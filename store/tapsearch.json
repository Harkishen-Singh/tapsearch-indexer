{
    "documentHashMap": {
        "186247402": "A coordination committee to find common ground between the vastly mismatched parties is also part of the deal. The Shiv Sena may also be asked to tone down its Hindutva rhetoric, which is anathema harkishen to the Congress. Earlier on Wednesday, Sonia Gandhi was urged by Kerala ally Indian Union Muslim League (IUML) to snub the Sena.",
        "410929361": "According to the deal in circulation, the Shiv Sena has to share chief ministership with the NCP, with its chief Uddhav Thackeray getting the first turn. Other details include two deputy chief ministers, one harkishen from the Congress. ",
        "1043891123": "Top Congress leaders went to Sharad Pawar's home while a senior Congress leader harkishen separately met the Shiv Sena's Sanjay Raut. ",
        "1118639836": "\"Some details\" of power sharing with the Sena need to be sorted out while a harkishen common minimum agenda is almost done, Congress sources said. ",
        "1122085995": "\n\nMANNING\nMarko Lukša \nwww.allitebooks.com\n\n\nKubernetes resources covered in the book\n* Cluster-level resource (not namespaced)\n** Also in other API versions; listed version is the one used in this book\n(continues on inside back cover)\nResource (abbr.) [API version]DescriptionSection\nNamespace* (ns) [v1]Enables organizing resources into non-overlapping \ngroups (for example, per tenant)\n3.7\nDeploying workloads\nPod (po) [v1]The basic deployable unit containing one or more \nprocesses in co-located containers\n3.1\nReplicaSet (rs) [apps/v1beta2**]Keeps one or more pod replicas running4.3\nReplicationController (rc) [v1]The older, less-powerful equivalent of a \nReplicaSet\n4.2\nJob [batch/v1]Runs pods that perform a completable task4.5\nCronJob [batch/v1beta1]Runs a scheduled job once or periodically4.6\nDaemonSet (ds) [apps/v1beta2**]Runs one pod replica per node (on all nodes or \nonly on those matching a node selector)\n4.4\nStatefulSet (sts) [apps/v1beta1**]Runs stateful pods with a stable identity10.2\nDeployment (deploy) [apps/v1beta1**]Declarative deployment and updates of pods9.3\nServices\nService (svc) [v1]Exposes one or more pods at a single and stable \nIP address and port pair\n5.1\nEndpoints (ep) [v1]Defines which pods (or other servers) are \nexposed through a service\n5.2.1\nIngress (ing) [extensions/v1beta1]Exposes one or more services to external clients \nthrough a single externally reachable IP address\n5.4\nConfig\nConfigMap (cm) [v1]A key-value map for storing non-sensitive config \noptions for apps and exposing it to them\n7.4\nSecret [v1]Like a ConfigMap, but for sensitive data7.5\nStorage\nPersistentVolume* (pv) [v1]Points to persistent storage that can be mounted \ninto a pod through a PersistentVolumeClaim\n6.5\nPersistentVolumeClaim (pvc) [v1]A request for and claim to a PersistentVolume6.5\nStorageClass* (sc) [storage.k8s.io/v1]Defines the type of dynamically-provisioned stor-\nage claimable in a PersistentVolumeClaim\n6.6\n \nwww.allitebooks.com\n\n\nKubernetes in Action\n \nwww.allitebooks.com\n\n\n \nwww.allitebooks.com\n\n\nKubernetes\nin Action\nMARKO LUKŠA\nMANNING\nSHELTER ISLAND\n \nwww.allitebooks.com\n\n\nFor online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2018 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nManning Publications Co.Development editor:   Elesha Hyde\n20 Baldwin RoadReview editor:   Aleksandar Dragosavljevic ́\nPO Box 761Technical development editor:   Jeanne Boyarsky\nShelter Island, NY 11964Project editor:   Kevin Sullivan\nCopyeditor:   Katie Petito\nProofreader:   Melody Dolab\nTechnical proofreader:   Antonio Magnaghi\nIllustrator:   Chuck Larson\nTypesetter:   Dennis Dalinnik\nCover designer:   Marija Tudor\nISBN: 9781617293726\nPrinted in the United States of America\n1  2  3  4  5  6  7  8  9  10  –  EBM  –  22  21  20  19  18  17\n \nwww.allitebooks.com\n\n\n To my parents, \nwho have always put their children’s needs above their own\n \nwww.allitebooks.com\n\n\n \n \nwww.allitebooks.com\n\n\nvii\nbrief contents\nPART 1OVERVIEW\n1■Introducing Kubernetes    1\n2■First steps with Docker and Kubernetes    25\nPART 2CORE CONCEPTS\n3■Pods: running containers in Kubernetes    55\n4■Replication and other controllers: deploying \nmanaged pods    84\n5■Services: enabling clients to discover and talk \nto pods    120\n6■Volumes: attaching disk storage to containers    159\n7■ConfigMaps and Secrets: configuring applications    191\n8■Accessing pod metadata and other resources from \napplications    225\n9■Deployments: updating applications declaratively     250\n10■StatefulSets: deploying replicated stateful \napplications    280\n \nwww.allitebooks.com\n\n\nBRIEF CONTENTSviii\nPART 3BEYOND THE BASICS\n11■Understanding Kubernetes internals    309\n12■Securing the Kubernetes API server    346\n13■Securing cluster nodes and the network    375\n14■Managing pods’ computational resources    404\n15■Automatic scaling of pods and cluster nodes    437\n16■Advanced scheduling    457\n17■Best practices for developing apps    477\n18■Extending Kubernetes    508\n \nwww.allitebooks.com\n\n\nix\ncontents\npreface    xxi\nacknowledgments    xxiii\nabout this book    xxv\nabout the author    xxix\nabout the cover illustration    xxx\nPART 1OVERVIEW\n1 \nIntroducing Kubernetes    1\n1.1Understanding the need for a system like Kubernetes    2\nMoving from monolithic apps to microservices    3\n■\nProviding a \nconsistent environment to applications    6\n■\nMoving to continuous \ndelivery: DevOps and NoOps    6\n1.2Introducing container technologies    7\nUnderstanding what containers are    8\n■\nIntroducing the Docker \ncontainer platform    12\n■\nIntroducing rkt—an alternative to Docker    15\n1.3Introducing Kubernetes    16\nUnderstanding its origins    16\n■\nLooking at Kubernetes from the \ntop of a mountain    16\n■\nUnderstanding the architecture of a \nKubernetes cluster    18\n■\nRunning an application in Kubernetes    19\nUnderstanding the benefits of using Kubernetes    21\n1.4Summary    23\n \n\nCONTENTS\nx\n2 \nFirst steps with Docker and Kubernetes    25\n2.1Creating, running, and sharing a container image    26\nInstalling Docker and running a Hello World container    26\nCreating a trivial Node.js app    28\n■\nCreating a Dockerfile \nfor the image    29\n■\nBuilding the container image    29\nRunning the container image    32\n■\nExploring the inside \nof a running container    33\n■\nStopping and removing a \ncontainer    34\n■\nPushing the image to an image registry    35\n2.2Setting up a Kubernetes cluster    36\nRunning a local single-node Kubernetes cluster with Minikube    37\nUsing a hosted Kubernetes cluster with Google Kubernetes \nEngine    38\n■\nSetting up an alias and command-line completion \nfor kubectl    41\n2.3Running your first app on Kubernetes    42\nDeploying your Node.js app    42\n■\nAccessing your web \napplication    45\n■\nThe logical parts of your system    47\nHorizontally scaling the application    48\n■\nExamining what \nnodes your app is running on    51\n■\nIntroducing the \nKubernetes dashboard    52\n2.4Summary    53\nPART 2CORE CONCEPTS\n3 \nPods: running containers in Kubernetes    55\n3.1Introducing pods    56\nUnderstanding why we need pods    56\n■\nUnderstanding pods    57\nOrganizing containers across pods properly    58\n3.2Creating pods from YAML or JSON descriptors    61\nExamining a YAML descriptor of an existing pod    61\n■\nCreating a \nsimple YAML descriptor for a pod    63\n■\nUsing kubectl create to \ncreate the pod    65\n■\nViewing application logs    65\n■\nSending \nrequests to the pod    66\n3.3Organizing pods with labels    67\nIntroducing labels    68\n■\nSpecifying labels when creating a pod    69\nModifying labels of existing pods    70\n3.4Listing subsets of pods through label selectors    71\nListing pods using a label selector    71\n■\nUsing multiple conditions \nin a label selector    72\n \n\nCONTENTS\nxi\n3.5Using labels and selectors to constrain pod \nscheduling    73\nUsing labels for categorizing worker nodes    74\n■\nScheduling pods to \nspecific nodes    74\n■\nScheduling to one specific node    75\n3.6Annotating pods    75\nLooking up an object’s annotations    75\n■\nAdding and modifying \nannotations    76\n3.7Using namespaces to group resources    76\nUnderstanding the need for namespaces    77\n■\nDiscovering other \nnamespaces and their pods    77\n■\nCreating a namespace    78\nManaging objects in other namespaces    79\n■\nUnderstanding \nthe isolation provided by namespaces    79\n3.8Stopping and removing pods    80\nDeleting a pod by name    80\n■\nDeleting pods using label \nselectors    80\n■\nDeleting pods by deleting the whole \nnamespace    80\n■\nDeleting all pods in a namespace, \nwhile keeping the namespace    81\n■\nDeleting (almost) \nall resources in a namespace    82\n3.9Summary    82\n4 \nReplication and other controllers: deploying managed pods    84\n4.1Keeping pods healthy    85\nIntroducing liveness probes    85\n■\nCreating an HTTP-based \nliveness probe    86\n■\nSeeing a liveness probe in action    87\nConfiguring additional properties of the liveness probe    88\nCreating effective liveness probes    89\n4.2Introducing ReplicationControllers    90\nThe operation of a ReplicationController    91\n■\nCreating a \nReplicationController    93\n■\nSeeing the ReplicationController \nin action    94\n■\nMoving pods in and out of the scope of a \nReplicationController    98\n■\nChanging the pod template    101\nHorizontally scaling pods    102\n■\nDeleting a \nReplicationController    103\n4.3Using ReplicaSets instead of ReplicationControllers    104\nComparing a ReplicaSet to a ReplicationController    105\nDefining a ReplicaSet    105\n■\nCreating and examining a \nReplicaSet    106\n■\nUsing the ReplicaSet’s more expressive \nlabel selectors    107\n■\nWrapping up ReplicaSets    108\n \n\nCONTENTS\nxii\n4.4Running exactly one pod on each node with \nDaemonSets    108\nUsing a DaemonSet to run a pod on every node    109\nUsing a DaemonSet to run pods only on certain nodes    109\n4.5Running pods that perform a single completable \ntask    112\nIntroducing the Job resource    112\n■\nDefining a Job resource    113\nSeeing a Job run a pod    114\n■\nRunning multiple pod instances \nin a Job    114\n■\nLimiting the time allowed for a Job pod to \ncomplete    116\n4.6Scheduling Jobs to run periodically or once \nin the future    116\nCreating a CronJob    116\n■\nUnderstanding how scheduled \njobs are run    117\n4.7Summary    118\n5 \nServices: enabling clients to discover and talk to pods    120\n5.1Introducing services    121\nCreating services    122\n■\nDiscovering services    128\n5.2Connecting to services living outside the cluster    131\nIntroducing service endpoints    131\n■\nManually configuring \nservice endpoints    132\n■\nCreating an alias for an external \nservice    134\n5.3Exposing services to external clients    134\nUsing a NodePort service    135\n■\nExposing a service through \nan external load balancer    138\n■\nUnderstanding the peculiarities \nof external connections    141\n5.4Exposing services externally through an Ingress \nresource    142\nCreating an Ingress resource    144\n■\nAccessing the service \nthrough the Ingress    145\n■\nExposing multiple services \nthrough the same Ingress    146\n■\nConfiguring Ingress to \nhandle TLS traffic    147\n5.5Signaling when a pod is ready to accept connections    149\nIntroducing readiness probes    149\n■\nAdding a readiness probe \nto a pod    151\n■\nUnderstanding what real-world readiness \nprobes should do    153\n \n\nCONTENTS\nxiii\n5.6Using a headless service for discovering individual \npods    154\nCreating a headless service    154\n■\nDiscovering pods \nthrough DNS    155\n■\nDiscovering all pods—even those \nthat aren’t ready    156\n5.7Troubleshooting services    156\n5.8Summary    157\n6 \nVolumes: attaching disk storage to containers    159\n6.1Introducing volumes    160\nExplaining volumes in an example    160\n■\nIntroducing available \nvolume types    162\n6.2Using volumes to share data between containers    163\nUsing an emptyDir volume    163\n■\nUsing a Git repository as the \nstarting point for a volume    166\n6.3Accessing files on the worker node’s filesystem    169\nIntroducing the hostPath volume    169\n■\nExamining system pods \nthat use hostPath volumes    170\n6.4Using persistent storage    171\nUsing a GCE Persistent Disk in a pod volume    171\n■\nUsing other \ntypes of volumes with underlying persistent storage    174\n6.5Decoupling pods from the underlying storage \ntechnology    176\nIntroducing PersistentVolumes and PersistentVolumeClaims    176\nCreating a PersistentVolume    177\n■\nClaiming a PersistentVolume \nby creating a PersistentVolumeClaim    179\n■\nUsing a \nPersistentVolumeClaim in a pod    181\n■\nUnderstanding the \nbenefits of using PersistentVolumes and claims    182\n■\nRecycling \nPersistentVolumes    183\n6.6Dynamic provisioning of PersistentVolumes    184\nDefining the available storage types through StorageClass \nresources    185\n■\nRequesting the storage class in a \nPersistentVolumeClaim    185\n■\nDynamic provisioning \nwithout specifying a storage class    187\n6.7Summary    190\n \n\nCONTENTS\nxiv\n7 \nConfigMaps and Secrets: configuring applications    191\n7.1Configuring containerized applications    191\n7.2Passing command-line arguments to containers    192\nDefining the command and arguments in Docker    193\nOverriding the command and arguments in Kubernetes    195\n7.3Setting environment variables for a container    196\nSpecifying environment variables in a container definition    197\nReferring to other environment variables in a variable’s value    198\nUnderstanding the drawback of hardcoding environment \nvariables    198\n7.4Decoupling configuration with a ConfigMap    198\nIntroducing ConfigMaps    198\n■\nCreating a ConfigMap    200\nPassing a ConfigMap entry to a container as an environment \nvariable    202\n■\nPassing all entries of a ConfigMap as environment \nvariables at once    204\n■\nPassing a ConfigMap entry as a \ncommand-line argument    204\n■\nUsing a configMap volume to \nexpose ConfigMap entries as files    205\n■\nUpdating an app’s config \nwithout having to restart the app    211\n7.5Using Secrets to pass sensitive data to containers    213\nIntroducing Secrets    214\n■\nIntroducing the default token \nSecret    214\n■\nCreating a Secret    216\n■\nComparing ConfigMaps \nand Secrets    217\n■\nUsing the Secret in a pod    218\nUnderstanding image pull Secrets    222\n7.6Summary    224\n8 \nAccessing pod metadata and other resources from \napplications    225\n8.1Passing metadata through the Downward API    226\nUnderstanding the available metadata    226\n■\nExposing metadata \nthrough environment variables    227\n■\nPassing metadata through \nfiles in a downwardAPI volume    230\n8.2Talking to the Kubernetes API server    233\nExploring the Kubernetes REST API    234\n■\nTalking to the API \nserver from within a pod    238\n■\nSimplifying API server \ncommunication with ambassador containers    243\n■\nUsing client \nlibraries to talk to the API server    246\n8.3Summary    249\n \n\nCONTENTS\nxv\n9 \nDeployments: updating applications declaratively    250\n9.1Updating applications running in pods    251\nDeleting old pods and replacing them with new ones    252\nSpinning up new pods and then deleting the old ones    252\n9.2Performing an automatic rolling update with a \nReplicationController    254\nRunning the initial version of the app    254\n■\nPerforming a rolling \nupdate with kubectl    256\n■\nUnderstanding why kubectl rolling-\nupdate is now obsolete    260\n9.3Using Deployments for updating apps declaratively    261\nCreating a Deployment    262\n■\nUpdating a Deployment    264\nRolling back a deployment    268\n■\nControlling the rate of the \nrollout    271\n■\nPausing the rollout process    273\n■\nBlocking \nrollouts of bad versions    274\n9.4Summary    279\n10 \nStatefulSets: deploying replicated stateful applications    280\n10.1Replicating stateful pods    281\nRunning multiple replicas with separate storage for each    281\nProviding a stable identity for each pod    282\n10.2Understanding StatefulSets    284\nComparing StatefulSets with ReplicaSets    284\n■\nProviding a \nstable network identity    285\n■\nProviding stable dedicated storage \nto each stateful instance    287\n■\nUnderstanding StatefulSet \nguarantees    289\n10.3Using a StatefulSet    290\nCreating the app and container image    290\n■\nDeploying the app \nthrough a StatefulSet    291\n■\nPlaying with your pods    295\n10.4Discovering peers in a StatefulSet    299\nImplementing peer discovery through DNS    301\n■\nUpdating a \nStatefulSet    302\n■\nTrying out your clustered data store    303\n10.5Understanding how StatefulSets deal with node \nfailures    304\nSimulating a node’s disconnection from the network    304\nDeleting the pod manually    306\n10.6Summary    307\n \n\nCONTENTS\nxvi\nPART 3BEYOND THE BASICS\n11 \nUnderstanding Kubernetes internals    309\n11.1Understanding the architecture    310\nThe distributed nature of Kubernetes components    310\nHow Kubernetes uses etcd    312\n■\nWhat the API server does    316\nUnderstanding how the API server notifies clients of resource \nchanges    318\n■\nUnderstanding the Scheduler    319\nIntroducing the controllers running in the Controller Manager    321\nWhat the Kubelet does    326\n■\nThe role of the Kubernetes Service \nProxy    327\n■\nIntroducing Kubernetes add-ons    328\n■\nBringing it \nall together    330\n11.2How controllers cooperate    330\nUnderstanding which components are involved    330\n■\nThe chain \nof events    331\n■\nObserving cluster events    332\n11.3Understanding what a running pod is    333\n11.4Inter-pod networking    335\nWhat the network must be like    335\n■\nDiving deeper into \nhow networking works    336\n■\nIntroducing the Container \nNetwork Interface    338\n11.5How services are implemented    338\nIntroducing the kube-proxy    339\n■\nHow kube-proxy uses iptables    339\n11.6Running highly available clusters    341\nMaking your apps highly available    341\n■\nMaking Kubernetes \nControl Plane components highly available    342\n11.7Summary    345\n12 \nSecuring the Kubernetes API server    346\n12.1Understanding authentication    346\nUsers and groups    347\n■\nIntroducing ServiceAccounts    348\nCreating ServiceAccounts    349\n■\nAssigning a ServiceAccount \nto a pod    351\n12.2Securing the cluster with role-based access control    353\nIntroducing the RBAC authorization plugin    353\n■\nIntroducing \nRBAC resources    355\n■\nUsing Roles and RoleBindings    358\nUsing ClusterRoles and ClusterRoleBindings    362\nUnderstanding default ClusterRoles and ClusterRoleBindings    371\nGranting authorization permissions wisely    373\n12.3Summary    373\n \n\nCONTENTS\nxvii\n13 \nSecuring cluster nodes and the network    375\n13.1Using the host node’s namespaces in a pod    376\nUsing the node’s network namespace in a pod    376\n■\nBinding to \na host port without using the host’s network namespace    377\nUsing the node’s PID and IPC namespaces    379\n13.2Configuring the container’s security context    380\nRunning a container as a specific user    381\n■\nPreventing a \ncontainer from running as root    382\n■\nRunning pods in \nprivileged mode    382\n■\nAdding individual kernel capabilities \nto a container    384\n■\nDropping capabilities from a container    385\nPreventing processes from writing to the container’s filesystem    386\nSharing volumes when containers run as different users    387\n13.3Restricting the use of security-related features \nin pods    389\nIntroducing the PodSecurityPolicy resource    389\n■\nUnderstanding \nrunAsUser, fsGroup, and supplementalGroups policies    392\nConfiguring allowed, default, and disallowed capabilities    394\nConstraining the types of volumes pods can use    395\n■\nAssigning \ndifferent PodSecurityPolicies to different users and groups    396\n13.4Isolating the pod network    399\nEnabling network isolation in a namespace    399\n■\nAllowing \nonly some pods in the namespace to connect to a server pod    400\nIsolating the network between Kubernetes namespaces    401\nIsolating using CIDR notation    402\n■\nLimiting the outbound \ntraffic of a set of pods    403\n13.5Summary    403\n14 \nManaging pods’ computational resources    404\n14.1Requesting resources for a pod’s containers    405\nCreating pods with resource requests    405\n■\nUnderstanding how \nresource requests affect scheduling    406\n■\nUnderstanding how CPU \nrequests affect CPU time sharing    411\n■\nDefining and requesting \ncustom resources    411\n14.2Limiting resources available to a container    412\nSetting a hard limit for the amount of resources a container \ncan use    412\n■\nExceeding the limits    414\n■\nUnderstanding \nhow apps in containers see limits    415\n14.3Understanding pod QoS classes    417\nDefining the QoS class for a pod    417\n■\nUnderstanding which \nprocess gets killed when memory is low    420\n \n\nCONTENTS\nxviii\n14.4Setting default requests and limits for pods per \nnamespace    421\nIntroducing the LimitRange resource    421\n■\nCreating a \nLimitRange object    422\n■\nEnforcing the limits    423\nApplying default resource requests and limits    424\n14.5Limiting the total resources available in \na namespace    425\nIntroducing the ResourceQuota object    425\n■\nSpecifying a quota \nfor persistent storage    427\n■\nLimiting the number of objects that can \nbe created    427\n■\nSpecifying quotas for specific pod states and/or \nQoS classes    429\n14.6Monitoring pod resource usage    430\nCollecting and retrieving actual resource usages    430\n■\nStoring \nand analyzing historical resource consumption statistics    432\n14.7Summary    435\n15 \nAutomatic scaling of pods and cluster nodes    437\n15.1Horizontal pod autoscaling    438\nUnderstanding the autoscaling process    438\n■\nScaling based \non CPU utilization    441\n■\nScaling based on memory \nconsumption    448\n■\nScaling based on other and custom \nmetrics    448\n■\nDetermining which metrics are appropriate for \nautoscaling    450\n■\nScaling down to zero replicas    450\n15.2Vertical pod autoscaling    451\nAutomatically configuring resource requests    451\n■\nModifying \nresource requests while a pod is running    451\n15.3Horizontal scaling of cluster nodes    452\nIntroducing the Cluster Autoscaler    452\n■\nEnabling the Cluster \nAutoscaler    454\n■\nLimiting service disruption during cluster \nscale-down    454\n15.4Summary    456\n16 \nAdvanced scheduling    457\n16.1Using taints and tolerations to repel pods from certain \nnodes    457\nIntroducing taints and tolerations    458\n■\nAdding custom taints to \na node    460\n■\nAdding tolerations to pods    460\n■\nUnderstanding \nwhat taints and tolerations can be used for    461\n \nwww.allitebooks.com\n\n\nCONTENTS\nxix\n16.2Using node affinity to attract pods to certain nodes    462\nSpecifying hard node affinity rules    463\n■\nPrioritizing nodes when \nscheduling a pod    465\n16.3Co-locating pods with pod affinity and anti-affinity    468\nUsing inter-pod affinity to deploy pods on the same node    468\nDeploying pods in the same rack, availability zone, or geographic \nregion    471\n■\nExpressing pod affinity preferences instead of hard \nrequirements    472\n■\nScheduling pods away from each other with \npod anti-affinity    474\n16.4Summary    476\n17 \nBest practices for developing apps    477\n17.1Bringing everything together    478\n17.2Understanding the pod’s lifecycle    479\nApplications must expect to be killed and relocated    479\nRescheduling of dead or partially dead pods    482\n■\nStarting \npods in a specific order    483\n■\nAdding lifecycle hooks    485\nUnderstanding pod shutdown    489\n17.3Ensuring all client requests are handled properly    492\nPreventing broken client connections when a pod is starting up    492\nPreventing broken connections during pod shut-down    493\n17.4Making your apps easy to run and manage in \nKubernetes    497\nMaking manageable container images    497\n■\nProperly \ntagging your images and using imagePullPolicy wisely    497\nUsing multi-dimensional instead of single-dimensional labels    498\nDescribing each resource through annotations    498\n■\nProviding \ninformation on why the process terminated    498\n■\nHandling \napplication logs    500\n17.5Best practices for development and testing    502\nRunning apps outside of Kubernetes during development    502\nUsing Minikube in development    503\n■\nVersioning and auto-\ndeploying resource manifests    504\n■\nIntroducing Ksonnet as an \nalternative to writing YAML/JSON manifests    505\n■\nEmploying \nContinuous Integration and Continuous Delivery (CI/CD)    506\n17.6Summary    506\n \n\nCONTENTS\nxx\n18 \nExtending Kubernetes    508\n18.1Defining custom API objects    508\nIntroducing CustomResourceDefinitions    509\n■\nAutomating \ncustom resources with custom controllers    513\n■\nValidating \ncustom objects    517\n■\nProviding a custom API server for your \ncustom objects    518\n18.2Extending Kubernetes with the Kubernetes Service \nCatalog    519\nIntroducing the Service Catalog    520\n■\nIntroducing the \nService Catalog API server and Controller Manager    521\nIntroducing Service Brokers and the OpenServiceBroker API    522\nProvisioning and using a service    524\n■\nUnbinding and \ndeprovisioning    526\n■\nUnderstanding what the Service \nCatalog brings    526\n18.3Platforms built on top of Kubernetes    527\nRed Hat OpenShift Container Platform    527\n■\nDeis Workflow \nand Helm    530\n18.4Summary    533\nappendix AUsing kubectl with multiple clusters    534\nappendix BSetting up a multi-node cluster with kubeadm    539\nappendix CUsing other container runtimes    552\nappendix DCluster Federation    556\nindex     561\n \n\nxxi\npreface\nAfter  working  at  Red  Hat  for  a  few  years,  in  late  2014  I  was  assigned  to  a  newly-\nestablished  team  called  Cloud  Enablement.  Our  task  was  to  bring  the  company’s\nrange of middleware products to the OpenShift Container Platform, which was then\nbeing  developed  on  top  of  Kubernetes.  At  that  time,  Kubernetes  was  still  in  its\ninfancy—version 1.0 hadn’t even been released yet.\n Our team had to get to know the ins and outs of Kubernetes quickly to set a proper\ndirection for our software and take advantage of everything Kubernetes had to offer.\nWhen faced with a problem, it was hard for us to tell if we were doing things wrong or\nmerely hitting one of the early Kubernetes bugs. \n  Both  Kubernetes  and  my  understanding  of  it  have  come  a  long  way  since  then.\nWhen I first started using it, most people hadn’t even heard of Kubernetes. Now, virtu-\nally  every  software  engineer  knows  about  it,  and  it  has  become  one  of  the  fastest-\ngrowing and most-widely-adopted ways of running applications in both the cloud and\non-premises datacenters. \n In my first month of dealing with Kubernetes, I wrote a two-part blog post about\nhow to run a JBoss WildFly application server cluster in OpenShift/Kubernetes. At the\ntime, I never could have imagined that a simple blog post would ultimately lead the\npeople  at  Manning  to  contact  me  about  whether  I  would  like  to  write  a  book  about\nKubernetes.  Of  course,  I  couldn’t  say  no  to  such  an  offer,  even  though  I  was  sure\nthey’d approached other people as well and would ultimately pick someone else.\n And yet, here we are. After more than a year and a half of writing and researching,\nthe book is done. It’s been an awesome journey. Writing a book about a technology is\n \n\nPREFACE\nxxii\nabsolutely the best way to get to know it in much greater detail than you’d learn as just\na user. As my knowledge of Kubernetes has expanded during the process and Kuber-\nnetes itself has evolved, I’ve constantly gone back to previous chapters I’ve written and\nadded additional information. I’m a perfectionist, so I’ll never really be absolutely sat-\nisfied with the book, but I’m happy to hear that a lot of readers of the Manning Early\nAccess Program (MEAP) have found it to be a great guide to Kubernetes.\n  My  aim  is  to  get  the  reader  to  understand  the  technology  itself  and  teach  them\nhow to use the tooling to effectively and efficiently develop and deploy apps to Kuber-\nnetes clusters. In the book, I don’t put much emphasis on how to actually set up and\nmaintain  a  proper  highly  available  Kubernetes  cluster,  but  the  last  part  should  give\nreaders a very solid understanding of what such a cluster consists of and should allow\nthem to easily comprehend additional resources that deal with this subject.\n I hope you’ll enjoy reading it, and that it teaches you how to get the most out of\nthe awesome system that is Kubernetes.\n \n\nxxiii\nacknowledgments\nBefore I started writing this book, I had no clue how many people would be involved\nin  bringing  it  from  a  rough  manuscript  to  a  published  piece  of  work.  This  means\nthere are a lot of people to thank.\n First, I’d like to thank Erin Twohey for approaching me about writing this book,\nand Michael Stephens from Manning, who had full confidence in my ability to write it\nfrom day one. His words of encouragement early on really motivated me and kept me\nmotivated throughout the last year and a half. \n  I  would  also  like  to  thank  my  initial  development  editor  Andrew  Warren,  who\nhelped  me  get  my  first  chapter  out  the  door, and Elesha Hyde, who took over from\nAndrew  and  worked  with  me  all  the  way  to  the  last  chapter.  Thank  you  for  bearing\nwith  me,  even  though  I’m  a  difficult  person  to  deal  with,  as  I  tend  to  drop  off  the\nradar fairly regularly. \n  I  would  also  like  to  thank  Jeanne  Boyarsky,  who  was  the  first  reviewer  to  read  and\ncomment on my chapters while I was writing them. Jeanne and Elesha were instrumen-\ntal  in  making  the  book  as  nice  as  it  hopefully  is.  Without  their  comments,  the  book\ncould never have received such good reviews from external reviewers and readers.\n I’d like to thank my technical proofreader, Antonio Magnaghi, and of course all\nmy external reviewers: Al Krinker, Alessandro Campeis, Alexander Myltsev, Csaba Sari,\nDavid  DiMaria,  Elias  Rangel,  Erisk  Zelenka,  Fabrizio  Cucci,  Jared  Duncan,  Keith\nDonaldson, Michael Bright, Paolo Antinori, Peter Perlepes, and Tiklu Ganguly. Their\npositive comments kept me going at times when I worried my writing was utterly awful\nand completely useless. On the other hand, their constructive criticism helped improve\n \n\nACKNOWLEDGMENTS\nxxiv\nsections that I’d quickly thrown together without enough effort. Thank you for point-\ning out the hard-to-understand sections and suggesting ways of improving the book.\nAlso,  thank  you  for  asking  the  right  questions,  which  made  me  realize  I  was  wrong\nabout two or three things in the initial versions of the manuscript.\n  I  also  need  to  thank  readers  who  bought  the  early  version  of  the  book  through\nManning’s MEAP program and voiced their comments in the online forum or reached\nout  to  me  directly—especially  Vimal  Kansal,  Paolo  Patierno,  and  Roland  Huß,  who\nnoticed  quite  a  few  inconsistencies  and  other  mistakes.  And  I  would  like  to  thank\neveryone at Manning who has been involved in getting this book published. Before I\nfinish,  I  also  need  to  thank  my  colleague  and  high  school  friend  Aleš  Justin,  who\nbrought  me  to  Red  Hat,  and  my  wonderful  colleagues  from  the  Cloud  Enablement\nteam. If I hadn’t been at Red Hat or in the team, I wouldn’t have been the one to write\nthis book.\n Lastly, I would like to thank my wife and my son, who were way too understanding\nand  supportive  over  the  last  18  months,  while  I  was  locked  in  my  office  instead  of\nspending time with them.\n Thank you all!\n \n\nxxv\nabout this book\nKubernetes in Action aims to make you a proficient user of Kubernetes. It teaches you\nvirtually all the concepts you need to understand to effectively develop and run appli-\ncations in a Kubernetes environment. \n Before diving into Kubernetes, the book gives an overview of container technolo-\ngies like Docker, including how to build containers, so that even readers who haven’t\nused  these  technologies  before  can  get  up  and  running.    It  then  slowly  guides  you\nthrough  most  of  what  you  need  to  know  about  Kubernetes—from  basic  concepts  to\nthings hidden below the surface.\nWho should read this book\nThe book focuses primarily on application developers, but it also provides an overview\nof  managing  applications  from  the  operational  perspective.  It’s  meant  for  anyone\ninterested in running and managing containerized applications on more than just a\nsingle server.\n  Both  beginner  and  advanced  software  engineers  who  want  to  learn  about  con-\ntainer technologies and orchestrating multiple related containers at scale will gain the\nexpertise necessary to develop, containerize, and run their applications in a Kuberne-\ntes environment. \n No previous exposure to either container technologies or Kubernetes is required.\nThe book explains the subject matter in a progressively detailed manner, and doesn’t\nuse any application source code that would be too hard for non-expert developers to\nunderstand. \n \n\nABOUT THIS BOOK\nxxvi\n  Readers,  however,  should  have  at  least  a  basic  knowledge  of  programming,  com-\nputer  networking,  and  running  basic  commands  in  Linux,  and  an  understanding  of\nwell-known computer protocols like HTTP. \nHow this book is organized: a roadmap\nThis book has three parts that cover 18 chapters.\n Part 1 gives a short introduction to Docker and Kubernetes, how to set up a Kuber-\nnetes cluster, and how to run a simple application in it. It contains two chapters:\n■Chapter 1 explains what Kubernetes is, how it came to be, and how it helps to\nsolve today’s problems of managing applications at scale.\n■Chapter 2 is a hands-on tutorial on how to build a container image and run it in\na Kubernetes cluster. It also explains how to run a local single-node Kubernetes\ncluster and a proper multi-node cluster in the cloud.\nPart 2 introduces the key concepts you must understand to run applications in Kuber-\nnetes. The chapters are as follows:\n■Chapter 3 introduces the fundamental building block in Kubernetes—the pod—\nand explains how to organize pods and other Kubernetes objects through labels. \n■Chapter 4 teaches you how Kubernetes keeps applications healthy by automati-\ncally  restarting  containers.  It  also  shows  how  to  properly  run  managed  pods,\nhorizontally  scale  them,  make  them  resistant  to  failures  of  cluster  nodes,  and\nrun them at a predefined time in the future or periodically.\n■Chapter  5  shows  how  pods  can  expose  the  service  they  provide  to  clients  run-\nning both inside and outside the cluster. It also shows how pods running in the\ncluster can discover and access services, regardless of whether they live in or out\nof the cluster. \n■Chapter 6 explains how multiple containers running in the same pod can share\nfiles and how you can manage persistent storage and make it accessible to pods. \n■Chapter 7 shows how to pass configuration data and sensitive information like\ncredentials to apps running inside pods.\n■Chapter 8 describes how applications can get information about the Kuberne-\ntes  environment  they’re  running  in  and  how  they  can  talk  to  Kubernetes  to\nalter the state of the cluster.\n■Chapter 9 introduces the concept of a Deployment and explains the proper way\nof running and updating applications in a Kubernetes environment.\n■Chapter 10 introduces a dedicated way of running stateful applications, which\nusually require a stable identity and state.\nPart  3  dives  deep  into  the  internals  of  a  Kubernetes  cluster,  introduces  some  addi-\ntional  concepts,  and  reviews  everything  you’ve  learned  in  the  first  two  parts  from  a\nhigher perspective. This is the last group of chapters:\n■Chapter 11 goes beneath the surface of Kubernetes and explains all the compo-\nnents  that  make  up  a  Kubernetes  cluster  and  what  each  of  them  does.  It  also\n \n\nABOUT THIS BOOK\nxxvii\nexplains  how  pods  communicate  through  the  network  and  how  services  per-\nform load balancing across multiple pods.\n■Chapter  12  explains  how  to  secure  your  Kubernetes  API  server,  and  by  exten-\nsion the cluster, using authentication and authorization. \n■Chapter  13  teaches  you  how  pods  can  access  the  node’s  resources  and  how  a\ncluster administrator can prevent pods from doing that.\n■Chapter  14  dives  into  constraining  the  computational  resources  each  applica-\ntion  is  allowed  to  consume,  configuring  the  applications’  Quality  of  Service\nguarantees,  and  monitoring  the  resource  usage  of  individual  applications.  It\nalso teaches you how to prevent users from consuming too many resources.\n■Chapter 15 discusses how Kubernetes can be configured to automatically scale\nthe number of running replicas of your application, and how it can also increase\nthe size of your cluster when your current number of cluster nodes can’t accept\nany additional applications. \n■Chapter  16  shows  how  to  ensure  pods  are  scheduled  only  to  certain  nodes  or\nhow to prevent them from being scheduled to others. It also shows how to make\nsure pods are scheduled together or how to prevent that from happening.\n■Chapter 17 teaches you how you should develop your applications to make them\ngood citizens of your cluster. It also gives you a few pointers on how to set up your\ndevelopment and testing workflows to reduce friction during development.\n■Chapter  18  shows  you  how  you  can  extend  Kubernetes  with  your  own  custom\nobjects  and  how  others  have  done  it  and  created  enterprise-class  application\nplatforms.\nAs  you  progress  through  these  chapters,  you’ll  not  only  learn  about  the  individual\nKubernetes building blocks, but also progressively improve your knowledge of using\nthe \nkubectl command-line tool.\nAbout the code\nWhile  this  book  doesn’t  contain  a  lot  of  actual  source  code,  it  does  contain  a  lot  of\nmanifests  of  Kubernetes  resources  in  YAML  format  and  shell  commands  along  with\ntheir outputs. All of this is formatted in a \nfixed-width font like this to separate it\nfrom ordinary text. \n Shell commands are mostly \nin bold, to clearly separate them from their output, but\nsometimes only the most important parts of the command or parts of the command’s\noutput are in bold for emphasis. In most cases, the command output has been reformat-\nted to make it fit into the limited space in the book. Also, because the Kubernetes CLI\ntool \nkubectl  is  constantly  evolving,  newer  versions  may  print  out  more  information\nthan what’s shown in the book. Don’t be confused if they don’t match exactly. \n Listings sometimes include a line-continuation marker (\n➥) to show that a line of\ntext wraps to the next line. They also include annotations, which highlight and explain\nthe most important parts. \n \n\nABOUT THIS BOOK\nxxviii\n  Within  text  paragraphs,  some  very  common  elements  such  as  Pod,  Replication-\nController, ReplicaSet, DaemonSet, and so forth are set in regular font to avoid over-\nproliferation of code font and help readability. In some places, “Pod” is capitalized\nto refer to the Pod resource, and lowercased to refer to the actual group of running\ncontainers.\n All the samples in the book have been tested with Kubernetes version 1.8 running\nin Google Kubernetes Engine and in a local cluster run with Minikube. The complete\nsource code and YAML manifests can be found at https://github.com/luksa/kubernetes-\nin-action  or  downloaded  from  the  publisher’s  website  at  www.manning.com/books/\nkubernetes-in-action.\nBook forum\nPurchase  of  Kubernetes  in  Action  includes  free  access  to  a  private  web  forum  run  by\nManning Publications where you can make comments about the book, ask technical\nquestions,  and  receive  help  from  the  author  and  from  other  users.  To  access  the\nforum, go to https://forums.manning.com/forums/kubernetes-in-action. You can also\nlearn  more  about  Manning’s  forums  and  the  rules  of  conduct  at  https://forums\n.manning.com/forums/about.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue  between  individual  readers  and  between  readers  and  the  author  can  take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe author, whose contribution to the forum remains voluntary (and unpaid). We sug-\ngest you try asking the author some challenging questions lest his interest stray! The\nforum and the archives of previous discussions will be accessible from the publisher’s\nwebsite as long as the book is in print.\nOther online resources\nYou can find a wide range of additional Kubernetes resources at the following locations:\n■The Kubernetes website at https://kubernetes.io\n■The Kubernetes Blog, which regularly posts interesting info (http://blog.kuber-\nnetes.io)\n■The Kubernetes community’s Slack channel at http://slack.k8s.io\n■The Kubernetes and Cloud Native Computing Foundation’s YouTube channels:\n–https://www.youtube.com/channel/UCZ2bu0qutTOM0tHYa_jkIwg \n–https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA \nTo  gain  a  deeper  understanding  of  individual  topics  or  even  to  help  contribute  to\nKubernetes, you can also check out any of the Kubernetes Special Interest Groups (SIGs)\nat https://github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs).\n And, finally, as Kubernetes is open source, there’s a wealth of information available\nin the Kubernetes source code itself. You’ll find it at https://github.com/kubernetes/\nkubernetes and related repositories. \n \n\nxxix\nabout the author\nMarko Lukša is a software engineer with more than 20 years of\nprofessional  experience  developing  everything  from  simple\nweb applications to full ERP systems, frameworks, and middle-\nware software. He took his first steps in programming back in\n1985,  at  the  age  of  six,  on  a  second-hand  ZX  Spectrum  com-\nputer his father had bought for him. In primary school, he was\nthe national champion in the Logo programming competition\nand attended summer coding camps, where he learned to pro-\ngram  in  Pascal.  Since  then,  he  has  developed  software  in  a\nwide range of programming languages.\n      In  high  school,  he  started  building  dynamic  websites  when\nthe  web  was  still  relatively  young.  He  then  moved  on  to  developing  software  for  the\nhealthcare  and  telecommunications  industries  at  a  local  company,  while  studying\ncomputer  science  at  the  University  of  Ljubljana,  Slovenia.  Eventually,  he  ended  up\nworking for Red Hat, initially developing an open source implementation of the Goo-\ngle App Engine API, which utilized Red Hat’s JBoss middleware products underneath.\nHe also worked in or contributed to projects like CDI/Weld, Infinispan/JBoss Data-\nGrid, and others.\n Since late 2014, he has been part of Red Hat’s Cloud Enablement team, where his\nresponsibilities  include  staying  up-to-date  on  new  developments  in  Kubernetes  and\nrelated technologies and ensuring the company’s middleware software utilizes the fea-\ntures of Kubernetes and OpenShift to their full potential.\n \n\nxxx\nabout the cover illustration\nThe figure on the cover of Kubernetes in Action is a “Member of the Divan,” the Turkish\nCouncil of State or governing body. The illustration is taken from a collection of cos-\ntumes of the Ottoman Empire published on January 1, 1802, by William Miller of Old\nBond Street, London. The title page is missing from the collection and we have been\nunable to track it down to date. The book’s table of contents identifies the figures in\nboth  English  and  French,  and  each  illustration  bears  the  names  of  two  artists  who\nworked on it, both of whom would no doubt be surprised to find their art gracing the\nfront cover of a computer programming book ... 200 years later.\n The collection was purchased by a Manning editor at an antiquarian flea market in\nthe “Garage” on West 26th Street in Manhattan. The seller was an American based in\nAnkara, Turkey, and the transaction took place just as he was packing up his stand for\nthe day. The Manning editor didn’t have on his person the substantial amount of cash\nthat  was  required  for  the  purchase,  and  a  credit  card  and  check  were  both  politely\nturned down. With the seller flying back to Ankara that evening, the situation was get-\nting hopeless. What was the solution? It turned out to be nothing more than an old-\nfashioned  verbal  agreement  sealed  with  a  handshake.  The  seller  proposed  that  the\nmoney be transferred to him by wire, and the editor walked out with the bank infor-\nmation on a piece of paper and the portfolio of images under his arm. Needless to say,\nwe transferred the funds the next day, and we remain grateful and impressed by this\nunknown person’s trust in one of us. It recalls something that might have happened a\nlong time ago. We at Manning celebrate the inventiveness, the initiative, and, yes, the\nfun of the computer business with book covers based on the rich diversity of regional\nlife of two centuries ago‚ brought back to life by the pictures from this collection.\n \n\n1\nIntroducing Kubernetes\nYears ago, most software applications were big monoliths, running either as a single\nprocess or as a small number of processes spread across a handful of servers. These\nlegacy  systems  are  still  widespread  today.  They  have  slow  release  cycles  and  are\nupdated relatively infrequently. At the end of every release cycle, developers pack-\nage up the whole system and hand it over to the ops team, who then deploys and\nmonitors it. In case of hardware failures, the ops team manually migrates it to the\nremaining healthy servers. \n Today, these big monolithic legacy applications are slowly being broken down\ninto  smaller,  independently  running  components  called  microservices.  Because\nThis chapter covers\nUnderstanding how software development and \ndeployment has changed over recent years\nIsolating applications and reducing environment \ndifferences using containers\nUnderstanding how containers and Docker are \nused by Kubernetes\nMaking developers’ and sysadmins’ jobs easier \nwith Kubernetes\n \n\n2CHAPTER 1Introducing Kubernetes\nmicroservices are decoupled from each other, they can be developed, deployed, updated,\nand scaled individually. This enables you to change components quickly and as often as\nnecessary to keep up with today’s rapidly changing business requirements. \n But with bigger numbers of deployable components and increasingly larger data-\ncenters,  it  becomes  increasingly  difficult  to  configure,  manage,  and  keep  the  whole\nsystem running smoothly. It’s much harder to figure out where to put each of those\ncomponents to achieve high resource utilization and thereby keep the hardware costs\ndown.  Doing  all  this  manually  is  hard  work.  We  need  automation,  which  includes\nautomatic  scheduling  of  those  components  to  our  servers,  automatic  configuration,\nsupervision, and failure-handling. This is where Kubernetes comes in.\n  Kubernetes  enables  developers  to  deploy  their  applications  themselves  and  as\noften as they want, without requiring any assistance from the operations (ops) team.\nBut Kubernetes doesn’t benefit only developers. It also helps the ops team by automat-\nically monitoring and rescheduling those apps in the event of a hardware failure. The\nfocus for system administrators (sysadmins) shifts from supervising individual apps to\nmostly supervising and managing Kubernetes and the rest of the infrastructure, while\nKubernetes itself takes care of the apps. \nNOTEKubernetes is Greek for pilot or helmsman (the person holding the\nship’s steering wheel). People pronounce Kubernetes in a few different ways.\nMany  pronounce  it  as  Koo-ber-nay-tace,  while  others  pronounce  it  more  like\nKoo-ber-netties.  No  matter  which  form  you  use,  people  will  understand  what\nyou mean.\nKubernetes abstracts away the hardware infrastructure and exposes your whole data-\ncenter as a single enormous computational resource. It allows you to deploy and run\nyour  software  components  without  having  to  know  about  the  actual  servers  under-\nneath. When deploying a multi-component application through Kubernetes, it selects\na  server  for  each  component,  deploys  it,  and  enables  it  to  easily  find  and  communi-\ncate with all the other components of your application. \n This makes Kubernetes great for most on-premises datacenters, but where it starts\nto shine is when it’s used in the largest datacenters, such as the ones built and oper-\nated by cloud providers. Kubernetes allows them to offer developers a simple platform\nfor deploying and running any type of application, while not requiring the cloud pro-\nvider’s own sysadmins to know anything about the tens of thousands of apps running\non their hardware. \n With more and more big companies accepting the Kubernetes model as the best\nway to run apps, it’s becoming the standard way of running distributed apps both in\nthe cloud, as well as on local on-premises infrastructure. \n1.1Understanding the need for a system like Kubernetes \nBefore you start getting to know Kubernetes in  detail,  let’s  take  a  quick  look  at  how\nthe  development  and  deployment  of  applications  has  changed  in  recent  years.  This\nchange is both a consequence of splitting big monolithic apps into smaller microservices\n \n\n3Understanding the need for a system like Kubernetes\nand  of  the  changes  in  the  infrastructure  that  runs  those  apps.  Understanding  these\nchanges will help you better see the benefits of using Kubernetes and container tech-\nnologies such as Docker.\n1.1.1Moving from monolithic apps to microservices\nMonolithic applications consist of components that are all tightly coupled together and\nhave to be developed, deployed, and managed as one entity, because they all run as a sin-\ngle  OS  process.  Changes  to  one  part  of  the  application  require  a  redeployment  of  the\nwhole application, and over time the lack of hard boundaries between the parts results\nin the increase of complexity and consequential deterioration of the quality of the whole\nsystem because of the unconstrained growth of inter-dependencies between these parts. \n  Running  a  monolithic  application  usually  requires  a  small  number  of  powerful\nservers that can provide enough resources for running the application. To deal with\nincreasing loads on the system, you then either have to vertically scale the servers (also\nknown as scaling up) by adding more CPUs, memory, and other server components,\nor  scale  the  whole  system  horizontally,  by  setting  up  additional  servers  and  running\nmultiple copies (or replicas) of an application (scaling out). While scaling up usually\ndoesn’t require any changes to the app, it gets expensive relatively quickly and in prac-\ntice always has an upper limit. Scaling out, on the other hand, is relatively cheap hard-\nware-wise,  but  may  require  big  changes  in  the  application  code  and  isn’t  always\npossible—certain parts of an application are extremely hard or next to impossible to\nscale  horizontally  (relational  databases,  for  example).  If  any  part  of  a  monolithic\napplication  isn’t  scalable,  the  whole  application  becomes  unscalable,  unless  you  can\nsplit up the monolith somehow.\nSPLITTING APPS INTO MICROSERVICES\nThese and other problems have forced us to start splitting complex monolithic appli-\ncations into smaller independently deployable components called microservices. Each\nmicroservice runs as an independent process (see figure 1.1) and communicates with\nother microservices through simple, well-defined interfaces (APIs).\nServer 1\nMonolithic application\nSingle process\nServer 1\nProcess 1.1\nProcess 1.2\nMicroservices-based application\nServer 2\nProcess 2.1\nProcess 2.2\nFigure 1.1   Components inside a monolithic application vs. standalone microservices\n \n\n4CHAPTER 1Introducing Kubernetes\nMicroservices communicate through synchronous protocols such as HTTP, over which\nthey usually expose RESTful (REpresentational State Transfer) APIs, or through asyn-\nchronous  protocols  such  as  AMQP  (Advanced  Message  Queueing  Protocol).  These\nprotocols are simple, well understood by most developers, and not tied to any specific\nprogramming language. Each microservice can be written in the language that’s most\nappropriate for implementing that specific microservice.\n Because each microservice is a standalone process with a relatively static external\nAPI, it’s possible to develop and deploy each microservice separately. A change to one\nof them doesn’t require changes or redeployment of any other service, provided that\nthe API doesn’t change or changes only in a backward-compatible way. \nSCALING MICROSERVICES\nScaling microservices, unlike monolithic systems, where you need to scale the system as\na whole, is done on a per-service basis, which means you have the option of scaling only\nthose  services  that  require  more  resources,  while  leaving  others  at  their  original  scale.\nFigure  1.2  shows  an  example.  Certain  components  are  replicated  and  run  as  multiple\nprocesses deployed on different servers, while others run as a single application process.\nWhen a monolithic application can’t be scaled out because one of its parts is unscal-\nable, splitting the app into microservices allows you to horizontally scale the parts that\nallow scaling out, and scale the parts that don’t, vertically instead of horizontally.\nServer 1\nProcess 1.1\nProcess 1.2\nProcess 1.3\nServer 2\nProcess 2.1\nProcess 2.2\nServer 3\nProcess 3.1\nProcess 3.2\nProcess 3.3\nServer 4\nProcess 4.1\nProcess 4.2\nProcess 2.3\nSingle instance\n(possibly not scalable)\nThree instances of\nthe same component\nFigure 1.2   Each microservice can be scaled individually.\n \n\n5Understanding the need for a system like Kubernetes\nDEPLOYING MICROSERVICES\nAs  always,  microservices  also  have  drawbacks.  When  your  system  consists  of  only  a\nsmall  number  of  deployable  components,  managing  those  components  is  easy.  It’s\ntrivial  to  decide  where  to  deploy  each  component,  because  there  aren’t  that  many\nchoices. When the number of those components increases, deployment-related deci-\nsions become increasingly difficult because not only does the number of deployment\ncombinations  increase,  but  the  number  of  inter-dependencies  between  the  compo-\nnents increases by an even greater factor. \n Microservices perform their work together as a team, so they need to find and talk\nto each other. When deploying them, someone or something needs to configure all of\nthem  properly  to  enable  them  to  work  together  as  a  single  system.  With  increasing\nnumbers of microservices, this becomes tedious and error-prone, especially when you\nconsider what the ops/sysadmin teams need to do when a server fails. \n Microservices also bring other problems, such as making it hard to debug and trace\nexecution  calls,  because  they  span  multiple  processes  and  machines.  Luckily,  these\nproblems are now being addressed with distributed tracing systems such as Zipkin. \nUNDERSTANDING THE DIVERGENCE OF ENVIRONMENT REQUIREMENTS\nAs  I’ve  already  mentioned,  components  in  a  microservices  architecture  aren’t  only\ndeployed independently, but are also developed that way. Because of their indepen-\ndence and the fact that it’s common to have separate teams developing each compo-\nnent, nothing impedes each team from using  different  libraries  and  replacing  them\nwhenever the need arises. The divergence of dependencies between application com-\nponents,  like  the  one  shown  in  figure  1.3,  where  applications  require  different  ver-\nsions of the same libraries, is inevitable.\nServer running a monolithic app\nMonolithic app\nLibrary B\nv2.4\nLibrary C\nv1.1\nLibrary A\nv1.0\nLibrary Y\nv3.2\nLibrary X\nv1.4\nServer running multiple apps\nLibrary B\nv2.4\nLibrary C\nv1.1\nLibrary C\nv2.0\nLibrary A\nv1.0\nLibrary A\nv2.2\nLibrary Y\nv4.0\nLibrary Y\nv3.2\nLibrary X\nv2.3\nLibrary X\nv1.4\nApp 1App 2App 3App 4\nRequires libraries\nRequires libraries\nFigure 1.3   Multiple applications running on the same host may have conflicting dependencies.\n \n\n6CHAPTER 1Introducing Kubernetes\nDeploying  dynamically  linked  applications  that  require  different  versions  of  shared\nlibraries,  and/or  require  other  environment  specifics,  can  quickly  become  a  night-\nmare  for  the  ops  team  who  deploys  and  manages  them  on  production  servers.  The\nbigger the number of components you need to deploy on the same host, the harder it\nwill be to manage all their dependencies to satisfy all their requirements. \n1.1.2Providing a consistent environment to applications\nRegardless  of  how  many  individual  components  you’re  developing  and  deploying,\none of the biggest problems that developers and operations teams always have to deal\nwith is the differences in the environments they run their apps in. Not only is there a\nhuge  difference  between  development  and  production  environments,  differences\neven exist between individual production machines. Another unavoidable fact is that\nthe environment of a single production machine will change over time. \n  These  differences  range  from  hardware  to  the  operating  system  to  the  libraries\nthat  are  available  on  each  machine.  Production  environments  are  managed  by  the\noperations  team,  while  developers  often  take  care  of  their  development  laptops  on\ntheir own. The difference is how much these two groups of people know about sys-\ntem  administration,  and  this  understandably  leads  to  relatively  big  differences\nbetween those two systems, not to mention that system administrators give much more\nemphasis on keeping the system up to date with the latest security patches, while a lot\nof developers don’t care about that as much. \n Also, production systems can run applications from multiple developers or devel-\nopment  teams,  which  isn’t  necessarily  true  for  developers’  computers.  A  production\nsystem must provide the proper environment to all applications it hosts, even though\nthey may require different, even conflicting, versions of libraries.\n To reduce the number of problems that only show up in production, it would be\nideal  if  applications  could  run  in  the  exact  same  environment  during  development\nand in production so they have the exact same operating system, libraries, system con-\nfiguration,  networking  environment,  and  everything  else.  You  also  don’t  want  this\nenvironment  to  change  too  much  over  time,  if  at  all.  Also,  if  possible,  you  want  the\nability  to  add  applications  to  the  same  server  without  affecting  any  of  the  existing\napplications on that server. \n1.1.3Moving to continuous delivery: DevOps and NoOps\nIn the last few years, we’ve also seen a shift in the whole application development pro-\ncess  and  how  applications  are  taken  care  of  in  production.  In  the  past,  the  develop-\nment team’s job was to create the application and hand it off to the operations team,\nwho  then  deployed  it,  tended  to  it,  and  kept  it  running.  But  now,  organizations  are\nrealizing it’s better to have the same team that develops the application also take part\nin deploying it and taking care of it over its whole lifetime. This means the developer,\nQA,  and  operations  teams  now  need  to  collaborate  throughout  the  whole  process.\nThis practice is called DevOps.\n \n\n7Introducing container technologies\nUNDERSTANDING THE BENEFITS\nHaving the developers more involved in running the application in production leads\nto  them  having  a  better  understanding  of  both  the  users’  needs  and  issues  and  the\nproblems  faced  by  the  ops  team  while  maintaining  the  app.  Application  developers\nare now also much more inclined to give users the app earlier and then use their feed-\nback to steer further development of the app. \n To release newer versions of applications more often, you need to streamline the\ndeployment  process.  Ideally,  you  want  developers  to  deploy  the  applications  them-\nselves  without  having  to  wait  for  the  ops  people.  But  deploying  an  application  often\nrequires  an  understanding  of  the  underlying  infrastructure  and  the  organization  of\nthe hardware in the datacenter. Developers don’t always know those details and, most\nof the time, don’t even want to know about them. \nLETTING DEVELOPERS AND SYSADMINS DO WHAT THEY DO BEST\nEven  though  developers  and  system  administrators  both  work  toward  achieving  the\nsame goal of running a successful software application as a service to its customers, they\nhave different individual goals and motivating factors. Developers love creating new fea-\ntures and improving the user experience. They don’t normally want to be the ones mak-\ning sure that the underlying operating system is up to date with all the security patches\nand things like that. They prefer to leave that up to the system administrators. \n The ops team is in charge of the production deployments and the hardware infra-\nstructure they run on. They care about system security, utilization, and other aspects\nthat aren’t a high priority for developers. The ops people don’t want to deal with the\nimplicit interdependencies of all the application components and don’t want to think\nabout  how  changes  to  either  the  underlying  operating  system  or  the  infrastructure\ncan affect the operation of the application as a whole, but they must.\n Ideally, you want the developers to deploy applications themselves without know-\ning  anything  about  the  hardware  infrastructure  and  without  dealing  with  the  ops\nteam. This is referred to as NoOps. Obviously, you still need someone to take care of\nthe  hardware  infrastructure,  but  ideally,  without  having  to  deal  with  peculiarities  of\neach application running on it. \n As you’ll see, Kubernetes enables us to achieve all of this. By abstracting away the\nactual hardware and exposing it as a single platform for deploying and running apps,\nit allows developers to configure and deploy their applications without any help from\nthe sysadmins and allows the sysadmins to focus on keeping the underlying infrastruc-\nture up and running, while not having to know anything about the actual applications\nrunning on top of it.\n1.2Introducing container technologies\nIn section 1.1 I presented a non-comprehensive list of problems facing today’s devel-\nopment and ops teams. While you have many ways of dealing with them, this book will\nfocus on how they’re solved with Kubernetes. \n \n\n8CHAPTER 1Introducing Kubernetes\n  Kubernetes  uses  Linux  container  technologies  to  provide  isolation  of  running\napplications,  so  before  we  dig  into  Kubernetes  itself,  you  need  to  become  familiar\nwith the basics of containers to understand what Kubernetes does itself, and what it\noffloads to container technologies like Docker or rkt (pronounced “rock-it”).\n1.2.1Understanding what containers are\nIn  section  1.1.1  we  saw  how  different  software  components  running  on  the  same\nmachine  will  require  different,  possibly  conflicting,  versions  of  dependent  libraries  or\nhave other different environment requirements in general. \n When an application is composed of only smaller numbers of large components,\nit’s completely acceptable to give a dedicated Virtual Machine (VM) to each compo-\nnent and isolate their environments by providing each of them with their own operat-\ning  system  instance.  But  when  these  components  start  getting  smaller  and  their\nnumbers start to grow, you can’t give each of them their own VM if you don’t want to\nwaste hardware resources and keep your hardware costs down. But it’s not only about\nwasting  hardware  resources.  Because  each  VM  usually  needs  to  be  configured  and\nmanaged  individually,  rising  numbers of VMs also lead to  wasting  human  resources,\nbecause they increase the system administrators’ workload considerably.\nISOLATING COMPONENTS WITH LINUX CONTAINER TECHNOLOGIES\nInstead of using virtual machines to isolate the environments of each microservice (or\nsoftware  processes  in  general),  developers  are  turning  to  Linux  container  technolo-\ngies. They allow you to run multiple services on the same host machine, while not only\nexposing a different environment to each of them, but also isolating them from each\nother, similarly to VMs, but with much less overhead.\n A process running in a container runs inside the host’s operating system, like all\nthe  other  processes  (unlike  VMs,  where  processes  run  in  separate  operating  sys-\ntems). But the process in the container is still isolated from other processes. To the\nprocess itself, it looks like it’s the only one running on the machine and in its oper-\nating system. \nCOMPARING VIRTUAL MACHINES TO CONTAINERS\nCompared  to  VMs,  containers  are  much  more  lightweight,  which  allows  you  to  run\nhigher numbers of software components on the same hardware, mainly because each\nVM needs to run its own set of system processes, which requires additional compute\nresources  in  addition  to  those  consumed  by  the  component’s  own  process.  A  con-\ntainer, on the other hand, is nothing more than a single isolated process running in\nthe  host  OS,  consuming  only  the  resources  that  the  app  consumes  and  without  the\noverhead of any additional processes. \n Because of the overhead of VMs, you often end up grouping multiple applications\ninto  each  VM  because  you  don’t  have  enough  resources  to  dedicate  a  whole  VM  to\neach app. When using containers, you can (and should) have one container for each\n \n\n9Introducing container technologies\napplication, as shown in figure 1.4. The end-result is that you can fit many more appli-\ncations on the same bare-metal machine.\nWhen you run three VMs on a host, you have three completely separate operating sys-\ntems running on and sharing the same bare-metal hardware. Underneath those VMs\nis the host’s OS and a hypervisor, which divides the physical hardware resources into\nsmaller sets of virtual resources that can be used by the operating system inside each\nVM. Applications running inside those VMs perform system calls to the guest OS’ ker-\nnel in the VM, and the kernel then performs x86 instructions on the host’s physical\nCPU through the hypervisor. \nNOTETwo types of hypervisors exist. Type 1 hypervisors don’t use a host OS,\nwhile Type 2 do.\nContainers, on the other hand, all perform system calls on the exact same kernel run-\nning in the host OS. This single kernel is the only one performing x86 instructions on\nthe host’s CPU. The CPU doesn’t need to do any kind of virtualization the way it does\nwith VMs (see figure 1.5).\n  The  main  benefit  of  virtual  machines  is  the  full  isolation  they  provide,  because\neach VM runs its own Linux kernel, while containers all call out to the same kernel,\nwhich  can  clearly  pose  a  security  risk.  If  you  have  a  limited  amount  of  hardware\nresources, VMs may only be an option when you have a small number of processes that\nApps running in three VMs\n(on a single machine)\nBare-metal machine\nVM 1VM 2VM 3\nApp A\nApp B\nApp C\nApp D\nApp E\nApp F\nGuest OSGuest OSGuest OS\nBare-metal machine\nHost OS\nHypervisor\nApps running in\nisolated containers\nContainer 1Container 2Container 3\nApp AApp BApp C\nContainer 4Container 5Container 6\nApp DApp EApp F\nContainer 7Container 8Container 9\nApp ...App ...App ...\nHost OS\nFigure 1.4   Using VMs to isolate groups of applications vs. isolating individual apps with containers\n \n\n10CHAPTER 1Introducing Kubernetes\nyou  want  to  isolate.  To  run  greater  numbers  of  isolated  processes  on  the  same\nmachine, containers are a much better choice because of their low overhead. Remem-\nber, each VM runs its own set of system services, while containers don’t, because they\nall run in the same OS. That also means that to run a container, nothing needs to be\nbooted up, as is the case in VMs. A process run in a container starts up immediately.\nApps running in multiple VMs\nVM 1\nApp\nA\nApp\nB\nKernel\nVirtual CPU\nHypervisor\nPhysical CPU\nKernel\nPhysical CPU\nVM 2\nApp\nD\nKernel\nVirtual CPU\nApp\nC\nApp\nE\nVM 3\nApp\nF\nKernel\nVirtual CPU\nApps running in isolated containers\nContainer\nA\nContainer\nB\nContainer\nC\nContainer\nD\nContainer\nE\nContainer\nF\nApp\nA\nApp\nB\nApp\nD\nApp\nE\nApp\nF\nApp\nC\nFigure 1.5   The difference between \nhow apps in VMs use the CPU vs. how \nthey use them in containers\n \n\n11Introducing container technologies\nINTRODUCING THE MECHANISMS THAT MAKE CONTAINER ISOLATION POSSIBLE \nBy this point, you’re probably wondering how exactly containers can isolate processes\nif they’re running on the same operating system. Two mechanisms make this possible.\nThe first one, Linux Namespaces, makes sure each process sees its own personal view of\nthe  system  (files,  processes,  network  interfaces,  hostname,  and  so  on).  The  second\none is Linux Control Groups (cgroups), which limit the amount of resources the process\ncan consume (CPU, memory, network bandwidth, and so on).\nISOLATING PROCESSES WITH LINUX NAMESPACES\nBy default, each Linux system initially has one single namespace. All system resources,\nsuch as filesystems, process IDs, user IDs, network interfaces, and others, belong to the\nsingle  namespace.  But  you  can  create  additional  namespaces  and  organize  resources\nacross them. When running a process, you run it inside one of those namespaces. The\nprocess  will  only  see  resources  that  are  inside  the  same  namespace.  Well,  multiple\nkinds of namespaces exist, so a process doesn’t belong to one namespace, but to one\nnamespace of each kind. \n The following kinds of namespaces exist:\nMount (mnt)\nProcess ID (pid)\nNetwork (net)\nInter-process communication (ipc)\nUTS\nUser ID (user)\nEach namespace kind is used to isolate a certain group of resources. For example, the\nUTS  namespace  determines  what  hostname  and  domain  name  the  process  running\ninside  that  namespace  sees.  By  assigning  two  different  UTS  namespaces  to  a  pair  of\nprocesses,  you  can  make  them  see  different  local  hostnames.  In  other  words,  to  the\ntwo processes, it will appear as though they are running on two different machines (at\nleast as far as the hostname is concerned). \n  Likewise,  what  Network  namespace  a  process  belongs  to  determines  which  net-\nwork  interfaces  the  application  running  inside  the  process  sees.  Each  network  inter-\nface  belongs  to  exactly  one  namespace,  but  can  be  moved  from  one  namespace  to\nanother.  Each  container  uses  its  own  Network  namespace,  and  therefore  each  con-\ntainer sees its own set of network interfaces.\n  This  should  give  you  a  basic  idea  of  how  namespaces  are  used  to  isolate  applica-\ntions running in containers from each other. \nLIMITING RESOURCES AVAILABLE TO A PROCESS\nThe  other  half  of  container  isolation  deals  with  limiting  the  amount  of  system\nresources a container can consume. This is achieved with cgroups, a Linux kernel fea-\nture that limits the resource usage of a process  (or  a  group  of  processes).  A  process\ncan’t  use  more  than  the  configured  amount  of  CPU,  memory,  network  bandwidth,\n \n\n12CHAPTER 1Introducing Kubernetes\nand  so  on.  This  way,  processes  cannot  hog  resources  reserved  for  other  processes,\nwhich is similar to when each process runs on a separate machine.\n1.2.2Introducing the Docker container platform\nWhile  container  technologies  have  been  around  for  a  long  time,  they’ve  become\nmore widely known with the rise of the Docker container platform. Docker was the\nfirst container system that made containers easily portable across different machines.\nIt  simplified  the  process  of  packaging  up  not  only  the  application  but  also  all  its\nlibraries and other dependencies, even the whole OS file system, into a simple, por-\ntable  package  that  can  be  used  to  provision  the  application  to  any  other  machine\nrunning Docker. \n  When  you  run  an  application  packaged  with  Docker,  it  sees  the  exact  filesystem\ncontents  that  you’ve  bundled  with  it.  It  sees  the  same  files  whether  it’s  running  on\nyour development machine or a production machine, even if it the production server\nis running a completely different Linux OS. The application won’t see anything from\nthe server it’s running on, so it doesn’t matter if the server has a completely different\nset of installed libraries compared to your development machine. \n  For  example,  if  you’ve  packaged  up  your  application  with  the  files  of  the  whole\nRed Hat Enterprise Linux (RHEL) operating system, the application will believe it’s\nrunning inside RHEL, both when you run it on your development computer that runs\nFedora and when you run it on a server running Debian or some other Linux distribu-\ntion. Only the kernel may be different.\n This is similar to creating a VM image by installing an operating system into a VM,\ninstalling  the  app  inside  it,  and  then  distributing  the  whole  VM  image  around  and\nrunning it. Docker achieves the same effect, but instead of using VMs to achieve app\nisolation,  it  uses  Linux  container  technologies  mentioned  in  the  previous  section  to\nprovide (almost) the same level of isolation that VMs do. Instead of using big mono-\nlithic VM images, it uses container images, which are usually smaller.\n  A  big  difference  between  Docker-based  container  images  and  VM  images  is  that\ncontainer images are composed of layers, which can be shared and reused across mul-\ntiple images. This means only certain layers of an image need to be downloaded if the\nother layers were already downloaded previously when running a different container\nimage that also contains the same layers.\nUNDERSTANDING DOCKER CONCEPTS\nDocker is a platform for packaging, distributing, and running applications. As we’ve\nalready stated, it allows you to package your application together with its whole envi-\nronment. This can be either a few libraries that the app requires or even all the files\nthat  are  usually  available  on  the  filesystem  of  an  installed  operating  system.  Docker\nmakes  it  possible  to  transfer  this  package  to  a  central  repository  from  which  it  can\nthen  be  transferred  to  any  computer  running  Docker  and  executed  there  (for  the\nmost part, but not always, as we’ll soon explain).\n \n\n13Introducing container technologies\n Three main concepts in Docker comprise this scenario:\nImages—A Docker-based container image is something you package your appli-\ncation and its environment into. It contains the filesystem that will be available\nto the application and other metadata, such as the path to the executable that\nshould be executed when the image is run. \nRegistries—A Docker Registry is a repository that stores your Docker images and\nfacilitates  easy  sharing  of  those  images  between  different  people  and  comput-\ners. When you build your image, you can either run it on the computer you’ve\nbuilt  it  on,  or  you  can  push  (upload)  the  image  to  a  registry  and  then  pull\n(download) it on another computer and run it there. Certain registries are pub-\nlic, allowing anyone to pull images from it, while others are private, only accessi-\nble to certain people or machines.\nContainers—A Docker-based container is a regular Linux container created from\na Docker-based container image. A running container is a process running on\nthe host running Docker, but it’s completely isolated from both the host and all\nother processes running on it. The process is also resource-constrained, mean-\ning it can only access and use the amount of resources (CPU, RAM, and so on)\nthat are allocated to it. \nBUILDING, DISTRIBUTING, AND RUNNING A DOCKER IMAGE\nFigure 1.6 shows all three concepts and how they relate to each other. The developer\nfirst  builds  an  image  and  then  pushes  it  to  a  registry.  The  image  is  thus  available  to\nanyone  who  can  access  the  registry.  They  can  then  pull  the  image  to  any  other\nmachine  running  Docker  and  run  the  image.  Docker  creates  an  isolated  container\nbased on the image and runs the binary executable specified as part of the image.\nDocker\nImage\nContainer\nImage registry\nImage\nDocker\nImage\nDevelopment machine\nProduction machine\n1. Developer tells\nDocker to build\nand push image\n2. Docker\nbuilds image\n4. Developer tells\nDocker on production\nmachine to run image\n3. Docker\npushes image\nto registry\n5. Docker pulls\nimage from\nregistry\n6. Docker runs\ncontainer from\nimage\nDeveloper\nFigure 1.6   Docker images, registries, and containers\n \n\n14CHAPTER 1Introducing Kubernetes\nCOMPARING VIRTUAL MACHINES AND DOCKER CONTAINERS\nI’ve  explained  how  Linux  containers  are  generally  like  virtual  machines,  but  much\nmore lightweight. Now let’s look at how Docker containers specifically compare to vir-\ntual machines (and how Docker images compare to VM images). Figure 1.7 again shows\nthe same six applications running both in VMs and as Docker containers.\nYou’ll  notice  that  apps  A  and  B  have  access  to  the  same  binaries  and  libraries  both\nwhen running in a VM and when running as two separate containers. In the VM, this\nis  obvious,  because  both  apps  see  the  same  filesystem  (that  of  the  VM).  But  we  said\nHost running multiple VMs\nVM 1\nApp\nA\nApp\nB\nBinaries and\nlibraries\n(Filesystem)\nGuest OS kernel\nHypervisor\nHost OS\nHost OS\nVM 2\nApp\nD\nGuest OS kernel\nApp\nC\nApp\nE\nVM 3\nApp\nF\nGuest OS kernel\nHost running multiple Docker containers\nContainer 1Container 2\nContainer 3\nContainer 4\nContainer 5\nContainer 6\nApp\nD\nApp\nE\nApp\nF\nApp\nC\nApp\nA\nApp\nB\nBinaries and\nlibraries\n(Filesystem)\nBinaries and\nlibraries\n(Filesystem)\nBinaries and\nlibraries\n(Filesystem)\nBinaries and\nlibraries\n(Filesystem)\nBinaries and\nlibraries\n(Filesystem)\nDocker\nFigure 1.7   Running six apps on \nthree VMs vs. running them in \nDocker containers\n \n\n15Introducing container technologies\nthat  each  container  has  its  own  isolated  filesystem.  How  can  both  app  A  and  app  B\nshare the same files?\nUNDERSTANDING IMAGE LAYERS\nI’ve already said that Docker images are composed of layers. Different images can con-\ntain  the  exact  same  layers  because  every  Docker  image  is  built  on  top  of  another\nimage  and  two  different  images  can  both  use  the  same  parent  image  as  their  base.\nThis speeds up the distribution of images across the network, because layers that have\nalready been transferred as part of the first image don’t need to be transferred again\nwhen transferring the other image. \n But layers don’t only make distribution more efficient, they also help reduce the\nstorage  footprint  of  images.  Each  layer  is  only  stored  once.  Two  containers  created\nfrom two images based on the same base layers can therefore read the same files, but\nif one of them writes over those files, the other one doesn’t see those changes. There-\nfore, even if they share files, they’re still isolated from each other. This works because\ncontainer image layers are read-only. When a container is run, a new writable layer is\ncreated on top of the layers in the image. When the process in the container writes to\na file located in one of the underlying layers, a copy of the whole file is created in the\ntop-most layer and the process writes to the copy. \nUNDERSTANDING THE PORTABILITY LIMITATIONS OF CONTAINER IMAGES\nIn theory, a container image can be run on any Linux machine running Docker, but\none small caveat exists—one related to the fact that all containers running on a host use\nthe host’s Linux kernel. If a containerized application requires a specific kernel version,\nit  may  not  work  on  every  machine.  If  a  machine  runs  a  different  version  of  the  Linux\nkernel or doesn’t have the same kernel modules available, the app can’t run on it.\n While containers are much more lightweight compared to VMs, they impose cer-\ntain  constraints  on  the  apps  running  inside  them.  VMs  have  no  such  constraints,\nbecause each VM runs its own kernel. \n And it’s not only about the kernel. It should also be clear that a containerized app\nbuilt  for  a  specific  hardware  architecture  can  only  run  on  other  machines  that  have\nthe same architecture. You can’t containerize an application built for the x86 architec-\nture and expect it to run on an ARM-based machine because it also runs Docker. You\nstill need a VM for that.\n1.2.3Introducing rkt—an alternative to Docker\nDocker was the first container platform that made containers mainstream. I hope I’ve\nmade it clear that Docker itself doesn’t provide process isolation. The actual isolation\nof  containers  is  done  at  the  Linux  kernel  level  using  kernel  features  such  as  Linux\nNamespaces and cgroups. Docker only makes it easy to use those features.\n After the success of Docker, the Open Container Initiative (OCI) was born to cre-\nate open industry standards around container formats and runtime. Docker is part\nof that initiative, as is rkt (pronounced “rock-it”), which is another Linux container\nengine. \n \n\n16CHAPTER 1Introducing Kubernetes\n Like Docker, rkt is a platform for running containers. It puts a strong emphasis on\nsecurity, composability, and conforming to open standards. It uses the OCI container\nimage format and can even run regular Docker container images. \n  This  book  focuses  on  using  Docker  as  the  container  runtime  for  Kubernetes,\nbecause  it  was  initially  the  only  one  supported  by  Kubernetes.  Recently,  Kubernetes\nhas also started supporting rkt, as well as others, as the container runtime. \n The reason I mention rkt at this point is so you don’t make the mistake of thinking\nKubernetes  is  a  container  orchestration  system  made  specifically  for  Docker-based\ncontainers. In fact, over the course of this book, you’ll realize that the essence of\nKubernetes isn’t orchestrating containers. It’s much more. Containers happen to be\nthe best way to run apps on different cluster nodes. With that in mind, let’s finally dive\ninto the core of what this book is all about—Kubernetes.\n1.3Introducing Kubernetes\nWe’ve  already  shown  that  as  the  number  of  deployable  application  components  in\nyour  system  grows,  it  becomes  harder  to  manage  them  all.  Google  was  probably  the\nfirst company that realized it needed a much better way of deploying and managing\ntheir software components and their infrastructure to scale globally. It’s one of only a\nfew companies in the world that runs hundreds of thousands of servers and has had to\ndeal  with  managing  deployments  on  such  a  massive  scale.  This  has  forced  them  to\ndevelop solutions for making the development and deployment of thousands of soft-\nware components manageable and cost-efficient.\n1.3.1Understanding its origins\nThrough the years, Google developed an internal system called Borg (and later a new\nsystem called Omega), that helped both application developers and system administra-\ntors  manage  those  thousands  of  applications  and  services.  In  addition  to  simplifying\nthe development and management, it also helped them achieve a much higher utiliza-\ntion of their infrastructure, which is important when your organization is that large.\nWhen you run hundreds of thousands of machines, even tiny improvements in utiliza-\ntion  mean  savings  in  the  millions  of  dollars,  so  the  incentives  for  developing  such  a\nsystem are clear.\n  After  having  kept  Borg  and  Omega  secret  for  a  whole  decade,  in  2014  Google\nintroduced  Kubernetes,  an  open-source  system  based  on  the  experience  gained\nthrough Borg, Omega, and other internal Google systems. \n1.3.2Looking at Kubernetes from the top of a mountain\nKubernetes is a software system that allows you to easily deploy and manage container-\nized applications on top of it. It relies on the features of Linux containers to run het-\nerogeneous   applications   without   having   to   know   any   internal   details   of   these\napplications and without having to manually deploy these applications on each host.\nBecause  these  apps  run  in  containers,  they  don’t  affect  other  apps  running  on  the\n \n\n17Introducing Kubernetes\nsame server, which is critical when you run applications for completely different orga-\nnizations  on  the  same  hardware.  This  is  of  paramount  importance  for  cloud  provid-\ners,  because  they  strive  for  the  best  possible  utilization  of  their  hardware  while  still\nhaving to maintain complete isolation of hosted applications.\n  Kubernetes  enables  you  to  run  your  software  applications  on  thousands  of  com-\nputer nodes as if all those nodes were a single, enormous computer. It abstracts away\nthe underlying infrastructure and, by doing so, simplifies development, deployment,\nand management for both development and the operations teams. \n Deploying applications through Kubernetes is always the same, whether your clus-\nter  contains  only  a  couple  of  nodes  or  thousands  of  them.  The  size  of  the  cluster\nmakes  no  difference  at  all.  Additional  cluster  nodes  simply  represent  an  additional\namount of resources available to deployed apps.\nUNDERSTANDING THE CORE OF WHAT KUBERNETES DOES\nFigure 1.8 shows the simplest possible view of a Kubernetes system. The system is com-\nposed of a master node and any number of worker nodes. When the developer sub-\nmits  a  list  of  apps  to  the  master,  Kubernetes  deploys  them  to  the  cluster  of  worker\nnodes. What node a component lands on doesn’t (and shouldn’t) matter—neither to\nthe developer nor to the system administrator.\nThe  developer  can  specify  that  certain  apps  must  run  together  and  Kubernetes  will\ndeploy them on the same worker node. Others will be spread around the cluster, but\nthey can talk to each other in the same way, regardless of where they’re deployed.\nHELPING DEVELOPERS FOCUS ON THE CORE APP FEATURES\nKubernetes can be thought of as an operating system for the cluster. It relieves appli-\ncation  developers  from  having  to  implement  certain  infrastructure-related  services\ninto their apps; instead they rely on Kubernetes to provide these services. This includes\nthings such as service discovery, scaling, load-balancing, self-healing, and even leader\nKubernetes\nmaster\nTens or thousands of worker nodes exposed\nas a single deployment platform\n1x\nApp descriptor\n5x\n2x\nDeveloper\nFigure 1.8   Kubernetes exposes the whole datacenter as a single deployment platform.\n \n\n18CHAPTER 1Introducing Kubernetes\nelection. Application developers can therefore focus on implementing the actual fea-\ntures of the applications and not waste time figuring out how to integrate them with\nthe infrastructure.\nHELPING OPS TEAMS ACHIEVE BETTER RESOURCE UTILIZATION\nKubernetes will run your containerized app somewhere in the cluster, provide infor-\nmation to its components on how to find each other, and keep all of them running.\nBecause  your  application  doesn’t  care  which  node  it’s  running  on,  Kubernetes  can\nrelocate  the  app  at  any  time,  and  by  mixing  and  matching  apps,  achieve  far  better\nresource utilization than is possible with manual scheduling.\n1.3.3Understanding the architecture of a Kubernetes cluster\nWe’ve seen a bird’s-eye view of Kubernetes’ architecture. Now let’s take a closer look at\nwhat a Kubernetes cluster is composed of. At the hardware level, a Kubernetes cluster\nis composed of many nodes, which can be split into two types: \nThe master node, which hosts the Kubernetes Control Plane that controls and man-\nages the whole Kubernetes system\nWorker nodes that run the actual applications you deploy\nFigure  1.9  shows  the  components  running  on  these  two  sets  of  nodes.  I’ll  explain\nthem next.\nTHE CONTROL PLANE\nThe  Control  Plane  is  what  controls  the  cluster  and  makes  it  function.  It  consists  of\nmultiple components that can run on a single master node or be split across multiple\nnodes and replicated to ensure high availability. These components are\nThe Kubernetes API Server, which you and the other Control Plane components\ncommunicate with\nControl Plane (master)\netcd\nAPI server\nkube-proxy\nWorker node(s)\nKubelet\nContainer Runtime\nScheduler\nController\nManager\nFigure 1.9   The components that make up a Kubernetes cluster\n \nwww.allitebooks.com\n\n\n19Introducing Kubernetes\nThe Scheduler, which schedules your apps (assigns a worker node to each deploy-\nable component of your application) \nThe Controller  Manager,  which  performs  cluster-level  functions,  such  as  repli-\ncating  components,  keeping  track  of  worker  nodes,  handling  node  failures,\nand so on\netcd,  a  reliable  distributed  data  store  that  persistently  stores  the  cluster\nconfiguration.\nThe  components  of  the  Control  Plane  hold  and control the state of the cluster, but\nthey don’t run your applications. This is done by the (worker) nodes.\nTHE NODES\nThe  worker  nodes  are  the  machines  that  run  your  containerized  applications.  The\ntask  of  running,  monitoring,  and  providing  services  to  your  applications  is  done  by\nthe following components:\nDocker, rkt, or another container runtime, which runs your containers\nThe Kubelet, which talks to the API server and manages containers on its node\nThe Kubernetes  Service  Proxy  (kube-proxy),  which  load-balances  network  traffic\nbetween application components\nWe’ll explain all these components in detail in chapter 11. I’m not a fan of explaining\nhow things work before first explaining what something does and teaching people to\nuse it. It’s like learning to drive a car. You don’t want to know what’s under the hood.\nYou first want to learn how to drive it from point A to point B. Only after you learn\nhow to do that do you become interested in how a car makes that possible. After all,\nknowing what’s under the hood may someday help you get the car moving again after\nit breaks down and leaves you stranded at the side of the road.\n1.3.4Running an application in Kubernetes\nTo run an application in Kubernetes, you first need to package it up into one or more\ncontainer images, push those images to an image registry, and then post a description\nof your app to the Kubernetes API server. \n The description includes information such as the container image or images that\ncontain  your  application  components,  how  those  components  are  related  to  each\nother,  and  which  ones  need  to  be  run  co-located  (together  on  the  same  node)  and\nwhich don’t. For each component, you can also specify how many copies (or replicas)\nyou  want  to  run.  Additionally,  the  description  also  includes  which  of  those  compo-\nnents  provide  a  service  to  either  internal  or  external  clients  and  should  be  exposed\nthrough a single IP address and made discoverable to the other components. \nUNDERSTANDING HOW THE DESCRIPTION RESULTS IN A RUNNING CONTAINER\nWhen  the  API  server  processes  your  app’s  description,  the  Scheduler  schedules  the\nspecified  groups  of  containers  onto  the  available  worker  nodes  based  on  computa-\ntional resources required by each group and the unallocated resources on each node\n \n\n20CHAPTER 1Introducing Kubernetes\nat  that  moment.  The  Kubelet  on  those  nodes  then  instructs  the  Container  Runtime\n(Docker, for example) to pull the required container images and run the containers. \n  Examine  figure  1.10  to  gain  a  better  understanding  of  how  applications  are\ndeployed in Kubernetes. The app descriptor lists four containers, grouped into three\nsets (these sets are called pods; we’ll explain what they are in chapter 3). The first two\npods  each  contain  only  a  single  container,  whereas  the  last  one  contains  two.  That\nmeans  both  containers  need  to  run  co-located  and  shouldn’t  be  isolated  from  each\nother. Next to each pod, you also see a number representing the number of replicas\nof each pod that need to run in parallel. After submitting the descriptor to Kuberne-\ntes,  it  will  schedule  the  specified  number  of  replicas  of  each  pod  to  the  available\nworker nodes. The Kubelets on the nodes will then tell Docker to pull the container\nimages from the image registry and run the containers.\nKEEPING THE CONTAINERS RUNNING\nOnce the application is running, Kubernetes continuously makes sure that the deployed\nstate of the application always matches the description you provided. For example, if\n1x\nApp descriptor\nLegend:\nContainer imageMultiple containers\nrunning “together”\n(not fully isolated)\n5x\n2x\nControl Plane\n(master)\nImage registry\nWorker nodes\n...\nkube-proxy\nDocker\nKubeletkube-proxy\nDocker\nKubelet\nContainer\n...\nkube-proxy\nDocker\nKubeletkube-proxy\nDocker\nKubelet\n...\nkube-proxy\nDocker\nKubeletkube-proxy\nDocker\nKubelet\nFigure 1.10   A basic overview of the Kubernetes architecture and an application running on top of it\n \n\n21Introducing Kubernetes\nyou specify that you always want five instances of a web server running, Kubernetes will\nalways  keep  exactly  five  instances  running.  If  one  of  those  instances  stops  working\nproperly,  like  when  its  process  crashes  or  when  it  stops  responding,  Kubernetes  will\nrestart it automatically. \n Similarly, if a whole worker node dies or becomes inaccessible, Kubernetes will\nselect new nodes for all the containers that were running on the node and run them\non the newly selected nodes.\nSCALING THE NUMBER OF COPIES\nWhile the application is running, you can decide you want to increase or decrease the\nnumber  of  copies,  and  Kubernetes  will  spin  up  additional  ones  or  stop  the  excess\nones, respectively. You can even leave the job of deciding the optimal number of cop-\nies to Kubernetes. It can automatically keep adjusting the number, based on real-time\nmetrics,  such  as  CPU  load,  memory  consumption,  queries  per  second,  or  any  other\nmetric your app exposes. \nHITTING A MOVING TARGET\nWe’ve  said  that  Kubernetes  may  need  to  move  your  containers  around  the  cluster.\nThis can occur when the node they were running on has failed or because they were\nevicted from a node to make room for other containers. If the container is providing a\nservice to external clients or other containers running in the cluster, how can they use\nthe container properly if it’s constantly moving around the cluster? And how can cli-\nents  connect  to  containers  providing  a  service  when  those  containers  are  replicated\nand spread across the whole cluster?\n To allow clients to easily find containers that provide a specific service, you can tell\nKubernetes which containers provide the same service and Kubernetes will expose all\nof  them  at  a  single  static  IP  address  and  expose  that  address  to  all  applications  run-\nning in the cluster. This is done through environment variables, but clients can also\nlook up the service IP through good old DNS. The kube-proxy will make sure connec-\ntions to the service are load balanced across all the containers that provide the service.\nThe IP address of the service stays constant, so clients can always connect to its con-\ntainers, even when they’re moved around the cluster.\n1.3.5Understanding the benefits of using Kubernetes\nIf  you  have  Kubernetes  deployed  on  all  your  servers,  the  ops  team  doesn’t  need  to\ndeal with deploying your apps anymore. Because a containerized application already\ncontains all it needs to run, the system administrators don’t need to install anything to\ndeploy and run the app. On any node where Kubernetes is deployed, Kubernetes can\nrun the app immediately without any help from the sysadmins. \nSIMPLIFYING APPLICATION DEPLOYMENT\nBecause  Kubernetes  exposes  all  its  worker  nodes  as  a  single  deployment  platform,\napplication developers can start deploying applications on their own and don’t need\nto know anything about the servers that make up the cluster. \n \n\n22CHAPTER 1Introducing Kubernetes\n In essence, all the nodes are now a single bunch of computational resources that\nare waiting for applications to consume them. A developer doesn’t usually care what\nkind  of  server  the  application  is  running  on,  as  long  as  the  server  can  provide  the\napplication with adequate system resources. \n  Certain  cases  do  exist  where  the  developer does care what kind of hardware the\napplication should run on. If the nodes are heterogeneous, you’ll find cases when you\nwant certain apps to run on nodes with certain capabilities and run other apps on oth-\ners.  For  example,  one  of  your  apps  may  require  being  run  on  a  system  with  SSDs\ninstead  of  HDDs,  while  other  apps  run  fine  on  HDDs.  In  such  cases,  you  obviously\nwant to ensure that particular app is always scheduled to a node with an SSD.\n Without using Kubernetes, the sysadmin would select one specific node that has an\nSSD and deploy the app there. But when using Kubernetes, instead of selecting a spe-\ncific node where your app should be run, it’s more appropriate to tell Kubernetes to\nonly choose among nodes with an SSD. You’ll learn how to do that in chapter 3.\nACHIEVING BETTER UTILIZATION OF HARDWARE\nBy setting up Kubernetes on your servers and using it to run your apps instead of run-\nning them manually, you’ve decoupled your  app  from  the  infrastructure.  When  you\ntell Kubernetes to run your application, you’re letting it choose the most appropriate\nnode  to  run  your  application  on  based  on  the  description  of  the  application’s\nresource requirements and the available resources on each node. \n By using containers and not tying the app down to a specific node in your cluster,\nyou’re allowing the app to freely move around the cluster at any time, so the different\napp  components  running  on  the  cluster  can  be  mixed  and  matched  to  be  packed\ntightly onto the cluster nodes. This ensures the node’s hardware resources are utilized\nas best as possible.\n The ability to move applications around the cluster at any time allows Kubernetes\nto utilize the infrastructure much better than what you can achieve manually. Humans\naren’t good at finding optimal combinations, especially when the number of all possi-\nble options is huge, such as when you have many application components and many\nserver  nodes  they  can  be  deployed  on.  Computers  can  obviously  perform  this  work\nmuch better and faster than humans. \nHEALTH CHECKING AND SELF-HEALING\nHaving a system that allows moving an application across the cluster at any time is also\nvaluable in the event of server failures. As your cluster size increases, you’ll deal with\nfailing computer components ever more frequently. \n Kubernetes monitors your app components and the nodes they run on and auto-\nmatically  reschedules  them  to  other  nodes  in  the  event  of  a  node  failure.  This  frees\nthe ops team from having to migrate app components manually and allows the team\nto immediately focus on fixing the node itself and returning it to the pool of available\nhardware resources instead of focusing on relocating the app.\n  If  your  infrastructure  has  enough  spare  resources  to  allow  normal  system  opera-\ntion even without the failed node, the ops team doesn’t even need to react to the failure\n \n\n23Summary\nimmediately,  such  as  at  3  a.m.  They  can  sleep  tight  and  deal  with  the  failed  node\nduring regular work hours.\nAUTOMATIC SCALING\nUsing  Kubernetes  to  manage  your  deployed  applications  also  means  the  ops  team\ndoesn’t need to constantly monitor the load of individual applications to react to sud-\nden  load  spikes.  As  previously  mentioned,  Kubernetes  can  be  told  to  monitor  the\nresources  used  by  each  application  and  to  keep  adjusting  the  number  of  running\ninstances of each application. \n If Kubernetes is running on cloud infrastructure, where adding additional nodes is\nas  easy  as  requesting  them  through  the  cloud  provider’s  API,  Kubernetes  can  even\nautomatically  scale  the  whole  cluster  size  up  or  down  based  on  the  needs  of  the\ndeployed applications.\nSIMPLIFYING APPLICATION DEVELOPMENT\nThe features described in the previous section mostly benefit the operations team. But\nwhat  about  the  developers?  Does  Kubernetes  bring  anything  to  their  table?  It  defi-\nnitely does.\n  If  you  turn  back  to  the  fact  that  apps  run  in  the  same  environment  both  during\ndevelopment and in production, this has a big effect on when bugs are discovered. We\nall agree the sooner you discover a bug, the easier it is to fix it, and fixing it requires\nless work. It’s the developers who do the fixing, so this means less work for them. \n  Then  there’s  the  fact  that  developers  don’t  need  to  implement  features  that  they\nwould usually implement. This includes discovery of services and/or peers in a clustered\napplication. Kubernetes does this instead of the app. Usually, the app only needs to look\nup  certain  environment  variables  or  perform  a  DNS  lookup.  If  that’s  not  enough,  the\napplication can query the Kubernetes API server directly to get that and/or other infor-\nmation.  Querying  the  Kubernetes  API  server  like  that  can  even  save  developers  from\nhaving to implement complicated mechanisms such as leader election.\n As a final example of what Kubernetes brings to the table, you also need to con-\nsider the increase in confidence developers will feel knowing that when a new version\nof their app is going to be rolled out, Kubernetes can automatically detect if the new\nversion  is  bad  and  stop  its  rollout  immediately.  This  increase  in  confidence  usually\naccelerates the continuous delivery of apps, which benefits the whole organization.\n1.4Summary\nIn  this  introductory  chapter,  you’ve  seen  how  applications  have  changed  in  recent\nyears  and  how  they  can  now  be  harder  to  deploy  and  manage.  We’ve  introduced\nKubernetes  and  shown  how  it,  together  with  Docker  and  other  container  platforms,\nhelps  deploy  and  manage  applications  and  the  infrastructure  they  run  on.  You’ve\nlearned that\nMonolithic  apps  are  easier  to  deploy,  but  harder  to  maintain  over  time  and\nsometimes impossible to scale.\n \n\n24CHAPTER 1Introducing Kubernetes\nMicroservices-based application architectures allow easier development of each\ncomponent, but are harder to deploy and configure to work as a single system.\nLinux containers provide much the same benefits as virtual machines, but are\nfar more lightweight and allow for much better hardware utilization.\nDocker improved on existing Linux container technologies by allowing easier and\nfaster provisioning of containerized apps together with their OS environments.\nKubernetes  exposes  the  whole  datacenter  as  a  single  computational  resource\nfor running applications.\nDevelopers  can  deploy  apps  through  Kubernetes  without  assistance  from\nsysadmins.\nSysadmins  can  sleep  better  by  having  Kubernetes  deal  with  failed  nodes  auto-\nmatically.\nIn the next chapter, you’ll get your hands dirty by building an app and running it in\nDocker and then in Kubernetes.\n \n\n25\nFirst steps with Docker\nand Kubernetes\nBefore you start learning about Kubernetes concepts in detail, let’s see how to cre-\nate a simple application, package it into a container image, and run it in a managed\nKubernetes cluster (in Google Kubernetes Engine) or in a local single-node cluster.\nThis should give you a slightly better overview of the whole Kubernetes system and\nwill  make  it  easier  to  follow  the  next  few  chapters,  where  we’ll  go  over  the  basic\nbuilding blocks and concepts in Kubernetes.\nThis chapter covers\nCreating, running, and sharing a container image \nwith Docker\nRunning a single-node Kubernetes cluster locally\nSetting up a Kubernetes cluster on Google \nKubernetes Engine\nSetting up and using the kubectl command-line \nclient\nDeploying an app on Kubernetes and scaling it \nhorizontally\n \n\n26CHAPTER 2First steps with Docker and Kubernetes\n2.1Creating, running, and sharing a container image\nAs you’ve already learned in the previous chapter, running applications in Kubernetes\nrequires them to be packaged into container images. We’ll do a basic introduction to\nusing Docker in case you haven’t used it yet. In the next few sections you’ll\n1Install Docker and run your first “Hello world” container \n2Create a trivial Node.js app that you’ll later deploy in Kubernetes\n3Package  the  app  into  a  container  image  so  you  can  then  run  it  as  an  isolated\ncontainer\n4Run a container based on the image\n5Push the image to Docker Hub so that anyone anywhere can run it\n2.1.1Installing Docker and running a Hello World container\nFirst,  you’ll  need  to  install  Docker  on  your  Linux  machine.  If  you  don’t  use  Linux,\nyou’ll need to start a Linux virtual machine (VM) and run Docker inside that VM. If\nyou’re using a Mac or Windows and install Docker per instructions, Docker will set up\na VM for you and run the Docker daemon inside that VM. The Docker client execut-\nable will be available on your host OS, and will communicate with the daemon inside\nthe VM. \n  To  install  Docker,  follow  the  instructions  at  http://docs.docker.com/engine/\ninstallation/ for your specific operating system. After completing the installation, you\ncan use the Docker client executable to run various Docker commands. For example,\nyou  could  try  pulling  and  running  an  existing image from Docker Hub, the public\nDocker  registry,  which  contains  ready-to-use  container  images  for  many  well-known\nsoftware packages. One of them is the \nbusybox image, which you’ll use to run a simple\necho \"Hello world\" command. \nRUNNING A HELLO WORLD CONTAINER\nIf you’re unfamiliar with busybox, it’s a single executable that combines many of the\nstandard UNIX command-line tools, such as \necho, ls, gzip, and so on. Instead of the\nbusybox image, you could also use any other full-fledged OS container image such as\nFedora, Ubuntu, or other similar images, as long as it includes the \necho executable.\n How do you run the \nbusybox image? You don’t need to download or install any-\nthing.  Use  the  \ndocker run  command  and  specify  what  image  to  download  and  run\nand (optionally) what command to execute, as shown in the following listing.\n$ docker run busybox echo \"Hello world\"\nUnable to find image 'busybox:latest' locally\nlatest: Pulling from docker.io/busybox\n9a163e0b8d13: Pull complete \nfef924a0204a: Pull complete\nDigest: sha256:97473e34e311e6c1b3f61f2a721d038d1e5eef17d98d1353a513007cf46ca6bd\nStatus: Downloaded newer image for docker.io/busybox:latest\nHello world\nListing 2.1   Running a Hello world container with Docker\n \n\n27Creating, running, and sharing a container image\nThis  doesn’t  look  that  impressive,  but  when  you  consider  that  the  whole  “app”  was\ndownloaded  and  executed  with  a  single  command,  without  you  having  to  install  that\napp or anything else, you’ll agree it’s awesome. In your case, the app was a single execut-\nable  (busybox),  but  it  might  as  well  have  been  an  incredibly  complex  app  with  many\ndependencies. The whole process of setting up and running the app would have been\nexactly the same. What’s also important is that the app was executed inside a container,\ncompletely isolated from all the other processes running on your machine.\nUNDERSTANDING WHAT HAPPENS BEHIND THE SCENES\nFigure  2.1  shows  exactly  what  happened  when  you  performed  the  docker run  com-\nmand. First, Docker checked to see if the \nbusybox:latest image was already present\non your local machine. It wasn’t, so Docker pulled it from the Docker Hub registry at\nhttp://docker.io. After the image was downloaded to your machine, Docker created a\ncontainer  from  that  image  and  ran  the  command  inside  it.  The  \necho  command\nprinted  the  text  to  STDOUT  and  then  the  process  terminated  and  the  container\nstopped.\nRUNNING OTHER IMAGES\nRunning other existing container images is much the same as how you ran the busybox\nimage. In fact, it’s often even simpler, because you usually don’t need to specify what\ncommand  to  execute,  the  way  you  did  in  the  example  (\necho \"Hello world\").  The\ncommand that should be executed is usually baked into the image itself, but you can\noverride  it  if  you  want.  After  searching  or  browsing  through  the  publicly  available\nimages  on  http://hub.docker.com or another public registry, you tell Docker to run\nthe image like this:\n$ docker run <image>\nFigure 2.1   Running echo “Hello world” in a container based on the busybox container image\nLocal machine\nDocker Hub\n1.docker run busybox\necho \"Hello world\"\n3. Docker pulls\nbusybox image\nfrom registry (if not\navailable locally)\n2. Docker checks if busybox\nimage is already stored locally\n4. Docker runs\necho \"Hello world\"\nin isolated container\nbusybox\nDockerbusybox\n \n\n28CHAPTER 2First steps with Docker and Kubernetes\nVERSIONING CONTAINER IMAGES\nAll software packages get updated, so more than a single version of a package usually\nexists. Docker supports having multiple versions or variants of the same image under\nthe same name. Each variant must have a unique tag. When referring to images with-\nout explicitly specifying the tag, Docker will assume you’re referring to the so-called\nlatest tag. To run a different version of the image, you may specify the tag along with\nthe image name like this:\n$ docker run <image>:<tag>\n2.1.2Creating a trivial Node.js app\nNow that you have a working Docker setup, you’re going to create an app. You’ll build\na trivial Node.js web application and package it into a container image. The applica-\ntion will accept HTTP requests and respond with the hostname of the machine it’s\nrunning  in.  This  way,  you’ll  see  that  an  app  running  inside  a  container  sees  its  own\nhostname and not that of the host machine, even though it’s running on the host like\nany other process. This will be useful later, when you deploy the app on Kubernetes\nand scale it out (scale it horizontally; that is, run multiple instances of the app). You’ll\nsee your HTTP requests hitting different instances of the app.\n Your app will consist of a single file called app.js with the contents shown in the fol-\nlowing listing.\nconst http = require('http');\nconst os = require('os');\nconsole.log(\"Kubia server starting...\");\nvar handler = function(request, response) {\n  console.log(\"Received request from \" + request.connection.remoteAddress);\n  response.writeHead(200);\n  response.end(\"You've hit \" + os.hostname() + \"\\n\");\n};\nvar www = http.createServer(handler);\nwww.listen(8080);\nIt should be clear what this code does. It starts up an HTTP server on port 8080. The\nserver responds with an HTTP response status code \n200 OK and the text \"You’ve hit\n<hostname>\"\n to every request. The request handler also logs the client’s IP address to\nthe standard output, which you’ll need later.\nNOTEThe  returned  hostname  is  the  server’s  actual  hostname,  not  the  one\nthe client sends in the HTTP request’s \nHost header.\nYou could now download and install Node.js and test your app directly, but this isn’t\nnecessary, because you’ll use Docker to package the app into a container image and\nListing 2.2   A simple Node.js app: app.js\n \n\n29Creating, running, and sharing a container image\nenable it to be run anywhere without having to download or install anything (except\nDocker, which does need to be installed on the machine you want to run the image on).\n2.1.3Creating a Dockerfile for the image\nTo package your app into an image, you first need to create a file called Dockerfile,\nwhich  will  contain  a  list  of  instructions  that  Docker  will  perform  when  building  the\nimage. The Dockerfile needs to be in the same directory as the app.js file and should\ncontain the commands shown in the following listing.\nFROM node:7\nADD app.js /app.js\nENTRYPOINT [\"node\", \"app.js\"]\nThe FROM  line  defines  the  container  image  you’ll  use  as  a  starting  point  (the  base\nimage you’re building on top of). In your case, you’re using the \nnode container image,\ntag \n7. In the second line, you’re adding your app.js file from your local directory into\nthe  root  directory  in  the  image,  under  the  same  name  (app.js).  Finally,  in  the  third\nline,  you’re  defining  what  command  should  be  executed  when  somebody  runs  the\nimage. In your case, the command is \nnode app.js.\n2.1.4Building the container image\nNow that you have your Dockerfile and the app.js file, you have everything you need\nto build your image. To build it, run the following Docker command:\n$ docker build -t kubia .\nFigure  2.2  shows  what  happens  during  the  build  process.  You’re  telling  Docker  to\nbuild an image called \nkubia based on the contents of the current directory (note the\ndot at the end of the build command). Docker will look for the Dockerfile in the direc-\ntory and build the image based on the instructions in the file.\nListing 2.3   A Dockerfile for building a container image for your app\nChoosing a base image\nYou may wonder why we chose this specific image as your base. Because your app\nis a Node.js app, you need your image to contain the \nnode binary executable to run\nthe app. You could have used any image that contains that binary, or you could have\neven  used  a  Linux  distro  base  image  such  as  \nfedora  or  ubuntu  and  installed\nNode.js into the container at image build time. But because the \nnode image is made\nspecifically for running Node.js apps, and includes everything you need to run your\napp, you’ll use that as the base image.\n \n\n30CHAPTER 2First steps with Docker and Kubernetes\nUNDERSTANDING HOW AN IMAGE IS BUILT\nThe build process isn’t performed by the Docker client. Instead, the contents of the\nwhole  directory  are  uploaded  to  the  Docker  daemon  and  the  image  is  built  there.\nThe client and daemon don’t need to be on the same machine at all. If you’re using\nDocker  on  a  non-Linux  OS,  the  client  is  on  your  host  OS,  but  the  daemon  runs\ninside a VM. Because all the files in the build directory are uploaded to the daemon,\nif it contains many large files and the daemon isn’t running locally, the upload may\ntake longer. \nTIPDon’t include any unnecessary files in the build directory, because they’ll\nslow  down  the  build  process—especially  when  the  Docker  daemon  is  on  a\nremote machine. \nDuring the build process, Docker will first pull the base image (node:7) from the pub-\nlic image repository (Docker Hub), unless the image has already been pulled and is\nstored on your machine. \nUNDERSTANDING IMAGE LAYERS\nAn image isn’t a single, big, binary blob, but is composed of multiple layers, which you\nmay  have  already  noticed  when  running  the  busybox  example  (there  were  multiple\nPull complete lines—one for each layer). Different images may share several layers,\nFigure 2.2   Building a new container image from a Dockerfile\nLocal machine\nDocker Hub\n1.docker build\nkubia .\n3. Docker pulls image\nnode:7.0 if it isn’t\nstored locally yet\n4. Build new\nimage\n2. Docker client uploads\ndirectory contents to daemon\nDockerfile\nDocker client\nDocker daemon\napp.js\nnode:7.0\nnode:7.0\nkubia:latest\n \n\n31Creating, running, and sharing a container image\nwhich  makes  storing  and  transferring  images  much  more  efficient.  For  example,  if\nyou create multiple images based on the same base image (such as \nnode:7 in the exam-\nple), all the layers comprising the base image will be stored only once. Also, when pull-\ning an image, Docker will download each layer individually. Several layers may already\nbe stored on your machine, so Docker will only download those that aren’t.\n You may think that each Dockerfile creates only a single new layer, but that’s not\nthe case. When building an image, a new layer is created for each individual command\nin the Dockerfile. During the build of your image, after pulling all the layers of the base\nimage,  Docker  will  create  a  new  layer  on  top  of  them  and  add  the  app.js  file  into  it.\nThen it will create yet another layer that will specify the command that should be exe-\ncuted when the image is run. This last layer will then be tagged as \nkubia:latest. This is\nshown in figure 2.3, which also shows how a different image called \nother:latest would\nuse the same layers of the Node.js image as your own image does.\nWhen the build process completes, you have a new image stored locally. You can see it\nby telling Docker to list all locally stored images, as shown in the following listing.\n$ docker images\nREPOSITORY   TAG      IMAGE ID           CREATED             VIRTUAL SIZE\nkubia        latest   d30ecc7419e7       1 minute ago        637.1 MB\n...\nCOMPARING BUILDING IMAGES WITH A DOCKERFILE VS. MANUALLY\nDockerfiles are the usual way of building container images with Docker, but you could\nalso build the image manually by running a container from an existing image, execut-\ning commands in the container, exiting the container, and committing the final state\nas a new image. This is exactly what happens when you build from a Dockerfile, but\nit’s performed automatically and is repeatable, which allows you to make changes to\nListing 2.4   Listing locally stored images\nFigure 2.3   Container images are composed of layers that can be shared among different images.\nRUN curl ...\nCMD node\nADD app.js/app.js\n...\nCMD node app.js\nkubia:latest image\n...\n...\nRUN apt-get ...\nother:latest image\n...\nnode:0.12 image\nbuildpack-deps:jessie image\n \n\n32CHAPTER 2First steps with Docker and Kubernetes\nthe Dockerfile and rebuild the image any time, without having to manually retype all\nthe commands again.\n2.1.5Running the container image\nYou can now run your image with the following command:\n$ docker run --name kubia-container -p 8080:8080 -d kubia\nThis  tells  Docker  to  run  a  new  container  called  kubia-container  from  the  kubia\nimage. The container will be detached from the console (-d flag), which means it will\nrun in the background. Port 8080 on the local machine will be mapped to port 8080\ninside  the  container  (\n-p 8080:8080  option),  so  you  can  access  the  app  through\nhttp://localhost:8080. \n If you’re not running the Docker daemon on your local machine (if you’re using a\nMac or Windows, the daemon is running inside a VM), you’ll need to use the host-\nname  or  IP  of  the  VM  running  the  daemon  instead  of  localhost.  You  can  look  it  up\nthrough the \nDOCKER_HOST environment variable.\nACCESSING YOUR APP\nNow try to access your application at http://localhost:8080 (be sure to replace local-\nhost with the hostname or IP of the Docker host if necessary): \n$ curl localhost:8080\nYou’ve hit 44d76963e8e1   \nThat’s the response from your app. Your tiny application is now running inside a con-\ntainer, isolated from everything else. As you can see, it’s returning \n44d76963e8e1 as its\nhostname, and not the actual hostname of your host machine. The hexadecimal num-\nber is the ID of the Docker container. \nLISTING ALL RUNNING CONTAINERS\nLet’s  list  all  running  containers  in  the  following  listing,  so  you  can  examine  the  list\n(I’ve  edited  the  output  to  make  it  more  readable—imagine  the  last  two  lines  as  the\ncontinuation of the first two).\n$ docker ps\nCONTAINER ID  IMAGE         COMMAND               CREATED        ...\n44d76963e8e1  kubia:latest  \"/bin/sh -c 'node ap  6 minutes ago  ...\n...  STATUS              PORTS                    NAMES\n...  Up 6 minutes        0.0.0.0:8080->8080/tcp   kubia-container\nA single container is running. For each container, Docker prints out its ID and name,\nthe  image  used  to  run  the  container,  and  the  command  that’s  executing  inside  the\ncontainer. \nListing 2.5   Listing running containers\n \n\n33Creating, running, and sharing a container image\nGETTING ADDITIONAL INFORMATION ABOUT A CONTAINER\nThe docker ps command only shows the most basic information about the containers.\nTo see additional information, you can use \ndocker inspect:\n$ docker inspect kubia-container\nDocker  will  print  out  a  long  JSON  containing  low-level  information  about  the  con-\ntainer. \n2.1.6Exploring the inside of a running container\nWhat  if  you  want  to  see  what  the  environment  is  like  inside  the  container?  Because\nmultiple  processes  can  run  inside  the  same  container,  you  can  always  run  an  addi-\ntional  process  in  it  to  see  what’s  inside.  You  can  even  run  a  shell,  provided  that  the\nshell’s binary executable is available in the image. \nRUNNING A SHELL INSIDE AN EXISTING CONTAINER\nThe Node.js image on which you’ve based your image contains the bash shell, so you\ncan run the shell inside the container like this:\n$ docker exec -it kubia-container bash\nThis will run bash inside the existing kubia-container container. The bash process\nwill have the same Linux namespaces as the main container process. This allows you\nto explore the container from within and see how Node.js and your app see the system\nwhen running inside the container. The \n-it option is shorthand for two options: \n-i,  which  makes  sure  STDIN  is  kept  open.  You  need  this  for  entering  com-\nmands into the shell. \n-t, which allocates a pseudo terminal (TTY).\nYou need both if you want the use the shell like you’re used to. (If you leave out the\nfirst one, you can’t type any commands, and if you leave out the second one, the com-\nmand prompt won’t be displayed and some commands will complain about the \nTERM\nvariable not being set.)\nEXPLORING THE CONTAINER FROM WITHIN\nLet’s see how to use the shell in the following listing to see the processes running in\nthe container.\nroot@44d76963e8e1:/# ps aux\nUSER  PID %CPU %MEM    VSZ   RSS TTY STAT START TIME COMMAND\nroot    1  0.0  0.1 676380 16504 ?   Sl   12:31 0:00 node app.js\nroot   10  0.0  0.0  20216  1924 ?   Ss   12:31 0:00 bash\nroot   19  0.0  0.0  17492  1136 ?   R+   12:38 0:00 ps aux\nYou see only three processes. You don’t see any other processes from the host OS. \nListing 2.6   Listing processes from inside a container\n \n\n34CHAPTER 2First steps with Docker and Kubernetes\nUNDERSTANDING THAT PROCESSES IN A CONTAINER RUN IN THE HOST OPERATING SYSTEM\nIf you now open another terminal and list the processes on the host OS itself, you will,\namong  all  other  host  processes,  also  see  the  processes  running  in  the  container,  as\nshown in listing 2.7. \nNOTEIf you’re using a Mac or Windows, you’ll need to log into the VM where\nthe Docker daemon is running to see these processes.\n$ ps aux | grep app.js\nUSER  PID %CPU %MEM    VSZ   RSS TTY STAT START TIME COMMAND\nroot  382  0.0  0.1 676380 16504 ?   Sl   12:31 0:00 node app.js\nThis proves that processes running in the container are running in the host OS. If you\nhave a keen eye, you may have noticed that the processes have different IDs inside the\ncontainer  vs.  on  the  host.  The  container  is  using  its  own  PID  Linux  namespace  and\nhas a completely isolated process tree, with its own sequence of numbers. \nTHE CONTAINER’S FILESYSTEM IS ALSO ISOLATED\nLike  having  an  isolated  process  tree,  each  container  also  has  an  isolated  filesystem.\nListing the contents of the root directory inside the container will only show the files\nin the container and will include all the files that are in the image plus any files that\nare created while the container is running (log files and similar), as shown in the fol-\nlowing listing.\nroot@44d76963e8e1:/# ls /\napp.js  boot  etc   lib    media  opt   root  sbin  sys  usr\nbin     dev   home  lib64  mnt    proc  run   srv   tmp  var\nIt contains the app.js file and other system directories that are part of the node:7 base\nimage you’re using. To exit the container, you exit the shell by running the \nexit com-\nmand and you’ll be returned to your host machine (like logging out of an ssh session,\nfor example).\nTIPEntering a running container like this is useful when debugging an app\nrunning in a container. When something’s wrong, the first thing you’ll want\nto explore is the actual state of the system your application sees. Keep in mind\nthat  an  application  will  not  only  see  its  own  unique  filesystem,  but  also  pro-\ncesses, users, hostname, and network interfaces.\n2.1.7Stopping and removing a container\nTo stop your app, you tell Docker to stop the kubia-container container:\n$ docker stop kubia-container\nListing 2.7   A container’s processes run in the host OS\nListing 2.8   A container has its own complete filesystem\n \n\n35Creating, running, and sharing a container image\nThis  will  stop  the  main  process  running  in  the  container  and  consequently  stop  the\ncontainer,  because  no  other  processes  are  running  inside  the  container.  The  con-\ntainer itself still exists and you can see it with \ndocker ps -a. The -a option prints out\nall the containers, those running and those that have been stopped. To truly remove a\ncontainer, you need to remove it with the \ndocker rm command:\n$ docker rm kubia-container\nThis deletes the container. All its contents are removed and it can’t be started again.\n2.1.8Pushing the image to an image registry\nThe image you’ve built has so far only been available on your local machine. To allow\nyou to run it on any other machine, you need to push the image to an external image\nregistry. For the sake of simplicity, you won’t set up a private image registry and will\ninstead push the image to Docker Hub (http://hub.docker.com), which is one of the\npublicly  available  registries.  Other  widely  used  such  registries  are  Quay.io  and  the\nGoogle Container Registry.\n Before you do that, you need to re-tag your image according to Docker Hub’s\nrules.  Docker  Hub  will  allow  you  to  push  an  image  if  the  image’s  repository  name\nstarts  with  your  Docker  Hub  ID.  You  create your Docker Hub ID by registering at\nhttp://hub.docker.com. I’ll use my own ID (\nluksa) in the following examples. Please\nchange every occurrence with your own ID.\nTAGGING AN IMAGE UNDER AN ADDITIONAL TAG\nOnce  you  know  your  ID,  you’re  ready  to  rename  your  image,  currently  tagged  as\nkubia, to luksa/kubia (replace luksa with your own Docker Hub ID):\n$ docker tag kubia luksa/kubia\nThis doesn’t rename the tag; it creates an additional tag for the same image. You can\nconfirm this by listing the images stored on your system with the \ndocker images com-\nmand, as shown in the following listing.\n$ docker images | head\nREPOSITORY        TAG      IMAGE ID        CREATED             VIRTUAL SIZE\nluksa/kubia       latest   d30ecc7419e7    About an hour ago   654.5 MB\nkubia             latest   d30ecc7419e7    About an hour ago   654.5 MB\ndocker.io/node    7.0      04c0ca2a8dad    2 days ago          654.5 MB\n...\nAs you can see, both kubia and luksa/kubia point to the same image ID, so they’re in\nfact one single image with two tags. \nListing 2.9   A container image can have multiple tags\n \n\n36CHAPTER 2First steps with Docker and Kubernetes\nPUSHING THE IMAGE TO DOCKER HUB\nBefore you can push the image to Docker Hub, you need to log in under your user ID\nwith  the  \ndocker login  command.  Once  you’re  logged  in,  you  can  finally  push  the\nyourid/kubia image to Docker Hub like this:\n$ docker push luksa/kubia\nRUNNING THE IMAGE ON A DIFFERENT MACHINE\nAfter  the  push  to  Docker  Hub  is  complete,  the  image  will  be  available  to  everyone.\nYou can now run the image on any machine running Docker by executing the follow-\ning command:\n$ docker run -p 8080:8080 -d luksa/kubia\nIt doesn’t get much simpler than that. And the best thing about this is that your appli-\ncation will have the exact same environment every time and everywhere it’s run. If it\nran  fine  on  your  machine,  it  should  run  as  well  on  every  other  Linux  machine.  No\nneed  to  worry  about  whether  the  host  machine  has  Node.js  installed  or  not.  In  fact,\neven if it does, your app won’t use it, because it will use the one installed inside the\nimage.\n2.2Setting up a Kubernetes cluster\nNow  that  you  have  your  app  packaged  inside  a  container  image  and  made  available\nthrough Docker Hub, you can deploy it in a Kubernetes cluster instead of running it\nin Docker directly. But first, you need to set up the cluster itself. \n  Setting  up  a  full-fledged,  multi-node  Kubernetes  cluster  isn’t  a  simple  task,  espe-\ncially  if  you’re  not  well-versed  in  Linux  and  networking  administration.  A  proper\nKubernetes  install  spans  multiple  physical  or  virtual  machines  and  requires  the  net-\nworking to be set up properly, so that all the containers running inside the Kuberne-\ntes cluster can connect to each other through the same flat networking space. \n A long list of methods exists for installing a Kubernetes cluster. These methods are\ndescribed in detail in the documentation at http://kubernetes.io. We’re not going to\nlist  all  of  them  here,  because  the  list  keeps  evolving,  but  Kubernetes  can  be  run  on\nyour  local  development  machine,  your  own  organization’s  cluster  of  machines,  on\ncloud providers providing virtual machines (Google Compute Engine, Amazon EC2,\nMicrosoft Azure, and so on), or by using a managed Kubernetes cluster such as Goo-\ngle Kubernetes Engine (previously known as Google Container Engine). \n In this chapter, we’ll cover two simple options for getting your hands on a running\nKubernetes  cluster.  You’ll  see  how  to  run  a  single-node  Kubernetes  cluster  on  your\nlocal machine and how to get access to a hosted cluster running on Google Kuberne-\ntes Engine (GKE). \n A third option, which covers installing a cluster with the \nkubeadm tool, is explained\nin appendix B. The instructions there show you how to set up a three-node Kubernetes\n \n\n37Setting up a Kubernetes cluster\ncluster  using  virtual  machines,  but  I  suggest  you  try  it  only  after  reading  the  first  11\nchapters of the book.\n Another option is to install Kubernetes on Amazon’s AWS (Amazon Web Services).\nFor this, you can look at the \nkops tool, which is built on top of kubeadm mentioned in\nthe  previous  paragraph,  and  is  available  at  http://github.com/kubernetes/kops.  It\nhelps you deploy production-grade, highly available Kubernetes clusters on AWS and\nwill eventually support other platforms as well (Google Kubernetes Engine, VMware,\nvSphere, and so on).\n2.2.1Running a local single-node Kubernetes cluster with Minikube\nThe  simplest  and  quickest  path  to  a  fully  functioning  Kubernetes  cluster  is  by  using\nMinikube.  Minikube  is  a  tool  that  sets  up  a  single-node  cluster  that’s  great  for  both\ntesting Kubernetes and developing apps locally. \n Although we can’t show certain Kubernetes features related to managing apps on\nmultiple  nodes,  the  single-node  cluster  should  be  enough  for  exploring  most  topics\ndiscussed in this book. \nINSTALLING MINIKUBE\nMinikube is a single binary that needs to be downloaded and put onto your path. It’s\navailable for OSX, Linux, and Windows. To install it, the best place to start is to go to\nthe  Minikube  repository  on  GitHub  (http://github.com/kubernetes/minikube)  and\nfollow the instructions there.\n For example, on OSX and Linux, Minikube can be downloaded and set up with a\nsingle command. For OSX, this is what the command looks like:\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/\n➥ v0.23.0/minikube-darwin-amd64 && chmod +x minikube && sudo mv minikube \n➥\n /usr/local/bin/\nOn  Linux,  you  download  a  different  release  (replace  “darwin”  with  “linux”  in  the\nURL). On Windows, you can download the file manually, rename it to minikube.exe,\nand put it onto your path. Minikube runs Kubernetes inside a VM run through either\nVirtualBox or KVM, so you also need to install one of them before you can start the\nMinikube cluster.\nSTARTING A KUBERNETES CLUSTER WITH MINIKUBE\nOnce you have Minikube installed locally, you can immediately start up the Kuberne-\ntes cluster with the command in the following listing.\n$ minikube start\nStarting local Kubernetes cluster...\nStarting VM...\nSSH-ing files into VM...\n...\nKubectl is now configured to use the cluster. \nListing 2.10   Starting a Minikube virtual machine\n \n\n38CHAPTER 2First steps with Docker and Kubernetes\nStarting the cluster takes more than a minute, so don’t interrupt the command before\nit completes. \nINSTALLING THE KUBERNETES CLIENT (KUBECTL)\nTo interact with Kubernetes, you also need the kubectl CLI client. Again, all you need\nto  do  is  download  it  and  put  it  on  your  path.  The  latest  stable  release  for  OSX,  for\nexample, can be downloaded and installed with the following command:\n$ curl -LO https://storage.googleapis.com/kubernetes-release/release\n➥\n  /$(curl -s https://storage.googleapis.com/kubernetes-release/release\n➥\n  /stable.txt)/bin/darwin/amd64/kubectl \n➥\n  && chmod +x kubectl \n➥\n  && sudo mv kubectl /usr/local/bin/\nTo download kubectl for Linux or Windows, replace darwin in the URL with either\nlinux or windows.\nNOTEIf  you’ll  be  using  multiple  Kubernetes  clusters  (for  example,  both\nMinikube  and  GKE),  refer  to  appendix  A  for  information  on  how  to  set  up\nand switch between different \nkubectl contexts.\nCHECKING TO SEE THE CLUSTER IS UP AND KUBECTL CAN TALK TO IT\nTo  verify  your  cluster  is  working,  you  can  use  the  kubectl cluster-info  command\nshown in the following listing.\n$ kubectl cluster-info\nKubernetes master is running at https://192.168.99.100:8443\nKubeDNS is running at https://192.168.99.100:8443/api/v1/proxy/...\nkubernetes-dashboard is running at https://192.168.99.100:8443/api/v1/...\nThis shows the cluster is up. It shows the URLs of the various Kubernetes components,\nincluding the API server and the web console. \nTIPYou can run minikube ssh to log into the Minikube VM and explore it\nfrom  the  inside.  For  example,  you  may  want  to  see  what  processes  are  run-\nning on the node.\n2.2.2Using a hosted Kubernetes cluster with Google Kubernetes Engine\nIf  you  want  to  explore  a  full-fledged  multi-node  Kubernetes  cluster  instead,  you  can\nuse a managed Google Kubernetes Engine (GKE) cluster. This way, you don’t need to\nmanually set up all the cluster nodes and networking, which is usually too much for\nsomeone making their first steps with Kubernetes. Using a managed solution such as\nGKE makes sure you don’t end up with a misconfigured, non-working, or partially work-\ning cluster.\nListing 2.11   Displaying cluster information\n \n\n39Setting up a Kubernetes cluster\nSETTING UP A GOOGLE CLOUD PROJECT AND DOWNLOADING THE NECESSARY CLIENT BINARIES\nBefore you can set up a new Kubernetes cluster, you need to set up your GKE environ-\nment. Because the process may change, I’m not listing the exact instructions here. To\nget  started,  please  follow  the  instructions  at  https://cloud.google.com/container-\nengine/docs/before-you-begin.\n Roughly, the whole procedure includes\n1Signing  up  for  a  Google  account,  in  the  unlikely  case  you  don’t  have  one\nalready.\n2Creating a project in the Google Cloud Platform Console. \n3Enabling billing. This does require your credit card info, but Google provides a\n12-month  free  trial.  And  they’re  nice  enough  to  not  start  charging  automati-\ncally after the free trial is over.)\n4Enabling the Kubernetes Engine API.\n5Downloading  and  installing  Google  Cloud  SDK.  (This  includes  the  gcloud\ncommand-line tool, which you’ll need to create a Kubernetes cluster.)\n6Installing  the  kubectl  command-line  tool  with  gcloud components install\nkubectl\n.\nNOTECertain  operations  (the  one  in  step 2, for example) may take a few\nminutes to complete, so relax and grab a coffee in the meantime.\nCREATING A KUBERNETES CLUSTER WITH THREE NODES\nAfter  completing  the  installation,  you  can  create  a  Kubernetes  cluster  with  three\nworker nodes using the command shown in the following listing.\n$ gcloud container clusters create kubia --num-nodes 3 \n➥\n --machine-type f1-micro\nCreating cluster kubia...done.\nCreated [https://container.googleapis.com/v1/projects/kubia1-\n1227/zones/europe-west1-d/clusters/kubia].\nkubeconfig entry generated for kubia.\nNAME   ZONE   MST_VER MASTER_IP     TYPE     NODE_VER NUM_NODES STATUS\nkubia  eu-w1d 1.5.3   104.155.92.30 f1-micro 1.5.3    3         RUNNING\nYou should now have a running Kubernetes cluster with three worker nodes as shown\nin figure 2.4. You’re using three nodes to help better demonstrate features that apply\nto multiple nodes. You can use a smaller number of nodes, if you want. \nGETTING AN OVERVIEW OF YOUR CLUSTER\nTo give you a basic idea of what your cluster looks like and how to interact with it, see\nfigure  2.4.  Each  node  runs  Docker,  the  Kubelet  and  the  kube-proxy.  You’ll  interact\nwith the cluster through the \nkubectl command line client, which issues REST requests\nto the Kubernetes API server running on the master node.\nListing 2.12   Creating a three-node cluster in GKE\n \n\n40CHAPTER 2First steps with Docker and Kubernetes\nCHECKING IF THE CLUSTER IS UP BY LISTING CLUSTER NODES\nYou’ll use the kubectl command now to list all the nodes in your cluster, as shown in\nthe following listing.\n$ kubectl get nodes\nNAME                      STATUS  AGE  VERSION\ngke-kubia-85f6-node-0rrx  Ready   1m    v1.5.3\ngke-kubia-85f6-node-heo1  Ready   1m    v1.5.3\ngke-kubia-85f6-node-vs9f  Ready   1m    v1.5.3\nThe kubectl get command can list all kinds of Kubernetes objects. You’ll use it con-\nstantly, but it usually shows only the most basic information for the listed objects. \nTIPYou can log into one of the nodes with gcloud compute ssh <node-name>\nto explore what’s running on the node.\nListing 2.13   Listing cluster nodes with kubectl\nFigure 2.4   How you’re interacting with your three-node Kubernetes cluster \nLocal dev machine\nREST call\nkubectlREST API server\nMaster node\n(IP 104.155.92.30)\nDocker\nKubeletkube-proxy\ngke-kubia-85f6-node-heo1\nDocker\nKubeletkube-proxy\ngke-kubia-85f6-node-vs9f\nDocker\nWorker nodes\nKubernetes cluster\nKubeletkube-proxy\ngke-kubia-85f6-node-0rrx\n \n\n41Setting up a Kubernetes cluster\nRETRIEVING ADDITIONAL DETAILS OF AN OBJECT\nTo see more detailed information about an object, you can use the kubectl describe\ncommand, which shows much more: \n$ kubectl describe node gke-kubia-85f6-node-0rrx\nI’m omitting the actual output of the describe command, because it’s fairly wide and\nwould be completely unreadable here in the book. The output shows the node’s sta-\ntus, its CPU and memory data, system information, containers running on the node,\nand much more.\n  In  the  previous  \nkubectl describe  example,  you  specified  the  name  of  the  node\nexplicitly, but you could also have performed a simple \nkubectl describe node without\ntyping the node’s name and it would print out a detailed description of all the nodes.\nTIPRunning the describe and get commands without specifying the name\nof the object comes in handy when only one object of a given type exists, so\nyou don’t waste time typing or copy/pasting the object’s name.\nWhile  we’re  talking  about  reducing  keystrokes,  let  me  give  you  additional  advice  on\nhow to make working with \nkubectl much easier, before we move on to running your\nfirst app in Kubernetes.\n2.2.3Setting up an alias and command-line completion for kubectl \nYou’ll  use  kubectl  often.  You’ll  soon  realize  that  having  to  type  the  full  command\nevery time is a real pain. Before you continue, take a minute to make your life easier\nby setting up an alias and tab completion for \nkubectl.\nCREATING AN ALIAS\nThroughout  the  book,  I’ll  always  be  using  the  full  name  of  the  kubectl  executable,\nbut  you  may  want  to  add  a  short  alias  such  as  \nk,  so  you  won’t  have  to  type  kubectl\nevery time. If you haven’t used aliases yet, here’s how you define one. Add the follow-\ning line to your \n~/.bashrc or equivalent file:\nalias k=kubectl\nNOTEYou may already have the k executable if you used gcloud to set up the\ncluster.\nCONFIGURING TAB COMPLETION FOR KUBECTL\nEven with a short alias such as k, you’ll still need to type way more than you’d like. Luck-\nily, the \nkubectl command can also output shell completion code for both the bash and\nzsh  shell.  It  doesn’t  enable  tab  completion  of  only  command  names,  but  also  of  the\nactual object names. For example, instead of having to write the whole node name in\nthe previous example, all you’d need to type is\n$ kubectl desc<TAB> no<TAB> gke-ku<TAB>\n \n\n42CHAPTER 2First steps with Docker and Kubernetes\nTo enable tab completion in bash, you’ll first need to install a package called bash-\ncompletion\n and then run the following command (you’ll probably also want to add it\nto \n~/.bashrc or equivalent):\n$ source <(kubectl completion bash)\nBut there’s one caveat. When you run the preceding command, tab completion will\nonly  work  when  you  use  the  full  \nkubectl  name  (it  won’t  work  when  you  use  the  k\nalias). To fix this, you need to transform the output of the kubectl completion com-\nmand a bit:\n$ source <(kubectl completion bash | sed s/kubectl/k/g)\nNOTEUnfortunately,  as  I’m  writing  this,  shell  completion  doesn’t  work  for\naliases on MacOS. You’ll have to use the full \nkubectl command name if you\nwant completion to work.\nNow  you’re  all  set  to  start  interacting  with  your  cluster  without  having  to  type  too\nmuch. You can finally run your first app on Kubernetes.\n2.3Running your first app on Kubernetes\nBecause this may be your first time, you’ll use the simplest possible way of running an\napp  on  Kubernetes.  Usually,  you’d  prepare  a  JSON  or  YAML  manifest,  containing  a\ndescription of all the components you want to deploy, but because we haven’t talked\nabout the types of components you can create in Kubernetes yet, you’ll use a simple\none-line command to get something running.\n2.3.1Deploying your Node.js app\nThe simplest way to deploy your app is to use the kubectl run command, which will\ncreate all the necessary components without having to deal with JSON or YAML. This\nway, we don’t need to dive into the structure of each object yet. Try to run the image\nyou created and pushed to Docker Hub earlier. Here’s how to run it in Kubernetes:\n$ kubectl run kubia --image=luksa/kubia --port=8080 --generator=run/v1\nreplicationcontroller \"kubia\" created\nThe --image=luksa/kubia  part  obviously  specifies  the  container  image  you  want  to\nrun, and the \n--port=8080 option tells Kubernetes that your app is listening on port\n8080. The last flag (\n--generator) does require an explanation, though. Usually, you\nwon’t  use  it,  but  you’re  using  it  here  so  Kubernetes  creates  a  ReplicationController\ninstead of a Deployment. You’ll learn what ReplicationControllers are later in the chap-\nter,  but  we  won’t  talk  about  Deployments  until  chapter  9.  That’s  why  I  don’t  want\nkubectl to create a Deployment yet.\n  As  the  previous  command’s  output  shows,  a  ReplicationController  called  \nkubia\nhas been created. As already mentioned, we’ll see what that is later in the chapter. For\n \n\n43Running your first app on Kubernetes\nnow,  let’s  start  from  the  bottom  and  focus  on  the  container  you  created  (you  can\nassume a container has been created, because you specified a container image in the\nrun command).\nINTRODUCING PODS\nYou may be wondering if you can see your container in a list showing all the running\ncontainers. Maybe something such as \nkubectl get containers? Well, that’s not exactly\nhow Kubernetes works. It doesn’t deal with individual containers directly. Instead, it\nuses the concept of multiple co-located containers. This group of containers is called\na Pod. \n A pod is a group of one or more tightly related containers that will always run\ntogether on the same worker node and in the same Linux namespace(s). Each pod\nis  like  a  separate  logical  machine  with  its  own  IP,  hostname,  processes,  and  so  on,\nrunning a single application. The application can be a single process, running in a\nsingle container, or it can be a main application process and additional supporting\nprocesses, each running in its own container. All the containers in a pod will appear\nto be running on the same logical machine, whereas containers in other pods, even\nif they’re running on the same worker node, will appear to be running on a differ-\nent one. \n To better understand the relationship between containers, pods, and nodes, exam-\nine figure 2.5. As you can see, each pod has its own IP and contains one or more con-\ntainers,  each  running  an  application  process.  Pods  are  spread  out  across  different\nworker nodes.\nLISTING PODS\nBecause you can’t list individual containers, since they’re not standalone Kubernetes\nobjects,  can  you  list  pods  instead?  Yes,  you  can.  Let’s  see  how  to  tell  \nkubectl  to  list\npods in the following listing.\nFigure 2.5   The relationship between containers, pods, and physical worker nodes\nWorker node 1\nPod 2\nIP: 10.1.0.2\nContainer 1\nContainer 2\nPod 3\nIP: 10.1.0.3\nContainer 1\nPod 1\nIP: 10.1.0.1\nContainer\nWorker node 2\nPod 5\nIP: 10.1.1.2\nContainer 1\nPod 6\nIP: 10.1.1.3\nContainer 1\nPod 4\nIP: 10.1.1.1\nContainer\nContainer 2Container 2\n \n\n44CHAPTER 2First steps with Docker and Kubernetes\n$ kubectl get pods\nNAME          READY     STATUS    RESTARTS   AGE\nkubia-4jfyf   0/1       Pending   0          1m\nThis is your pod. Its status is still Pending and the pod’s single container is shown as\nnot ready yet (this is what the \n0/1 in the READY column means). The reason why the\npod  isn’t  running  yet  is  because  the  worker  node  the  pod  has  been  assigned  to  is\ndownloading the container image before it can run it. When the download is finished,\nthe  pod’s  container  will  be  created  and  then  the  pod  will  transition  to  the  \nRunning\nstate, as shown in the following listing.\n$ kubectl get pods\nNAME          READY     STATUS    RESTARTS   AGE\nkubia-4jfyf   1/1       Running   0          5m\nTo see more information about the pod, you can also use the kubectl describe pod\ncommand, like you did earlier for one of the worker nodes. If the pod stays stuck in\nthe Pending status, it might be that Kubernetes can’t pull the image from the registry.\nIf you’re using your own image, make sure it’s marked as public on Docker Hub. To\nmake sure the image can be pulled successfully, try pulling the image manually with\nthe \ndocker pull command on another machine. \nUNDERSTANDING WHAT HAPPENED BEHIND THE SCENES\nTo help you visualize what transpired, look at figure 2.6. It shows both steps you had to\nperform  to  get  a  container  image  running  inside  Kubernetes.  First,  you  built  the\nimage and pushed it to Docker Hub. This was necessary because building the image\non your local machine only makes it available on your local machine, but you needed\nto make it accessible to the Docker daemons running on your worker nodes.\n  When  you  ran  the  \nkubectl  command,  it  created  a  new  ReplicationController\nobject in the cluster by sending a REST HTTP request to the Kubernetes API server.\nThe ReplicationController then created a new pod, which was then scheduled to one\nof the worker nodes by the Scheduler. The Kubelet on that node saw that the pod was\nscheduled  to  it  and  instructed  Docker  to  pull  the  specified  image  from  the  registry\nbecause the image wasn’t available locally. After downloading the image, Docker cre-\nated and ran the container. \n The other two nodes are displayed to show context. They didn’t play any role in\nthe process, because the pod wasn’t scheduled to them.\nDEFINITIONThe  term  scheduling  means  assigning  the  pod  to  a  node.  The\npod is run immediately, not at a time in the future as the term might lead you\nto believe.\nListing 2.14   Listing pods\nListing 2.15   Listing pods again to see if the pod’s status has changed\n \n\n45Running your first app on Kubernetes\n2.3.2Accessing your web application\nWith your pod running, how do you access it? We mentioned that each pod gets its\nown  IP  address,  but  this  address  is  internal  to  the  cluster  and  isn’t  accessible  from\noutside of it. To make the pod accessible from the outside, you’ll expose it through a\nService object. You’ll create a special service of type \nLoadBalancer, because if you cre-\nate a regular service (a \nClusterIP service), like the pod, it would also only be accessi-\nble from inside the cluster. By creating a \nLoadBalancer-type service, an external load\nbalancer will be created and you can connect to the pod through the load balancer’s\npublic IP. \nCREATING A SERVICE OBJECT\nTo create the service, you’ll tell Kubernetes to expose the ReplicationController you\ncreated earlier:\n$ kubectl expose rc kubia --type=LoadBalancer --name kubia-http\nservice \"kubia-http\" exposed\nFigure 2.6   Running the luksa/kubia container image in Kubernetes\nLocal dev\nmachine\nkubectl\nREST API server\nScheduler\nMaster node(s)\nDocker\nKubelet\ngke-kubia-85f6-node-0rrx\nDocker\nKubelet\ngke-kubia-85f6-node-heo1\nDocker\nKubelet\ngke-kubia-85f6-node-vs9f\nDocker Hub\n3.kubectl run kubia\n--image=luksa/kubia\n--port=8080\n4.issueskubectl\nREST call\n5. Pod created\nand scheduled\nto a worker node\n7. Kubelet\ninstructs\nDocker\nto run the\nimage\n8. Docker pulls\nand runs\nluksa/kubia\n6. Kubelet\nis notified\n1.docker push\nluksa/kubia\n2. Image\nluksa/kubia\nis pushed to\nDocker Hub\nDocker\npod kubia-4jfyf\n \n\n46CHAPTER 2First steps with Docker and Kubernetes\nNOTEWe’re  using  the  abbreviation  rc  instead  of  replicationcontroller.\nMost resource types have an abbreviation like this so you don’t have to type\nthe full name (for example, \npo for pods, svc for services, and so on).\nLISTING SERVICES\nThe expose  command’s  output  mentions  a  service  called  kubia-http.  Services  are\nobjects like Pods and Nodes, so you can see the newly created Service object by run-\nning the \nkubectl get services command, as shown in the following listing.\n$ kubectl get services\nNAME         CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes   10.3.240.1     <none>        443/TCP         34m\nkubia-http   10.3.246.185   <pending>     8080:31348/TCP  4s\nThe  list  shows  two  services.  Ignore  the  kubernetes  service  for  now  and  take  a  close\nlook at the \nkubia-http service you created. It doesn’t have an external IP address yet,\nbecause it takes time for the load balancer to be created by the cloud infrastructure\nKubernetes is running on. Once the load balancer is up, the external IP address of the\nservice should be displayed. Let’s wait a while and list the services again, as shown in\nthe following listing.\n$ kubectl get svc\nNAME         CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes   10.3.240.1     <none>        443/TCP         35m\nkubia-http   10.3.246.185   104.155.74.57 8080:31348/TCP  1m\nAha,  there’s  the  external  IP.  Your  application  is  now  accessible  at  http://104.155.74\n.57:8080 from anywhere in the world. \nNOTEMinikube  doesn’t  support  LoadBalancer  services,  so  the  service  will\nnever  get  an  external  IP.  But  you  can  access  the  service  anyway  through  its\nexternal port. How to do that is described in the next section’s tip.\nACCESSING YOUR SERVICE THROUGH ITS EXTERNAL IP\nYou can now send requests to your pod through the service’s external IP and port:\n$ curl 104.155.74.57:8080\nYou’ve hit kubia-4jfyf \nWoohoo! Your app is now running somewhere in your three-node Kubernetes cluster\n(or  a  single-node  cluster  if  you’re  using  Minikube).  If  you  don’t  count  the  steps\nrequired to set up the whole cluster, all it took was two simple commands to get your\napp running and to make it accessible to users across the world.\nListing 2.16   Listing Services\nListing 2.17   Listing services again to see if an external IP has been assigned\n \n\n47Running your first app on Kubernetes\nTIPWhen  using  Minikube,  you  can  get  the  IP  and  port  through  which  you\ncan access the service by running \nminikube service kubia-http.\nIf you look closely, you’ll see that the app is reporting the name of the pod as its host-\nname. As already mentioned, each pod behaves like a separate independent machine\nwith  its  own  IP  address  and  hostname.  Even  though  the  application  is  running  in\nthe worker node’s operating system, to the app it appears as though it’s running on\na  separate  machine  dedicated  to  the  app  itself—no  other  processes  are  running\nalongside it.\n2.3.3The logical parts of your system\nUntil now, I’ve mostly explained the actual physical components of your system. You\nhave  three  worker  nodes,  which  are  VMs  running  Docker  and  the  Kubelet,  and  you\nhave a master node that controls the whole system. Honestly, we don’t know if a single\nmaster node is hosting all the individual components of the Kubernetes Control Plane\nor if they’re split across multiple nodes. It doesn’t really matter, because you’re only\ninteracting with the API server, which is accessible at a single endpoint.\n  Besides  this  physical  view  of  the  system,  there’s  also  a  separate,  logical  view  of  it.\nI’ve already mentioned Pods, ReplicationControllers, and Services. All of them will be\nexplained in the next few chapters, but let’s quickly look at how they fit together and\nwhat roles they play in your little setup.\nUNDERSTANDING HOW THE REPLICATIONCONTROLLER, THE POD, AND THE SERVICE FIT TOGETHER\nAs  I’ve  already  explained,  you’re  not  creating  and  working  with  containers  directly.\nInstead, the basic building block in Kubernetes is the pod. But, you didn’t really cre-\nate any pods either, at least not directly. By running the \nkubectl run command you\ncreated  a  ReplicationController,  and  this  ReplicationController  is  what  created  the\nactual  Pod  object.  To  make  that  pod  accessible  from  outside  the  cluster,  you  told\nKubernetes to expose all the pods managed by that ReplicationController as a single\nService. A rough picture of all three elements is presented in figure 2.7.\nFigure 2.7   Your system consists of a ReplicationController, a Pod, and a Service.\nPod: kubia-4jfyf\nIP: 10.1.0.1\nContainer\nPort\n8080\nService: kubia-http\nInternal IP: 10.3.246.185\nExternal IP: 104.155.74.57\nReplicationController: kubia\nReplicas: 1\nPort\n8080\nIncoming\nrequest\n \n\n48CHAPTER 2First steps with Docker and Kubernetes\nUNDERSTANDING THE POD AND ITS CONTAINER\nThe main and most important component in your system is the pod. It contains only a\nsingle  container,  but  generally  a  pod  can  contain  as  many  containers  as  you  want.\nInside the container is your Node.js process, which is bound to port 8080 and is wait-\ning for HTTP requests. The pod has its own unique private IP address and hostname. \nUNDERSTANDING THE ROLE OF THE REPLICATIONCONTROLLER\nThe next component is the kubia ReplicationController. It makes sure there’s always\nexactly one instance of your pod running. Generally, ReplicationControllers are used\nto replicate pods (that is, create multiple copies of a pod) and keep them running. In\nyour  case,  you  didn’t  specify  how  many  pod  replicas  you  want,  so  the  Replication-\nController  created  a  single  one.  If  your  pod  were  to  disappear  for  any  reason,  the\nReplicationController would create a new pod to replace the missing one. \nUNDERSTANDING WHY YOU NEED A SERVICE\nThe  third  component  of  your  system  is  the  kubia-http  service.  To  understand  why\nyou  need  services,  you  need  to  learn  a  key  detail  about  pods.  They’re  ephemeral.  A\npod may disappear at any time—because the node it’s running on has failed, because\nsomeone deleted the pod, or because the pod was evicted from an otherwise healthy\nnode.  When  any  of  those  occurs,  a  missing  pod  is  replaced  with  a  new  one  by  the\nReplicationController,  as  described  previously.  This  new  pod  gets  a  different  IP\naddress from the pod it’s replacing. This is where services come in—to solve the prob-\nlem of ever-changing pod IP addresses, as well as exposing multiple pods at a single\nconstant IP and port pair. \n When a service is created, it gets a static IP, which never changes during the lifetime of\nthe service. Instead of connecting to pods directly, clients should connect to the service\nthrough its constant IP address. The service makes sure one of the pods receives the con-\nnection, regardless of where the pod is currently running (and what its IP address is). \n Services represent a static location for a group of one or more pods that all provide\nthe same service. Requests coming to the IP and port of the service will be forwarded\nto the IP and port of one of the pods belonging to the service at that moment.  \n2.3.4Horizontally scaling the application\nYou now have a running application, monitored and kept running by a Replication-\nController  and  exposed  to  the  world  through  a  service.  Now  let’s  make  additional\nmagic happen. \n One of the main benefits of using Kubernetes is the simplicity with which you can\nscale your deployments. Let’s see how easy it is to scale up the number of pods. You’ll\nincrease the number of running instances to three. \n Your pod is managed by a ReplicationController. Let’s see it with the \nkubectl get\ncommand:\n$ kubectl get replicationcontrollers\nNAME        DESIRED    CURRENT   AGE\nkubia       1          1         17m\n \nwww.allitebooks.com\n\n\n49Running your first app on Kubernetes\nThe  list  shows  a  single  ReplicationController  called  kubia.  The  DESIRED  column\nshows  the  number  of  pod  replicas  you  want  the  ReplicationController  to  keep,\nwhereas the \nCURRENT column shows the actual number of pods currently running. In\nyour  case,  you  wanted  to  have  a  single  replica  of  the  pod  running,  and  exactly  one\nreplica is currently running. \nINCREASING THE DESIRED REPLICA COUNT\nTo  scale  up  the  number  of  replicas  of  your  pod,  you  need  to  change  the  desired\nreplica count on the ReplicationController like this:\n$ kubectl scale rc kubia --replicas=3\nreplicationcontroller \"kubia\" scaled\nYou’ve now told Kubernetes to make sure three instances of your pod are always run-\nning. Notice that you didn’t instruct Kubernetes what action to take. You didn’t tell it\nto  add  two  more  pods.  You  only  set  the  new  desired  number  of  instances  and  let\nKubernetes determine what actions it needs to take to achieve the requested state. \n  This  is  one  of  the  most  fundamental  Kubernetes  principles.  Instead  of  telling\nKubernetes exactly what actions it should perform, you’re only declaratively changing\nthe  desired  state  of  the  system  and  letting  Kubernetes  examine  the  current  actual\nstate and reconcile it with the desired state. This is true across all of Kubernetes.\nSEEING THE RESULTS OF THE SCALE-OUT\nBack to your replica count increase. Let’s list the ReplicationControllers again to see\nthe updated replica count:\n$ kubectl get rc\nNAME        DESIRED    CURRENT   READY   AGE\nkubia       3          3         2       17m\nBecause  the  actual  number  of  pods  has  already  been  increased  to  three  (as  evident\nfrom the \nCURRENT column), listing all the pods should now show three pods instead\nof one:\n$ kubectl get pods\nNAME          READY     STATUS    RESTARTS   AGE\nkubia-hczji   1/1       Running   0          7s\nkubia-iq9y6   0/1       Pending   0          7s\nkubia-4jfyf   1/1       Running   0          18m\nListing all the resource types with kubectl get\nYou’ve been using the same basic kubectl get command to list things in your cluster.\nYou’ve  used  this  command  to  list  Node,  Pod,  Service  and  ReplicationController\nobjects. You can get a list of all the possible object types by invoking \nkubectl get\nwithout  specifying  the  type.  You  can  then  use  those  types  with  various  kubectl\ncommands such as get, describe, and so on. The list also shows the abbreviations\nI mentioned earlier.\n \n\n50CHAPTER 2First steps with Docker and Kubernetes\nAs you can see, three pods exist instead of one. Two are already running, one is still\npending,  but  should  be  ready  in  a  few  moments,  as  soon  as  the  container  image  is\ndownloaded and the container is started. \n As you can see, scaling an application is incredibly simple. Once your app is run-\nning  in  production  and  a  need  to  scale  the  app  arises,  you  can  add  additional\ninstances with a single command without having to install and run additional copies\nmanually. \n Keep in mind that the app itself needs to support being scaled horizontally. Kuber-\nnetes doesn’t magically make your app scalable; it only makes it trivial to scale the app\nup or down. \nSEEING REQUESTS HIT ALL THREE PODS WHEN HITTING THE SERVICE\nBecause you now have multiple instances of your app running, let’s see what happens\nif you hit the service URL again. Will you always hit the same app instance or not?\n$ curl 104.155.74.57:8080\nYou’ve hit kubia-hczji\n$ curl 104.155.74.57:8080\nYou’ve hit kubia-iq9y6\n$ curl 104.155.74.57:8080\nYou’ve hit kubia-iq9y6\n$ curl 104.155.74.57:8080\nYou’ve hit kubia-4jfyf   \nRequests are hitting different pods randomly. This is what services in Kubernetes do\nwhen more than one pod instance backs them. They act as a load balancer standing in\nfront  of  multiple  pods.  When  there’s  only  one  pod,  services  provide  a  static  address\nfor  the  single  pod.  Whether  a  service  is  backed  by  a  single  pod  or  a  group  of  pods,\nthose pods come and go as they’re moved around the cluster, which means their IP\naddresses  change,  but  the  service  is  always  there  at  the  same  address.  This  makes  it\neasy  for  clients  to  connect  to  the  pods,  regardless  of  how  many  exist  and  how  often\nthey change location.\nVISUALIZING THE NEW STATE OF YOUR SYSTEM\nLet’s  visualize  your  system  again  to  see  what’s  changed  from  before.  Figure  2.8\nshows  the  new  state  of  your  system.  You  still  have  a  single  service  and  a  single\nReplicationController, but you now have three instances of your pod, all managed\nby  the  ReplicationController.  The  service  no  longer  sends  all  requests  to  a  single\npod, but spreads them across all three pods as shown in the experiment with \ncurl\nin the previous section.\n As an exercise, you can now try spinning up additional instances by increasing the\nReplicationController’s replica count even further and then scaling back down.\n \n\n51Running your first app on Kubernetes\n2.3.5Examining what nodes your app is running on \nYou may be wondering what nodes your pods have been scheduled to. In the Kuber-\nnetes world, what node a pod is running on isn’t that important, as long as it gets\nscheduled to a node that can provide the CPU and memory the pod needs to run\nproperly. \n Regardless of the node they’re scheduled to, all the apps running inside contain-\ners have the same type of OS environment. Each pod has its own IP and can talk to\nany other pod, regardless of whether that other pod is also running on the same node\nor on a different one. Each pod is provided with the requested amount of computa-\ntional  resources,  so  whether  those  resources  are  provided  by  one  node  or  another\ndoesn’t make any difference. \nDISPLAYING THE POD IP AND THE POD’S NODE WHEN LISTING PODS\nIf you’ve been paying close attention, you probably noticed that the kubectl get pods\ncommand doesn’t even show any information about the nodes the pods are scheduled\nto. This is because it’s usually not an important piece of information.\n But you can request additional columns to display using the \n-o wide option. When\nlisting pods, this option shows the pod’s IP and the node the pod is running on:\n$ kubectl get pods -o wide\nNAME          READY   STATUS    RESTARTS   AGE   IP         NODE\nkubia-hczji   1/1     Running   0          7s    10.1.0.2   gke-kubia-85...\nPod: kubia-4jfyf\nIP: 10.1.0.1\nContainer\nService: kubia-http\nInternal IP: 10.3.246.185\nExternal IP: 104.155.74.57\nPort\n8080\nIncoming\nrequest\nPod: kubia-hczji\nIP: 10.1.0.2\nContainer\nPod: kubia-iq9y6\nIP: 10.1.0.3\nContainer\nReplicationController: kubia\nReplicas: 3\nPort\n8080\nPort\n8080\nPort\n8080\nFigure 2.8   Three instances of a pod managed by the same ReplicationController and exposed \nthrough a single service IP and port.\n \n\n52CHAPTER 2First steps with Docker and Kubernetes\nINSPECTING OTHER DETAILS OF A POD WITH KUBECTL DESCRIBE\nYou  can  also  see  the  node  by  using  the  kubectl describe  command,  which  shows\nmany other details of the pod, as shown in the following listing.\n$ kubectl describe pod kubia-hczji\nName:        kubia-hczji\nNamespace:   default\nNode:        gke-kubia-85f6-node-vs9f/10.132.0.3    \nStart Time:  Fri, 29 Apr 2016 14:12:33 +0200\nLabels:      run=kubia\nStatus:      Running\nIP:          10.1.0.2\nControllers: ReplicationController/kubia\nContainers:  ...\nConditions:\n  Type       Status\n  Ready      True \nVolumes: ...\nEvents: ...\nThis  shows,  among  other  things,  the  node  the  pod  has  been  scheduled  to,  the  time\nwhen it was started, the image(s) it’s running, and other useful information.\n2.3.6Introducing the Kubernetes dashboard\nBefore we wrap up this initial hands-on chapter, let’s look at another way of exploring\nyour Kubernetes cluster.\n Up to now, you’ve only been using the \nkubectl command-line tool. If you’re more\ninto graphical web user interfaces, you’ll be glad to hear that Kubernetes also comes\nwith a nice (but still evolving) web dashboard.\n The dashboard allows you to list all the Pods, ReplicationControllers, Services, and\nother objects deployed in your cluster, as well as to create, modify, and delete them.\nFigure 2.9 shows the dashboard.\n Although you won’t use the dashboard in this book, you can open it up any time to\nquickly see a graphical view of what’s deployed in your cluster after you create or mod-\nify objects through \nkubectl.\nACCESSING THE DASHBOARD WHEN RUNNING KUBERNETES IN GKE\nIf  you’re  using  Google  Kubernetes  Engine,  you  can  find  out  the  URL  of  the  dash-\nboard through the \nkubectl cluster-info command, which we already introduced:\n$ kubectl cluster-info | grep dashboard\nkubernetes-dashboard is running at https://104.155.108.191/api/v1/proxy/\n➥\n namespaces/kube-system/services/kubernetes-dashboard\nListing 2.18   Describing a pod with kubectl describe\nHere’s the node the pod \nhas been scheduled to.\n \n\n53Summary\nIf  you  open  this  URL  in  a  browser,  you’re  presented  with  a  username  and  password\nprompt. You’ll find the username and password by running the following command:\n$ gcloud container clusters describe kubia | grep -E \"(username|password):\"\n  password: 32nENgreEJ632A12          \n  username: admin                     \nACCESSING THE DASHBOARD WHEN USING MINIKUBE\nTo open the dashboard in your browser when using Minikube to run your Kubernetes\ncluster, run the following command:\n$ minikube dashboard\nThe dashboard will open in your default browser. Unlike with GKE, you won’t need to\nenter any credentials to access it.\n2.4Summary\nHopefully, this initial hands-on chapter has shown you that Kubernetes isn’t a compli-\ncated platform to use, and you’re ready to learn in depth about all the things it can\nprovide. After reading this chapter, you should now know how to\nPull and run any publicly available container image\nPackage your apps into container images and make them available to anyone by\npushing the images to a remote image registry\nFigure 2.9   Screenshot of the Kubernetes web-based dashboard\nThe username and password \nfor the dashboard\n \n\n54CHAPTER 2First steps with Docker and Kubernetes\nEnter a running container and inspect its environment\nSet up a multi-node Kubernetes cluster on Google Kubernetes Engine\nConfigure an alias and tab completion for the kubectl command-line tool\nList and inspect Nodes, Pods, Services, and ReplicationControllers in a Kuber-\nnetes cluster\nRun a container in Kubernetes and make it accessible from outside the cluster\nHave a basic sense of how Pods, ReplicationControllers, and Services relate to\none another\nScale an app horizontally by changing the ReplicationController’s replica count\nAccess the web-based Kubernetes dashboard on both Minikube and GKE \n \n\n55\nPods: running\ncontainers in Kubernetes\nThe  previous  chapter  should  have  given  you  a  rough  picture  of  the  basic  compo-\nnents you create in Kubernetes and at least an outline of what they do. Now, we’ll\nstart  reviewing  all  types  of  Kubernetes  objects  (or  resources)  in  greater  detail,  so\nyou’ll understand when, how, and why to use each of them. We’ll start with pods,\nbecause  they’re  the  central,  most  important,  concept  in  Kubernetes.  Everything\nelse either manages, exposes, or is used by pods. \nThis chapter covers\nCreating, running, and stopping pods\nOrganizing pods and other resources with labels\nPerforming an operation on all pods with a \nspecific label\nUsing namespaces to split pods into non-\noverlapping groups\nScheduling pods onto specific types of worker \nnodes\n \n\n56CHAPTER 3Pods: running containers in Kubernetes\n3.1Introducing pods\nYou’ve already learned that a pod is a co-located group of containers and represents\nthe basic building block in Kubernetes. Instead of deploying containers individually,\nyou always deploy and operate on a pod of containers. We’re not implying that a pod\nalways includes more than one container—it’s common for pods to contain only a sin-\ngle container. The key thing about pods is that when a pod does contain multiple con-\ntainers,  all  of  them  are  always  run  on  a  single  worker  node—it  never  spans  multiple\nworker nodes, as shown in figure 3.1.\n3.1.1Understanding why we need pods\nBut why do we even need pods? Why can’t we use containers directly? Why would we\neven need to run multiple containers together? Can’t we put all our processes into a\nsingle container? We’ll answer those questions now.\nUNDERSTANDING WHY MULTIPLE CONTAINERS ARE BETTER THAN ONE CONTAINER RUNNING \nMULTIPLE PROCESSES\nImagine  an  app  consisting  of  multiple  processes  that  either  communicate  through\nIPC  (Inter-Process  Communication)  or  through  locally  stored  files,  which  requires\nthem to run on the same machine. Because in Kubernetes you always run processes in\ncontainers  and  each  container  is  much  like  an  isolated  machine,  you  may  think  it\nmakes sense to run multiple processes in a single container, but you shouldn’t do that. \n Containers are designed to run only a single process per container (unless the\nprocess itself spawns child processes). If you run multiple unrelated processes in a\nsingle container, it is your responsibility to keep all those processes running, man-\nage their logs, and so on. For example, you’d have to include a mechanism for auto-\nmatically restarting individual processes if they crash. Also, all those processes would\nlog to the same standard output, so you’d have a hard time figuring out what pro-\ncess logged what. \nNode 1\nPod 2\nIP: 10.1.0.2\nContainer 1\nContainer 2\nPod 1\nIP: 10.1.0.1\nContainer\nNode 2\nPod 4\nIP: 10.1.1.2\nContainer 1\nPod 5\nIP: 10.1.1.3\nContainer 1\nContainer 2\nPod 3\nContainer 1Container 2\nFigure 3.1   All containers of a pod run on the same node. A pod never spans two nodes.\n \n\n57Introducing pods\n Therefore, you need to run each process in its own container. That’s how Docker\nand Kubernetes are meant to be used. \n3.1.2Understanding pods\nBecause you’re not supposed to group multiple processes into a single container, it’s\nobvious you need another higher-level construct that will allow you to bind containers\ntogether and manage them as a single unit. This is the reasoning behind pods. \n A pod of containers allows you to run closely related processes together and pro-\nvide them with (almost) the same environment as if they were all running in a single\ncontainer, while keeping them somewhat isolated. This way, you get the best of both\nworlds.  You  can  take  advantage  of  all  the  features  containers  provide,  while  at  the\nsame time giving the processes the illusion of running together. \nUNDERSTANDING THE PARTIAL ISOLATION BETWEEN CONTAINERS OF THE SAME POD\nIn  the  previous  chapter,  you  learned  that  containers  are  completely  isolated  from\neach other, but now you see that you want to isolate groups of containers instead of\nindividual  ones.  You  want  containers  inside  each  group  to  share  certain  resources,\nalthough not all, so that they’re not fully isolated. Kubernetes achieves this by config-\nuring Docker to have all containers of a pod share the same set of Linux namespaces\ninstead of each container having its own set. \n Because all containers of a pod run under the same Network and UTS namespaces\n(we’re  talking  about  Linux  namespaces  here),  they  all  share  the  same  hostname  and\nnetwork interfaces. Similarly, all containers of a pod run under the same IPC namespace\nand can communicate through IPC. In the latest Kubernetes and Docker versions, they\ncan also share the same PID namespace, but that feature isn’t enabled by default. \nNOTEWhen  containers  of  the  same  pod  use  separate  PID  namespaces,  you\nonly see the container’s own processes when running \nps aux in the container.\nBut when it comes to the filesystem, things are a little different. Because most of the\ncontainer’s  filesystem  comes  from  the  container  image,  by  default,  the  filesystem  of\neach container is fully isolated from other containers. However, it’s possible to have\nthem  share  file  directories  using  a  Kubernetes  concept  called  a  Volume,  which  we’ll\ntalk about in chapter 6.\nUNDERSTANDING HOW CONTAINERS SHARE THE SAME IP AND PORT SPACE\nOne thing to stress here is that because containers in a pod run in the same Network\nnamespace, they share the same IP address and port space. This means processes run-\nning  in  containers  of  the  same  pod  need  to  take  care  not  to  bind  to  the  same  port\nnumbers  or  they’ll  run  into  port  conflicts.  But  this  only  concerns  containers  in  the\nsame  pod.  Containers  of  different  pods  can  never  run  into  port  conflicts,  because\neach  pod  has  a  separate  port  space.  All  the  containers  in  a  pod  also  have  the  same\nloopback network interface, so a container can communicate with other containers in\nthe same pod through localhost.\n \n\n58CHAPTER 3Pods: running containers in Kubernetes\nINTRODUCING THE FLAT INTER-POD NETWORK\nAll pods in a Kubernetes cluster reside in a single flat, shared, network-address space\n(shown in figure 3.2), which means every pod can access every other pod at the other\npod’s IP address. No NAT (Network Address Translation) gateways exist between them.\nWhen two pods send network packets between each other, they’ll each see the actual\nIP address of the other as the source IP in the packet.\nConsequently, communication between pods is always simple. It doesn’t matter if two\npods  are  scheduled  onto  a  single  or  onto  different  worker  nodes;  in  both  cases  the\ncontainers  inside  those  pods  can  communicate  with  each  other  across  the  flat  NAT-\nless network, much like computers on a local area network (LAN), regardless of the\nactual inter-node network topology. Like a computer on a LAN, each pod gets its own\nIP address and is accessible from all other pods through this network established spe-\ncifically for pods. This is usually achieved through an additional software-defined net-\nwork layered on top of the actual network.\n To sum up what’s been covered in this section: pods are logical hosts and behave\nmuch like physical hosts or VMs in the non-container world. Processes running in the\nsame pod are like processes running on the same physical or virtual machine, except\nthat each process is encapsulated in a container. \n3.1.3Organizing containers across pods properly\nYou should think of pods as separate machines, but where each one hosts only a cer-\ntain app. Unlike the old days, when we used to cram all sorts of apps onto the same\nhost, we don’t do that with pods. Because pods are relatively lightweight, you can have\nas many as you need without incurring almost any overhead. Instead of stuffing every-\nthing into a single pod, you should organize apps into multiple pods, where each one\ncontains only tightly related components or processes.\nNode 1\nPod A\nIP: 10.1.1.6\nContainer 1\nContainer 2\nPod B\nIP: 10.1.1.7\nContainer 1\nContainer 2\nNode 2\nFlat network\nPod C\nIP: 10.1.2.5\nContainer 1\nContainer 2\nPod D\nIP: 10.1.2.7\nContainer 1\nContainer 2\nFigure 3.2   Each pod gets a routable IP address and all other pods see the pod under \nthat IP address.\n \n\n59Introducing pods\n  Having  said  that,  do  you  think  a  multi-tier  application  consisting  of  a  frontend\napplication server and a backend database should be configured as a single pod or as\ntwo pods?\nSPLITTING MULTI-TIER APPS INTO MULTIPLE PODS\nAlthough  nothing  is  stopping  you  from  running  both  the  frontend  server  and  the\ndatabase in a single pod with two containers, it isn’t the most appropriate way. We’ve\nsaid that all containers of the same pod always run co-located, but do the web server\nand the database really need to run on the same machine? The answer is obviously no,\nso you don’t want to put them into a single pod. But is it wrong to do so regardless? In\na way, it is.\n  If  both  the  frontend  and  backend  are  in  the  same  pod,  then  both  will  always  be\nrun on the same machine. If you have a two-node Kubernetes cluster and only this sin-\ngle  pod,  you’ll  only  be  using  a  single  worker  node  and  not  taking  advantage  of  the\ncomputational resources (CPU and memory) you have at your disposal on the second\nnode. Splitting the pod into two would allow Kubernetes to schedule the frontend to\none  node  and  the  backend  to  the  other  node,  thereby  improving  the  utilization  of\nyour infrastructure.\nSPLITTING INTO MULTIPLE PODS TO ENABLE INDIVIDUAL SCALING\nAnother reason why you shouldn’t put them both into a single pod is scaling. A pod is\nalso  the  basic  unit  of  scaling.  Kubernetes  can’t  horizontally  scale  individual  contain-\ners; instead, it scales whole pods. If your pod consists of a frontend and a backend con-\ntainer, when you scale up the number of instances of the pod to, let’s say, two, you end\nup with two frontend containers and two backend containers. \n  Usually,  frontend  components  have  completely  different  scaling  requirements\nthan the backends, so we tend to scale them individually. Not to mention the fact that\nbackends such as databases are usually much harder to scale compared to (stateless)\nfrontend web servers. If you need to scale a container individually, this is a clear indi-\ncation that it needs to be deployed in a separate pod. \nUNDERSTANDING WHEN TO USE MULTIPLE CONTAINERS IN A POD\nThe main reason to put multiple containers into a single pod is when the application\nconsists of one main process and one or more complementary processes, as shown in\nfigure 3.3.\nPod\nMain container\nSupporting\ncontainer 1\nSupporting\ncontainer 2\nVolume\nFigure 3.3   Pods should contain tightly coupled \ncontainers, usually a main container and containers \nthat support the main one.\n \n\n60CHAPTER 3Pods: running containers in Kubernetes\nFor example, the main container in a pod could be a web server that serves files from\na  certain  file  directory,  while  an  additional  container  (a  sidecar  container)  periodi-\ncally  downloads  content  from  an  external  source  and  stores  it  in  the  web  server’s\ndirectory. In chapter 6 you’ll see that you need to use a Kubernetes Volume that you\nmount into both containers. \n Other examples of sidecar containers include log rotators and collectors, data pro-\ncessors, communication adapters, and others.\nDECIDING WHEN TO USE MULTIPLE CONTAINERS IN A POD\nTo  recap  how  containers  should  be  grouped  into  pods—when  deciding  whether  to\nput two containers into a single pod or into two separate pods, you always need to ask\nyourself the following questions:\nDo they need to be run together or can they run on different hosts?\nDo they represent a single whole or are they independent components?\nMust they be scaled together or individually? \nBasically,  you  should  always  gravitate  toward  running  containers  in  separate  pods,\nunless a specific reason requires them to be part of the same pod. Figure 3.4 will help\nyou memorize this.\nAlthough pods can contain multiple containers, to keep things simple for now, you’ll\nonly  be  dealing  with  single-container  pods  in  this  chapter.  You’ll  see  how  multiple\ncontainers are used in the same pod later, in chapter 6. \nPod\nFrontend\nprocess\nBackend\nprocess\nContainer\nPod\nFrontend\nprocess\nFrontend\ncontainer\nFrontend pod\nFrontend\nprocess\nFrontend\ncontainer\nBackend pod\nBackend\nprocess\nBackend\ncontainer\nBackend\nprocess\nBackend\ncontainer\nFigure 3.4   A container shouldn’t run multiple processes. A pod shouldn’t contain multiple \ncontainers if they don’t need to run on the same machine.\n \n\n61Creating pods from YAML or JSON descriptors\n3.2Creating pods from YAML or JSON descriptors\nPods and other Kubernetes resources are usually created by posting a JSON or YAML\nmanifest to the Kubernetes REST API endpoint. Also, you can use other, simpler ways\nof  creating  resources,  such  as  the  \nkubectl run  command  you  used  in  the  previous\nchapter,  but  they  usually  only  allow  you  to configure a limited set of properties, not\nall. Additionally, defining all your Kubernetes objects from YAML files makes it possi-\nble to store them in a version control system, with all the benefits it brings.\n To configure all aspects of each type of resource, you’ll need to know and under-\nstand the Kubernetes API object definitions. You’ll get to know most of them as you\nlearn about each resource type throughout this book. We won’t explain every single\nproperty, so you should also refer to the Kubernetes API reference documentation at\nhttp://kubernetes.io/docs/reference/ when creating objects.\n3.2.1Examining a YAML descriptor of an existing pod\nYou already have some existing pods you created in the previous chapter, so let’s look\nat what a YAML definition for one of those pods looks like. You’ll use the \nkubectl get\ncommand  with  the  -o yaml  option  to  get  the  whole  YAML  definition  of  the  pod,  as\nshown in the following listing.\n$ kubectl get po kubia-zxzij -o yaml\napiVersion: v1                         \nkind: Pod                                       \nmetadata:                                                 \n  annotations:                                            \n    kubernetes.io/created-by: ...                         \n  creationTimestamp: 2016-03-18T12:37:50Z                 \n  generateName: kubia-                                    \n  labels:                                                 \n    run: kubia                                            \n  name: kubia-zxzij                                       \n  namespace: default                                      \n  resourceVersion: \"294\"                                  \n  selfLink: /api/v1/namespaces/default/pods/kubia-zxzij   \n  uid: 3a564dc0-ed06-11e5-ba3b-42010af00004               \nspec:                                                   \n  containers:                                           \n  - image: luksa/kubia                                  \n    imagePullPolicy: IfNotPresent                       \n    name: kubia                                         \n    ports:                                              \n    - containerPort: 8080                               \n      protocol: TCP                                     \n    resources:                                          \n      requests:                                         \n        cpu: 100m                                       \nListing 3.1   Full YAML of a deployed pod\nKubernetes API version used \nin this YAML descriptor\nType of Kubernetes \nobject/resource\nPod metadata (name, \nlabels, annotations, \nand so on)\nPod specification/\ncontents (list of \npod’s containers, \nvolumes, and so on)\n \n\n62CHAPTER 3Pods: running containers in Kubernetes\n    terminationMessagePath: /dev/termination-log      \n    volumeMounts:                                     \n    - mountPath: /var/run/secrets/k8s.io/servacc      \n      name: default-token-kvcqa                       \n      readOnly: true                                  \n  dnsPolicy: ClusterFirst                             \n  nodeName: gke-kubia-e8fe08b8-node-txje              \n  restartPolicy: Always                               \n  serviceAccount: default                             \n  serviceAccountName: default                         \n  terminationGracePeriodSeconds: 30                   \n  volumes:                                            \n  - name: default-token-kvcqa                         \n    secret:                                           \n      secretName: default-token-kvcqa                 \nstatus:                                                   \n  conditions:                                             \n  - lastProbeTime: null                                   \n    lastTransitionTime: null                              \n    status: \"True\"                                        \n    type: Ready                                           \n  containerStatuses:                                      \n  - containerID: docker://f0276994322d247ba...            \n    image: luksa/kubia                                    \n    imageID: docker://4c325bcc6b40c110226b89fe...         \n    lastState: {}                                         \n    name: kubia                                           \n    ready: true                                           \n    restartCount: 0                                       \n    state:                                                \n      running:                                            \n        startedAt: 2016-03-18T12:46:05Z                   \n  hostIP: 10.132.0.4                                      \n  phase: Running                                          \n  podIP: 10.0.2.3                                         \n  startTime: 2016-03-18T12:44:32Z                         \nI know this looks complicated, but it becomes simple once you understand the basics\nand know how to distinguish between the important parts and the minor details. Also,\nyou can take comfort in the fact that when creating a new pod, the YAML you need to\nwrite is much shorter, as you’ll see later.\nINTRODUCING THE MAIN PARTS OF A POD DEFINITION\nThe  pod  definition  consists  of  a  few  parts.  First,  there’s  the  Kubernetes  API  version\nused in the YAML and the type of resource the YAML is describing. Then, three\nimportant sections are found in almost all Kubernetes resources:\nMetadata  includes  the  name,  namespace,  labels,  and  other  information  about\nthe pod.\nSpec contains the actual description of the pod’s contents, such as the pod’s con-\ntainers, volumes, and other data.\nPod specification/\ncontents (list of \npod’s containers, \nvolumes, and so on)\nDetailed status \nof the pod and \nits containers\n \n\n63Creating pods from YAML or JSON descriptors\nStatus  contains  the  current  information  about  the  running  pod,  such  as  what\ncondition  the  pod  is  in,  the  description  and  status  of  each  container,  and  the\npod’s internal IP and other basic info.\nListing 3.1 showed a full description of a running pod, including its status. The \nstatus\npart  contains  read-only  runtime  data  that  shows  the  state  of  the  resource  at  a  given\nmoment. When creating a new pod, you never need to provide the \nstatus part. \n  The  three  parts  described  previously  show  the  typical  structure  of  a  Kubernetes\nAPI  object.  As  you’ll  see  throughout  the  book,  all  other  objects  have  the  same  anat-\nomy. This makes understanding new objects relatively easy.\n Going through all the individual properties in the previous YAML doesn’t make\nmuch sense, so, instead, let’s see what the most basic YAML for creating a pod looks\nlike. \n3.2.2Creating a simple YAML descriptor for a pod\nYou’re  going  to  create  a  file  called  kubia-manual.yaml  (you  can  create  it  in  any\ndirectory  you  want),  or  download  the  book’s  code  archive,  where  you’ll  find  the\nfile inside the Chapter03 directory. The following listing shows the entire contents\nof the file.\napiVersion: v1         \nkind: Pod                             \nmetadata:     \n  name: kubia-manual         \nspec: \n  containers: \n  - image: luksa/kubia          \n    name: kubia         \n    ports: \n    - containerPort: 8080     \n      protocol: TCP\nI’m sure you’ll agree this is much simpler than the definition in listing 3.1. Let’s exam-\nine this descriptor in detail. It conforms to the \nv1 version of the Kubernetes API. The\ntype  of  resource  you’re  describing  is  a  \npod,  with  the  name  kubia-manual.  The  pod\nconsists  of  a  single  container  based  on  the  \nluksa/kubia  image.  You’ve  also  given  a\nname to the container and indicated that it’s listening on port \n8080. \nSPECIFYING CONTAINER PORTS\nSpecifying  ports  in  the  pod  definition  is  purely  informational.  Omitting  them  has  no\neffect  on  whether  clients  can  connect  to  the  pod  through  the  port  or  not.  If  the  con-\nListing 3.2   A basic pod manifest: kubia-manual.yaml\nDescriptor conforms\nto version v1 of\nKubernetes API\nYou’re \ndescribing a pod.\nThe name \nof the pod\nContainer image to create \nthe container from\nName of the container\nThe port the app \nis listening on\n \n\n64CHAPTER 3Pods: running containers in Kubernetes\ntainer  is  accepting  connections  through  a  port  bound  to  the  0.0.0.0  address,  other\npods can always connect to it, even if the port isn’t listed in the pod spec explicitly. But\nit  makes  sense  to  define  the  ports  explicitly  so  that  everyone  using  your  cluster  can\nquickly  see  what  ports  each  pod  exposes.  Explicitly  defining  ports  also  allows  you  to\nassign a name to each port, which can come in handy, as you’ll see later in the book.\nUsing kubectl explain to discover possible API object fields\nWhen  preparing  a  manifest,  you  can  either  turn  to  the  Kubernetes  reference\ndocumentation  at  http://kubernetes.io/docs/api  to  see  which  attributes  are\nsupported by each API object, or you can use the \nkubectl explain command.\nFor  example,  when  creating  a  pod  manifest  from  scratch,  you  can  start  by  asking\nkubectl to explain pods:\n$ kubectl explain pods\nDESCRIPTION:\nPod is a collection of containers that can run on a host. This resource \nis created by clients and scheduled onto hosts.\nFIELDS:\n   kind      <string>\n     Kind is a string value representing the REST resource this object\n     represents...\n   metadata  <Object>\n     Standard object's metadata...\n   spec      <Object>\n     Specification of the desired behavior of the pod...\n   status    <Object>\n     Most recently observed status of the pod. This data may not be up to\n     date...\nKubectl  prints  out  the  explanation  of  the  object  and  lists  the  attributes  the  object\ncan  contain.  You  can  then  drill  deeper  to  find  out  more  about  each  attribute.  For\nexample, you can examine the \nspec attribute like this:\n$ kubectl explain pod.spec\nRESOURCE: spec <Object>\nDESCRIPTION:\n    Specification of the desired behavior of the pod...\n    podSpec is a description of a pod.\nFIELDS:\n   hostPID   <boolean>\n     Use the host's pid namespace. Optional: Default to false.\n   ...\n   volumes   <[]Object>\n     List of volumes that can be mounted by containers belonging to the\n     pod.\n \n\n65Creating pods from YAML or JSON descriptors\n3.2.3Using kubectl create to create the pod\nTo create the pod from your YAML file, use the kubectl create command:\n$ kubectl create -f kubia-manual.yaml\npod \"kubia-manual\" created\nThe kubectl create -f command is used for creating any resource (not only pods)\nfrom a YAML or JSON file. \nRETRIEVING THE WHOLE DEFINITION OF A RUNNING POD\nAfter creating the pod, you can ask Kubernetes for the full YAML of the pod. You’ll\nsee  it’s  similar  to  the  YAML  you  saw  earlier.  You’ll  learn  about  the  additional  fields\nappearing in the returned definition in the next sections. Go ahead and use the fol-\nlowing command to see the full descriptor of the pod:\n$ kubectl get po kubia-manual -o yaml\nIf you’re more into JSON, you can also tell kubectl to return JSON instead of YAML\nlike this (this works even if you used YAML to create the pod):\n$ kubectl get po kubia-manual -o json\nSEEING YOUR NEWLY CREATED POD IN THE LIST OF PODS\nYour pod has been created, but how do you know if it’s running? Let’s list pods to see\ntheir statuses:\n$ kubectl get pods\nNAME            READY   STATUS    RESTARTS   AGE\nkubia-manual    1/1     Running   0          32s\nkubia-zxzij     1/1     Running   0          1d    \nThere’s your kubia-manual pod. Its status shows that it’s running. If you’re like me,\nyou’ll probably want to confirm that’s true by talking to the pod. You’ll do that in a\nminute. First, you’ll look at the app’s log to check for any errors.\n3.2.4Viewing application logs\nYour  little  Node.js  application  logs  to  the  process’s  standard  output.  Containerized\napplications usually log to the standard output and standard error stream instead of\n   Containers  <[]Object> -required-\n     List of containers belonging to the pod. Containers cannot currently\n     Be added or removed. There must be at least one container in a pod.\n     Cannot be updated. More info:\n     http://releases.k8s.io/release-1.4/docs/user-guide/containers.md\n \n\n66CHAPTER 3Pods: running containers in Kubernetes\nwriting their logs to files. This is to allow users to view logs of different applications in\na simple, standard way. \n The container runtime (Docker in your case) redirects those streams to files and\nallows you to get the container’s log by running\n$ docker logs <container id>\nYou could use ssh to log into the node where your pod is running and retrieve its logs\nwith \ndocker logs, but Kubernetes provides an easier way. \nRETRIEVING A POD’S LOG WITH KUBECTL LOGS\nTo see your pod’s log (more precisely, the container’s log) you run the following com-\nmand on your local machine (no need to \nssh anywhere):\n$ kubectl logs kubia-manual\nKubia server starting...\nYou haven’t sent any web requests to your Node.js app, so the log only shows a single\nlog statement about the server starting up. As you can see, retrieving logs of an appli-\ncation  running  in  Kubernetes  is  incredibly  simple  if  the  pod  only  contains  a  single\ncontainer. \nNOTEContainer logs are automatically rotated daily and every time the log file\nreaches 10MB in size. The \nkubectl logs command only shows the log entries\nfrom the last rotation.\nSPECIFYING THE CONTAINER NAME WHEN GETTING LOGS OF A MULTI-CONTAINER POD\nIf your pod includes multiple containers, you have to explicitly specify the container\nname by including the \n-c <container name> option when running kubectl logs. In\nyour \nkubia-manual pod, you set the container’s name to kubia, so if additional con-\ntainers exist in the pod, you’d have to get its logs like this:\n$ kubectl logs kubia-manual -c kubia\nKubia server starting...\nNote that you can only retrieve container logs of pods that are still in existence. When\na pod is deleted, its logs are also deleted. To make a pod’s logs available even after the\npod is deleted, you need to set up centralized, cluster-wide logging, which stores all\nthe logs into a central store. Chapter 17 explains how centralized logging works.\n3.2.5Sending requests to the pod\nThe pod is now running—at least that’s what kubectl get and your app’s log say. But\nhow  do  you  see  it  in  action?  In  the  previous  chapter,  you  used  the  \nkubectl expose\ncommand to create a service to gain access to the pod externally. You’re not going to\ndo that now, because a whole chapter is dedicated to services, and you have other ways\nof connecting to a pod for testing and debugging purposes. One of them is through\nport forwarding.\n \n\n67Organizing pods with labels\nFORWARDING A LOCAL NETWORK PORT TO A PORT IN THE POD\nWhen you want to talk to a specific pod without going through a service (for debug-\nging or other reasons), Kubernetes allows you to configure port forwarding to the\npod.  This  is  done  through  the  \nkubectl port-forward  command.  The  following\ncommand will forward your machine’s local port \n8888 to port 8080 of your kubia-\nmanual\n pod:\n$ kubectl port-forward kubia-manual 8888:8080\n... Forwarding from 127.0.0.1:8888 -> 8080\n... Forwarding from [::1]:8888 -> 8080\nThe port forwarder is running and you can now connect to your pod through the\nlocal port. \nCONNECTING TO THE POD THROUGH THE PORT FORWARDER\nIn a different terminal, you can now use curl to send an HTTP request to your pod\nthrough the \nkubectl port-forward proxy running on localhost:8888:\n$ curl localhost:8888\nYou’ve hit kubia-manual\nFigure 3.5 shows an overly simplified view of what happens when you send the request.\nIn reality, a couple of additional components sit between the \nkubectl process and the\npod, but they aren’t relevant right now.\nUsing  port  forwarding  like  this  is  an  effective  way  to  test  an  individual  pod.  You’ll\nlearn about other similar methods throughout the book. \n3.3Organizing pods with labels\nAt  this  point,  you  have  two  pods  running  in  your  cluster.  When  deploying  actual\napplications,  most  users  will  end  up  running  many  more  pods.  As  the  number  of\npods  increases,  the  need  for  categorizing  them  into  subsets  becomes  more  and\nmore evident.\n For example, with microservices architectures, the number of deployed microser-\nvices  can  easily  exceed  20  or  more.  Those  components  will  probably  be  replicated\nKubernetes cluster\nPort\n8080\nLocal machine\nkubectl\nport-forward\nprocess\ncurl\nPort\n8888\nPod:\nkubia-manual\nFigure 3.5   A simplified view of what happens when you use curl with kubectl port-forward\n \n\n68CHAPTER 3Pods: running containers in Kubernetes\n(multiple  copies  of  the  same  component  will  be  deployed)  and  multiple  versions  or\nreleases (stable, beta, canary, and so on) will run concurrently. This can lead to hun-\ndreds of pods in the system. Without a mechanism for organizing them, you end up\nwith  a  big,  incomprehensible  mess,  such  as  the  one  shown  in  figure  3.6.  The  figure\nshows pods of multiple microservices, with several running multiple replicas, and others\nrunning different releases of the same microservice.\nIt’s evident you need a way of organizing them into smaller groups based on arbitrary\ncriteria, so every developer and system administrator dealing with your system can eas-\nily see which pod is which. And you’ll want to operate on every pod belonging to a cer-\ntain group with a single action instead of having to perform the action for each pod\nindividually. \n Organizing pods and all other Kubernetes objects is done through labels.\n3.3.1Introducing labels\nLabels  are  a  simple,  yet  incredibly  powerful,  Kubernetes  feature  for  organizing  not\nonly pods, but all other Kubernetes resources. A label is an arbitrary key-value pair you\nattach to a resource, which is then utilized when selecting resources using label selectors\n(resources are filtered based on whether they include the label specified in the selec-\ntor). A resource can have more than one label, as long as the keys of those labels are\nunique  within  that  resource.  You  usually  attach  labels  to  resources  when  you  create\nthem,  but  you  can  also  add  additional  labels  or  even  modify  the  values  of  existing\nlabels later without having to recreate the resource. \nUI pod\nUI pod\nUI pod\nAccount\nService\npod\nProduct\nCatalog\npod\nProduct\nCatalog\npod\nProduct\nCatalog\npod\nShopping\nCart\npod\nShopping\nCart\npod\nOrder\nService\npod\nUI pod\nUI pod\nProduct\nCatalog\npod\nProduct\nCatalog\npod\nOrder\nService\npod\nAccount\nService\npod\nProduct\nCatalog\npod\nProduct\nCatalog\npod\nOrder\nService\npod\nFigure 3.6   Uncategorized pods in a microservices architecture\n \n\n69Organizing pods with labels\n Let’s turn back to the microservices example from figure 3.6. By adding labels to\nthose  pods,  you  get  a  much-better-organized  system  that  everyone  can  easily  make\nsense of. Each pod is labeled with two labels:\napp, which specifies which app, component, or microservice the pod belongs to. \nrel, which shows whether the application running in the pod is a stable, beta,\nor a canary release.\nDEFINITIONA canary release is when you deploy a new version of an applica-\ntion  next  to  the  stable  version,  and  only  let  a  small  fraction  of  users  hit  the\nnew version to see how it behaves before rolling it out to all users. This pre-\nvents bad releases from being exposed to too many users.\nBy  adding  these  two  labels,  you’ve  essentially  organized  your  pods  into  two  dimen-\nsions (horizontally by app and vertically by release), as shown in figure 3.7.\nEvery developer or ops person with access to  your  cluster  can  now  easily  see  the  sys-\ntem’s structure and where each pod fits in by looking at the pod’s labels.\n3.3.2Specifying labels when creating a pod\nNow, you’ll see labels in action by creating a new pod with two labels. Create a new file\ncalled kubia-manual-with-labels.yaml with the contents of the following listing.\napiVersion: v1                                         \nkind: Pod                                              \nmetadata:                                              \n  name: kubia-manual-v2\nListing 3.3   A pod with labels: kubia-manual-with-labels.yaml\nUI pod\napp: ui\nrel: stable\nrel=stable\napp=ui\nAccount\nService\npod\napp: as\nrel: stable\napp=as\napp: pc\nrel: stable\napp=pc\napp: sc\nrel: stable\napp=sc\napp: os\nrel: stable\napp=os\nProduct\nCatalog\npod\nShopping\nCart\npod\nOrder\nService\npod\nUI pod\napp: ui\nrel: beta\nrel=beta\napp: pc\nrel: beta\napp: os\nrel: beta\nProduct\nCatalog\npod\nOrder\nService\npod\nrel=canary\nAccount\nService\npod\napp: as\nrel: canary\napp: pc\nrel: canary\napp: os\nrel: canary\nProduct\nCatalog\npod\nOrder\nService\npod\nFigure 3.7   Organizing pods in a microservices architecture with pod labels\n \n\n70CHAPTER 3Pods: running containers in Kubernetes\n  labels:    \n    creation_method: manual          \n    env: prod                        \nspec: \n  containers: \n  - image: luksa/kubia\n    name: kubia\n    ports: \n    - containerPort: 8080\n      protocol: TCP\nYou’ve  included  the  labels  creation_method=manual  and  env=data.labels  section.\nYou’ll create this pod now:\n$ kubectl create -f kubia-manual-with-labels.yaml\npod \"kubia-manual-v2\" created\nThe kubectl get pods  command  doesn’t  list  any  labels  by  default,  but  you  can  see\nthem by using the \n--show-labels switch:\n$ kubectl get po --show-labels\nNAME            READY  STATUS   RESTARTS  AGE LABELS\nkubia-manual    1/1    Running  0         16m <none>\nkubia-manual-v2 1/1    Running  0         2m  creat_method=manual,env=prod\nkubia-zxzij     1/1    Running  0         1d  run=kubia\nInstead of listing all labels, if you’re only interested in certain labels, you can specify\nthem with the \n-L switch and have each displayed in its own column. List pods again\nand show the columns for the two labels you’ve attached to your \nkubia-manual-v2 pod:\n$ kubectl get po -L creation_method,env\nNAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV\nkubia-manual    1/1     Running   0          16m   <none>            <none>\nkubia-manual-v2 1/1     Running   0          2m    manual            prod\nkubia-zxzij     1/1     Running   0          1d    <none>            <none>\n3.3.3Modifying labels of existing pods\nLabels can also be added to and modified on existing pods. Because the kubia-man-\nual\n pod was also created manually, let’s add the creation_method=manual label to it: \n$ kubectl label po kubia-manual creation_method=manual\npod \"kubia-manual\" labeled\nNow, let’s also change the env=prod label to env=debug on the kubia-manual-v2 pod,\nto see how existing labels can be changed.\nNOTEYou need to use the --overwrite option when changing existing labels.\n$ kubectl label po kubia-manual-v2 env=debug --overwrite\npod \"kubia-manual-v2\" labeled\nTwo labels are \nattached to the pod.\n \n\n71Listing subsets of pods through label selectors\nList the pods again to see the updated labels:\n$ kubectl get po -L creation_method,env\nNAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV\nkubia-manual    1/1     Running   0          16m   manual            <none>\nkubia-manual-v2 1/1     Running   0          2m    manual            debug\nkubia-zxzij     1/1     Running   0          1d    <none>            <none>\nAs  you  can  see,  attaching  labels  to  resources  is  trivial,  and  so  is  changing  them  on\nexisting resources. It may not be evident right now, but this is an incredibly powerful\nfeature, as you’ll see in the next chapter. But first, let’s see what you can do with these\nlabels, in addition to displaying them when listing pods.\n3.4Listing subsets of pods through label selectors\nAttaching labels to resources so you can see the labels next to each resource when list-\ning  them  isn’t  that  interesting.  But  labels  go  hand  in  hand  with  label  selectors.  Label\nselectors allow you to select a subset of pods tagged with certain labels and perform an\noperation on those pods. A label selector is a criterion, which filters resources based\non whether they include a certain label with a certain value. \n A label selector can select resources based on whether the resource\nContains (or doesn’t contain) a label with a certain key\nContains a label with a certain key and value\nContains  a  label  with  a  certain  key,  but  with  a  value  not  equal  to  the  one  you\nspecify\n3.4.1Listing pods using a label selector\nLet’s use label selectors on the pods you’ve created so far. To see all pods you created\nmanually (you labeled them with \ncreation_method=manual), do the following:\n$ kubectl get po -l creation_method=manual\nNAME              READY     STATUS    RESTARTS   AGE\nkubia-manual      1/1       Running   0          51m\nkubia-manual-v2   1/1       Running   0          37m\nTo list all pods that include the env label, whatever its value is:\n$ kubectl get po -l env\nNAME              READY     STATUS    RESTARTS   AGE\nkubia-manual-v2   1/1       Running   0          37m\nAnd those that don’t have the env label:\n$ kubectl get po -l '!env'\nNAME           READY     STATUS    RESTARTS   AGE\nkubia-manual   1/1       Running   0          51m\nkubia-zxzij    1/1       Running   0          10d\n \n\n72CHAPTER 3Pods: running containers in Kubernetes\nNOTEMake sure to use single quotes around !env, so the bash shell doesn’t\nevaluate the exclamation mark.\nSimilarly, you could also match pods with the following label selectors:\ncreation_method!=manual to select pods with the creation_method label with\nany value other than \nmanual\nenv in (prod,devel) to select pods with the env  label  set  to  either  prod  or\ndevel\nenv notin (prod,devel) to select pods with the env label set to any value other\nthan \nprod or devel\nTurning  back  to  the  pods  in  the  microservices-oriented  architecture  example,  you\ncould  select  all  pods  that  are  part  of  the  product  catalog  microservice  by  using  the\napp=pc label selector (shown in the following figure).\n3.4.2Using multiple conditions in a label selector\nA  selector  can  also  include  multiple  comma-separated  criteria.  Resources  need  to\nmatch all of them to match the selector. If, for example, you want to select only pods\nrunning the beta release of the product catalog microservice, you’d use the following\nselector: \napp=pc,rel=beta (visualized in figure 3.9).\n Label selectors aren’t useful only for listing pods, but also for performing actions\non a subset of all pods. For example, later in the chapter, you’ll see how to use label\nselectors  to  delete  multiple  pods  at  once.  But  label  selectors  aren’t  used  only  by\nkubectl. They’re also used internally, as you’ll see next.\nUI pod\napp: ui\nrel: stable\nrel=stable\napp=ui\nAccount\nService\npod\napp: as\nrel: stable\napp=as\napp: pc\nrel: stable\napp=pc\napp: sc\nrel: stable\napp=sc\napp: os\nrel: stable\napp=os\nProduct\nCatalog\npod\nShopping\nCart\npod\nOrder\nService\npod\nUI pod\napp: ui\nrel: beta\nrel=beta\napp: pc\nrel: beta\napp: os\nrel: beta\nProduct\nCatalog\npod\nOrder\nService\npod\nrel=canary\nAccount\nService\npod\napp: as\nrel: canary\napp: pc\nrel: canary\napp: os\nrel: canary\nProduct\nCatalog\npod\nOrder\nService\npod\nFigure 3.8   Selecting the product catalog microservice pods using the “app=pc” label selector\n \n\n73Using labels and selectors to constrain pod scheduling\n3.5Using labels and selectors to constrain pod scheduling\nAll the pods you’ve created so far have been scheduled pretty much randomly across\nyour worker nodes. As I’ve mentioned in the previous chapter, this is the proper way\nof working in a Kubernetes cluster. Because Kubernetes exposes all the nodes in the\ncluster as a single, large deployment platform, it shouldn’t matter to you what node a\npod  is  scheduled  to.  Because  each  pod  gets  the  exact  amount  of  computational\nresources it requests (CPU, memory, and so on) and its accessibility from other pods\nisn’t at all affected by the node the pod is scheduled to, usually there shouldn’t be any\nneed for you to tell Kubernetes exactly where to schedule your pods. \n Certain cases exist, however, where you’ll want to have at least a little say in where\na pod should be scheduled. A good example is when your hardware infrastructure\nisn’t homogenous. If part of your worker nodes have spinning hard drives, whereas\nothers have SSDs, you may want to schedule certain pods to one group of nodes and\nthe rest to the other. Another example is when you need to schedule pods perform-\ning intensive GPU-based computation only to nodes that provide the required GPU\nacceleration. \n You never want to say specifically what node a pod should be scheduled to, because\nthat  would  couple  the  application  to  the  infrastructure,  whereas  the  whole  idea  of\nKubernetes is hiding the actual infrastructure from the apps that run on it. But if you\nwant to have a say in where a pod should be scheduled, instead of specifying an exact\nnode,  you  should  describe  the  node  requirements  and  then  let  Kubernetes  select  a\nnode  that  matches  those  requirements.  This  can  be  done  through  node  labels  and\nnode label selectors. \nUI pod\napp: ui\nrel: stable\nrel=stable\napp=ui\nAccount\nService\npod\napp: as\nrel: stable\napp=as\napp: pc\nrel: stable\napp=pc\napp: sc\nrel: stable\napp=sc\napp: os\nrel: stable\napp=os\nProduct\nCatalog\npod\nShopping\nCart\npod\nOrder\nService\npod\nUI pod\napp: ui\nrel: beta\nrel=beta\napp: pc\nrel: beta\napp: os\nrel: beta\nProduct\nCatalog\npod\nOrder\nService\npod\nrel=canary\nAccount\nService\npod\napp: as\nrel: canary\napp: pc\nrel: canary\napp: os\nrel: canary\nProduct\nCatalog\npod\nOrder\nService\npod\nFigure 3.9   Selecting pods with multiple label selectors\n \n\n74CHAPTER 3Pods: running containers in Kubernetes\n3.5.1Using labels for categorizing worker nodes\nAs  you  learned  earlier,  pods  aren’t  the  only  Kubernetes  resource  type  that  you  can\nattach a label to. Labels can be attached to  any  Kubernetes  object,  including  nodes.\nUsually, when the ops team adds a new node to the cluster, they’ll categorize the node\nby attaching labels specifying the type of hardware the node provides or anything else\nthat may come in handy when scheduling pods. \n Let’s imagine one of the nodes in your cluster contains a GPU meant to be used\nfor general-purpose GPU computing. You want to add a label to the node showing this\nfeature. You’re going to add the label \ngpu=true to one of your nodes (pick one out of\nthe list returned by \nkubectl get nodes):\n$ kubectl label node gke-kubia-85f6-node-0rrx gpu=true\nnode \"gke-kubia-85f6-node-0rrx\" labeled\nNow you can use a label selector when listing the nodes, like you did before with pods.\nList only nodes that include the label \ngpu=true:\n$ kubectl get nodes -l gpu=true\nNAME                      STATUS AGE\ngke-kubia-85f6-node-0rrx  Ready  1d\nAs expected, only one node has this label. You can also try listing all the nodes and tell\nkubectl to display an additional column showing the values of each node’s gpu label\n(\nkubectl get nodes -L gpu).\n3.5.2Scheduling pods to specific nodes\nNow imagine you want to deploy a new pod that needs a GPU to perform its work.\nTo  ask  the  scheduler  to  only  choose  among  the  nodes  that  provide  a  GPU,  you’ll\nadd a node selector to the pod’s YAML. Create a file called kubia-gpu.yaml with the\nfollowing listing’s contents and then use \nkubectl create -f kubia-gpu.yaml to cre-\nate the pod.\napiVersion: v1                                         \nkind: Pod                                              \nmetadata:                                              \n  name: kubia-gpu\nspec: \n  nodeSelector:               \n    gpu: \"true\"               \n  containers: \n  - image: luksa/kubia\n    name: kubia\nListing 3.4   Using a label selector to schedule a pod to a specific node: kubia-gpu.yaml\nnodeSelector tells Kubernetes \nto deploy this pod only to \nnodes containing the \ngpu=true label.\n \n\n75Annotating pods\nYou’ve added a nodeSelector field under the spec section. When you create the pod,\nthe  scheduler  will  only  choose  among  the  nodes  that  contain  the  \ngpu=true  label\n(which is only a single node in your case). \n3.5.3Scheduling to one specific node\nSimilarly, you could also schedule a pod to an exact node, because each node also has\na unique label with the key \nkubernetes.io/hostname and value set to the actual host-\nname of the node. But setting the \nnodeSelector to a specific node by the hostname\nlabel  may  lead  to  the  pod  being  unschedulable  if  the  node  is  offline.  You  shouldn’t\nthink in terms of individual nodes. Always think about logical groups of nodes that sat-\nisfy certain criteria specified through label selectors.\n  This  was  a  quick  demonstration  of  how  labels  and  label  selectors  work  and  how\nthey can be used to influence the operation of Kubernetes. The importance and use-\nfulness of label selectors will become even more evident when we talk about Replication-\nControllers and Services in the next two chapters. \nNOTEAdditional  ways  of  influencing  which  node  a  pod  is  scheduled  to  are\ncovered in chapter 16.\n3.6Annotating pods\nIn addition to labels, pods and other objects can also contain annotations. Annotations\nare also key-value pairs, so in essence, they’re similar to labels, but they aren’t meant to\nhold  identifying  information.  They  can’t  be  used  to  group  objects  the  way  labels  can.\nWhile  objects  can  be  selected  through  label  selectors,  there’s  no  such  thing  as  an\nannotation selector. \n On the other hand, annotations can hold much larger pieces of information and\nare primarily meant to be used by tools. Certain annotations are automatically added\nto objects by Kubernetes, but others are added by users manually.\n Annotations are also commonly used when introducing new features to Kuberne-\ntes. Usually, alpha and beta versions of new features don’t introduce any new fields to\nAPI  objects.  Annotations  are  used  instead  of  fields,  and  then  once  the  required  API\nchanges have become clear and been agreed upon by the Kubernetes developers, new\nfields are introduced and the related annotations deprecated.\n A great use of annotations is adding descriptions for each pod or other API object,\nso that everyone using the cluster can quickly look up information about each individ-\nual  object.  For  example,  an  annotation  used to specify the name of the person who\ncreated the object can make collaboration between everyone working on the cluster\nmuch easier.\n3.6.1Looking up an object’s annotations\nLet’s  see  an  example  of  an  annotation  that  Kubernetes  added  automatically  to  the\npod  you  created  in  the  previous  chapter.  To  see  the  annotations,  you’ll  need  to\n \n\n76CHAPTER 3Pods: running containers in Kubernetes\nrequest the full YAML of the pod or use the kubectl describe command. You’ll use the\nfirst option in the following listing.\n$ kubectl get po kubia-zxzij -o yaml\napiVersion: v1\nkind: pod\nmetadata:\n  annotations:\n    kubernetes.io/created-by: |\n      {\"kind\":\"SerializedReference\", \"apiVersion\":\"v1\", \n      \"reference\":{\"kind\":\"ReplicationController\", \"namespace\":\"default\", ...\nWithout going into too many details, as you can see, the kubernetes.io/created-by\nannotation holds JSON data about the object that created the pod. That’s not some-\nthing you’d want to put into a label. Labels should be short, whereas annotations can\ncontain relatively large blobs of data (up to 256 KB in total).\nNOTEThe kubernetes.io/created-by  annotations  was  deprecated  in  ver-\nsion 1.8 and will be removed in 1.9, so you will no longer see it in the YAML.\n3.6.2Adding and modifying annotations\nAnnotations can obviously be added to pods at creation time, the same way labels can.\nThey can also be added to or modified on existing pods later. The simplest way to add\nan annotation to an existing object is through the \nkubectl annotate command. \n You’ll try adding an annotation to your \nkubia-manual pod now:\n$ kubectl annotate pod kubia-manual mycompany.com/someannotation=\"foo bar\"\npod \"kubia-manual\" annotated\nYou  added  the  annotation  mycompany.com/someannotation  with  the  value  foo bar.\nIt’s a good idea to use this format for annotation keys to prevent key collisions. When\ndifferent tools or libraries add annotations to objects, they may accidentally override\neach other’s annotations if they don’t use unique prefixes like you did here.\n You can use \nkubectl describe to see the annotation you added:\n$ kubectl describe pod kubia-manual\n...\nAnnotations:    mycompany.com/someannotation=foo bar\n...\n3.7Using namespaces to group resources\nLet’s turn back to labels for a moment. We’ve seen how they organize pods and other\nobjects  into  groups.  Because  each  object  can  have  multiple  labels,  those  groups  of\nobjects can overlap. Plus, when working with the cluster (through \nkubectl for example),\nif you don’t explicitly specify a label selector, you’ll always see all objects. \nListing 3.5   A pod’s annotations\n \n\n77Using namespaces to group resources\n But what about times when you want to split objects into separate, non-overlapping\ngroups? You may want to only operate inside one group at a time. For this and other\nreasons,  Kubernetes  also  groups  objects  into  namespaces.  These  aren’t  the  Linux\nnamespaces  we  talked  about  in  chapter  2,  which  are  used  to  isolate  processes  from\neach other. Kubernetes namespaces provide a scope for objects names. Instead of hav-\ning all your resources in one single namespace, you can split them into multiple name-\nspaces, which also allows you to use the same resource names multiple times (across\ndifferent namespaces).\n3.7.1Understanding the need for namespaces\nUsing multiple namespaces allows you to split complex systems with numerous com-\nponents into smaller distinct groups. They can also be used for separating resources\nin a multi-tenant environment, splitting up resources into production, development,\nand QA environments, or in any other way you may need. Resource names only need\nto be unique within a namespace. Two different namespaces can contain resources of\nthe same name. But, while most types of resources are namespaced, a few aren’t. One\nof them is the Node resource, which is global and not tied to a single namespace.\nYou’ll learn about other cluster-level resources in later chapters.\n Let’s see how to use namespaces now.\n3.7.2Discovering other namespaces and their pods\nFirst, let’s list all namespaces in your cluster:\n$ kubectl get ns\nNAME          LABELS    STATUS    AGE\ndefault       <none>    Active    1h\nkube-public   <none>    Active    1h\nkube-system   <none>    Active    1h\nUp to this point, you’ve operated only in the default namespace. When listing resources\nwith  the  \nkubectl get  command,  you’ve  never  specified  the  namespace  explicitly,  so\nkubectl  always  defaulted  to  the  default  namespace,  showing  you  only  the  objects  in\nthat namespace. But as you can see from the list, the \nkube-public and the kube-system\nnamespaces  also  exist.  Let’s  look  at  the  pods  that  belong  to  the  kube-system  name-\nspace, by telling \nkubectl to list pods in that namespace only:\n$ kubectl get po --namespace kube-system\nNAME                                 READY     STATUS    RESTARTS   AGE\nfluentd-cloud-kubia-e8fe-node-txje   1/1       Running   0          1h\nheapster-v11-fz1ge                   1/1       Running   0          1h\nkube-dns-v9-p8a4t                    0/4       Pending   0          1h\nkube-ui-v4-kdlai                     1/1       Running   0          1h\nl7-lb-controller-v0.5.2-bue96        2/2       Running   92         1h\nTIPYou can also use -n instead of --namespace.\n \n\n78CHAPTER 3Pods: running containers in Kubernetes\nYou’ll learn about these pods later in the book (don’t worry if the pods shown here\ndon’t match the ones on your system exactly). It’s clear from the name of the name-\nspace  that  these  are  resources  related  to  the  Kubernetes  system  itself.  By  having\nthem in this separate namespace, it keeps everything nicely organized. If they were\nall in the default namespace, mixed in with the resources you create yourself, you’d\nhave a hard time seeing what belongs where, and you might inadvertently delete sys-\ntem resources. \n Namespaces enable you to separate resources that don’t belong together into non-\noverlapping groups. If several users or groups of users are using the same Kubernetes\ncluster, and they each manage their own distinct set of resources, they should each use\ntheir own namespace. This way, they don’t need to take any special care not to inad-\nvertently modify or delete the other users’ resources and don’t need to concern them-\nselves with name conflicts, because namespaces provide a scope for resource names,\nas has already been mentioned.\n  Besides isolating resources, namespaces are also used for allowing only certain users\naccess  to  particular  resources  and  even  for  limiting  the  amount  of  computational\nresources available to individual users. You’ll learn about this in chapters 12 through 14.\n3.7.3Creating a namespace\nA namespace is a Kubernetes resource like any other, so you can create it by posting a\nYAML file to the Kubernetes API server. Let’s see how to do this now. \nCREATING A NAMESPACE FROM A YAML FILE\nFirst, create a custom-namespace.yaml file with the following listing’s contents (you’ll\nfind the file in the book’s code archive).\napiVersion: v1\nkind: Namespace         \nmetadata:\n  name: custom-namespace  \nNow, use kubectl to post the file to the Kubernetes API server:\n$ kubectl create -f custom-namespace.yaml\nnamespace \"custom-namespace\" created\nCREATING A NAMESPACE WITH KUBECTL CREATE NAMESPACE\nAlthough writing a file like the previous one isn’t a big deal, it’s still a hassle. Luckily,\nyou can also create namespaces with the dedicated \nkubectl create namespace com-\nmand, which is quicker than writing a YAML file. By having you create a YAML mani-\nfest for the namespace, I wanted to reinforce the idea that everything in Kubernetes\nListing 3.6   A YAML definition of a namespace: custom-namespace.yaml\nThis says you’re \ndefining a namespace.\nThis is the name \nof the namespace.\n \n\n79Using namespaces to group resources\nhas a corresponding API object that you can create, read, update, and delete by post-\ning a YAML manifest to the API server.\n You could have created the namespace like this:\n$ kubectl create namespace custom-namespace\nnamespace \"custom-namespace\" created\nNOTEAlthough  most  objects’  names  must  conform  to  the  naming  conven-\ntions specified in RFC 1035 (Domain names), which means they may contain\nonly  letters,  digits,  dashes,  and  dots,  namespaces  (and  a  few  others)  aren’t\nallowed to contain dots. \n3.7.4Managing objects in other namespaces\nTo create resources in the namespace you’ve created, either add a namespace: custom-\nnamespace\n entry to the metadata section, or specify the namespace when creating the\nresource with the \nkubectl create command:\n$ kubectl create -f kubia-manual.yaml -n custom-namespace\npod \"kubia-manual\" created\nYou now have two pods with the same name (kubia-manual). One is in the default\nnamespace, and the other is in your custom-namespace.\n When listing, describing, modifying, or deleting objects in other namespaces, you\nneed to pass the \n--namespace (or -n) flag to kubectl. If you don’t specify the name-\nspace, \nkubectl performs the action in the default namespace configured in the cur-\nrent \nkubectl context. The current context’s namespace and the current context itself\ncan be changed through \nkubectl config commands. To learn more about managing\nkubectl contexts, refer to appendix A. \nTIPTo quickly switch to a different namespace, you can set up the following\nalias: \nalias kcd='kubectl config set-context $(kubectl config current-\ncontext)\n --namespace '. You can then switch between namespaces using kcd\nsome-namespace\n.\n3.7.5Understanding the isolation provided by namespaces\nTo wrap up this section about namespaces, let me explain what namespaces don’t pro-\nvide—at  least  not  out  of  the  box.  Although  namespaces  allow  you  to  isolate  objects\ninto distinct groups, which allows you to operate only on those belonging to the speci-\nfied namespace, they don’t provide any kind of isolation of running objects. \n For example, you may think that when different users deploy pods across different\nnamespaces, those pods are isolated from each other and can’t communicate, but that’s\nnot  necessarily  the  case.  Whether  namespaces  provide  network  isolation  depends  on\nwhich  networking  solution  is  deployed  with  Kubernetes.  When  the  solution  doesn’t\nprovide  inter-namespace  network  isolation,  if  a  pod  in  namespace  \nfoo  knows  the  IP\n \n\n80CHAPTER 3Pods: running containers in Kubernetes\naddress of a pod in namespace bar, there is nothing preventing it from sending traffic,\nsuch as HTTP requests, to the other pod. \n3.8Stopping and removing pods\nYou’ve  created  a  number  of  pods,  which  should  all  still  be  running.  You  have  four\npods running in the \ndefault namespace and one pod in custom-namespace. You’re\ngoing to stop all of them now, because you don’t need them anymore.\n3.8.1Deleting a pod by name\nFirst, delete the kubia-gpu pod by name:\n$ kubectl delete po kubia-gpu\npod \"kubia-gpu\" deleted\nBy deleting a pod, you’re instructing Kubernetes to terminate all the containers that are\npart of that pod. Kubernetes sends a \nSIGTERM signal to the process and waits a certain\nnumber of seconds (30 by default) for it to shut down gracefully. If it doesn’t shut down\nin  time,  the  process  is  then  killed  through  \nSIGKILL.  To  make  sure  your  processes  are\nalways shut down gracefully, they need to handle the \nSIGTERM signal properly. \nTIPYou can also delete more than one pod by specifying multiple, space-sep-\narated names (for example, \nkubectl delete po pod1 pod2).\n3.8.2Deleting pods using label selectors\nInstead of specifying each pod to delete by name, you’ll now use what you’ve learned\nabout  label  selectors  to  stop  both  the  \nkubia-manual  and  the  kubia-manual-v2  pod.\nBoth  pods  include  the  \ncreation_method=manual  label,  so  you  can  delete  them  by\nusing a label selector:\n$ kubectl delete po -l creation_method=manual\npod \"kubia-manual\" deleted\npod \"kubia-manual-v2\" deleted \nIn  the  earlier  microservices  example,  where  you  had  tens  (or  possibly  hundreds)  of\npods,  you  could,  for  instance,  delete  all  canary pods at once by specifying the\nrel=canary label selector (visualized in figure 3.10):\n$ kubectl delete po -l rel=canary\n3.8.3Deleting pods by deleting the whole namespace\nOkay, back to your real pods. What about the pod in the custom-namespace? You no\nlonger  need  either  the  pods  in  that  namespace,  or  the  namespace  itself.  You  can\n \n\n81Stopping and removing pods\ndelete the whole namespace (the pods will be deleted along with the namespace auto-\nmatically), using the following command:\n$ kubectl delete ns custom-namespace\nnamespace \n\"custom-namespace\" deleted\n3.8.4Deleting all pods in a namespace, while keeping the namespace\nYou’ve  now  cleaned  up  almost  everything.  But  what  about  the  pod  you  created  with\nthe \nkubectl run command in chapter 2? That one is still running:\n$ kubectl get pods\nNAME            READY   STATUS    RESTARTS   AGE\nkubia-zxzij     1/1     Running   0          1d    \nThis time, instead of deleting the specific pod, tell Kubernetes to delete all pods in the\ncurrent namespace by using the \n--all option:\n$ kubectl delete po --all\npod \"kubia-zxzij\" deleted\nNow, double check that no pods were left running:\n$ kubectl get pods\nNAME            READY   STATUS        RESTARTS   AGE\nkubia-09as0     1/1     Running       0          1d    \nkubia-zxzij     1/1     Terminating   0          1d    \nUI pod\napp: ui\nrel: stable\nrel=stable\napp=ui\nAccount\nService\npod\napp: as\nrel: stable\napp=as\napp: pc\nrel: stable\napp=pc\napp: sc\nrel: stable\napp=sc\napp: os\nrel: stable\napp=os\nProduct\nCatalog\npod\nShopping\nCart\npod\nOrder\nService\npod\nUI pod\napp: ui\nrel: beta\nrel=beta\napp: pc\nrel: beta\napp: os\nrel: beta\nProduct\nCatalog\npod\nOrder\nService\npod\nrel=canary\nAccount\nService\npod\napp: as\nrel: canary\napp: pc\nrel: canary\napp: os\nrel: canary\nProduct\nCatalog\npod\nOrder\nService\npod\nFigure 3.10   Selecting and deleting all canary pods through the rel=canary label selector\n \n\n82CHAPTER 3Pods: running containers in Kubernetes\nWait, what!?! The kubia-zxzij pod is terminating, but a new pod called kubia-09as0,\nwhich  wasn’t  there  before,  has  appeared.  No  matter  how  many  times  you  delete  all\npods, a new pod called kubia-something will emerge. \n You may remember you created your first pod with the \nkubectl run command. In\nchapter  2,  I  mentioned  that  this  doesn’t  create  a  pod  directly,  but  instead  creates  a\nReplicationController, which then creates the pod. As soon as you delete a pod cre-\nated  by  the  ReplicationController,  it  immediately  creates  a  new  one.  To  delete  the\npod, you also need to delete the ReplicationController. \n3.8.5Deleting (almost) all resources in a namespace\nYou  can  delete  the  ReplicationController  and  the  pods,  as  well  as  all  the  Services\nyou’ve  created,  by  deleting  all  resources  in  the  current  namespace  with  a  single\ncommand:\n$ kubectl delete all --all\npod \"kubia-09as0\" deleted\nreplicationcontroller \"kubia\" deleted\nservice \"kubernetes\" deleted\nservice \"kubia-http\" deleted\nThe first all in the command specifies that you’re deleting resources of all types, and\nthe \n--all option specifies that you’re deleting all resource instances instead of speci-\nfying them by name (you already used this option when you ran the previous delete\ncommand).\nNOTEDeleting  everything  with  the  all  keyword  doesn’t  delete  absolutely\neverything. Certain resources (like Secrets, which we’ll introduce in chapter 7)\nare preserved and need to be deleted explicitly.\nAs it deletes resources, \nkubectl will print the name of every resource it deletes. In the\nlist, you should see the \nkubia ReplicationController and the kubia-http Service you\ncreated in chapter 2. \nNOTEThe kubectl delete all --all command also deletes the kubernetes\nService, but it should be recreated automatically in a few moments.\n3.9Summary\nAfter  reading  this  chapter,  you  should  now  have  a  decent  knowledge  of  the  central\nbuilding block in Kubernetes. Every other concept you’ll learn about in the next few\nchapters is directly related to pods. \n In this chapter, you’ve learned\nHow to decide whether certain containers should be grouped together in a pod\nor not.\n \n\n83Summary\nPods  can  run  multiple  processes  and  are  similar  to  physical  hosts  in  the  non-\ncontainer world.\nYAML  or  JSON  descriptors  can  be  written  and  used  to  create  pods  and  then\nexamined to see the specification of a pod and its current state.\nLabels and label selectors should be used to organize pods and easily perform\noperations on multiple pods at once.\nYou can use node labels and selectors to schedule pods only to nodes that have\ncertain features.\nAnnotations  allow  attaching  larger  blobs  of  data  to  pods  either  by  people  or\ntools and libraries.\nNamespaces  can  be  used  to  allow  different  teams  to  use  the  same  cluster  as\nthough they were using separate Kubernetes clusters.\nHow to use the kubectl explain command to quickly look up the information\non any Kubernetes resource. \nIn  the  next  chapter,  you’ll  learn  about  ReplicationControllers  and  other  resources\nthat manage pods.\n \n\n84\nReplication and other\ncontrollers: deploying\nmanaged pods\nAs  you’ve  learned  so  far,  pods  represent  the  basic  deployable  unit  in  Kubernetes.\nYou know how to create, supervise, and manage them manually. But in real-world\nuse  cases,  you  want  your  deployments  to  stay  up  and  running  automatically  and\nremain healthy without any manual intervention. To do this, you almost never cre-\nate pods directly. Instead, you create other types of resources, such as Replication-\nControllers or Deployments, which then create and manage the actual pods.\n When you create unmanaged pods (such as the ones you created in the previ-\nous chapter), a cluster node is selected to run the pod and then its containers are\nrun  on  that  node.  In  this  chapter,  you’ll  learn  that  Kubernetes  then  monitors\nThis chapter covers\nKeeping pods healthy\nRunning multiple instances of the same pod\nAutomatically rescheduling pods after a node fails\nScaling pods horizontally\nRunning system-level pods on each cluster node\nRunning batch jobs\nScheduling jobs to run periodically or once in \nthe future\n \n\n85Keeping pods healthy\nthose  containers  and  automatically  restarts  them  if  they  fail.  But  if  the  whole  node\nfails,  the  pods  on  the  node  are  lost  and  will not be replaced with  new  ones,  unless\nthose pods are managed by the previously mentioned ReplicationControllers or simi-\nlar. In this chapter, you’ll learn how Kubernetes checks if a container is still alive and\nrestarts it if it isn’t. You’ll also learn how to run managed pods—both those that run\nindefinitely and those that perform a single task and then stop. \n4.1Keeping pods healthy\nOne of the main benefits of using Kubernetes is the ability to give it a list of contain-\ners and let it keep those containers running somewhere in the cluster. You do this by\ncreating  a  Pod  resource  and  letting  Kubernetes  pick  a  worker  node  for  it  and  run\nthe pod’s containers on that node. But what if one of those containers dies? What if\nall containers of a pod die? \n As soon as a pod is scheduled to a node, the Kubelet on that node will run its con-\ntainers  and,  from  then  on,  keep  them  running  as  long  as  the  pod  exists.  If  the  con-\ntainer’s   main   process   crashes,   the   Kubelet   will   restart   the   container.   If   your\napplication  has  a  bug  that  causes  it  to  crash  every  once  in  a  while,  Kubernetes  will\nrestart it automatically, so even without doing anything special in the app itself, run-\nning the app in Kubernetes automatically gives it the ability to heal itself. \n But sometimes apps stop working without their process crashing. For example, a\nJava  app  with  a  memory  leak  will  start  throwing  OutOfMemoryErrors,  but  the  JVM\nprocess  will  keep  running.  It  would  be  great  to  have  a  way  for  an  app  to  signal  to\nKubernetes that it’s no longer functioning properly and have Kubernetes restart it. \n We’ve said that a container that crashes is restarted automatically, so maybe you’re\nthinking you could catch these types of errors in the app and exit the process when\nthey occur. You can certainly do that, but it still doesn’t solve all your problems. \n For example, what about those situations when your app stops responding because\nit falls into an infinite loop or a deadlock? To make sure applications are restarted in\nsuch cases, you must check an application’s health from the outside and not depend\non the app doing it internally. \n4.1.1Introducing liveness probes\nKubernetes can check if a container is still alive through liveness probes. You can specify\na liveness probe for each container in the pod’s specification. Kubernetes will periodi-\ncally execute the probe and restart the container if the probe fails. \nNOTEKubernetes also supports readiness probes, which we’ll learn about in the\nnext chapter. Be sure not to confuse the two. They’re used for two different\nthings.\nKubernetes can probe a container using one of the three mechanisms:\nAn HTTP  GET  probe  performs  an  HTTP  GET  request  on  the  container’s  IP\naddress, a port and path you specify. If the probe receives a response, and the\n \n\n86CHAPTER 4Replication and other controllers: deploying managed pods\nresponse code doesn’t represent an error (in other words, if the HTTP response\ncode is 2xx or 3xx), the probe is considered successful. If the server returns an\nerror response code or if it doesn’t respond at all, the probe is considered a fail-\nure and the container will be restarted as a result.\nA TCP Socket probe tries to open a TCP connection to the specified port of the\ncontainer. If the connection is established successfully, the probe is successful.\nOtherwise, the container is restarted.\nAn Exec probe executes an arbitrary command inside the container and checks\nthe command’s exit status code. If the status code is 0, the probe is successful.\nAll other codes are considered failures. \n4.1.2Creating an HTTP-based liveness probe\nLet’s see how to add a liveness probe to your Node.js app. Because it’s a web app, it\nmakes sense to add a liveness probe that will check whether its web server is serving\nrequests. But because this particular Node.js app is too simple to ever fail, you’ll need\nto make the app fail artificially. \n  To  properly  demo  liveness  probes,  you’ll  modify  the  app  slightly  and  make  it\nreturn a 500 Internal Server Error HTTP status code for each request after the fifth\none—your  app  will  handle  the  first  five  client  requests  properly  and  then  return  an\nerror on every subsequent request. Thanks to the liveness probe, it should be restarted\nwhen that happens, allowing it to properly handle client requests again.\n  You  can  find  the  code  of  the  new  app  in  the  book’s  code  archive  (in  the  folder\nChapter04/kubia-unhealthy). I’ve pushed the container image to Docker Hub, so you\ndon’t need to build it yourself. \n You’ll create a new pod that includes an HTTP GET liveness probe. The following\nlisting shows the YAML for the pod.\napiVersion: v1\nkind: pod\nmetadata:\n  name: kubia-liveness\nspec:\n  containers:\n  - image: luksa/kubia-unhealthy   \n    name: kubia\n    livenessProbe:                 \n      httpGet:                     \n        path: /                     \n        port: 8080       \nListing 4.1   Adding a liveness probe to a pod: kubia-liveness-probe.yaml\nThis is the image \ncontaining the \n(somewhat) \nbroken app.\nA liveness probe that will \nperform an HTTP GET\nThe path to \nrequest in the \nHTTP request\nThe network port\nthe probe should\nconnect to\n \n\n87Keeping pods healthy\nThe pod descriptor defines an httpGet liveness probe, which tells Kubernetes to peri-\nodically perform HTTP GET requests on path \n/ on port 8080 to determine if the con-\ntainer is still healthy. These requests start as soon as the container is run.\n  After  five  such  requests  (or  actual  client  requests),  your  app  starts  returning\nHTTP status code 500, which Kubernetes will treat as a probe failure, and will thus\nrestart the container. \n4.1.3Seeing a liveness probe in action\nTo see what the liveness probe does, try creating the pod now. After about a minute and\na half, the container will be restarted. You can see that by running \nkubectl get:\n$ kubectl get po kubia-liveness\nNAME             READY     STATUS    RESTARTS   AGE\nkubia-liveness   1/1       Running   1          2m\nThe RESTARTS column shows that the pod’s container has been restarted once (if you\nwait another minute and a half, it gets restarted again, and then the cycle continues\nindefinitely).\nYou can see why the container had to be restarted by looking at what \nkubectl describe\nprints out, as shown in the following listing.\n$ kubectl describe po kubia-liveness\nName:           kubia-liveness\n...\nContainers:\n  kubia:\n    Container ID:       docker://480986f8\n    Image:              luksa/kubia-unhealthy\n    Image ID:           docker://sha256:2b208508\n    Port:\n    State:              Running                            \n      Started:          Sun, 14 May 2017 11:41:40 +0200    \nObtaining the application log of a crashed container\nIn the previous chapter, you learned how to print the application’s log with kubectl\nlogs\n. If your container is restarted, the kubectl logs command will show the log of\nthe current container. \nWhen you want to figure out why the previous container terminated, you’ll want to\nsee those logs instead of the current container’s logs. This can be done by using\nthe \n--previous option:\n$ kubectl logs mypod --previous\nListing 4.2   A pod’s description after its container is restarted\nThe container is \ncurrently running.\n \n\n88CHAPTER 4Replication and other controllers: deploying managed pods\n    Last State:         Terminated                         \n      Reason:           Error                              \n      Exit Code:        137                                \n      Started:          Mon, 01 Jan 0001 00:00:00 +0000    \n      Finished:         Sun, 14 May 2017 11:41:38 +0200    \n    Ready:              True\n    Restart Count:      1                                 \n    Liveness:           http-get http://:8080/ delay=0s timeout=1s\n                        period=10s #success=1 #failure=3\n    ...\nEvents:\n... Killing container with id docker://95246981:pod \"kubia-liveness ...\"\n    container \"kubia\" is unhealthy, it will be killed and re-created.\nYou  can  see  that  the  container  is  currently  running,  but  it  previously  terminated\nbecause of an error. The exit code was \n137, which has a special meaning—it denotes\nthat the process was terminated by an external signal. The number \n137 is a sum of two\nnumbers: \n128+x, where x is the signal number sent to the process that caused it to ter-\nminate. In the example, \nx equals 9, which is the number of the SIGKILL signal, mean-\ning the process was killed forcibly.\n  The  events  listed  at  the  bottom  show  why  the  container  was  killed—Kubernetes\ndetected the container was unhealthy, so it killed and re-created it. \nNOTEWhen a container is killed, a completely new container is created—it’s\nnot the same container being restarted again.\n4.1.4Configuring additional properties of the liveness probe\nYou  may  have  noticed  that  kubectl describe  also  displays  additional  information\nabout the liveness probe:\nLiveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 \n          \n➥ #failure=3\nBeside the liveness probe options you specified explicitly, you can also see additional\nproperties, such as \ndelay, timeout, period, and so on. The delay=0s part shows that\nthe  probing  begins  immediately  after  the  container  is  started.  The  \ntimeout  is  set  to\nonly  1  second,  so  the  container  must  return  a  response  in  1  second  or  the  probe  is\ncounted  as  failed.  The  container  is  probed  every  10  seconds  (\nperiod=10s)  and  the\ncontainer is restarted after the probe fails three consecutive times (\n#failure=3). \n  These  additional  parameters  can  be  customized  when  defining  the  probe.  For\nexample,  to  set  the  initial  delay,  add  the  \ninitialDelaySeconds  property  to  the  live-\nness probe as shown in the following listing.\n   livenessProbe:          \n     httpGet:              \n       path: /             \nListing 4.3   A liveness probe with an initial delay: kubia-liveness-probe-initial-delay.yaml\nThe previous \ncontainer terminated \nwith an error and \nexited with code 137.\nThe container \nhas been \nrestarted once.\n \n\n89Keeping pods healthy\n       port: 8080          \n     initialDelaySeconds: 15   \nIf you don’t set the initial delay, the prober will start probing the container as soon as\nit starts, which usually leads to the probe failing, because the app isn’t ready to start\nreceiving  requests.  If  the  number  of  failures  exceeds  the  failure  threshold,  the  con-\ntainer is restarted before it’s even able to start responding to requests properly. \nTIPAlways remember to set an initial delay to account for your app’s startup\ntime.\nI’ve  seen  this  on  many  occasions  and  users  were  confused  why  their  container  was\nbeing restarted. But if they’d used \nkubectl describe, they’d have seen that the con-\ntainer terminated with exit code 137 or 143, telling them that the pod was terminated\nexternally. Additionally, the listing of the pod’s events would show that the container\nwas killed because of a failed liveness probe. If you see this happening at pod startup,\nit’s because you failed to set \ninitialDelaySeconds appropriately.\nNOTEExit code 137 signals that the process was killed by an external signal\n(exit code is 128 + 9 (SIGKILL). Likewise, exit code 143 corresponds to 128 +\n15 (SIGTERM).\n4.1.5Creating effective liveness probes\nFor pods running in production, you should always define a liveness probe. Without\none, Kubernetes has no way of knowing whether your app is still alive or not. As long\nas the process is still running, Kubernetes will consider the container to be healthy. \nWHAT A LIVENESS PROBE SHOULD CHECK\nYour simplistic liveness probe simply checks if the server is responding. While this may\nseem overly simple, even a liveness probe like this does wonders, because it causes the\ncontainer  to  be  restarted  if  the  web  server  running  within  the  container  stops\nresponding to HTTP requests. Compared to having no liveness probe, this is a major\nimprovement, and may be sufficient in most cases.\n But for a better liveness check, you’d configure the probe to perform requests on a\nspecific URL path (\n/health, for example) and have the app perform an internal sta-\ntus check of all the vital components running inside the app to ensure none of them\nhas died or is unresponsive. \nTIPMake sure the /health HTTP endpoint doesn’t require authentication;\notherwise  the  probe  will  always  fail,  causing  your  container  to  be  restarted\nindefinitely.\nBe sure to check only the internals of the app and nothing influenced by an external\nfactor. For example, a frontend web server’s liveness probe shouldn’t return a failure\nwhen the server can’t connect to the backend database. If the underlying cause is in\nthe  database  itself,  restarting  the  web  server  container  will  not  fix  the  problem.\nKubernetes will wait 15 seconds \nbefore executing the first probe.\n \n\n90CHAPTER 4Replication and other controllers: deploying managed pods\nBecause the liveness probe will fail again, you’ll end up with the container restarting\nrepeatedly until the database becomes accessible again. \nKEEPING PROBES LIGHT\nLiveness probes shouldn’t use too many computational resources and shouldn’t take\ntoo  long  to  complete.  By  default,  the  probes  are  executed  relatively  often  and  are\nonly allowed one second to complete. Having a probe that does heavy lifting can slow\ndown your container considerably. Later in the book, you’ll also learn about how to\nlimit CPU time available to a container. The probe’s CPU time is counted in the con-\ntainer’s CPU time quota, so having a heavyweight liveness probe will reduce the CPU\ntime available to the main application processes.\nTIPIf you’re running a Java app in your container, be sure to use an HTTP\nGET liveness probe instead of an Exec probe, where you spin up a whole new\nJVM to get the liveness information. The same goes for any JVM-based or sim-\nilar  applications,  whose  start-up  procedure  requires  considerable  computa-\ntional resources.\nDON’T BOTHER IMPLEMENTING RETRY LOOPS IN YOUR PROBES\nYou’ve already seen that the failure threshold for the probe is configurable and usu-\nally the probe must fail multiple times before the container is killed. But even if you\nset the failure threshold to 1, Kubernetes will retry the probe several times before con-\nsidering it a single failed attempt. Therefore, implementing your own retry loop into\nthe probe is wasted effort.\nLIVENESS PROBE WRAP-UP\nYou  now  understand  that  Kubernetes  keeps  your  containers  running  by  restarting\nthem if they crash or if their liveness probes fail. This job is performed by the Kubelet\non the node hosting the pod—the Kubernetes Control Plane components running on\nthe master(s) have no part in this process. \n But if the node itself crashes, it’s the Control Plane that must create replacements for\nall the pods that went down with the node. It doesn’t do that for pods that you create\ndirectly. Those pods aren’t managed by anything except by the Kubelet, but because the\nKubelet runs on the node itself, it can’t do anything if the node fails. \n  To  make  sure  your  app  is  restarted  on  another  node,  you  need  to  have  the  pod\nmanaged by a ReplicationController or similar mechanism, which we’ll discuss in the\nrest of this chapter. \n4.2Introducing ReplicationControllers\nA  ReplicationController  is  a  Kubernetes  resource  that  ensures  its  pods  are  always\nkept running. If the pod disappears for any reason, such as in the event of a node\ndisappearing  from  the  cluster  or  because  the  pod  was  evicted  from  the  node,  the\nReplicationController notices the missing pod and creates a replacement pod. \n Figure 4.1 shows what happens when a node goes down and takes two pods with it.\nPod A was created directly and is therefore an unmanaged pod, while pod B is managed\n \n\n91Introducing ReplicationControllers\nby  a  ReplicationController.  After  the  node  fails,  the  ReplicationController  creates  a\nnew pod (pod B2) to replace the missing pod B, whereas pod A is lost completely—\nnothing will ever recreate it.\n The ReplicationController in the figure manages only a single pod, but Replication-\nControllers, in general, are meant to create and manage multiple copies (replicas) of a\npod. That’s where ReplicationControllers got their name from. \n4.2.1The operation of a ReplicationController\nA ReplicationController constantly monitors the list of running pods and makes sure\nthe actual number of pods of a “type” always matches the desired number. If too few\nsuch pods are running, it creates new replicas from a pod template. If too many such\npods are running, it removes the excess replicas. \n You might be wondering how there can be more than the desired number of repli-\ncas. This can happen for a few reasons: \nSomeone creates a pod of the same type manually.\nSomeone changes an existing pod’s “type.”\nSomeone decreases the desired number of pods, and so on.\nNode 1\nNode 1 fails\nPod A\nPod B\nNode 2\nVarious\nother pods\nCreates and\nmanages\nNode 1\nPod A\nPod B\nNode 2\nVarious\nother pods\nReplicationControllerReplicationController\nPod A goes down with Node 1 and is\nnot recreated, because there is no\nReplicationController overseeing it.\nRC notices pod B is\nmissing and creates\na new pod instance.\nPod B2\nFigure 4.1   When a node fails, only pods backed by a ReplicationController are recreated.\n \n\n92CHAPTER 4Replication and other controllers: deploying managed pods\nI’ve  used  the  term  pod  “type”  a  few  times.  But  no  such  thing  exists.  Replication-\nControllers don’t operate on pod types, but on sets of pods that match a certain label\nselector (you learned about them in the previous chapter). \nINTRODUCING THE CONTROLLER’S RECONCILIATION LOOP\nA  ReplicationController’s  job  is  to  make  sure  that  an  exact  number  of  pods  always\nmatches its label selector. If it doesn’t, the ReplicationController takes the appropriate\naction to reconcile the actual with the desired number. The operation of a Replication-\nController is shown in figure 4.2.\nUNDERSTANDING THE THREE PARTS OF A REPLICATIONCONTROLLER\nA ReplicationController has three essential parts (also shown in figure 4.3):\nA label selector, which determines what pods are in the ReplicationController’s scope\nA replica count, which specifies the desired number of pods that should be running\nA pod template, which is used when creating new pod replicas\nStart\nCompare\nmatched vs.\ndesired pod\ncount\nFind pods\nmatching the\nlabel selector\nCreate additional\npod(s) from\ncurrent template\nDelete the\nexcess pod(s)\nToo many\nJust enough\nToo few\nFigure 4.2   A ReplicationController’s reconciliation loop\napp: kubia\nPod\nPod template\nReplicationController: kubia\nPod selector:\napp=kubia\nReplicas: 3\nFigure 4.3   The three key parts of a \nReplicationController (pod selector, \nreplica count, and pod template)\n \n\n93Introducing ReplicationControllers\nA  ReplicationController’s  replica  count,  the  label  selector,  and  even  the  pod  tem-\nplate  can  all  be  modified  at  any  time,  but only changes to the replica count affect\nexisting pods. \nUNDERSTANDING THE EFFECT OF CHANGING THE CONTROLLER’S LABEL SELECTOR OR POD TEMPLATE\nChanges to the label selector and the pod template have no effect on existing pods.\nChanging  the  label  selector  makes  the  existing  pods  fall  out  of  the  scope  of  the\nReplicationController,  so  the  controller  stops  caring  about  them.  ReplicationCon-\ntrollers also don’t care about the actual “contents” of its pods (the container images,\nenvironment  variables,  and  other  things)  after  they  create  the  pod.  The  template\ntherefore only affects new pods created by this ReplicationController. You can think\nof it as a cookie cutter for cutting out new pods.\nUNDERSTANDING THE BENEFITS OF USING A REPLICATIONCONTROLLER\nLike many things in Kubernetes, a ReplicationController, although an incredibly sim-\nple concept, provides or enables the following powerful features:\nIt  makes  sure  a  pod  (or  multiple  pod  replicas)  is  always  running  by  starting  a\nnew pod when an existing one goes missing.\nWhen a cluster node fails, it creates replacement replicas for all the pods that\nwere  running  on  the  failed  node  (those  that  were  under  the  Replication-\nController’s control).\nIt  enables  easy  horizontal  scaling  of  pods—both  manual  and  automatic  (see\nhorizontal pod auto-scaling in chapter 15).\nNOTEA  pod  instance  is  never  relocated  to  another  node.  Instead,  the\nReplicationController creates a completely new pod instance that has no rela-\ntion to the instance it’s replacing. \n4.2.2Creating a ReplicationController\nLet’s  look  at  how  to  create  a  ReplicationController  and  then  see  how  it  keeps  your\npods  running.  Like  pods  and  other  Kubernetes  resources,  you  create  a  Replication-\nController by posting a JSON or YAML descriptor to the Kubernetes API server.\n  You’re  going  to  create  a  YAML  file  called  kubia-rc.yaml  for  your  Replication-\nController, as shown in the following listing.\napiVersion: v1\nkind: ReplicationController     \nmetadata:\n  name: kubia                      \nspec:\n  replicas: 3                     \n  selector:              \n    app: kubia           \nListing 4.4   A YAML definition of a ReplicationController: kubia-rc.yaml\nThis manifest defines a \nReplicationController (RC)\nThe name of this \nReplicationController\nThe desired number \nof pod instances\nThe pod selector determining \nwhat pods the RC is operating on\n \n\n94CHAPTER 4Replication and other controllers: deploying managed pods\n  template:                        \n    metadata:                      \n      labels:                      \n        app: kubia                 \n    spec:                          \n      containers:                  \n      - name: kubia                \n        image: luksa/kubia         \n        ports:                     \n        - containerPort: 8080      \nWhen  you  post  the  file  to  the  API  server,  Kubernetes  creates  a  new  Replication-\nController  named  \nkubia,  which  makes  sure  three  pod  instances  always  match  the\nlabel selector \napp=kubia. When there aren’t enough pods, new pods will be created\nfrom the provided pod template. The contents of the template are almost identical to\nthe pod definition you created in the previous chapter. \n  The  pod  labels  in  the  template  must  obviously  match  the  label  selector  of  the\nReplicationController;  otherwise  the  controller  would  create  new  pods  indefinitely,\nbecause spinning up a new pod wouldn’t bring the actual replica count any closer to\nthe desired number of replicas. To prevent such scenarios, the API server verifies the\nReplicationController definition and will not accept it if it’s misconfigured.\n Not specifying the selector at all is also an option. In that case, it will be configured\nautomatically from the labels in the pod template. \nTIPDon’t specify a pod selector when defining a ReplicationController. Let\nKubernetes  extract  it  from  the  pod  template.  This  will  keep  your  YAML\nshorter and simpler.\nTo  create  the  ReplicationController,  use  the  \nkubectl create  command,  which  you\nalready know:\n$ kubectl create -f kubia-rc.yaml\nreplicationcontroller \"kubia\" created\nAs  soon  as  the  ReplicationController  is  created,  it  goes  to  work.  Let’s  see  what\nit does.\n4.2.3Seeing the ReplicationController in action\nBecause  no  pods  exist  with  the  app=kubia  label,  the  ReplicationController  should\nspin up three new pods from the pod template. List the pods to see if the Replication-\nController has done what it’s supposed to:\n$ kubectl get pods\nNAME          READY     STATUS              RESTARTS   AGE\nkubia-53thy   0/1       ContainerCreating   0          2s\nkubia-k0xz6   0/1       ContainerCreating   0          2s\nkubia-q3vkg   0/1       ContainerCreating   0          2s\nThe pod template \nfor creating new \npods\n \n\n95Introducing ReplicationControllers\nIndeed, it has! You wanted three pods, and it created three pods. It’s now managing\nthose  three  pods.  Next  you’ll  mess  with  them  a  little  to  see  how  the  Replication-\nController responds. \nSEEING THE REPLICATIONCONTROLLER RESPOND TO A DELETED POD\nFirst, you’ll delete one of the pods manually to see how the ReplicationController spins\nup a new one immediately, bringing the number of matching pods back to three:\n$ kubectl delete pod kubia-53thy\npod \"kubia-53thy\" deleted\nListing the pods again shows four of them, because the one you deleted is terminat-\ning, and a new pod has already been created:\n$ kubectl get pods\nNAME          READY     STATUS              RESTARTS   AGE\nkubia-53thy   1/1       Terminating         0          3m\nkubia-oini2   0/1       ContainerCreating   0          2s\nkubia-k0xz6   1/1       Running             0          3m\nkubia-q3vkg   1/1       Running             0          3m\nThe ReplicationController has done its job again. It’s a nice little helper, isn’t it?\nGETTING INFORMATION ABOUT A REPLICATIONCONTROLLER\nNow,  let’s  see  what  information  the  kubectl get  command  shows  for  Replication-\nControllers:\n$ kubectl get rc\nNAME      DESIRED   CURRENT   READY     AGE\nkubia     3         3         2         3m\nNOTEWe’re using rc as a shorthand for replicationcontroller.\nYou  see  three  columns  showing  the  desired  number  of  pods,  the  actual  number  of\npods, and how many of them are ready (you’ll learn what that means in the next chap-\nter, when we talk about readiness probes).\n  You  can  see  additional  information  about  your  ReplicationController  with  the\nkubectl describe command, as shown in the following listing.\n$ kubectl describe rc kubia\nName:           kubia\nNamespace:      default\nSelector:       app=kubia\nLabels:         app=kubia\nAnnotations:    <none>\nReplicas:       3 current / 3 desired               \nPods Status:    4 Running / 0 Waiting / 0 Succeeded / 0 Failed  \nPod Template:\n  Labels:       app=kubia\n  Containers:   ...\nListing 4.5   Displaying details of a ReplicationController with kubectl describe\nThe actual vs. the \ndesired number of \npod instances\nNumber of \npod instances \nper pod \nstatus\n \n\n96CHAPTER 4Replication and other controllers: deploying managed pods\n  Volumes:      <none>\nEvents:                                                   \nFrom                    Type      Reason           Message\n----                    -------  ------            -------\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-53thy\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-k0xz6\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-q3vkg\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-oini2\nThe  current  number  of  replicas  matches  the  desired  number,  because  the  controller\nhas already created a new pod. It shows four running pods because a pod that’s termi-\nnating is still considered running, although it isn’t counted in the current replica count. \n  The  list  of  events  at  the  bottom  shows  the  actions  taken  by  the  Replication-\nController—it has created four pods so far.\nUNDERSTANDING EXACTLY WHAT CAUSED THE CONTROLLER TO CREATE A NEW POD\nThe controller is responding to the deletion of a pod by creating a new replacement\npod (see figure 4.4). Well, technically, it isn’t responding to the deletion itself, but the\nresulting state—the inadequate number of pods.\n  While  a  ReplicationController  is  immediately  notified  about  a  pod  being  deleted\n(the API server allows clients to watch for changes to resources and resource lists), that’s\nnot what causes it to create a replacement pod. The notification triggers the controller\nto check the actual number of pods and take appropriate action.\nThe events \nrelated to this \nReplicationController\nBefore deletionAfter deletion\nReplicationController: kubia\nReplicas: 3\nSelector:app=kubia\napp: kubia\nPod:\nkubia-q3vkg\napp: kubia\nPod:\nkubia-oini2\n[ContainerCreating]           [Terminating]\napp: kubia\nPod:\nkubia-k0xz6\napp: kubia\nPod:\nkubia-53thy\nReplicationController: kubia\nReplicas: 3\nSelector:app=kubia\napp: kubia\nPod:\nkubia-q3vkg\napp: kubia\nPod:\nkubia-k0xz6\napp: kubia\nPod:\nkubia-53thy\nDeletekubia-53thy\nFigure 4.4   If a pod disappears, the ReplicationController sees too few pods and creates a new replacement pod.\n \n\n97Introducing ReplicationControllers\nRESPONDING TO A NODE FAILURE\nSeeing  the  ReplicationController  respond  to  the  manual  deletion  of  a  pod  isn’t  too\ninteresting, so let’s look at a better example. If you’re using Google Kubernetes Engine\nto run these examples, you have a three-node Kubernetes cluster. You’re going to dis-\nconnect one of the nodes from the network to simulate a node failure.\nNOTEIf you’re using Minikube, you can’t do this exercise, because you only\nhave one node that acts both as a master and a worker node.\nIf a node fails in the non-Kubernetes world, the ops team would need to migrate the\napplications  running  on  that  node  to  other  machines  manually.  Kubernetes,  on  the\nother hand, does that automatically. Soon after the ReplicationController detects that\nits pods are down, it will spin up new pods to replace them. \n Let’s see this in action. You need to \nssh into one of the nodes with the gcloud\ncompute\n ssh command and then shut down its network interface with sudo ifconfig\neth0\n down, as shown in the following listing.\nNOTEChoose a node that runs at least one of your pods by listing pods with\nthe \n-o wide option.\n$ gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko\nEnter passphrase for key '/home/luksa/.ssh/google_compute_engine':\nWelcome to Kubernetes v1.6.4!\n...\nluksa@gke-kubia-default-pool-b46381f1-zwko ~ $ sudo ifconfig eth0 down\nWhen you shut down the network interface, the ssh session will stop responding, so\nyou need to open up another terminal or hard-exit from the \nssh session. In the new\nterminal  you  can  list  the  nodes  to  see  if  Kubernetes  has  detected  that  the  node  is\ndown. This takes a minute or so. Then, the node’s status is shown as \nNotReady:\n$ kubectl get node\nNAME                                   STATUS     AGE\ngke-kubia-default-pool-b46381f1-opc5   Ready      5h\ngke-kubia-default-pool-b46381f1-s8gj   Ready      5h\ngke-kubia-default-pool-b46381f1-zwko   NotReady   5h    \nIf you list the pods now, you’ll still see the same three pods as before, because Kuber-\nnetes waits a while before rescheduling pods (in case the node is unreachable because\nof a temporary network glitch or because the Kubelet is restarting). If the node stays\nunreachable  for  several  minutes,  the  status  of  the  pods  that  were  scheduled  to  that\nnode  changes  to  \nUnknown.  At  that  point,  the  ReplicationController  will  immediately\nspin up a new pod. You can see this by listing the pods again:\nListing 4.6   Simulating a node failure by shutting down its network interface\nNode isn’t ready, \nbecause it’s \ndisconnected from \nthe network\n \n\n98CHAPTER 4Replication and other controllers: deploying managed pods\n$ kubectl get pods\nNAME          READY   STATUS    RESTARTS   AGE\nkubia-oini2   1/1     Running   0          10m\nkubia-k0xz6   1/1     Running   0          10m\nkubia-q3vkg   1/1     Unknown   0          10m    \nkubia-dmdck   1/1     Running   0          5s    \nLooking at the age of the pods, you see that the kubia-dmdck pod is new. You again\nhave three pod instances running, which means the ReplicationController has again\ndone its job of bringing the actual state of the system to the desired state. \n The same thing happens if a node fails (either breaks down or becomes unreach-\nable).  No  immediate  human  intervention  is  necessary.  The  system  heals  itself\nautomatically. \n To bring the node back, you need to reset it with the following command:\n$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko\nWhen the node boots up again, its status should return to Ready, and the pod whose\nstatus was \nUnknown will be deleted.\n4.2.4Moving pods in and out of the scope of a ReplicationController\nPods  created  by  a  ReplicationController  aren’t  tied  to  the  ReplicationController  in\nany way. At any moment, a ReplicationController manages pods that match its label\nselector. By changing a pod’s labels, it can be removed from or added to the scope\nof a ReplicationController. It can even be moved from one ReplicationController to\nanother.\nTIPAlthough a pod isn’t tied to a ReplicationController, the pod does refer-\nence  it  in  the  \nmetadata.ownerReferences  field,  which  you  can  use  to  easily\nfind which ReplicationController a pod belongs to.\nIf you change a pod’s labels so they no longer match a ReplicationController’s label\nselector,  the  pod  becomes  like  any  other  manually  created  pod.  It’s  no  longer  man-\naged by anything. If the node running the pod fails, the pod is obviously not resched-\nuled.  But  keep  in  mind  that  when  you  changed  the  pod’s  labels,  the  replication\ncontroller noticed one pod was missing and spun up a new pod to replace it.\n  Let’s  try  this  with  your  pods.  Because  your  ReplicationController  manages  pods\nthat have the \napp=kubia label, you need to either remove this label or change its value\nto move the pod out of the ReplicationController’s scope. Adding another label will\nhave no effect, because the ReplicationController doesn’t care if the pod has any addi-\ntional labels. It only cares whether the pod has all the labels referenced in the label\nselector. \nThis pod’s status is \nunknown, because its \nnode is unreachable.\nThis pod was created \nfive seconds ago.\n \n\n99Introducing ReplicationControllers\nADDING LABELS TO PODS MANAGED BY A REPLICATIONCONTROLLER\nLet’s confirm that a ReplicationController doesn’t care if you add additional labels to\nits managed pods:\n$ kubectl label pod kubia-dmdck type=special\npod \"kubia-dmdck\" labeled\n$ kubectl get pods --show-labels\nNAME          READY   STATUS    RESTARTS   AGE   LABELS\nkubia-oini2   1/1     Running   0          11m   app=kubia\nkubia-k0xz6   1/1     Running   0          11m   app=kubia\nkubia-dmdck   1/1     Running   0          1m    app=kubia,type=special\nYou’ve added the type=special label to one of the pods. Listing all pods again shows\nthe same three pods as before, because no change occurred as far as the Replication-\nController is concerned.\nCHANGING THE LABELS OF A MANAGED POD\nNow, you’ll change the app=kubia label to something else. This will make the pod no\nlonger match the ReplicationController’s label selector, leaving it to only match two\npods. The ReplicationController should therefore start a new pod to bring the num-\nber back to three:\n$ kubectl label pod kubia-dmdck app=foo --overwrite\npod \"kubia-dmdck\" labeled\nThe --overwrite argument is necessary; otherwise kubectl will only print out a warn-\ning and won’t change the label, to prevent you from inadvertently changing an exist-\ning label’s value when your intent is to add a new one. \n Listing all the pods again should now show four pods: \n$ kubectl get pods -L app\nNAME         READY  STATUS             RESTARTS  AGE  APP\nkubia-2qneh  0/1    ContainerCreating  0         2s   kubia   \nkubia-oini2  1/1    Running            0         20m  kubia\nkubia-k0xz6  1/1    Running            0         20m  kubia\nkubia-dmdck  1/1    Running            0         10m  foo    \nNOTEYou’re using the -L app option to display the app label in a column.\nThere, you now have four pods altogether: one that isn’t managed by your Replication-\nController and three that are. Among them is the newly created pod.\n Figure 4.5 illustrates what happened when you changed the pod’s labels so they no\nlonger matched the ReplicationController’s pod selector. You can see your three pods\nand your ReplicationController. After you change the pod’s label from \napp=kubia to\napp=foo, the ReplicationController no longer cares about the pod. Because the con-\ntroller’s  replica  count  is  set  to  3  and  only  two  pods  match  the  label  selector,  the\nNewly created pod that replaces\nthe pod you removed from the\nscope of the ReplicationController\nPod no longer \nmanaged by the \nReplicationController\n \n\n100CHAPTER 4Replication and other controllers: deploying managed pods\nReplicationController  spins  up  pod  kubia-2qneh  to  bring  the  number  back  up  to\nthree. Pod \nkubia-dmdck is now completely independent and will keep running until\nyou delete it manually (you can do that now, because you don’t need it anymore).\nREMOVING PODS FROM CONTROLLERS IN PRACTICE\nRemoving  a  pod  from  the  scope  of  the  ReplicationController comes  in  handy  when\nyou  want  to  perform  actions  on  a  specific  pod.  For  example,  you  might  have  a  bug\nthat causes your pod to start behaving badly after a specific amount of time or a spe-\ncific event. If you know a pod is malfunctioning, you can take it out of the Replication-\nController’s  scope,  let  the  controller  replace  it  with  a  new  one,  and  then  debug  or\nplay with the pod in any way you want. Once you’re done, you delete the pod. \nCHANGING THE REPLICATIONCONTROLLER’S LABEL SELECTOR\nAs  an  exercise  to  see  if  you  fully  understand  ReplicationControllers,  what  do  you\nthink  would  happen  if  instead  of  changing  the  labels  of  a  pod,  you  modified  the\nReplicationController’s label selector? \n If your answer is that it would make all  the  pods  fall  out  of  the  scope  of  the\nReplicationController, which would result in it creating three new pods, you’re abso-\nlutely right. And it shows that you understand how ReplicationControllers work. \n Kubernetes does allow you to change a ReplicationController’s label selector, but\nthat’s not the case for the other resources that are covered in the second half of this\nInitial stateAfter re-labelling\nRe-labelkubia-dmdck\napp: kubia\nPod:\nkubia-oini2\napp: kubia\nPod:\nkubia-2qneh\n[ContainerCreating]\nPod:\nkubia-dmdck\napp: kubia\nPod:\nkubia-k0xz6\napp: kubia\ntype: special\ntype: special\napp: fooapp: kubia\nPod:\nkubia-dmdck\napp: kubia\nPod:\nkubia-k0xz6\nReplicationController: kubia\nReplicas: 3\nSelector:app=kubia\nReplicationController: kubia\nReplicas: 3\nSelector:app=kubia\nPod:\nkubia-oini2\nFigure 4.5   Removing a pod from the scope of a ReplicationController by changing its labels \n \n\n101Introducing ReplicationControllers\nchapter and which are also used for managing pods. You’ll never change a controller’s\nlabel selector, but you’ll regularly change its pod template. Let’s take a look at that.\n4.2.5Changing the pod template\nA ReplicationController’s pod template can be modified at any time. Changing the pod\ntemplate is like replacing a cookie cutter with another one. It will only affect the cookies\nyou cut out afterward and will have no effect on the ones you’ve already cut (see figure\n4.6).  To  modify  the  old  pods,  you’d  need  to  delete  them  and  let  the  Replication-\nController replace them with new ones based on the new template.\nAs an exercise, you can try editing the ReplicationController and adding a label to the\npod template. You can edit the ReplicationController with the following command:\n$ kubectl edit rc kubia\nThis will open the ReplicationController’s YAML definition in your default text editor.\nFind the pod template section and add an additional label to the metadata. After you\nsave your changes and exit the editor, \nkubectl will update the ReplicationController\nand print the following message:\nreplicationcontroller \"kubia\" edited\nYou can now list pods and their labels again and confirm that they haven’t changed.\nBut if you delete the pods and wait for their replacements to be created, you’ll see the\nnew label.\n Editing a ReplicationController like this to change the container image in the pod\ntemplate, deleting the existing pods, and letting them be replaced with new ones from\nthe new template could be used for upgrading pods, but you’ll learn a better way of\ndoing that in chapter 9. \nReplication\nController\nReplicas: 3\nTemplate:\nABC\nReplication\nController\nReplicas: 3\nTemplate:\nA\nReplication\nController\nReplicas: 3\nTemplate:\nA\nReplication\nController\nReplicas: 3\nTemplate:\nD\nABCABCAB\nChange\ntemplate\nDelete\na pod\nRC creates\nnew pod\nFigure 4.6   Changing a ReplicationController’s pod template only affects pods created afterward and has no \neffect on existing pods.\n \n\n102CHAPTER 4Replication and other controllers: deploying managed pods\n4.2.6Horizontally scaling pods\nYou’ve seen how ReplicationControllers make sure a specific number of pod instances\nis always running. Because it’s incredibly simple to change the desired number of rep-\nlicas, this also means scaling pods horizontally is trivial. \n Scaling the number of pods up or down is as easy as changing the value of the rep-\nlicas  field  in  the  ReplicationController  resource.  After  the  change,  the  Replication-\nController will either see too many pods exist (when scaling down) and delete part of\nthem, or see too few of them (when scaling up) and create additional pods. \nSCALING UP A REPLICATIONCONTROLLER\nYour  ReplicationController  has  been  keeping  three  instances  of  your  pod  running.\nYou’re  going  to  scale  that  number  up  to  10  now.  As  you  may  remember,  you’ve\nalready  scaled  a  ReplicationController  in  chapter  2.  You  could  use  the  same  com-\nmand as before:\n$ kubectl scale rc kubia --replicas=10\nBut you’ll do it differently this time. \nSCALING A REPLICATIONCONTROLLER BY EDITING ITS DEFINITION\nInstead of using the kubectl scale command, you’re going to scale it in a declarative\nway by editing the ReplicationController’s definition:\n$ kubectl edit rc kubia\nWhen the text editor opens, find the spec.replicas field and change its value to 10,\nas shown in the following listing.\n# Please edit the object below. Lines beginning with a '#' will be ignored,\n# and an empty file will abort the edit. If an error occurs while saving \n# this file will be reopened with the relevant failures.\napiVersion: v1\nkind: ReplicationController\nConfiguring kubectl edit to use a different text editor\nYou can tell kubectl to use a text editor of your choice by setting the KUBE_EDITOR\nenvironment variable. For example, if you’d like to use nano for editing Kubernetes\nresources,  execute  the  following  command  (or  put  it  into  your  \n~/.bashrc  or  an\nequivalent file):\nexport KUBE_EDITOR=\"/usr/bin/nano\"\nIf the KUBE_EDITOR environment variable isn’t set, kubectl edit falls back to using\nthe default editor, usually configured through the \nEDITOR environment variable.\nListing 4.7   Editing the RC in a text editor by running kubectl edit\n \n\n103Introducing ReplicationControllers\nmetadata:\n  ...\nspec:\n  replicas: 3        \n  selector:\n    app: kubia\n  ...\nWhen you save the file and close the editor, the ReplicationController is updated and\nit immediately scales the number of pods to 10:\n$ kubectl get rc\nNAME      DESIRED   CURRENT   READY     AGE\nkubia     10        10        4         21m\nThere you go. If the kubectl scale command makes it look as though you’re telling\nKubernetes exactly what to do, it’s now much clearer that you’re making a declarative\nchange to the desired state of the ReplicationController and not telling Kubernetes to\ndo something.\nSCALING DOWN WITH THE KUBECTL SCALE COMMAND\nNow scale back down to 3. You can use the kubectl scale command:\n$ kubectl scale rc kubia --replicas=3\nAll this command does is modify the spec.replicas field of the ReplicationController’s\ndefinition—like when you changed it through \nkubectl edit. \nUNDERSTANDING THE DECLARATIVE APPROACH TO SCALING\nHorizontally scaling pods in Kubernetes is a matter of stating your desire: “I want to\nhave x number of instances running.” You’re not telling Kubernetes what or how to do\nit. You’re just specifying the desired state. \n This declarative approach makes interacting with a Kubernetes cluster easy. Imag-\nine if you had to manually determine the current number of running instances and\nthen  explicitly  tell  Kubernetes  how  many  additional  instances  to  run.  That’s  more\nwork and is much more error-prone. Changing a simple number is much easier, and\nin chapter 15, you’ll learn that even that can be done by Kubernetes itself if you\nenable horizontal pod auto-scaling. \n4.2.7Deleting a ReplicationController\nWhen you delete a ReplicationController through kubectl delete, the pods are also\ndeleted. But because pods created by a ReplicationController aren’t an integral part\nof  the  ReplicationController,  and  are  only  managed  by  it,  you  can  delete  only  the\nReplicationController and leave the pods running, as shown in figure 4.7.\n This may be useful when you initially have a set of pods managed by a Replication-\nController,  and  then  decide  to  replace  the  ReplicationController  with  a  ReplicaSet,\nfor  example  (you’ll  learn  about  them  next.).  You  can  do  this  without  affecting  the\nChange the number 3 \nto number 10 in \nthis line.\n \n\n104CHAPTER 4Replication and other controllers: deploying managed pods\npods  and  keep  them  running  without  interruption  while  you  replace  the  Replication-\nController that manages them. \n  When  deleting  a  ReplicationController  with  \nkubectl delete,  you  can  keep  its\npods running by passing the \n--cascade=false option to the command. Try that now:\n$ kubectl delete rc kubia --cascade=false\nreplicationcontroller \"kubia\" deleted\nYou’ve deleted the ReplicationController so the pods are on their own. They are no\nlonger  managed.  But  you  can  always  create  a  new  ReplicationController  with  the\nproper label selector and make them managed again.\n4.3Using ReplicaSets instead of ReplicationControllers\nInitially, ReplicationControllers were the only Kubernetes component for replicating\npods  and  rescheduling  them  when  nodes  failed.  Later,  a  similar  resource  called  a\nReplicaSet  was  introduced.  It’s  a  new  generation  of  ReplicationController  and\nreplaces it completely (ReplicationControllers will eventually be deprecated). \n You could have started this chapter by creating a ReplicaSet instead of a Replication-\nController, but I felt it would be a good idea to start with what was initially available in\nKubernetes. Plus, you’ll still see ReplicationControllers used in the wild, so it’s good\nfor you to know about them. That said, you should always create ReplicaSets instead\nof  ReplicationControllers  from  now  on.  They’re  almost  identical,  so  you  shouldn’t\nhave any trouble using them instead. \nBefore the RC deletionAfter the RC deletion\nDelete RC\nPod:\nkubia-q3vkg\nPod:\nkubia-53thy\nPod:\nkubia-k0xz6\nPod:\nkubia-q3vkg\nPod:\nkubia-53thy\nPod:\nkubia-k0xz6\nReplicationController: kubia\nReplicas: 3\nSelector:app=kubia\napp: kubiaapp: kubia\napp: kubia\napp: kubiaapp: kubia\napp: kubia\nFigure 4.7   Deleting a replication controller with --cascade=false leaves pods unmanaged.\n \n\n105Using ReplicaSets instead of ReplicationControllers\n  You  usually  won’t  create  them  directly,  but  instead  have  them  created  automati-\ncally when you create the higher-level Deployment resource, which you’ll learn about\nin chapter 9. In any case, you should understand ReplicaSets, so let’s see how they dif-\nfer from ReplicationControllers.\n4.3.1Comparing a ReplicaSet to a ReplicationController\nA ReplicaSet behaves exactly like a ReplicationController, but it has more expressive\npod selectors. Whereas a ReplicationController’s label selector only allows matching\npods that include a certain label, a ReplicaSet’s selector also allows matching pods\nthat  lack  a  certain  label  or  pods  that  include  a  certain  label  key,  regardless  of\nits value.\n  Also,  for  example,  a  single  ReplicationController  can’t  match  pods  with  the  label\nenv=production and those with the label env=devel at the same time. It can only match\neither pods with the \nenv=production label or pods with the env=devel label. But a sin-\ngle ReplicaSet can match both sets of pods and treat them as a single group. \n Similarly, a ReplicationController can’t match pods based merely on the presence\nof a label key, regardless of its value, whereas a ReplicaSet can. For example, a Replica-\nSet can match all pods that include a label with the key \nenv, whatever its actual value is\n(you can think of it as \nenv=*).\n4.3.2Defining a ReplicaSet\nYou’re going to create a ReplicaSet now to see how the orphaned pods that were cre-\nated by your ReplicationController and then abandoned earlier can now be adopted\nby a ReplicaSet. First, you’ll rewrite your ReplicationController into a ReplicaSet by\ncreating  a  new  file  called  kubia-replicaset.yaml  with  the  contents  in  the  following\nlisting.\napiVersion: apps/v1beta2      \nkind: ReplicaSet                    \nmetadata:\n  name: kubia\nspec:\n  replicas: 3\n  selector:\n    matchLabels:                 \n      app: kubia                 \n  template:                        \n    metadata:                      \n      labels:                      \n        app: kubia                 \n    spec:                          \n      containers:                  \n      - name: kubia                \n        image: luksa/kubia         \nListing 4.8   A YAML definition of a ReplicaSet: kubia-replicaset.yaml\nReplicaSets aren’t part of the v1 \nAPI, but belong to the apps API \ngroup and version v1beta2.\nYou’re using the simpler matchLabels \nselector here, which is much like a \nReplicationController’s selector.\nThe template is \nthe same as in the \nReplicationController.\n \n\n106CHAPTER 4Replication and other controllers: deploying managed pods\nThe  first  thing  to  note  is  that  ReplicaSets  aren’t  part  of  the  v1  API,  so  you  need  to\nensure you specify the proper \napiVersion when creating the resource. You’re creating a\nresource  of  type  ReplicaSet  which  has  much  the  same  contents  as  the  Replication-\nController you created earlier. \n  The  only  difference  is  in  the  selector.  Instead  of  listing  labels  the  pods  need  to\nhave  directly  under  the  \nselector  property,  you’re  specifying  them  under  selector\n.matchLabels\n. This is the simpler (and less expressive) way of defining label selectors\nin a ReplicaSet. Later, you’ll look at the more expressive option, as well.\nBecause you still have three pods matching the \napp=kubia selector running from ear-\nlier, creating this ReplicaSet will not cause any new pods to be created. The ReplicaSet\nwill take those existing three pods under its wing. \n4.3.3Creating and examining a ReplicaSet\nCreate the ReplicaSet from the YAML file with the kubectl create command. After\nthat, you can examine the ReplicaSet with \nkubectl get and kubectl describe:\n$ kubectl get rs\nNAME      DESIRED   CURRENT   READY     AGE\nkubia     3         3         3         3s\nTIPUse rs shorthand, which stands for replicaset.\n$ kubectl describe rs\nName:           kubia\nNamespace:      default\nSelector:       app=kubia\nLabels:         app=kubia\nAnnotations:    <none>\nReplicas:       3 current / 3 desired\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:       app=kubia\nAbout the API version attribute\nThis is your first opportunity to see that the apiVersion property specifies two things:\nThe API group (which is apps in this case)\nThe actual API version (v1beta2)\nYou’ll see throughout the book that certain Kubernetes resources are in what’s called\nthe core API group, which doesn’t need to be specified in the \napiVersion field (you\njust  specify  the  version—for  example,  you’ve  been  using  \napiVersion: v1  when\ndefining Pod resources). Other resources, which were introduced in later Kubernetes\nversions, are categorized into several API groups. Look at the inside of the book’s\ncovers to see all resources and their respective API groups.\n \n\n107Using ReplicaSets instead of ReplicationControllers\n  Containers:   ...\n  Volumes:      <none>\nEvents:         <none>\nAs  you  can  see,  the  ReplicaSet  isn’t  any  different  from  a  ReplicationController.  It’s\nshowing it has three replicas matching the selector. If you list all the pods, you’ll see\nthey’re still the same three pods you had before. The ReplicaSet didn’t create any new\nones. \n4.3.4Using the ReplicaSet’s more expressive label selectors\nThe  main  improvements  of  ReplicaSets  over  ReplicationControllers  are  their  more\nexpressive label selectors. You intentionally used the simpler \nmatchLabels selector in\nthe first ReplicaSet example to see that ReplicaSets  are  no  different  from  Replication-\nControllers. Now, you’ll rewrite the selector to use the more powerful \nmatchExpressions\nproperty, as shown in the following listing.\n selector:\n   matchExpressions:                 \n     - key: app           \n       operator: In                  \n       values:                       \n         - kubia                     \nNOTEOnly the selector is shown. You’ll find the whole ReplicaSet definition\nin the book’s code archive.\nYou can add additional expressions to the selector. As in the example, each expression\nmust contain a \nkey, an operator, and possibly (depending on the operator) a list of\nvalues. You’ll see four valid operators:\nIn—Label’s value must match one of the specified values.\nNotIn—Label’s value must not match any of the specified values.\nExists—Pod must include a label with the specified key (the value isn’t import-\nant). When using this operator, you shouldn’t specify the \nvalues field.\nDoesNotExist—Pod must not include a label with the specified key. The values\nproperty must not be specified.\nIf you specify multiple expressions, all those expressions must evaluate to true for the\nselector to match a pod. If you specify both \nmatchLabels and matchExpressions, all\nthe  labels  must  match  and  all  the  expressions  must  evaluate  to  true  for  the  pod  to\nmatch the selector.\nListing 4.9   A matchExpressions selector: kubia-replicaset-matchexpressions.yaml\nThis selector requires the pod to \ncontain a label with the “app” key.\nThe label’s value \nmust be “kubia”.\n \n\n108CHAPTER 4Replication and other controllers: deploying managed pods\n4.3.5Wrapping up ReplicaSets\nThis was a quick introduction to ReplicaSets as an alternative to ReplicationControllers.\nRemember, always use them instead of ReplicationControllers, but you may still find\nReplicationControllers in other people’s deployments.\n  Now,  delete  the  ReplicaSet  to  clean  up  your  cluster  a  little.  You  can  delete  the\nReplicaSet the same way you’d delete a ReplicationController:\n$ kubectl delete rs kubia\nreplicaset \"kubia\" deleted\nDeleting the ReplicaSet should delete all the pods. List the pods to confirm that’s\nthe case. \n4.4Running exactly one pod on each node with \nDaemonSets\nBoth ReplicationControllers and ReplicaSets are used for running a specific number\nof pods deployed anywhere in the Kubernetes cluster. But certain cases exist when you\nwant a pod to run on each and every node in the cluster (and each node needs to run\nexactly one instance of the pod, as shown in figure 4.8).\n  Those  cases  include  infrastructure-related  pods  that  perform  system-level  opera-\ntions. For example, you’ll want to run a log collector and a resource monitor on every\nnode. Another good example is Kubernetes’ own kube-proxy process, which needs to\nrun on all nodes to make services work.\nNode 1\nPod\nPod\nPod\nReplicaSet\nReplicas: 5\nNode 2\nPod\nPod\nNode 3\nPod\nDaemonSet\nExactly one replica\non each node\nNode 4\nPod\nPod\nPod\nFigure 4.8   DaemonSets run only a single pod replica on each node, whereas ReplicaSets \nscatter them around the whole cluster randomly. \n \n\n109Running exactly one pod on each node with DaemonSets\nOutside  of  Kubernetes,  such  processes  would  usually  be  started  through  system  init\nscripts or the systemd daemon during node boot up. On Kubernetes nodes, you can\nstill use systemd to run your system processes, but then you can’t take advantage of all\nthe features Kubernetes provides. \n4.4.1Using a DaemonSet to run a pod on every node\nTo  run  a  pod  on  all  cluster  nodes,  you  create  a  DaemonSet  object,  which  is  much\nlike a ReplicationController or a ReplicaSet, except that pods created by a Daemon-\nSet  already  have  a  target  node  specified  and  skip  the  Kubernetes  Scheduler.  They\naren’t scattered around the cluster randomly. \n A DaemonSet makes sure it creates as many pods as there are nodes and deploys\neach one on its own node, as shown in figure 4.8.\n Whereas a ReplicaSet (or ReplicationController) makes sure that a desired num-\nber  of  pod  replicas  exist  in  the  cluster,  a  DaemonSet  doesn’t  have  any  notion  of  a\ndesired replica count. It doesn’t need it because its job is to ensure that a pod match-\ning its pod selector is running on each node. \n  If  a  node  goes  down,  the  DaemonSet  doesn’t  cause  the  pod  to  be  created  else-\nwhere.  But  when  a  new  node  is  added  to  the  cluster,  the  DaemonSet  immediately\ndeploys  a  new  pod  instance  to  it.  It  also  does  the  same  if  someone  inadvertently\ndeletes one of the pods, leaving the node without the DaemonSet’s pod. Like a Replica-\nSet, a DaemonSet creates the pod from the pod template configured in it.\n4.4.2Using a DaemonSet to run pods only on certain nodes\nA DaemonSet deploys pods to all nodes in the cluster, unless you specify that the pods\nshould only run on a subset of all the nodes. This is done by specifying the \nnode-\nSelector\n  property  in  the  pod  template,  which  is  part  of  the  DaemonSet  definition\n(similar to the pod template in a ReplicaSet or ReplicationController). \n You’ve already used node selectors to deploy a pod onto specific nodes in chapter 3.\nA node selector in a DaemonSet is similar—it defines the nodes the DaemonSet must\ndeploy its pods to. \nNOTELater in the book, you’ll learn that nodes can be made unschedulable,\npreventing pods from being deployed to them. A DaemonSet will deploy pods\neven to such nodes, because the unschedulable attribute is only used by the\nScheduler,  whereas  pods  managed  by  a  DaemonSet  bypass  the  Scheduler\ncompletely.  This  is  usually  desirable,  because  DaemonSets  are  meant  to  run\nsystem services, which usually need to run even on unschedulable nodes.\nEXPLAINING DAEMONSETS WITH AN EXAMPLE\nLet’s  imagine  having  a  daemon  called  ssd-monitor  that  needs  to  run  on  all  nodes\nthat  contain  a  solid-state  drive  (SSD).  You’ll  create  a  DaemonSet  that  runs  this  dae-\nmon on all nodes that are marked as having an SSD. The cluster administrators have\nadded  the  \ndisk=ssd  label  to  all  such  nodes,  so  you’ll  create  the  DaemonSet  with  a\nnode selector that only selects nodes with that label, as shown in figure 4.9.\n \n\n110CHAPTER 4Replication and other controllers: deploying managed pods\nCREATING A DAEMONSET YAML DEFINITION\nYou’ll  create  a  DaemonSet  that  runs  a  mock  ssd-monitor  process,  which  prints\n“SSD OK” to the standard output every five seconds. I’ve already prepared the mock\ncontainer image and pushed it to Docker Hub, so you can use it instead of building\nyour own. Create the YAML for the DaemonSet, as shown in the following listing.\napiVersion: apps/v1beta2      \nkind: DaemonSet                     \nmetadata:\n  name: ssd-monitor\nspec:                            \n  selector:\n    matchLabels:\n      app: ssd-monitor\n  template:\n    metadata:\n      labels:\n        app: ssd-monitor\n    spec:\n      nodeSelector:                \n        disk: ssd                  \n      containers:\n      - name: main\n        image: luksa/ssd-monitor\nYou’re defining a DaemonSet that will run a pod with a single container based on the\nluksa/ssd-monitor container image. An instance of this pod will be created for each\nnode that has the \ndisk=ssd label.\nListing 4.10   A YAML for a DaemonSet: ssd-monitor-daemonset.yaml\nNode 1\nPod:\nssd-monitor\nNode 2Node 3\nDaemonSet:\nsssd-monitor\nNode selector:\ndisk=ssd\nNode 4\ndisk: ssddisk: ssddisk: ssd\nUnschedulable\nPod:\nssd-monitor\nPod:\nssd-monitor\nFigure 4.9   Using a DaemonSet with a node selector to deploy system pods only on certain \nnodes\nDaemonSets are in the \napps API group, \nversion v1beta2.\nThe pod template includes a \nnode selector, which selects \nnodes with the disk=ssd label.\n \n\n111Running exactly one pod on each node with DaemonSets\nCREATING THE DAEMONSET\nYou’ll create the DaemonSet like you always create resources from a YAML file:\n$ kubectl create -f ssd-monitor-daemonset.yaml\ndaemonset \"ssd-monitor\" created\nLet’s see the created DaemonSet:\n$ kubectl get ds\nNAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE-SELECTOR  \nssd-monitor   0        0        0      0           0          disk=ssd\nThose zeroes look strange. Didn’t the DaemonSet deploy any pods? List the pods:\n$ kubectl get po\nNo resources found.\nWhere are the pods? Do you know what’s going on? Yes, you forgot to label your nodes\nwith the \ndisk=ssd label. No problem—you can do that now. The DaemonSet should\ndetect  that  the  nodes’  labels  have  changed  and  deploy  the  pod  to  all  nodes  with  a\nmatching label. Let’s see if that’s true. \nADDING THE REQUIRED LABEL TO YOUR NODE(S)\nRegardless if you’re using Minikube, GKE, or another multi-node cluster, you’ll need\nto list the nodes first, because you’ll need to know the node’s name when labeling it:\n$ kubectl get node\nNAME       STATUS    AGE       VERSION\nminikube   Ready     4d        v1.6.0\nNow, add the disk=ssd label to one of your nodes like this:\n$ kubectl label node minikube disk=ssd\nnode \"minikube\" labeled\nNOTEReplace minikube with the name of one of your nodes if you’re not\nusing Minikube.\nThe DaemonSet should have created one pod now. Let’s see:\n$ kubectl get po\nNAME                READY     STATUS    RESTARTS   AGE\nssd-monitor-hgxwq   1/1       Running   0          35s\nOkay; so far so good. If you have multiple nodes and you add the same label to further\nnodes, you’ll see the DaemonSet spin up pods for each of them. \nREMOVING THE REQUIRED LABEL FROM THE NODE\nNow, imagine you’ve made a mistake and have mislabeled one of the nodes. It has a\nspinning disk drive, not an SSD. What happens if you change the node’s label?\n$ kubectl label node minikube disk=hdd --overwrite\nnode \"minikube\" labeled\n \n\n112CHAPTER 4Replication and other controllers: deploying managed pods\nLet’s see if the change has any effect on the pod that was running on that node:\n$ kubectl get po\nNAME                READY     STATUS        RESTARTS   AGE\nssd-monitor-hgxwq   1/1       Terminating   0          4m\nThe  pod  is  being  terminated.  But  you  knew  that  was  going  to  happen,  right?  This\nwraps up your exploration of DaemonSets, so you may want to delete your \nssd-monitor\nDaemonSet. If you still have any other daemon pods running, you’ll see that deleting\nthe DaemonSet deletes those pods as well. \n4.5Running pods that perform a single completable task \nUp to now, we’ve only talked about pods than need to run continuously. You’ll have\ncases  where  you  only  want  to  run  a  task  that  terminates  after  completing  its  work.\nReplicationControllers,  ReplicaSets,  and  DaemonSets  run  continuous  tasks  that  are\nnever considered completed. Processes in such pods are restarted when they exit. But\nin a completable task, after its process terminates, it should not be restarted again. \n4.5.1Introducing the Job resource\nKubernetes includes support for this through the Job resource, which is similar to the\nother resources we’ve discussed in this chapter, but it allows you to run a pod whose\ncontainer isn’t restarted when the process running inside finishes successfully. Once it\ndoes, the pod is considered complete. \n In the event of a node failure, the pods on that node that are managed by a Job will\nbe rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of\nthe process itself (when the process returns an error exit code), the Job can be config-\nured to either restart the container or not.\n Figure 4.10 shows how a pod created by a Job is rescheduled to a new node if the\nnode  it  was  initially  scheduled  to  fails.  The  figure  also  shows  both  a  managed  pod,\nwhich isn’t rescheduled, and a pod backed by a ReplicaSet, which is.\n For example, Jobs are useful for ad hoc tasks, where it’s crucial that the task fin-\nishes properly. You could run the task in an unmanaged pod and wait for it to finish,\nbut in the event of a node failing or the pod being evicted from the node while it is\nperforming its task, you’d need to manually recreate it. Doing this manually doesn’t\nmake sense—especially if the job takes hours to complete. \n An example of such a job would be if  you  had  data  stored  somewhere  and  you\nneeded to transform and export it somewhere. You’re going to emulate this by run-\nning a container image built on top of the \nbusybox image, which invokes the sleep\ncommand for two minutes. I’ve already built the image and pushed it to Docker Hub,\nbut you can peek into its Dockerfile in the book’s code archive.\n \n\n113Running pods that perform a single completable task\n4.5.2Defining a Job resource\nCreate the Job manifest as in the following listing.\napiVersion: batch/v1        \nkind: Job                   \nmetadata:\n  name: batch-job\nspec:                                \n  template: \n    metadata:\n      labels:                        \n        app: batch-job               \n    spec:\n      restartPolicy: OnFailure         \n      containers:\n      - name: main\n        image: luksa/batch-job\nJobs are part of the batch  API  group  and  v1  API  version.  The  YAML  defines  a\nresource  of  type  Job  that  will  run  the  \nluksa/batch-job  image,  which  invokes  a  pro-\ncess that runs for exactly 120 seconds and then exits. \n  In  a  pod’s  specification,  you  can  specify  what  Kubernetes  should  do  when  the\nprocesses running in the container finish. This is done through the \nrestartPolicy\nListing 4.11   A YAML definition of a Job: exporter.yaml\nNode 1\nPod A (unmanaged)\nPod B (managed by a ReplicaSet)\nPod C (managed by a Job)\nNode 2\nNode 1 failsJob C2 finishes\nTime\nPod B2 (managed by a ReplicaSet)\nPod C2 (managed by a Job)\nPod A isn’t rescheduled,\nbecause there is nothing\nmanaging it.\nFigure 4.10   Pods managed by Jobs are rescheduled until they finish successfully.\nJobs are in the batch \nAPI group, version v1.\nYou’re not specifying a pod \nselector (it will be created \nbased on the labels in the \npod template).\nJobs can’t use the \ndefault restart policy, \nwhich is Always.\n \n\n114CHAPTER 4Replication and other controllers: deploying managed pods\npod spec property, which defaults to Always. Job pods can’t use the default policy,\nbecause they’re not meant to run indefinitely. Therefore, you need to explicitly set\nthe restart policy to either \nOnFailure or Never. This setting is what prevents the con-\ntainer from being restarted when it finishes (not the fact that the pod is being man-\naged by a Job resource).\n4.5.3Seeing a Job run a pod\nAfter you create this Job with the kubectl create command, you should see it start up\na pod immediately:\n$ kubectl get jobs\nNAME        DESIRED   SUCCESSFUL   AGE\nbatch-job   1         0            2s\n$ kubectl get po\nNAME              READY     STATUS    RESTARTS   AGE\nbatch-job-28qf4   1/1       Running   0          4s\nAfter the two minutes have passed, the pod will no longer show up in the pod list and\nthe Job will be marked as completed. By default, completed pods aren’t shown when\nyou list pods, unless you use the \n--show-all (or -a) switch:\n$ kubectl get po -a\nNAME              READY     STATUS      RESTARTS   AGE\nbatch-job-28qf4   0/1       Completed   0          2m\nThe reason the pod isn’t deleted when it completes is to allow you to examine its logs;\nfor example:\n$ kubectl logs batch-job-28qf4\nFri Apr 29 09:58:22 UTC 2016 Batch job starting\nFri Apr 29 10:00:22 UTC 2016 Finished succesfully\nThe pod will be deleted when you delete it or the Job that created it. Before you do\nthat, let’s look at the Job resource again:\n$ kubectl get job\nNAME        DESIRED   SUCCESSFUL   AGE\nbatch-job   1         1            9m\nThe Job is shown as having completed successfully. But why is that piece of informa-\ntion shown as a number instead of as \nyes or true? And what does the DESIRED column\nindicate? \n4.5.4Running multiple pod instances in a Job\nJobs may be configured to create more than one pod instance and run them in paral-\nlel or sequentially. This is done by setting the \ncompletions and the parallelism prop-\nerties in the Job spec.\n \n\n115Running pods that perform a single completable task\nRUNNING JOB PODS SEQUENTIALLY\nIf you need a Job to run more than once, you set completions to how many times you\nwant the Job’s pod to run. The following listing shows an example.\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: multi-completion-batch-job\nspec:\n  completions: 5                  \n  template:\n    <template is the same as in listing 4.11>\nThis Job will run five pods one after the other. It initially creates one pod, and when\nthe pod’s container finishes, it creates the second pod, and so on, until five pods com-\nplete successfully. If one of the pods fails, the Job creates a new pod, so the Job may\ncreate more than five pods overall.\nRUNNING JOB PODS IN PARALLEL\nInstead of running single Job pods one after the other, you can also make the Job run\nmultiple  pods  in  parallel.  You  specify  how  many  pods  are  allowed  to  run  in  parallel\nwith the \nparallelism  Job spec property, as shown in the following listing.\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: multi-completion-batch-job\nspec:\n  completions: 5                    \n  parallelism: 2                    \n  template:\n    <same as in listing 4.11>\nBy setting parallelism to 2, the Job creates two pods and runs them in parallel:\n$ kubectl get po\nNAME                               READY   STATUS     RESTARTS   AGE\nmulti-completion-batch-job-lmmnk   1/1     Running    0          21s\nmulti-completion-batch-job-qx4nq   1/1     Running    0          21s\nAs soon as one of them finishes, the Job will run the next pod, until five pods finish\nsuccessfully.\nListing 4.12   A Job requiring multiple completions: multi-completion-batch-job.yaml\nListing 4.13   Running Job pods in parallel: multi-completion-parallel-batch-job.yaml\nSetting completions to \n5 makes this Job run \nfive pods sequentially.\nThis job must ensure \nfive pods complete \nsuccessfully.\nUp to two pods \ncan run in parallel.\n \n\n116CHAPTER 4Replication and other controllers: deploying managed pods\nSCALING A JOB\nYou can even change a Job’s parallelism property while the Job is running. This is\nsimilar  to  scaling  a  ReplicaSet  or  ReplicationController,  and  can  be  done  with  the\nkubectl scale command:\n$ kubectl scale job multi-completion-batch-job --replicas 3\njob \"multi-completion-batch-job\" scaled\nBecause you’ve increased parallelism from 2 to 3, another pod is immediately spun\nup, so three pods are now running.\n4.5.5Limiting the time allowed for a Job pod to complete\nWe need to discuss one final thing about Jobs. How long should the Job wait for a pod\nto  finish?  What  if  the  pod  gets  stuck  and  can’t  finish  at  all  (or  it  can’t  finish  fast\nenough)?\n A pod’s time can be limited by setting the \nactiveDeadlineSeconds property in the\npod spec. If the pod runs longer than that, the system will try to terminate it and will\nmark the Job as failed. \nNOTEYou  can  configure  how  many  times  a  Job  can  be  retried  before  it  is\nmarked as failed by specifying the \nspec.backoffLimit field in the Job mani-\nfest. If you don't explicitly specify it, it defaults to 6.\n4.6Scheduling Jobs to run periodically or once \nin the future\nJob resources run their pods immediately when you create the Job resource. But many\nbatch jobs need to be run at a specific time in the future or repeatedly in the specified\ninterval.  In  Linux-  and  UNIX-like  operating  systems,  these  jobs  are  better  known  as\ncron jobs. Kubernetes supports them, too.\n  A  cron  job  in  Kubernetes  is  configured  by  creating  a  CronJob  resource.  The\nschedule for running the job is specified in the well-known cron format, so if you’re\nfamiliar with regular cron jobs, you’ll understand Kubernetes’ CronJobs in a matter\nof seconds.\n At the configured time, Kubernetes will create a Job resource according to the Job\ntemplate configured in the CronJob object. When the Job resource is created, one or\nmore pod replicas will be created and started according to the Job’s pod template, as\nyou learned in the previous section. There’s nothing more to it.\n Let’s look at how to create CronJobs. \n4.6.1Creating a CronJob\nImagine you need to run the batch job from your previous example every 15 minutes.\nTo do that, create a CronJob resource with the following specification.\n \n \n\n117Scheduling Jobs to run periodically or once in the future\napiVersion: batch/v1beta1               \nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  schedule: \"0,15,30,45 * * * *\"           \n  jobTemplate:\n    spec:\n      template:                            \n        metadata:                          \n          labels:                          \n            app: periodic-batch-job        \n        spec:                              \n          restartPolicy: OnFailure         \n          containers:                      \n          - name: main                     \n            image: luksa/batch-job         \nAs you can see, it’s not too complicated. You’ve specified a schedule and a template\nfrom which the Job objects will be created. \nCONFIGURING THE SCHEDULE\nIf  you’re  unfamiliar  with  the  cron  schedule  format,  you’ll  find  great  tutorials  and\nexplanations online, but as a quick introduction, from left to right, the schedule con-\ntains the following five entries:\nMinute\nHour\nDay of month\nMonth\nDay of week.\nIn the example, you want to run the job every 15 minutes, so the schedule needs to be\n\"0,15,30,45 * * * *\", which means at the 0, 15, 30 and 45 minutes mark of every hour\n(first  asterisk),  of  every  day  of  the  month  (second  asterisk),  of  every  month  (third\nasterisk) and on every day of the week (fourth asterisk). \n If, instead, you wanted it to run every 30 minutes, but only on the first day of the\nmonth, you’d set the schedule to \n\"0,30 * 1 * *\", and if you want it to run at 3AM every\nSunday, you’d set it to \n\"0 3 * * 0\" (the last zero stands for Sunday).\nCONFIGURING THE JOB TEMPLATE\nA  CronJob  creates  Job  resources  from  the  jobTemplate  property  configured  in  the\nCronJob spec, so refer to section 4.5 for more information on how to configure it.\n4.6.2Understanding how scheduled jobs are run\nJob resources will be created from the CronJob resource at approximately the sched-\nuled time. The Job then creates the pods. \nListing 4.14   YAML for a CronJob resource: cronjob.yaml\nAPI group is batch, \nversion is v1beta1\nThis job should run at the \n0, 15, 30 and 45 minutes of \nevery hour, every day.\nThe template for the \nJob resources that \nwill be created by \nthis CronJob\n \n\n118CHAPTER 4Replication and other controllers: deploying managed pods\n It may happen that the Job or pod is created and run relatively late. You may have\na hard requirement for the job to not be started too far over the scheduled time. In\nthat case, you can specify a deadline by specifying the \nstartingDeadlineSeconds field\nin the CronJob specification as shown in the following listing.\napiVersion: batch/v1beta1\nkind: CronJob\nspec:\n  schedule: \"0,15,30,45 * * * *\"\n  startingDeadlineSeconds: 15    \n  ...\nIn the example in listing 4.15, one of the times the job is supposed to run is 10:30:00.\nIf  it  doesn’t  start  by  10:30:15  for  whatever  reason,  the  job  will  not  run  and  will  be\nshown as Failed. \n In normal circumstances, a CronJob always creates only a single Job for each exe-\ncution configured in the schedule, but it may happen that two Jobs are created at the\nsame time, or none at all. To combat the first problem, your jobs should be idempo-\ntent  (running  them  multiple  times  instead  of  once  shouldn’t  lead  to  unwanted\nresults). For the second problem, make sure that the next job run performs any work\nthat should have been done by the previous (missed) run.\n4.7Summary\nYou’ve  now  learned  how  to  keep  pods  running  and  have  them  rescheduled  in  the\nevent of node failures. You should now know that\nYou can specify a liveness probe to have Kubernetes restart your container as\nsoon  as  it’s  no  longer  healthy  (where  the  app  defines  what’s  considered\nhealthy).\nPods shouldn’t be created directly, because they will not be re-created if they’re\ndeleted  by  mistake,  if  the  node  they’re  running  on  fails,  or  if  they’re  evicted\nfrom the node.\nReplicationControllers  always  keep  the  desired  number  of  pod  replicas\nrunning.\nScaling pods horizontally is as easy as changing the desired replica count on a\nReplicationController.\nPods  aren’t  owned  by  the  ReplicationControllers  and  can  be  moved  between\nthem if necessary.\nA ReplicationController creates new pods from a pod template. Changing the\ntemplate has no effect on existing pods.\nListing 4.15   Specifying a startingDeadlineSeconds for a CronJob\nAt the latest, the pod must \nstart running at 15 seconds \npast the scheduled time.\n \n\n119Summary\nReplicationControllers  should  be  replaced  with  ReplicaSets  and  Deployments,\nwhich provide the same functionality, but with additional powerful features.\nReplicationControllers and ReplicaSets schedule pods to random cluster nodes,\nwhereas  DaemonSets  make  sure  every  node  runs  a  single  instance  of  a  pod\ndefined in the DaemonSet.\nPods  that  perform  a  batch  task  should  be  created  through  a  Kubernetes  Job\nresource, not directly or through a ReplicationController or similar object.\nJobs that need to run sometime in the future can be created through CronJob\nresources. \n \n\n120\nServices: enabling\nclients to discover\nand talk to pods\nYou’ve learned about pods and how to deploy them through ReplicaSets and similar\nresources  to  ensure  they  keep  running.  Although  certain  pods  can  do  their  work\nindependently of an external stimulus, many applications these days are meant to\nrespond to external requests. For example, in the case of microservices, pods will\nusually respond to HTTP requests coming either from other pods inside the cluster\nor from clients outside the cluster. \n Pods need a way of finding other pods if they want to consume the services they\nprovide.  Unlike  in  the  non-Kubernetes  world,  where  a  sysadmin  would  configure\nThis chapter covers\nCreating Service resources to expose a group of \npods at a single address\nDiscovering services in the cluster\nExposing services to external clients\nConnecting to external services from inside the \ncluster\nControlling whether a pod is ready to be part of \nthe service or not\nTroubleshooting services\n \n\n121Introducing services\neach client app by specifying the exact IP address or hostname of the server providing\nthe service in the client’s configuration files, doing the same in Kubernetes wouldn’t\nwork, because\nPods are ephemeral—They may come and go at any time, whether it’s because a\npod  is  removed  from  a  node  to  make  room  for  other  pods,  because  someone\nscaled down the number of pods, or because a cluster node has failed.\nKubernetes assigns an IP address to a pod after the pod has been scheduled to a node\nand before it’s started—Clients thus can’t know the IP address of the server pod\nup front.\nHorizontal  scaling  means  multiple  pods  may  provide  the  same  service—Each  of  those\npods has its own IP address. Clients shouldn’t care how many pods are backing\nthe service and what their IPs are. They shouldn’t have to keep a list of all the\nindividual  IPs  of  pods.  Instead,  all  those  pods  should  be  accessible  through  a\nsingle IP address.\nTo  solve  these  problems,  Kubernetes  also  provides  another  resource  type—Services—\nthat we’ll discuss in this chapter.\n5.1Introducing services\nA  Kubernetes  Service  is  a  resource  you  create  to  make  a  single,  constant  point  of\nentry to a group of pods providing the same service. Each service has an IP address\nand port that never change while the service exists. Clients can open connections to\nthat IP and port, and those connections are then routed to one of the pods backing\nthat service. This way, clients of a service don’t need to know the location of individ-\nual pods providing the service, allowing those pods to be moved around the cluster\nat any time. \nEXPLAINING SERVICES WITH AN EXAMPLE\nLet’s revisit the example where you have a frontend web server and a backend data-\nbase  server.  There  may  be  multiple  pods  that  all  act  as  the  frontend,  but  there  may\nonly be a single backend database pod. You need to solve two problems to make the\nsystem function:\nExternal clients need to connect to the frontend pods without caring if there’s\nonly a single web server or hundreds.\nThe frontend pods need to connect to the backend database. Because the data-\nbase runs inside a pod, it may be moved around the cluster over time, causing\nits IP address to change. You don’t want to reconfigure the frontend pods every\ntime the backend database is moved.\nBy  creating  a  service  for  the  frontend  pods  and  configuring  it  to  be  accessible  from\noutside the cluster, you expose a single, constant IP address through which external\nclients can connect to the pods. Similarly, by also creating a service for the backend\npod,  you  create  a  stable  address  for  the  backend  pod.  The  service  address  doesn’t\n \n\n122CHAPTER 5Services: enabling clients to discover and talk to pods\nchange even if the pod’s IP address changes. Additionally, by creating the service, you\nalso enable the frontend pods to easily find the backend service by its name through\neither environment variables or DNS. All the components of your system (the two ser-\nvices, the two sets of pods backing those services, and the interdependencies between\nthem) are shown in figure 5.1.\nYou now understand the basic idea behind services. Now, let’s dig deeper by first see-\ning how they can be created.\n5.1.1Creating services\nAs you’ve seen, a service can be backed by more than one pod. Connections to the ser-\nvice  are  load-balanced  across  all  the  backing  pods.  But  how  exactly  do  you  define\nwhich pods are part of the service and which aren’t? \n  You  probably  remember  label  selectors  and  how  they’re  used  in  Replication-\nControllers and other pod controllers to specify which pods belong to the same set.\nThe same mechanism is used by services in the same way, as you can see in figure 5.2.\n In the previous chapter, you created a ReplicationController which then ran three\ninstances  of  the  pod  containing  the  Node.js  app.  Create  the  ReplicationController\nagain  and  verify  three  pod  instances  are  up  and  running.  After  that,  you’ll  create  a\nService for those three pods. \nFrontend pod 1\nIP: 2.1.1.1\nExternal client\nFrontend pod 2\nIP: 2.1.1.2\nFrontend pod 3\nIP: 2.1.1.3\nBackend pod\nIP: 2.1.1.4\nFrontend service\nIP: 1.1.1.1\nBackend service\nIP: 1.1.1.2\nFrontend components\nBackend components\nFigure 5.1   Both internal and external clients usually connect to pods through services.\n \n\n123Introducing services\nCREATING A SERVICE THROUGH KUBECTL EXPOSE\nThe easiest way to create a service is through kubectl expose, which you’ve already\nused  in  chapter  2  to  expose  the  ReplicationController  you  created  earlier.  The\nexpose command created a Service resource with the same pod selector as the one\nused by the ReplicationController, thereby exposing all its pods through a single IP\naddress and port. \n  Now,  instead  of  using  the  \nexpose  command,  you’ll  create  a  service  manually  by\nposting a YAML to the Kubernetes API server. \nCREATING A SERVICE THROUGH A YAML DESCRIPTOR\nCreate a file called kubia-svc.yaml with the following listing’s contents.\napiVersion: v1\nkind: Service             \nmetadata:\n  name: kubia              \nspec:\n  ports:\n  - port: 80              \n    targetPort: 8080       \n  selector:                 \n    app: kubia              \nYou’re defining a service called kubia, which will accept connections on port 80 and\nroute  each  connection  to  port  8080  of  one  of  the  pods  matching  the  \napp=kubia\nlabel selector. \n Go ahead and create the service by posting the file using \nkubectl create.\nListing 5.1   A definition of a service: kubia-svc.yaml\napp: kubia\nPod: kubia-q3vkg\nPod: kubia-k0xz6\nPod: kubia-53thy\nClient\nService: kubia\nSelector:app=kubia\napp: kubia\napp: kubia\nFigure 5.2   Label selectors \ndetermine which pods belong \nto the Service.\nThe port this service \nwill be available on\nThe container port the \nservice will forward to\nAll pods with the app=kubia \nlabel will be part of this service.\n \n\n124CHAPTER 5Services: enabling clients to discover and talk to pods\nEXAMINING YOUR NEW SERVICE\nAfter posting the YAML, you can list all Service resources in your namespace and see\nthat an internal cluster IP has been assigned to your service:\n$ kubectl get svc\nNAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\nkubernetes   10.111.240.1     <none>        443/TCP   30d\nkubia        10.111.249.153   <none>        80/TCP    6m     \nThe  list  shows  that  the  IP  address  assigned  to  the  service  is  10.111.249.153.  Because\nthis is the cluster IP, it’s only accessible from inside the cluster. The primary purpose\nof services is exposing groups of pods to other pods in the cluster, but you’ll usually\nalso want to expose services externally. You’ll see how to do that later. For now, let’s\nuse your service from inside the cluster and see what it does.\nTESTING YOUR SERVICE FROM WITHIN THE CLUSTER\nYou can send requests to your service from within the cluster in a few ways:\nThe obvious way is to create a pod that will send the request to the service’s\ncluster  IP  and  log  the  response.  You  can  then  examine  the  pod’s  log  to  see\nwhat the service’s response was.\nYou can ssh into one of the Kubernetes nodes and use the curl command.\nYou can execute the curl command inside one of your existing pods through\nthe \nkubectl exec command.\nLet’s go for the last option, so you also learn how to run commands in existing pods. \nREMOTELY EXECUTING COMMANDS IN RUNNING CONTAINERS\nThe kubectl exec  command  allows  you  to  remotely  run  arbitrary  commands  inside\nan existing container of a pod. This comes in handy when you want to examine the\ncontents,  state,  and/or  environment  of  a  container.  List  the  pods  with  the  \nkubectl\nget\n pods command and choose one as your target for the exec command (in the fol-\nlowing example, I’ve chosen the \nkubia-7nog1 pod as the target). You’ll also need to\nobtain the cluster IP of your service (using \nkubectl get svc, for example). When run-\nning the following commands yourself, be sure to replace the pod name and the ser-\nvice IP with your own: \n$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153\nYou’ve hit kubia-gzwli\nIf you’ve used ssh to execute commands on a remote system before, you’ll recognize\nthat \nkubectl exec isn’t much different.\n \n \n \n \nHere’s your \nservice.\n \n\n125Introducing services\nLet’s  go  over  what  transpired  when  you  ran  the  command.  Figure  5.3  shows  the\nsequence of events. You instructed Kubernetes to execute the \ncurl command inside the\ncontainer  of  one  of  your  pods.  Curl  sent  an  HTTP  request  to  the  service  IP,  which  is\nbacked  by  three  pods.  The  Kubernetes  service  proxy  intercepted  the  connection,\nselected a random pod among the three pods, and forwarded the request to it. Node.js\nrunning inside that pod then handled the request and returned an HTTP response con-\ntaining the pod’s name. Curl then printed the response to the standard output, which\nwas intercepted and printed to its standard output on your local machine by \nkubectl.\nWhy the double dash?\nThe  double  dash  (--)  in  the  command  signals  the  end  of  command  options  for\nkubectl. Everything after the double dash is the command that should be executed\ninside  the  \npod.  Using  the  double  dash  isn’t  necessary  if  the  command  has  no\narguments that start with a dash. But in your case, if you don’t use the double dash\nthere, the \n-s option would be interpreted as an option for kubectl exec and would\nresult in the following strange and highly misleading error:\n$ kubectl exec kubia-7nog1 curl -s http://10.111.249.153\nThe connection to the server 10.111.249.153 was refused – did you \nspecify the right host or port?\nThis  has  nothing  to  do  with  your  service  refusing  the  connection.  It’s  because\nkubectl is not able to connect to an API server at 10.111.249.153 (the -s option\nis used to tell \nkubectl to connect to a different API server than the default).\n3. Curl sends HTTP\nGET request\n4. Service redirects HTTP\nconnection to a randomly\nselected pod\n2. Curl is executed\ninside the container\nrunning node.js\n6. The output of the\ncommand is sentcurl\nback to kubectl and\nprinted by it\n5. HTTP response is\nsent back to curl\nPod: kubia-7nog1\nContainer\nnode.js\ncurl http://\n10.111.249.153\nPod: kubia-gzwli\nContainer\nnode.js\nPod: kubia-5fje3\nContainer\nnode.js\n1.kubectl exec\nService: kubia\n10.111.249.153:80\nFigure 5.3   Using kubectl exec to test out a connection to the service by running curl in one of the pods\n \n\n126CHAPTER 5Services: enabling clients to discover and talk to pods\nIn the previous example, you executed the curl command as a separate process, but\ninside the pod’s main container. This isn’t much different from the actual main pro-\ncess in the container talking to the service.\nCONFIGURING SESSION AFFINITY ON THE SERVICE\nIf you execute the same command a few more times, you should hit a different pod\nwith  every  invocation,  because  the  service  proxy  normally  forwards  each  connection\nto  a  randomly  selected  backing  pod,  even  if  the  connections  are  coming  from  the\nsame client. \n  If,  on  the  other  hand,  you  want  all  requests  made  by  a  certain  client  to  be  redi-\nrected to the same pod every time, you can set the service’s \nsessionAffinity property\nto \nClientIP (instead of None, which is the default), as shown in the following listing.\napiVersion: v1\nkind: Service             \nspec:\n  sessionAffinity: ClientIP\n  ...\nThis makes the service proxy redirect all requests originating from the same client IP\nto the same pod. As an exercise, you can create an additional service with session affin-\nity set to \nClientIP and try sending requests to it.\n Kubernetes supports only two types of service session affinity: \nNone and ClientIP.\nYou  may  be  surprised  it  doesn’t  have  a  cookie-based  session  affinity  option,  but  you\nneed to understand that Kubernetes services don’t operate at the HTTP level. Services\ndeal with TCP and UDP packets and don’t care about the payload they carry. Because\ncookies are a construct of the HTTP protocol, services don’t know about them, which\nexplains why session affinity cannot be based on cookies. \nEXPOSING MULTIPLE PORTS IN THE SAME SERVICE\nYour service exposes only a single port, but services can also support multiple ports. For\nexample,  if  your  pods  listened  on  two  ports—let’s  say  8080  for  HTTP  and  8443  for\nHTTPS—you  could  use  a  single  service  to  forward  both  port  80  and  443  to  the  pod’s\nports 8080 and 8443. You don’t need to create two different services in such cases. Using\na single, multi-port service exposes all the service’s ports through a single cluster IP.\nNOTEWhen creating a service with multiple ports, you must specify a name\nfor each port.\nThe spec for a multi-port service is shown in the following listing.\napiVersion: v1\nkind: Service             \nmetadata:\n  name: kubia              \nListing 5.2   A example of a service with ClientIP session affinity configured\nListing 5.3   Specifying multiple ports in a service definition\n \n\n127Introducing services\nspec:\n  ports:\n  - name: http              \n    port: 80                \n    targetPort: 8080        \n  - name: https             \n    port: 443               \n    targetPort: 8443        \n  selector:                 \n    app: kubia              \nNOTEThe label selector applies to the service as a whole—it can’t be config-\nured for each port individually. If you want different ports to map to different\nsubsets of pods, you need to create two services.\nBecause your \nkubia pods don’t listen on multiple ports, creating a multi-port service\nand a multi-port pod is left as an exercise to you.\nUSING NAMED PORTS\nIn all these examples, you’ve referred to the target port by its number, but you can also\ngive a name to each pod’s port and refer to it by name in the service spec. This makes\nthe service spec slightly clearer, especially if the port numbers aren’t well-known.\n For example, suppose your pod defines names for its ports as shown in the follow-\ning listing.\nkind: Pod\nspec:\n  containers:\n  - name: kubia\n    ports:\n    - name: http               \n      containerPort: 8080      \n    - name: https              \n      containerPort: 8443      \nYou can then refer to those ports by name in the service spec, as shown in the follow-\ning listing.\napiVersion: v1\nkind: Service             \nspec:\n  ports:\n  - name: http              \n    port: 80                \n    targetPort: http        \n  - name: https             \n    port: 443               \n    targetPort: https       \nListing 5.4   Specifying port names in a pod definition\nListing 5.5   Referring to named ports in a service\nPort 80 is mapped to \nthe pods’ port 8080.\nPort 443 is mapped to \npods’ port 8443.\nThe label selector always \napplies to the whole service.\nContainer’s port \n8080 is called http\nPort 8443 is called https.\nPort 80 is mapped to the \ncontainer’s port called http.\nPort 443 is mapped to the container’s \nport, whose name is https.\n \n\n128CHAPTER 5Services: enabling clients to discover and talk to pods\nBut why should you even bother with naming ports? The biggest benefit of doing so is\nthat it enables you to change port numbers later without having to change the service\nspec.  Your  pod  currently  uses  port  8080  for  http,  but  what  if  you  later  decide  you’d\nlike to move that to port 80? \n If you’re using named ports, all you need to do is change the port number in the\npod spec (while keeping the port’s name unchanged). As you spin up pods with the\nnew  ports,  client  connections  will  be  forwarded  to  the  appropriate  port  numbers,\ndepending on the pod receiving the connection (port 8080 on old pods and port 80\non the new ones).\n5.1.2Discovering services\nBy creating a service, you now have a single and stable IP address and port that you\ncan  hit  to  access  your  pods.  This  address  will  remain  unchanged  throughout  the\nwhole lifetime of the service. Pods behind this service may come and go, their IPs may\nchange, their number can go up or down, but they’ll always be accessible through the\nservice’s single and constant IP address. \n But how do the client pods know the IP and port of a service? Do you need to cre-\nate the service first, then manually look up its IP address and pass the IP to the config-\nuration options of the client pod? Not really. Kubernetes also provides ways for client\npods to discover a service’s IP and port.\nDISCOVERING SERVICES THROUGH ENVIRONMENT VARIABLES\nWhen a pod is started, Kubernetes initializes a set of environment variables pointing\nto each service that exists at that moment. If you create the service before creating the\nclient pods, processes in those pods can get the IP address and port of the service by\ninspecting their environment variables. \n Let’s see what those environment variables look like by examining the environment\nof one of your running pods. You’ve already learned that you can use the \nkubectl exec\ncommand to run a command in the pod, but because you created the service only\nafter your pods had been created, the environment variables for the service couldn’t\nhave been set yet. You’ll need to address that first.\n Before you can see environment variables for your service, you first need to delete\nall  the  pods  and  let  the  ReplicationController  create  new  ones.  You  may  remember\nyou can delete all pods without specifying their names like this:\n$ kubectl delete po --all\npod \"kubia-7nog1\" deleted\npod \"kubia-bf50t\" deleted\npod \"kubia-gzwli\" deleted\nNow you can list the new pods (I’m sure you know how to do that) and pick one as\nyour  target  for  the  \nkubectl exec  command.  Once  you’ve  selected  your  target  pod,\nyou can list environment variables by running the \nenv command inside the container,\nas shown in the following listing.\n \n\n129Introducing services\n$ kubectl exec kubia-3inly env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=kubia-3inly\nKUBERNETES_SERVICE_HOST=10.111.240.1\nKUBERNETES_SERVICE_PORT=443\n...\nKUBIA_SERVICE_HOST=10.111.249.153             \nKUBIA_SERVICE_PORT=80                            \n...\nTwo services are defined in your cluster: the kubernetes and the kubia service (you\nsaw this earlier with the \nkubectl get svc command); consequently, two sets of service-\nrelated environment variables are in the list. Among the variables that pertain to the\nkubia service you created at the beginning of the chapter, you’ll see the KUBIA_SERVICE\n_HOST\n and the KUBIA_SERVICE_PORT environment variables, which hold the IP address\nand port of the \nkubia service, respectively. \n Turning back to the frontend-backend example we started this chapter with, when\nyou have a frontend pod that requires the use of a backend database server pod, you\ncan  expose  the  backend  pod  through  a  service  called  \nbackend-database  and  then\nhave the frontend pod look up its IP address and port through the environment vari-\nables \nBACKEND_DATABASE_SERVICE_HOST and BACKEND_DATABASE_SERVICE_PORT.\nNOTEDashes  in  the  service  name  are  converted  to  underscores  and  all  let-\nters are uppercased when the service name is used as the prefix in the envi-\nronment variable’s name. \nEnvironment variables are one way of looking up the IP and port of a service, but isn’t\nthis  usually  the  domain  of  DNS?  Why  doesn’t  Kubernetes  include  a  DNS  server  and\nallow you to look up service IPs through DNS instead? As it turns out, it does!\nDISCOVERING SERVICES THROUGH DNS\nRemember in chapter 3 when you listed pods in the kube-system namespace? One of\nthe  pods  was  called  \nkube-dns.  The  kube-system  namespace  also  includes  a  corre-\nsponding service with the same name.\n As the name suggests, the pod runs a DNS server, which all other pods running in\nthe  cluster  are  automatically  configured  to  use  (Kubernetes  does  that  by  modifying\neach container’s \n/etc/resolv.conf file). Any DNS query performed by a process run-\nning in a pod will be handled by Kubernetes’ own DNS server, which knows all the ser-\nvices running in your system. \nNOTEWhether  a  pod  uses  the  internal  DNS  server  or  not  is  configurable\nthrough the \ndnsPolicy property in each pod’s spec.\nEach service gets a DNS entry in the internal DNS server, and client pods that know\nthe name of the service can access it through its fully qualified domain name (FQDN)\ninstead of resorting to environment variables. \nListing 5.6   Service-related environment variables in a container\nHere’s the cluster \nIP of the service.\nAnd here’s the port the \nservice is available on.\n \n\n130CHAPTER 5Services: enabling clients to discover and talk to pods\nCONNECTING TO THE SERVICE THROUGH ITS FQDN\nTo revisit the frontend-backend example, a frontend pod can connect to the backend-\ndatabase service by opening a connection to the following FQDN:\nbackend-database.default.svc.cluster.local\nbackend-database  corresponds  to  the  service  name,  default  stands  for  the  name-\nspace  the  service  is  defined  in,  and  \nsvc.cluster.local  is  a  configurable  cluster\ndomain suffix used in all cluster local service names. \nNOTEThe  client  must  still  know  the  service’s  port  number.  If  the  service  is\nusing a standard port (for example, 80 for HTTP or 5432 for Postgres), that\nshouldn’t be a problem. If not, the client can get the port number from the\nenvironment variable.\nConnecting to a service can be even simpler than that. You can omit the \nsvc.cluster\n.local\n suffix and even the namespace, when the frontend pod is in the same name-\nspace  as  the  database  pod.  You  can  thus  refer  to  the  service  simply  as  \nbackend-\ndatabase\n. That’s incredibly simple, right?\n Let’s try this. You’ll try to access the \nkubia service through its FQDN instead of its\nIP. Again, you’ll need to do that inside an existing pod. You already know how to use\nkubectl exec to run a single command in a pod’s container, but this time, instead of\nrunning the \ncurl command directly, you’ll run the bash shell instead, so you can then\nrun multiple commands in the container. This is similar to what you did in chapter 2\nwhen  you  entered  the  container  you  ran  with  Docker  by  using  the  \ndocker exec -it\nbash\n command. \nRUNNING A SHELL IN A POD’S CONTAINER\nYou  can  use  the  kubectl exec  command  to  run  bash  (or  any  other  shell)  inside  a\npod’s  container.  This  way  you’re  free  to  explore  the  container  as  long  as  you  want,\nwithout having to perform a \nkubectl exec for every command you want to run.\nNOTEThe shell’s binary executable must be available in the container image\nfor this to work.\nTo use the shell properly, you need to pass the \n-it option to kubectl exec:\n$ kubectl exec -it kubia-3inly bash\nroot@kubia-3inly:/# \nYou’re now inside the container. You can use the curl command to access the kubia\nservice in any of the following ways:\nroot@kubia-3inly:/# curl http://kubia.default.svc.cluster.local\nYou’ve hit kubia-5asi2\nroot@kubia-3inly:/# curl http://kubia.default\nYou’ve hit kubia-3inly\n \n\n131Connecting to services living outside the cluster\nroot@kubia-3inly:/# curl http://kubia\nYou’ve hit kubia-8awf3\nYou can hit your service by using the service’s name as the hostname in the requested\nURL. You can omit the namespace and the \nsvc.cluster.local suffix because of how\nthe DNS resolver inside each pod’s container is configured. Look at the /etc/resolv.conf\nfile in the container and you’ll understand:\nroot@kubia-3inly:/# cat /etc/resolv.conf\nsearch default.svc.cluster.local svc.cluster.local cluster.local ...\nUNDERSTANDING WHY YOU CAN’T PING A SERVICE IP\nOne last thing before we move on. You know how to create services now, so you’ll soon\ncreate your own. But what if, for whatever reason, you can’t access your service?\n You’ll probably try to figure out what’s wrong by entering an existing pod and try-\ning to access the service like you did in the last example. Then, if you still can’t access\nthe service with a simple \ncurl command, maybe you’ll try to ping the service IP to see\nif it’s up. Let’s try that now:\nroot@kubia-3inly:/# ping kubia\nPING kubia.default.svc.cluster.local (10.111.249.153): 56 data bytes\n^C--- kubia.default.svc.cluster.local ping statistics ---\n54 packets transmitted, 0 packets received, 100% packet loss\nHmm. curl-ing the service works, but pinging it doesn’t. That’s because the service’s\ncluster IP is a virtual IP, and only has meaning when combined with the service port.\nWe’ll explain what that means and how services work in chapter 11. I wanted to men-\ntion that here because it’s the first thing users do when they try to debug a broken\nservice and it catches most of them off guard.\n5.2Connecting to services living outside the cluster\nUp to now, we’ve talked about services backed by one or more pods running inside\nthe  cluster.  But  cases  exist  when  you’d  like  to  expose  external  services  through  the\nKubernetes  services  feature.  Instead  of  having  the  service  redirect  connections  to\npods in the cluster, you want it to redirect to external IP(s) and port(s). \n This allows you to take advantage of both service load balancing and service discov-\nery.  Client  pods  running  in  the  cluster  can  connect  to  the  external  service  like  they\nconnect to internal services.\n5.2.1Introducing service endpoints\nBefore  going  into  how  to  do  this,  let  me  first  shed  more  light  on  services.  Services\ndon’t  link  to  pods  directly.  Instead,  a  resource  sits  in  between—the  Endpoints\nresource. You may have already noticed endpoints if you used the \nkubectl describe\ncommand on your service, as shown in the following listing.\n \n\n132CHAPTER 5Services: enabling clients to discover and talk to pods\n$ kubectl describe svc kubia\nName:                kubia\nNamespace:           default\nLabels:              <none>\nSelector:            app=kubia         \nType:                ClusterIP\nIP:                  10.111.249.153\nPort:                <unset> 80/TCP\nEndpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   \nSession Affinity:    None\nNo events.\nAn Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a ser-\nvice. The Endpoints resource is like any other Kubernetes resource, so you can display\nits basic info with \nkubectl get:\n$ kubectl get endpoints kubia\nNAME    ENDPOINTS                                         AGE\nkubia   10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h\nAlthough  the  pod  selector  is  defined  in  the  service  spec,  it’s  not  used  directly  when\nredirecting  incoming  connections.  Instead,  the  selector  is  used  to  build  a  list  of  IPs\nand ports, which is then stored in the Endpoints resource. When a client connects to a\nservice,  the  service  proxy  selects  one  of  those  IP  and  port  pairs  and  redirects  the\nincoming connection to the server listening at that location.\n5.2.2Manually configuring service endpoints\nYou may have probably realized this already, but having the service’s endpoints decou-\npled from the service allows them to be configured and updated manually. \n  If  you  create  a  service  without  a  pod  selector,  Kubernetes  won’t  even  create  the\nEndpoints resource (after all, without a selector, it can’t know which pods to include\nin  the  service).  It’s  up  to  you  to  create  the  Endpoints  resource  to  specify  the  list  of\nendpoints for the service.\n To create a service with manually managed endpoints, you need to create both a\nService and an Endpoints resource. \nCREATING A SERVICE WITHOUT A SELECTOR\nYou’ll first create the YAML for the service itself, as shown in the following listing.\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-service     \nspec:                       \n  ports:\n  - port: 80                  \nListing 5.7   Full details of a service displayed with kubectl describe\nListing 5.8   A service without a pod selector: external-service.yaml\nThe service’s pod \nselector is used to \ncreate the list of \nendpoints.\nThe list of pod\nIPs and ports\nthat represent\nthe endpoints of\nthis service\nThe name of the service must \nmatch the name of the Endpoints \nobject (see next listing).\nThis service has no \nselector defined.\n \n\n133Connecting to services living outside the cluster\nYou’re defining a service called external-service that will accept incoming connec-\ntions on port 80. You didn’t define a pod selector for the service.\nCREATING AN ENDPOINTS RESOURCE FOR A SERVICE WITHOUT A SELECTOR\nEndpoints are a separate resource and not an attribute of a service. Because you cre-\nated the service without a selector, the corresponding Endpoints resource hasn’t been\ncreated  automatically,  so  it’s  up  to  you  to  create  it.  The  following  listing  shows  its\nYAML manifest.\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: external-service      \nsubsets:\n  - addresses:\n    - ip: 11.11.11.11         \n    - ip: 22.22.22.22         \n    ports:\n    - port: 80      \nThe Endpoints object needs to have the same name as the service and contain the list\nof  target  IP  addresses  and  ports  for  the  service.  After  both  the  Service  and  the  End-\npoints resource are posted to the server, the service is ready to be used like any regular\nservice with a pod selector. Containers created after the service is created will include\nthe environment variables for the service, and all connections to its IP:port pair will be\nload balanced between the service’s endpoints. \n Figure 5.4 shows three pods connecting to the service with external endpoints.\nIf you later decide to migrate the external service to pods running inside Kubernetes,\nyou can add a selector to the service, thereby making its Endpoints managed automat-\nically.  The  same  is  also  true  in  reverse—by  removing  the  selector  from  a  Service,\nListing 5.9   A manually created Endpoints resource: external-service-endpoints.yaml\nThe name of the Endpoints object \nmust match the name of the \nservice (see previous listing).\nThe IPs of the endpoints that the \nservice will forward connections to\nThe target port of the endpoints\nPodPodPod\nExternal server 1\nIP: 11.11.11.11:80\nExternal server 2\nIP: 22.22.22.22:80\nService\n10.111.249.214:80\nKubernetes cluster\nInternet\nFigure 5.4   Pods consuming a service with two external endpoints.\n \n\n134CHAPTER 5Services: enabling clients to discover and talk to pods\nKubernetes stops updating its Endpoints. This means a service IP address can remain\nconstant while the actual implementation of the service is changed. \n5.2.3Creating an alias for an external service\nInstead  of  exposing  an  external  service  by  manually  configuring  the  service’s  End-\npoints, a simpler method allows you to refer to an external service by its fully qualified\ndomain name (FQDN).\nCREATING AN EXTERNALNAME SERVICE\nTo create a service that serves as an alias for an external service, you create a Service\nresource with the \ntype field set to ExternalName. For example, let’s imagine there’s a\npublic API available at api.somecompany.com. You can define a service that points to\nit as shown in the following listing.\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-service\nspec:\n  type: ExternalName                       \n  externalName: someapi.somecompany.com     \n  ports:\n  - port: 80\nAfter  the  service  is  created,  pods  can  connect  to  the  external  service  through  the\nexternal-service.default.svc.cluster.local  domain  name  (or  even  external-\nservice\n)  instead  of  using  the  service’s  actual  FQDN.  This  hides  the  actual  service\nname  and  its  location  from  pods  consuming  the  service,  allowing  you  to  modify  the\nservice definition and point it to a different service any time later, by only changing\nthe \nexternalName attribute or by changing the type back to ClusterIP and creating\nan Endpoints object for the service—either manually or by specifying a label selector\non the service and having it created automatically.\n \nExternalName  services  are  implemented  solely  at  the  DNS  level—a  simple  CNAME\nDNS record is created for the service. Therefore, clients connecting to the service will\nconnect  to  the  external  service  directly,  bypassing  the  service  proxy  completely.  For\nthis reason, these types of services don’t even get a cluster IP. \nNOTEA CNAME  record  points  to  a  fully  qualified  domain  name  instead  of  a\nnumeric IP address.\n5.3Exposing services to external clients\nUp to now, we’ve only talked about how services can be consumed by pods from inside\nthe cluster. But you’ll also want to expose certain services, such as frontend webserv-\ners, to the outside, so external clients can access them, as depicted in figure 5.5.\nListing 5.10   An ExternalName-type service: external-service-externalname.yaml\nService type is set \nto ExternalName\nThe fully qualified domain \nname of the actual service\n \n\n135Exposing services to external clients\nYou have a few ways to make a service accessible externally:\nSetting  the  service  type  to NodePort—For  a  NodePort  service,  each  cluster  node\nopens a port on the node itself (hence the name) and redirects traffic received\non  that  port  to  the  underlying  service.  The  service  isn’t  accessible  only  at  the\ninternal cluster IP and port, but also through a dedicated port on all nodes. \nSetting  the  service  type  to LoadBalancer, an  extension  of  the NodePort type—This\nmakes  the  service  accessible  through  a  dedicated  load  balancer,  provisioned\nfrom the cloud infrastructure Kubernetes is running on. The load balancer redi-\nrects traffic to the node port across all the nodes. Clients connect to the service\nthrough the load balancer’s IP.\nCreating  an Ingress resource,  a  radically  different  mechanism  for  exposing  multiple  ser-\nvices through a single IP address—It operates at the HTTP level (network layer 7)\nand can thus offer more features than layer 4 services can. We’ll explain Ingress\nresources in section 5.4. \n5.3.1Using a NodePort service\nThe first method of exposing a set of pods to external clients is by creating a service\nand setting its type to \nNodePort. By creating a NodePort service, you make Kubernetes\nreserve a port on all its nodes (the same port number is used across all of them) and\nforward incoming connections to the pods that are part of the service. \n This is similar to a regular service (their actual type is \nClusterIP), but a NodePort\nservice  can  be  accessed  not  only  through  the  service’s  internal  cluster  IP,  but  also\nthrough any node’s IP and the reserved node port. \n This will make more sense when you try interacting with a \nNodePort service.\nCREATING A NODEPORT SERVICE\nYou’ll now create a NodePort service to see how you can use it. The following listing\nshows the YAML for the service.\n \nKubernetes cluster\nExternal clientService\nPodPodPod\nFigure 5.5   Exposing a service to external clients\n \n\n136CHAPTER 5Services: enabling clients to discover and talk to pods\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia-nodeport\nspec:\n  type: NodePort            \n  ports:\n  - port: 80                 \n    targetPort: 8080        \n    nodePort: 30123        \n  selector:\n    app: kubia\nYou set the type to NodePort and specify the node port this service should be bound to\nacross all cluster nodes. Specifying the port isn’t mandatory; Kubernetes will choose a\nrandom port if you omit it. \nNOTEWhen  you  create  the  service  in  GKE,  kubectl  prints  out  a  warning\nabout having to configure firewall rules. We’ll see how to do that soon. \nEXAMINING YOUR NODEPORT SERVICE\nLet’s see the basic information of your service to learn more about it:\n$ kubectl get svc kubia-nodeport\nNAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nkubia-nodeport   10.111.254.223   <nodes>       80:30123/TCP   2m\nLook at the EXTERNAL-IP column. It shows <nodes>, indicating the service is accessible\nthrough  the  IP  address  of  any  cluster  node.  The  \nPORT(S)  column  shows  both  the\ninternal port of the cluster IP (\n80) and the node port (30123). The service is accessi-\nble at the following addresses:\n10.11.254.223:80\n<1st node’s IP>:30123\n<2nd node’s IP>:30123, and so on.\nFigure  5.6  shows  your  service  exposed  on  port  30123  of  both  of  your  cluster  nodes\n(this applies if you’re running this on GKE; Minikube only has a single node, but the\nprinciple  is  the  same).  An  incoming  connection  to  one  of  those  ports  will  be  redi-\nrected to a randomly selected pod, which may or may not be the one running on the\nnode the connection is being made to. \n \n \n \nListing 5.11   A NodePort service definition: kubia-svc-nodeport.yaml\nSet the service \ntype to NodePort.\nThis is the port of the \nservice’s internal cluster IP.\nThis is the target port \nof the backing pods.\nThe service will be accessible \nthrough port 30123 of each of \nyour cluster nodes.\n \n\n137Exposing services to external clients\nA connection received on port 30123 of the first node might be forwarded either to\nthe pod running on the first node or to one of the pods running on the second node.\nCHANGING FIREWALL RULES TO LET EXTERNAL CLIENTS ACCESS OUR NODEPORT SERVICE\nAs  I’ve  mentioned  previously,  before  you  can  access  your  service  through  the  node\nport,  you  need  to  configure  the  Google  Cloud  Platform’s  firewalls  to  allow  external\nconnections to your nodes on that port. You’ll do this now:\n$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123\nCreated [https://www.googleapis.com/compute/v1/projects/kubia-\n1295/global/firewalls/kubia-svc-rule].\nNAME            NETWORK  SRC_RANGES  RULES      SRC_TAGS  TARGET_TAGS\nkubia-svc-rule  default  0.0.0.0/0   tcp:30123\nYou can access your service through port 30123 of one of the node’s IPs. But you need\nto figure out the IP of a node first. Refer to the sidebar on how to do that.\n \n \n \nKubernetes cluster\nExternal client\nPod\nNode 2\nIP: 130.211.99.206\nNode 1\nIP: 130.211.97.55\nPort 30123\nPort 8080\nPod\nPort 8080\nPod\nPort 30123\nPort 8080\nService\nFigure 5.6   An external client connecting to a NodePort service either through Node 1 or 2\n \n\n138CHAPTER 5Services: enabling clients to discover and talk to pods\nOnce you know the IPs of your nodes, you can try accessing your service through them:\n$ curl http://130.211.97.55:30123\nYou've hit kubia-ym8or\n$ curl http://130.211.99.206:30123\nYou've hit kubia-xueq1\nTIPWhen  using  Minikube,  you  can  easily  access  your  NodePort  services\nthrough  your  browser  by  running  \nminikube service <service-name> [-n\n<namespace>]\n.\nAs you can see, your pods are now accessible to the whole internet through port 30123\non any of your nodes. It doesn’t matter what node a client sends the request to. But if\nyou only point your clients to the first node, when that node fails, your clients can’t\naccess the service anymore. That’s why it makes sense to put a load balancer in front\nof  the  nodes  to  make  sure  you’re  spreading  requests  across  all  healthy  nodes  and\nnever sending them to a node that’s offline at that moment. \n  If  your  Kubernetes  cluster  supports  it  (which  is  mostly  true  when  Kubernetes  is\ndeployed  on  cloud  infrastructure),  the  load  balancer  can  be  provisioned  automati-\ncally by creating a \nLoadBalancer instead of a NodePort service. We’ll look at this next.\n5.3.2Exposing a service through an external load balancer\nKubernetes clusters running on cloud providers usually support the automatic provi-\nsion of a load balancer from the cloud infrastructure. All you need to do is set the\nUsing JSONPath to get the IPs of all your nodes \nYou can find the IP in the JSON or YAML descriptors of the nodes. But instead of\nsifting through the relatively large JSON, you can tell \nkubectl to print out only the\nnode IP instead of the whole service definition: \n$ kubectl get nodes -o jsonpath='{.items[*].status.\n➥ addresses[?(@.type==\"ExternalIP\")].address}'\n130.211.97.55 130.211.99.206\nYou’re  telling  kubectl  to  only  output  the  information  you  want  by  specifying  a\nJSONPath. You’re probably familiar with XPath and how it’s used with XML. JSONPath\nis basically XPath for JSON. The JSONPath in the previous example instructs \nkubectl\nto do the following:\nGo through all the elements in the items attribute.\nFor each element, enter the status attribute.\nFilter  elements  of  the  addresses  attribute,  taking  only  those  that  have  the\ntype attribute set to ExternalIP.\nFinally, print the address attribute of the filtered elements.\nTo learn more about how to use JSONPath with \nkubectl, refer to the documentation\nat http://kubernetes.io/docs/user-guide/jsonpath. \n \n\n139Exposing services to external clients\nservice’s  type  to  LoadBalancer  instead  of  NodePort.  The  load  balancer  will  have  its\nown  unique,  publicly  accessible  IP  address  and  will  redirect  all  connections  to  your\nservice. You can thus access your service through the load balancer’s IP address. \n  If  Kubernetes  is  running  in  an  environment  that  doesn’t  support  \nLoadBalancer\nservices, the load balancer will not be provisioned, but the service will still behave like\na \nNodePort service. That’s because a LoadBalancer service is an extension of a Node-\nPort\n service. You’ll run this example on Google Kubernetes Engine, which supports\nLoadBalancer services. Minikube doesn’t, at least not as of this writing. \nCREATING A LOADBALANCER SERVICE\nTo create a service with a load balancer in front, create the service from the following\nYAML manifest, as shown in the following listing.\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia-loadbalancer\nspec:\n  type: LoadBalancer          \n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: kubia\nThe service type is set to LoadBalancer instead of NodePort. You’re not specifying a spe-\ncific node port, although you could (you’re letting Kubernetes choose one instead). \nCONNECTING TO THE SERVICE THROUGH THE LOAD BALANCER\nAfter  you  create  the  service,  it  takes  time  for  the  cloud  infrastructure  to  create  the\nload balancer and write its IP address into the Service object. Once it does that, the IP\naddress will be listed as the external IP address of your service:\n$ kubectl get svc kubia-loadbalancer\nNAME                 CLUSTER-IP       EXTERNAL-IP      PORT(S)         AGE\nkubia-loadbalancer   10.111.241.153   130.211.53.173   80:32143/TCP    1m\nIn this case, the load balancer is available at IP 130.211.53.173, so you can now access\nthe service at that IP address:\n$ curl http://130.211.53.173\nYou've hit kubia-xueq1\nSuccess! As you may have noticed, this time you didn’t need to mess with firewalls the\nway you had to before with the \nNodePort service.\nListing 5.12   A LoadBalancer-type service: kubia-svc-loadbalancer.yaml\nThis type of service obtains \na load balancer from the \ninfrastructure hosting the \nKubernetes cluster.\n \n\n140CHAPTER 5Services: enabling clients to discover and talk to pods\nSee  figure  5.7  to  see  how  HTTP  requests  are  delivered  to  the  pod.  External  clients\n(\ncurl in your case) connect to port 80 of the load balancer and get routed to the\nSession affinity and web browsers\nBecause your service is now exposed externally, you may try accessing it with your\nweb browser. You’ll see something that may strike you as odd—the browser will hit\nthe  exact  same  pod  every  time.  Did  the  service’s  session  affinity  change  in  the\nmeantime? With \nkubectl explain, you can double-check that the service’s session\naffinity  is  still  set  to  \nNone,  so  why  don’t  different  browser  requests  hit  different\npods, as is the case when using \ncurl?\nLet me explain what’s happening. The browser is using keep-alive connections and\nsends  all  its  requests  through  a  single  connection,  whereas  \ncurl  opens  a  new\nconnection every time. Services work at the connection level, so when a connection to a\nservice is first opened, a random pod is selected and then all network packets belonging\nto that connection are all sent to that single pod. Even if session affinity is set to \nNone,\nusers will always hit the same pod (until the connection is closed).\nKubernetes cluster\nExternal client\nLoad balancer\nIP: 130.211.53.173:80\nPod\nNode 2\nIP: 130.211.99.206\nNode 1\nIP: 130.211.97.55\nPort 32143\nPort 8080\nPod\nPort 8080\nPod\nPort 32143\nPort 8080\nService\nFigure 5.7   An external client connecting to a LoadBalancer service\n \n\n141Exposing services to external clients\nimplicitly assigned node port on one of the nodes. From there, the connection is for-\nwarded to one of the pod instances.\n  As  already  mentioned,  a  \nLoadBalancer-type  service  is  a  NodePort  service  with  an\nadditional infrastructure-provided load balancer. If you use \nkubectl describe to dis-\nplay additional info about the service, you’ll see that a node port has been selected for\nthe service. If you were to open the firewall for this port, the way you did in the previ-\nous section about \nNodePort services, you could access the service through the node\nIPs as well.\nTIPIf  you’re  using  Minikube,  even  though  the  load  balancer  will  never  be\nprovisioned,  you  can  still  access  the  service  through  the  node  port  (at  the\nMinikube VM’s IP address).\n5.3.3Understanding the peculiarities of external connections\nYou  must  be  aware  of  several  things  related  to  externally  originating  connections  to\nservices. \nUNDERSTANDING AND PREVENTING UNNECESSARY NETWORK HOPS\nWhen  an  external  client  connects  to  a  service  through  the  node  port  (this  also\nincludes cases when it goes through the load balancer first), the randomly chosen\npod may or may not be running on the same node that received the connection. An\nadditional  network  hop  is  required  to  reach  the  pod,  but  this  may  not  always  be\ndesirable. \n You can prevent this additional hop by configuring the service to redirect external\ntraffic only to pods running on the node that received the connection. This is done by\nsetting the \nexternalTrafficPolicy field in the service’s spec section:\nspec:\n  externalTrafficPolicy: Local\n  ...\nIf  a  service  definition  includes  this  setting  and  an  external  connection  is  opened\nthrough the service’s node port, the service proxy will choose a locally running pod. If\nno  local  pods  exist,  the  connection  will  hang  (it  won’t  be  forwarded  to  a  random\nglobal  pod,  the  way  connections  are  when  not  using  the  annotation).  You  therefore\nneed  to  ensure  the  load  balancer  forwards  connections  only  to  nodes  that  have  at\nleast one such pod.\n Using this annotation also has other drawbacks. Normally, connections are spread\nevenly across all the pods, but when using this annotation, that’s no longer the case.\n  Imagine  having  two  nodes  and  three  pods.  Let’s  say  node  A  runs  one  pod  and\nnode B runs the other two. If the load balancer spreads connections evenly across the\ntwo nodes, the pod on node A will receive 50% of all connections, but the two pods on\nnode B will only receive 25% each, as shown in figure 5.8.\n \n\n142CHAPTER 5Services: enabling clients to discover and talk to pods\nBEING AWARE OF THE NON-PRESERVATION OF THE CLIENT’S IP\nUsually, when clients inside the cluster connect to a service, the pods backing the ser-\nvice can obtain the client’s IP address. But when the connection is received through a\nnode port, the packets’ source IP is changed, because Source Network Address Trans-\nlation (SNAT) is performed on the packets. \n The backing pod can’t see the actual client’s IP, which may be a problem for some\napplications that need to know the client’s IP. In the case of a web server, for example,\nthis means the access log won’t show the browser’s IP.\n The \nLocal external traffic policy described in the previous section affects the pres-\nervation of the client’s IP, because there’s no additional hop between the node receiv-\ning the connection and the node hosting the target pod (SNAT isn’t performed).\n5.4Exposing services externally through an Ingress \nresource\nYou’ve  now  seen  two  ways  of  exposing  a  service  to  clients  outside  the  cluster,  but\nanother method exists—creating an Ingress resource.\nDEFINITIONIngress  (noun)—The  act  of  going  in  or  entering;  the  right  to\nenter; a means or place of entering; entryway. \nLet me first explain why you need another way to access Kubernetes services from the\noutside. \nUNDERSTANDING WHY INGRESSES ARE NEEDED\nOne  important  reason  is  that  each  LoadBalancer  service  requires  its  own  load  bal-\nancer with its own public IP address, whereas an Ingress only requires one, even when\nproviding  access  to  dozens  of  services.  When  a  client  sends  an  HTTP  request  to  the\nIngress, the host and path in the request determine which service the request is for-\nwarded to, as shown in figure 5.9.\n \n50%\n50%50%\n25%25%\nNode A\nPod\nNode B\nPod\nPod\nLoad balancer\nFigure 5.8   A Service using \nthe \nLocal external traffic \npolicy may lead to uneven \nload distribution across pods.\n \n\n143Exposing services externally through an Ingress resource\nIngresses operate at the application layer of the network stack (HTTP) and can pro-\nvide features such as cookie-based session affinity and the like, which services can’t.\nUNDERSTANDING THAT AN INGRESS CONTROLLER IS REQUIRED\nBefore  we  go  into  the  features  an  Ingress  object  provides,  let  me  emphasize  that  to\nmake Ingress resources work, an Ingress controller needs to be running in the cluster.\nDifferent Kubernetes environments use different implementations of the controller,\nbut several don’t provide a default controller at all. \n For example, Google Kubernetes Engine uses Google Cloud Platform’s own HTTP\nload-balancing features to provide the Ingress functionality. Initially, Minikube didn’t\nprovide a controller out of the box, but it now includes an add-on that can be enabled\nto  let  you  try  out  the  Ingress  functionality.  Follow  the  instructions  in  the  following\nsidebar to ensure it’s enabled.\nEnabling the Ingress add-on in Minikube\nIf you’re using Minikube to run the examples in this book, you’ll need to ensure the\nIngress add-on is enabled. You can check whether it is by listing all the add-ons:\n$ minikube addons list\n- default-storageclass: enabled\n- kube-dns: enabled\n- heapster: disabled\n- ingress: disabled               \n- registry-creds: disabled\n- addon-manager: enabled\n- dashboard: enabled\nYou’ll  learn  about  what  these  add-ons  are  throughout  the  book,  but  it  should  be\npretty clear what the \ndashboard and the kube-dns add-ons do. Enable the Ingress\nadd-on so you can see Ingresses in action:\n$ minikube addons enable ingress\ningress was successfully enabled\nPodPodPod\nPodPodPod\nPodPodPod\nPodPodPod\nIngress\nClient\nService\nkubia.example.com/kubia\nfoo.example.com\nkubia.example.com/foo\nService\nbar.example.com\nService\nService\nFigure 5.9   Multiple services can be exposed through a single Ingress.\nThe Ingress add-on \nisn’t enabled.\n \n\n144CHAPTER 5Services: enabling clients to discover and talk to pods\nTIPThe --all-namespaces option mentioned in the sidebar is handy when\nyou don’t know what namespace your pod (or other type of resource) is in, or\nif you want to list resources across all namespaces.\n5.4.1Creating an Ingress resource\nYou’ve  confirmed  there’s  an  Ingress  controller  running  in  your  cluster,  so  you  can\nnow create an Ingress resource. The following listing shows what the YAML manifest\nfor the Ingress looks like.\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: kubia\nspec:\n  rules:\n  - host: kubia.example.com             \n    http:\n      paths:\n      - path: /                           \n        backend:\n          serviceName: kubia-nodeport     \n          servicePort: 80                 \nThis defines an Ingress with a single rule, which makes sure all HTTP requests received\nby the Ingress controller, in which the host \nkubia.example.com is requested, will be\nsent to the \nkubia-nodeport service on port 80. \n(continued)\nThis  should  have  spun  up  an  Ingress  controller  as  another  pod.  Most  likely,  the\ncontroller pod will be in the \nkube-system namespace, but not necessarily, so list all\nthe running pods across all namespaces by using the \n--all-namespaces option:\n$ kubectl get po --all-namespaces\nNAMESPACE    NAME                            READY  STATUS    RESTARTS AGE\ndefault      kubia-rsv5m                     1/1    Running   0        13h\ndefault      kubia-fe4ad                     1/1    Running   0        13h\ndefault      kubia-ke823                     1/1    Running   0        13h\nkube-system  default-http-backend-5wb0h      1/1    Running   0        18m\nkube-system  kube-addon-manager-minikube     1/1    Running   3        6d\nkube-system  kube-dns-v20-101vq              3/3    Running   9        6d\nkube-system  kubernetes-dashboard-jxd9l      1/1    Running   3        6d\nkube-system  nginx-ingress-controller-gdts0  1/1    Running   0        18m\nAt the bottom of the output, you see the Ingress controller pod. The name suggests\nthat  Nginx  (an  open-source  HTTP  server  and  reverse  proxy)  is  used  to  provide  the\nIngress functionality.\nListing 5.13   An Ingress resource definition: kubia-ingress.yaml\nThis Ingress maps the \nkubia.example.com domain \nname to your service.\nAll requests will be sent to \nport 80 of the kubia-\nnodeport service.\n \n\n145Exposing services externally through an Ingress resource\nNOTEIngress controllers on cloud providers (in GKE, for example) require\nthe  Ingress  to  point  to  a  \nNodePort  service.  But  that’s  not  a  requirement  of\nKubernetes itself.\n5.4.2Accessing the service through the Ingress\nTo  access  your  service  through  http://kubia.example.com, you’ll need to make sure\nthe domain name resolves to the IP of the Ingress controller. \nOBTAINING THE IP ADDRESS OF THE INGRESS\nTo look up the IP, you need to list Ingresses:\n$ kubectl get ingresses\nNAME      HOSTS               ADDRESS          PORTS     AGE\nkubia     kubia.example.com   192.168.99.100   80        29m\nNOTEWhen running on cloud providers, the address may take time to appear,\nbecause the Ingress controller provisions a load balancer behind the scenes.\nThe IP is shown in the \nADDRESS column. \nENSURING THE HOST CONFIGURED IN THE INGRESS POINTS TO THE INGRESS’ IP ADDRESS\nOnce  you  know  the  IP,  you  can  then  either  configure  your  DNS  servers  to  resolve\nkubia.example.com  to  that  IP  or  you  can  add  the  following  line  to  \n/etc/hosts  (or\nC:\\windows\\system32\\drivers\\etc\\hosts on Windows):\n192.168.99.100    kubia.example.com\nACCESSING PODS THROUGH THE INGRESS\nEverything  is  now  set  up,  so  you  can  access  the  service  at  http://kubia.example.com\n(using a browser or \ncurl):\n$ curl http://kubia.example.com\nYou've hit kubia-ke823\nYou’ve successfully accessed the service through an Ingress. Let’s take a better look at\nhow that unfolded.\nUNDERSTANDING HOW INGRESSES WORK\nFigure  5.10  shows  how  the  client  connected  to  one  of  the  pods  through  the  Ingress\ncontroller. The client first performed a DNS lookup of kubia.example.com, and the\nDNS server (or the local operating system) returned the IP of the Ingress controller.\nThe  client  then  sent  an  HTTP  request  to  the  Ingress  controller  and  specified\nkubia.example.com in the Host header. From that header, the controller determined\nwhich  service  the  client  is  trying  to  access,  looked  up  the  pod  IPs  through  the  End-\npoints object associated with the service, and forwarded the client’s request to one of\nthe pods.\n As you can see, the Ingress controller didn’t forward the request to the service. It\nonly used it to select a pod. Most, if not all, controllers work like this. \n \n\n146CHAPTER 5Services: enabling clients to discover and talk to pods\n5.4.3Exposing multiple services through the same Ingress\nIf you look at the Ingress spec closely, you’ll see that both rules and paths are arrays,\nso  they  can  contain  multiple  items.  An  Ingress  can  map  multiple  hosts  and  paths  to\nmultiple services, as you’ll see next. Let’s focus on \npaths first. \nMAPPING DIFFERENT SERVICES TO DIFFERENT PATHS OF THE SAME HOST\nYou  can  map  multiple  paths  on  the  same  host  to  different  services,  as  shown  in  the\nfollowing listing.\n...\n  - host: kubia.example.com\n    http:\n      paths:\n      - path: /kubia                \n        backend:                    \n          serviceName: kubia        \n          servicePort: 80           \n      - path: /foo                \n        backend:                  \n          serviceName: bar        \n          servicePort: 80         \nIn this case, requests will be sent to two different services, depending on the path in\nthe requested URL. Clients can therefore reach two different services through a single\nIP address (that of the Ingress controller).\nListing 5.14   Ingress exposing multiple services on same host, but different paths\nNode A\nPod\nNode B\nPodPod\nIngress\ncontroller\nEndpointsServiceIngress\nClient\n2. Client sends HTTP GET\nrequest with header\nHost: kubia.example.com\n3. Controller sends\nrequest to one of\nthe pods.\n1. Client looks up\nkubia.example.com\nDNS\nFigure 5.10   Accessing pods through an Ingress\nRequests to kubia.example.com/kubia \nwill be routed to the kubia service.\nRequests to kubia.example.com/bar \nwill be routed to the bar service.\n \n\n147Exposing services externally through an Ingress resource\nMAPPING DIFFERENT SERVICES TO DIFFERENT HOSTS\nSimilarly, you can use an Ingress to map to different services based on the host in the\nHTTP request instead of (only) the path, as shown in the next listing.\nspec:\n  rules:\n  - host: foo.example.com          \n    http:\n      paths:\n      - path: / \n        backend:\n          serviceName: foo         \n          servicePort: 80\n  - host: bar.example.com          \n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: bar         \n          servicePort: 80\nRequests  received  by  the  controller  will  be  forwarded  to  either  service  foo  or  bar,\ndepending  on  the  \nHost  header  in  the  request  (the  way  virtual  hosts  are  handled  in\nweb  servers).  DNS  needs  to  point  both  the  foo.example.com  and  the  bar.exam-\nple.com domain names to the Ingress controller’s IP address. \n5.4.4Configuring Ingress to handle TLS traffic\nYou’ve seen how an Ingress forwards HTTP traffic. But what about HTTPS? Let’s take\na quick look at how to configure Ingress to support TLS. \nCREATING A TLS CERTIFICATE FOR THE INGRESS\nWhen a client opens a TLS connection to an Ingress controller, the controller termi-\nnates the TLS connection. The communication between the client and the controller\nis  encrypted,  whereas  the  communication  between  the  controller  and  the  backend\npod isn’t. The application running in the pod doesn’t need to support TLS. For exam-\nple, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress\ncontroller take care of everything related to TLS. To enable the controller to do that,\nyou need to attach a certificate and a private key to the Ingress. The two need to be\nstored  in  a  Kubernetes  resource  called  a  Secret,  which  is  then  referenced  in  the\nIngress manifest. We’ll explain Secrets in detail in chapter 7. For now, you’ll create the\nSecret without paying too much attention to it.\n First, you need to create the private key and certificate:\n$ openssl genrsa -out tls.key 2048\n$ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj \n➥ /CN=kubia.example.com\nListing 5.15   Ingress exposing multiple services on different hosts\nRequests for \nfoo.example.com will be \nrouted to service foo.\nRequests for \nbar.example.com will be \nrouted to service bar.\n \n\n148CHAPTER 5Services: enabling clients to discover and talk to pods\nThen you create the Secret from the two files like this:\n$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key\nsecret \"tls-secret\" created\nThe  private  key  and  the  certificate  are  now  stored  in  the  Secret  called  tls-secret.\nNow,  you  can  update  your  Ingress  object  so it will also accept HTTPS requests for\nkubia.example.com. The Ingress manifest should now look like the following listing.\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: kubia\nspec:\n  tls:                           \n  - hosts:                        \n    - kubia.example.com           \n    secretName: tls-secret       \n  rules:\n  - host: kubia.example.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: kubia-nodeport\n          servicePort: 80\nTIPInstead of deleting the Ingress and re-creating it from the new file, you\ncan  invoke  \nkubectl apply -f kubia-ingress-tls.yaml,  which  updates  the\nIngress resource with what’s specified in the file.\nSigning certificates through the CertificateSigningRequest resource\nInstead  of  signing  the  certificate  ourselves,  you  can  get  the  certificate  signed  by\ncreating a \nCertificateSigningRequest (CSR) resource. Users or their applications\ncan create a regular certificate request, put it into a CSR, and then either a human\noperator or an automated process can approve the request like this:\n$ kubectl certificate approve <name of the CSR> \nThe signed certificate can then be retrieved from the CSR’s status.certificate\nfield. \nNote that a certificate signer component must be running in the cluster; otherwise\ncreating \nCertificateSigningRequest and approving or denying them won’t have\nany effect.\nListing 5.16   Ingress handling TLS traffic: kubia-ingress-tls.yaml\nThe whole TLS configuration \nis under this attribute.\nTLS connections will be accepted for \nthe kubia.example.com hostname.\nThe private key and the certificate \nshould be obtained from the tls-\nsecret you created previously.\n \n\n149Signaling when a pod is ready to accept connections\nYou can now use HTTPS to access your service through the Ingress:\n$ curl -k -v https://kubia.example.com/kubia\n* About to connect() to kubia.example.com port 443 (#0)\n...\n* Server certificate:\n*   subject: CN=kubia.example.com\n...\n> GET /kubia HTTP/1.1\n> ...\nYou've hit kubia-xueq1\nThe command’s output shows the response from the app, as well as the server certifi-\ncate you configured the Ingress\n with.\nNOTESupport for Ingress features varies between the different Ingress con-\ntroller  implementations,  so  check  the  implementation-specific  documenta-\ntion to see what’s supported. \nIngresses  are  a  relatively  new  Kubernetes  feature,  so  you  can  expect  to  see  many\nimprovements and new features in the future. Although they currently support only\nL7 (HTTP/HTTPS) load balancing, support for L4 load balancing is also planned.\n5.5Signaling when a pod is ready to accept connections\nThere’s  one  more  thing  we  need  to  cover  regarding  both  Services  and  Ingresses.\nYou’ve already learned that pods are included as endpoints of a service if their labels\nmatch the service’s pod selector. As soon as a new pod with proper labels is created, it\nbecomes part of the service and requests start to be redirected to the pod. But what if\nthe pod isn’t ready to start serving requests immediately? \n The pod may need time to load either configuration or data, or it may need to per-\nform a warm-up procedure to prevent the first user request from taking too long and\naffecting the user experience. In such cases you don’t want the pod to start receiving\nrequests  immediately,  especially  when  the  already-running  instances  can  process\nrequests properly and quickly. It makes sense to not forward requests to a pod that’s in\nthe process of starting up until it’s fully ready.\n5.5.1Introducing readiness probes\nIn the previous chapter you learned about liveness probes and how they help keep\nyour  apps  healthy  by  ensuring  unhealthy  containers  are  restarted  automatically.\nSimilar  to  liveness  probes,  Kubernetes  allows  you  to  also  define  a  readiness  probe\nfor your pod.\n The readiness probe is invoked periodically and determines whether the specific\npod should receive client requests or not. When a container’s readiness probe returns\nsuccess, it’s signaling that the container is ready to accept requests. \n This notion of being ready is obviously something that’s specific to each container.\nKubernetes can merely check if the app running in the container responds to a simple\n \n\n150CHAPTER 5Services: enabling clients to discover and talk to pods\nGET /  request  or  it  can  hit  a  specific  URL  path,  which  causes  the  app  to  perform  a\nwhole list of checks to determine if it’s ready. Such a detailed readiness probe, which\ntakes the app’s specifics into account, is the app developer’s responsibility. \nTYPES OF READINESS PROBES\nLike liveness probes, three types of readiness probes exist:\nAn Exec  probe,  where  a  process  is  executed.  The  container’s  status  is  deter-\nmined by the process’ exit status code.\nAn HTTP GET probe, which sends an HTTP GET request to the container and\nthe  HTTP  status  code  of  the  response  determines  whether  the  container  is\nready or not.\nA TCP  Socket  probe,  which  opens  a  TCP  connection  to  a  specified  port  of  the\ncontainer. If the connection is established, the container is considered ready.\nUNDERSTANDING THE OPERATION OF READINESS PROBES\nWhen a container is started, Kubernetes can be configured to wait for a configurable\namount  of  time  to  pass  before  performing  the  first  readiness  check.  After  that,  it\ninvokes the probe periodically and acts based on the result of the readiness probe. If a\npod reports that it’s not ready, it’s removed from the service. If the pod then becomes\nready again, it’s re-added. \n Unlike liveness probes, if a container fails the readiness check, it won’t be killed or\nrestarted.  This  is  an  important  distinction  between  liveness  and  readiness  probes.\nLiveness probes keep pods healthy by killing off unhealthy containers and replacing\nthem with new, healthy ones, whereas readiness probes make sure that only pods that\nare  ready  to  serve  requests  receive  them.  This  is  mostly  necessary  during  container\nstart up, but it’s also useful after the container has been running for a while. \n As you can see in figure 5.11, if a pod’s readiness probe fails, the pod is removed\nfrom the Endpoints object. Clients connecting to the service will not be redirected to\nthe  pod.  The  effect  is  the  same  as  when  the  pod  doesn’t  match  the  service’s  label\nselector at all.\nEndpoints\nService\nSelector:app=kubia\napp: kubia\nPod: kubia-q3vkg\napp: kubia\nPod: kubia-k0xz6\napp: kubia\nPod: kubia-53thy\nNot ready\nThis pod is no longer\nan endpoint, because its\nreadiness probe has failed.\nFigure 5.11   A pod whose readiness probe fails is removed as an endpoint of a service.\n \n\n151Signaling when a pod is ready to accept connections\nUNDERSTANDING WHY READINESS PROBES ARE IMPORTANT\nImagine  that  a  group  of  pods  (for  example,  pods  running  application  servers)\ndepends on a service provided by another pod (a backend database, for example). If\nat  any  point  one  of  the  frontend  pods  experiences  connectivity  problems  and  can’t\nreach the database anymore, it may be wise for its readiness probe to signal to Kuber-\nnetes that the pod isn’t ready to serve any requests at that time. If other pod instances\naren’t experiencing the same type of connectivity issues, they can serve requests nor-\nmally. A readiness probe makes sure clients only talk to those healthy pods and never\nnotice there’s anything wrong with the system.\n5.5.2Adding a readiness probe to a pod\nNext you’ll add a readiness probe to your existing pods by modifying the Replication-\nController’s pod template. \nADDING A READINESS PROBE TO THE POD TEMPLATE\nYou’ll use the kubectl edit command to add the probe to the pod template in your\nexisting ReplicationController:\n$ kubectl edit rc kubia\nWhen the ReplicationController’s YAML opens in the text editor, find the container\nspecification in the pod template and add the following readiness probe definition to\nthe first container under \nspec.template.spec.containers. The YAML should look\nlike the following listing.\napiVersion: v1\nkind: ReplicationController\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      containers:\n      - name: kubia\n        image: luksa/kubia\n        readinessProbe:       \n          exec:               \n            command:          \n            - ls              \n            - /var/ready      \n        ...\nThe readiness probe will periodically perform the command ls /var/ready inside the\ncontainer. The \nls command returns exit code zero if the file exists, or a non-zero exit\ncode otherwise. If the file exists, the readiness probe will succeed; otherwise, it will fail. \nListing 5.17   RC creating a pod with a readiness probe: kubia-rc-readinessprobe.yaml\nA readinessProbe may \nbe defined for each \ncontainer in the pod.\n \n\n152CHAPTER 5Services: enabling clients to discover and talk to pods\n The reason you’re defining such a strange readiness probe is so you can toggle its\nresult by creating or removing the file in question. The file doesn’t exist yet, so all the\npods should now report not being ready, right? Well, not exactly. As you may remem-\nber from the previous chapter, changing a ReplicationController’s pod template has\nno effect on existing pods. \n In other words, all your existing pods still  have  no  readiness  probe  defined.  You\ncan see this by listing the pods with \nkubectl get pods and looking at the READY col-\numn.  You  need  to  delete  the  pods  and  have  them  re-created  by  the  Replication-\nController.  The  new  pods  will  fail  the  readiness  check  and  won’t  be  included  as\nendpoints of the service until you create the /var/ready file in each of them. \nOBSERVING AND MODIFYING THE PODS’ READINESS STATUS\nList the pods again and inspect whether they’re ready or not:\n$ kubectl get po\nNAME          READY     STATUS    RESTARTS   AGE\nkubia-2r1qb   0/1       Running   0          1m\nkubia-3rax1   0/1       Running   0          1m\nkubia-3yw4s   0/1       Running   0          1m\nThe READY column shows that none of the containers are ready. Now make the readi-\nness  probe  of  one  of  them  start  returning  success  by  creating  the  \n/var/ready  file,\nwhose existence makes your mock readiness probe succeed:\n$ kubectl exec kubia-2r1qb -- touch /var/ready\nYou’ve  used  the  kubectl exec  command  to  execute  the  touch  command  inside  the\ncontainer of the \nkubia-2r1qb pod. The touch command creates the file if it doesn’t\nyet  exist.  The  pod’s  readiness  probe  command  should  now  exit  with  status  code  0,\nwhich means the probe is successful, and the pod should now be shown as ready. Let’s\nsee if it is:\n$ kubectl get po kubia-2r1qb\nNAME          READY     STATUS    RESTARTS   AGE\nkubia-2r1qb   0/1       Running   0          2m\nThe pod still isn’t ready. Is there something wrong or is this the expected result? Take\na more detailed look at the pod with \nkubectl describe. The output should contain\nthe following line:\nReadiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1\n➥ #failure=3\nThe  readiness  probe  is  checked  periodically—every  10  seconds  by  default.  The  pod\nisn’t ready because the readiness probe hasn’t been invoked yet. But in 10 seconds at\nthe  latest,  the  pod  should  become  ready  and  its  IP  should  be  listed  as  the  only  end-\npoint of the service (run \nkubectl get endpoints kubia-loadbalancer to confirm). \n \n\n153Signaling when a pod is ready to accept connections\nHITTING THE SERVICE WITH THE SINGLE READY POD\nYou can now hit the service URL a few times to see that each and every request is redi-\nrected to this one pod:\n$ curl http://130.211.53.173\nYou’ve hit kubia-2r1qb\n$ curl http://130.211.53.173\nYou’ve hit kubia-2r1qb\n...\n$ curl http://130.211.53.173\nYou’ve hit kubia-2r1qb\nEven  though  there  are  three  pods  running,  only  a  single  pod  is  reporting  as  being\nready and is therefore the only pod receiving requests. If you now delete the file, the\npod will be removed from the service again. \n5.5.3Understanding what real-world readiness probes should do\nThis mock readiness probe is useful only for demonstrating what readiness probes do.\nIn the real world, the readiness probe should return success or failure depending on\nwhether the app can (and wants to) receive client requests or not. \n Manually removing pods from services should be performed by either deleting the\npod or changing the pod’s labels instead of manually flipping a switch in the probe. \nTIPIf  you  want  to  add  or  remove  a  pod  from  a  service  manually,  add\nenabled=true as a label to your pod and to the label selector of your service.\nRemove the label when you want to remove the pod from the service.\nALWAYS DEFINE A READINESS PROBE\nBefore we conclude this section, there are two final notes about readiness probes that\nI  need  to  emphasize.  First,  if  you  don’t  add  a  readiness  probe  to  your  pods,  they’ll\nbecome  service  endpoints  almost  immediately.  If  your  application  takes  too  long  to\nstart listening for incoming connections, client requests hitting the service will be for-\nwarded to the pod while it’s still starting up and not ready to accept incoming connec-\ntions. Clients will therefore see “Connection refused” types of errors. \nTIPYou should always define a readiness probe, even if it’s as simple as send-\ning an HTTP request to the base URL. \nDON’T INCLUDE POD SHUTDOWN LOGIC INTO YOUR READINESS PROBES\nThe  other  thing  I  need  to  mention  applies  to  the  other  end  of  the  pod’s  life  (pod\nshutdown) and is also related to clients experiencing connection errors. \n When a pod is being shut down, the app running in it usually stops accepting con-\nnections as soon as it receives the termination signal. Because of this, you might think\nyou  need  to  make  your  readiness  probe  start  failing  as  soon  as  the  shutdown  proce-\ndure is initiated, ensuring the pod is removed from all services it’s part of. But that’s\nnot  necessary,  because  Kubernetes  removes  the  pod  from  all  services  as  soon  as  you\ndelete the pod.\n \n\n154CHAPTER 5Services: enabling clients to discover and talk to pods\n5.6Using a headless service for discovering individual pods\nYou’ve seen how services can be used to provide a stable IP address allowing clients to\nconnect  to  pods  (or  other  endpoints)  backing  each  service.  Each  connection  to  the\nservice  is  forwarded  to  one  randomly  selected  backing  pod.  But  what  if  the  client\nneeds  to  connect  to  all  of  those  pods?  What  if  the  backing  pods  themselves  need  to\neach  connect  to  all  the  other  backing  pods?  Connecting  through  the  service  clearly\nisn’t the way to do this. What is?\n For a client to connect to all pods, it needs to figure out the the IP of each individ-\nual pod. One option is to have the client call the Kubernetes API server and get the\nlist of pods and their IP addresses through an API call, but because you should always\nstrive to keep your apps Kubernetes-agnostic, using the API server isn’t ideal. \n Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually,\nwhen you perform a DNS lookup for a service, the DNS server returns a single IP—the\nservice’s cluster IP. But if you tell Kubernetes you don’t need a cluster IP for your service\n(you do this by setting the \nclusterIP field to None in the service specification), the DNS\nserver will return the pod IPs instead of the single service IP.\n Instead of returning a single DNS \nA record, the DNS server will return multiple A\nrecords for the service, each pointing to the IP of an individual pod backing the ser-\nvice at that moment. Clients can therefore do a simple DNS \nA record lookup and get\nthe IPs of all the pods that are part of the service. The client can then use that infor-\nmation to connect to one, many, or all of them.\n5.6.1Creating a headless service\nSetting  the  clusterIP  field  in  a  service  spec  to  None  makes  the  service  headless,  as\nKubernetes  won’t  assign  it  a  cluster  IP  through  which  clients  could  connect  to  the\npods backing it. \n You’ll create a headless service called \nkubia-headless now. The following listing\nshows its definition.\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia-headless\nspec:\n  clusterIP: None       \n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: kubia\nAfter you create the service with kubectl create, you can inspect it with kubectl get\nand kubectl describe. You’ll see it has no cluster IP and its endpoints include (part of)\nListing 5.18   A headless service: kubia-svc-headless.yaml\nThis makes the \nservice headless.\n \n\n155Using a headless service for discovering individual pods\nthe pods matching its pod selector. I say “part of” because your pods contain a readi-\nness  probe,  so  only  pods  that  are  ready  will  be  listed  as  endpoints  of  the  service.\nBefore continuing, please make sure at least two pods report being ready, by creating\nthe \n/var/ready file, as in the previous example:\n$ kubectl exec <pod name> -- touch /var/ready\n5.6.2Discovering pods through DNS\nWith your pods ready, you can now try performing a DNS lookup to see if you get the\nactual pod IPs or not. You’ll need to perform the lookup from inside one of the pods.\nUnfortunately, your \nkubia container image doesn’t include the nslookup (or the dig)\nbinary, so you can’t use it to perform the DNS lookup.\n All you’re trying to do is perform a DNS lookup from inside a pod running in the\ncluster.  Why  not  run  a  new  pod  based  on  an  image  that  contains  the  binaries  you\nneed?  To  perform  DNS-related  actions,  you  can  use  the  \ntutum/dnsutils  container\nimage, which is available on Docker Hub and contains both the \nnslookup and the dig\nbinaries. To run the pod, you can go through the whole process of creating a YAML\nmanifest  for  it  and  passing  it  to  \nkubectl create,  but  that’s  too  much  work,  right?\nLuckily, there’s a faster way.\nRUNNING A POD WITHOUT WRITING A YAML MANIFEST\nIn chapter 1, you already created pods without writing a YAML manifest by using the\nkubectl run command. But this time you want to create only a pod—you don’t need\nto create a ReplicationController to manage the pod. You can do that like this:\n$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1\n➥ --command -- sleep infinity\npod \"dnsutils\" created\nThe trick is in the --generator=run-pod/v1 option, which tells kubectl to create the\npod directly, without any kind of ReplicationController or similar behind it. \nUNDERSTANDING DNS A RECORDS RETURNED FOR A HEADLESS SERVICE\nLet’s use the newly created pod to perform a DNS lookup:\n$ kubectl exec dnsutils nslookup kubia-headless\n...\nName:    kubia-headless.default.svc.cluster.local\nAddress: 10.108.1.4 \nName:    kubia-headless.default.svc.cluster.local\nAddress: 10.108.2.5 \nThe  DNS  server  returns  two  different  IPs  for  the  kubia-headless.default.svc\n.cluster.local\n FQDN. Those are the IPs of the two pods that are reporting being\nready.  You  can  confirm  this  by  listing  pods  with  \nkubectl get pods -o wide,  which\nshows the pods’ IPs. \n \n\n156CHAPTER 5Services: enabling clients to discover and talk to pods\n This is different from what DNS returns for regular (non-headless) services, such\nas for your \nkubia service, where the returned IP is the service’s cluster IP:\n$ kubectl exec dnsutils nslookup kubia\n...\nName:    kubia.default.svc.cluster.local\nAddress: 10.111.249.153\nAlthough headless services may seem different from regular services, they aren’t that\ndifferent  from  the  clients’  perspective.  Even  with  a  headless  service,  clients  can  con-\nnect to its pods by connecting to the service’s DNS name, as they can with regular ser-\nvices.  But  with  headless  services,  because  DNS  returns  the  pods’  IPs,  clients  connect\ndirectly to the pods, instead of through the service proxy. \nNOTEA headless services still provides load balancing across pods, but through\nthe DNS round-robin mechanism instead of through the service proxy.\n5.6.3Discovering all pods—even those that aren’t ready\nYou’ve  seen  that  only  pods  that  are  ready  become  endpoints  of  services.  But  some-\ntimes you want to use the service discovery mechanism to find all pods matching the\nservice’s label selector, even those that aren’t ready. \n Luckily, you don’t have to resort to querying the Kubernetes API server. You can\nuse the DNS lookup mechanism to find even those unready pods. To tell Kubernetes\nyou want all pods added to a service, regardless of the pod’s readiness status, you must\nadd the following annotation to the service:\nkind: Service\nmetadata:\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: \"true\"\nWARNINGAs the annotation name suggests, as I’m writing this, this is an alpha\nfeature. The Kubernetes Service API already supports a new service spec field\ncalled \npublishNotReadyAddresses, which will replace the tolerate-unready-\nendpoints\n annotation. In Kubernetes version 1.9.0, the field is not honored yet\n(the annotation is what determines whether unready endpoints are included in\nthe DNS or not). Check the documentation to see whether that’s changed.\n5.7Troubleshooting services\nServices  are  a  crucial  Kubernetes  concept  and  the  source  of  frustration  for  many\ndevelopers. I’ve seen many developers lose heaps of time figuring out why they can’t\nconnect to their pods through the service IP or FQDN. For this reason, a short look at\nhow to troubleshoot services is in order.\n When you’re unable to access your pods through the service, you should start by\ngoing through the following list:\n \n\n157Summary\nFirst,  make  sure  you’re  connecting  to  the  service’s  cluster  IP  from  within  the\ncluster, not from the outside.\nDon’t  bother  pinging  the  service  IP  to  figure  out  if  the  service  is  accessible\n(remember, the service’s cluster IP is a virtual IP and pinging it will never work).\nIf  you’ve  defined  a  readiness  probe,  make  sure  it’s  succeeding;  otherwise  the\npod won’t be part of the service.\nTo confirm that a pod is part of the service, examine the corresponding End-\npoints object with \nkubectl get endpoints.\nIf you’re trying to access the service through its FQDN or a part of it (for exam-\nple,  myservice.mynamespace.svc.cluster.local  or  myservice.mynamespace)  and\nit doesn’t work, see if you can access it using its cluster IP instead of the FQDN.\nCheck  whether  you’re  connecting  to  the  port  exposed  by  the  service  and  not\nthe target port.\nTry connecting to the pod IP directly to confirm your pod is accepting connec-\ntions on the correct port.\nIf you can’t even access your app through the pod’s IP, make sure your app isn’t\nonly binding to localhost.\nThis should help you resolve most of your service-related problems. You’ll learn much\nmore  about  how  services  work  in  chapter  11.  By  understanding  exactly  how  they’re\nimplemented, it should be much easier for you to troubleshoot them.\n5.8Summary\nIn this chapter, you’ve learned how to create Kubernetes Service resources to expose\nthe  services  available  in  your  application,  regardless  of  how  many  pod  instances  are\nproviding each service. You’ve learned how Kubernetes\nExposes multiple pods that match a certain label selector under a single, stable\nIP address and port\nMakes services accessible from inside the cluster by default, but allows you to\nmake the service accessible from outside the cluster by setting its type to either\nNodePort or LoadBalancer\nEnables pods to discover services together with their IP addresses and ports by\nlooking up environment variables\nAllows  discovery  of  and  communication  with  services  residing  outside  the\ncluster by creating a Service resource without specifying a selector, by creating\nan associated Endpoints resource instead\nProvides  a  DNS  CNAME  alias  for  external  services  with  the  ExternalName  ser-\nvice type\nExposes multiple HTTP services through a single Ingress (consuming a sin-\ngle IP)\n \n\n158CHAPTER 5Services: enabling clients to discover and talk to pods\nUses a pod container’s readiness probe to determine whether a pod should or\nshouldn’t be included as a service endpoint\nEnables discovery of pod IPs through DNS when you create a headless service\nAlong with getting a better understanding of services, you’ve also learned how to\nTroubleshoot them\nModify firewall rules in Google Kubernetes/Compute Engine\nExecute commands in pod containers through kubectl exec \nRun a bash shell in an existing pod’s container\nModify Kubernetes resources through the kubectl apply command\nRun an unmanaged ad hoc pod with kubectl run --generator=run-pod/v1\n \n\n159\nVolumes: attaching\ndisk storage to containers\nIn the previous three chapters, we introduced pods and other Kubernetes resources\nthat interact with them, namely ReplicationControllers, ReplicaSets, DaemonSets,\nJobs, and Services. Now, we’re going back inside the pod to learn how its containers\ncan access external disk storage and/or share storage between them.\n We’ve said that pods are similar to logical hosts where processes running inside\nthem  share  resources  such  as  CPU,  RAM,  network  interfaces,  and  others.  One\nwould expect the processes to also share disks, but that’s not the case. You’ll remem-\nber  that  each  container  in  a  pod  has  its  own  isolated  filesystem,  because  the  file-\nsystem comes from the container’s image.\nThis chapter covers\nCreating multi-container pods\nCreating a volume to share disk storage between \ncontainers\nUsing a Git repository inside a pod\nAttaching persistent storage such as a GCE \nPersistent Disk to pods\nUsing pre-provisioned persistent storage\nDynamic provisioning of persistent storage\n \n\n160CHAPTER 6Volumes: attaching disk storage to containers\n Every new container starts off with the exact set of files that was added to the image\nat build time. Combine this with the fact that containers in a pod get restarted (either\nbecause  the  process  died  or  because  the  liveness  probe  signaled  to  Kubernetes  that\nthe container wasn’t healthy anymore) and you’ll realize that the new container will\nnot  see  anything  that  was  written  to  the  filesystem  by  the  previous  container,  even\nthough the newly started container runs in the same pod.\n In certain scenarios you want the new container to continue where the last one fin-\nished, such as when restarting a process on a physical machine. You may not need (or\nwant) the whole filesystem to be persisted, but you do want to preserve the directories\nthat hold actual data.\n Kubernetes provides this by defining storage volumes. They aren’t top-level resources\nlike pods, but are instead defined as a part of a pod and share the same lifecycle as the\npod. This means a volume is created when the pod is started and is destroyed when\nthe  pod  is  deleted.  Because  of  this,  a  volume’s  contents  will  persist  across  container\nrestarts. After a container is restarted, the new container can see all the files that were\nwritten to the volume by the previous container. Also, if a pod contains multiple con-\ntainers, the volume can be used by all of them at once. \n6.1Introducing volumes\nKubernetes volumes are a component of a pod and are thus defined in the pod’s spec-\nification—much like containers. They aren’t a standalone Kubernetes object and can-\nnot be created or deleted on their own. A volume is available to all containers in the\npod, but it must be mounted in each container that needs to access it. In each con-\ntainer, you can mount the volume in any location of its filesystem.\n6.1.1Explaining volumes in an example\nImagine you have a pod with three containers (shown in figure 6.1). One container\nruns a web server that serves HTML pages from the /var/htdocs directory and stores\nthe access log to /var/logs. The second container runs an agent that creates HTML\nfiles and stores them in /var/html. The third container processes the logs it finds in\nthe /var/logs directory (rotates them, compresses them, analyzes them, or whatever).\n Each container has a nicely defined single responsibility, but on its own each con-\ntainer wouldn’t be of much use. Creating a pod with these three containers without\nthem  sharing  disk  storage  doesn’t  make  any  sense,  because  the  content  generator\nwould  write  the  generated  HTML  files  inside  its  own  container  and  the  web  server\ncouldn’t access those files, as it runs in a separate isolated container. Instead, it would\nserve an empty directory or whatever you put in the /var/htdocs directory in its con-\ntainer image. Similarly, the log rotator would never have anything to do, because its\n/var/logs directory would always remain empty with nothing writing logs there. A pod\nwith these three containers and no volumes basically does nothing.\n But if you add two volumes to the pod and mount them at appropriate paths inside\nthe three containers, as shown in figure 6.2, you’ve created a system that’s much more\n \n\n161Introducing volumes\nPod\nContainer: WebServer\nFilesystem\nWebserver\nprocess\nWrites\nReads\n/\nvar/\nhtdocs/\nlogs/\nContainer: ContentAgent\nFilesystem\nContentAgent\nprocess\nWrites\n/\nvar/\nhtml/\nContainer: LogRotator\nFilesystem\nLogRotator\nprocess\nReads\n/\nvar/\nlogs/\nFigure 6.1   Three containers of the \nsame pod without shared storage\nPod\nContainer: WebServer\nFilesystem\n/\nvar/\nhtdocs/\nlogs/\nContainer: ContentAgent\nFilesystem\n/\nvar/\nhtml/\nContainer: LogRotator\nFilesystem\n/\nvar/\nlogs/\nVolume:\npublicHtml\nVolume:\nlogVol\nFigure 6.2   Three containers sharing two \nvolumes mounted at various mount paths\n \n\n162CHAPTER 6Volumes: attaching disk storage to containers\nthan the sum of its parts. Linux allows you to mount a filesystem at arbitrary locations\nin the file tree. When you do that, the contents of the mounted filesystem are accessi-\nble in the directory it’s mounted into. By mounting the same volume into two contain-\ners, they can operate on the same files. In your case, you’re mounting two volumes in\nthree containers. By doing this, your three containers can work together and do some-\nthing useful. Let me explain how.\n First, the pod has a volume called \npublicHtml. This volume is mounted in the Web-\nServer\n  container  at  /var/htdocs,  because  that’s  the  directory  the  web  server  serves\nfiles  from.  The  same  volume  is  also  mounted  in  the  \nContentAgent  container,  but  at\n/var/html, because that’s where the agent writes the files to. By mounting this single vol-\nume like that, the web server will now serve the content generated by the content agent.\n Similarly, the pod also has a volume called \nlogVol for storing logs. This volume is\nmounted  at  /var/logs  in  both  the  \nWebServer  and  the  LogRotator  containers.  Note\nthat it isn’t mounted in the \nContentAgent container. The container cannot access its\nfiles,  even  though  the  container  and  the  volume  are  part  of  the  same  pod.  It’s  not\nenough to define a volume in the pod; you need to define a \nVolumeMount inside the\ncontainer’s spec also, if you want the container to be able to access it.\n The two volumes in this example can both initially be empty, so you can use a type\nof volume called \nemptyDir. Kubernetes also supports other types of volumes that are\neither  populated  during  initialization  of  the  volume  from  an  external  source,  or  an\nexisting directory is mounted inside the volume. This process of populating or mount-\ning a volume is performed before the pod’s containers are started. \n A volume is bound to the lifecycle of a pod and will stay in existence only while the\npod  exists,  but  depending  on  the  volume  type,  the  volume’s  files  may  remain  intact\neven after the pod and volume disappear, and can later be mounted into a new vol-\nume. Let’s see what types of volumes exist.\n6.1.2Introducing available volume types\nA  wide  variety  of  volume  types  is  available.  Several  are  generic,  while  others  are  spe-\ncific to the actual storage technologies used underneath. Don’t worry if you’ve never\nheard of those technologies—I hadn’t heard of at least half of them. You’ll probably\nonly use volume types for the technologies you already know and use. Here’s a list of\nseveral of the available volume types:\nemptyDir—A simple empty directory used for storing transient data.\nhostPath—Used  for  mounting  directories  from  the  worker  node’s  filesystem\ninto the pod.\ngitRepo—A volume initialized by checking out the contents of a Git repository.\nnfs—An NFS share mounted into the pod.\ngcePersistentDisk  (Google  Compute  Engine  Persistent  Disk),  awsElastic-\nBlockStore\n  (Amazon  Web  Services  Elastic  Block  Store  Volume),  azureDisk\n(Microsoft  Azure  Disk  Volume)—Used  for  mounting  cloud  provider-specific\nstorage.\n \n\n163Using volumes to share data between containers\ncinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphere-\nVolume\n, photonPersistentDisk, scaleIO—Used  for  mounting  other  types  of\nnetwork storage.\nconfigMap, secret, downwardAPI—Special types of volumes used to expose cer-\ntain Kubernetes resources and cluster information to the pod.\npersistentVolumeClaim—A  way  to  use  a  pre-  or  dynamically  provisioned  per-\nsistent storage. (We’ll talk about them in the last section of this chapter.)\nThese volume types serve various purposes. You’ll learn about some of them in the\nfollowing sections. Special types of volumes (\nsecret, downwardAPI, configMap) are\ncovered in the next two chapters, because they aren’t used for storing data, but for\nexposing Kubernetes metadata to apps running in the pod. \n A single pod can use multiple volumes of different types at the same time, and, as\nwe’ve  mentioned  before,  each  of  the  pod’s  containers  can  either  have  the  volume\nmounted or not.\n6.2Using volumes to share data between containers\nAlthough a volume can prove useful even when used by a single container, let’s first\nfocus on how it’s used for sharing data between multiple containers in a pod.\n6.2.1Using an emptyDir volume\nThe simplest volume type is the emptyDir volume, so let’s look at it in the first exam-\nple of how to define a volume in a pod. As the name suggests, the volume starts out as\nan empty directory. The app running inside the pod can then write any files it needs\nto it. Because the volume’s lifetime is tied to that of the pod, the volume’s contents are\nlost when the pod is deleted.\n  An  \nemptyDir  volume  is  especially  useful  for  sharing  files  between  containers\nrunning in the same pod. But it can also be used by a single container for when a con-\ntainer  needs  to  write  data  to  disk  temporarily,  such  as  when  performing  a  sort\noperation on a large dataset, which can’t fit into the available memory. The data could\nalso be written to the container’s filesystem itself (remember the top read-write layer\nin  a  container?),  but  subtle  differences  exist  between  the  two  options.  A  container’s\nfilesystem may not even be writable (we’ll talk about this toward the end of the book),\nso writing to a mounted volume might be the only option. \nUSING AN EMPTYDIR VOLUME IN A POD\nLet’s revisit the previous example where a web server, a content agent, and a log rota-\ntor  share  two  volumes,  but  let’s  simplify  a  bit.  You’ll  build  a  pod  with  only  the  web\nserver container and the content agent and a single volume for the HTML. \n You’ll use Nginx as the web server and the UNIX \nfortune command to generate\nthe HTML content. The \nfortune command prints out a random quote every time you\nrun it. You’ll create a script that invokes the \nfortune command every 10 seconds and\nstores  its  output  in  index.html.  You’ll  find  an  existing  Nginx  image  available  on\n \n\n164CHAPTER 6Volumes: attaching disk storage to containers\nDocker Hub, but you’ll need to either create the fortune  image  yourself  or  use  the\none I’ve already built and pushed to Docker Hub under \nluksa/fortune. If you want a\nrefresher on how to build Docker images, refer to the sidebar.\nCREATING THE POD\nNow that you have the two images required to run your pod, it’s time to create the pod\nmanifest. Create a file called fortune-pod.yaml with the contents shown in the follow-\ning listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune\nspec:\n  containers:\nBuilding the fortune container image\nHere’s how to build the image. Create a new directory called fortune and then inside\nit, create a \nfortuneloop.sh shell script with the following contents:\n#!/bin/bash\ntrap \"exit\" SIGINT\nmkdir /var/htdocs\nwhile :\ndo\n  echo $(date) Writing fortune to /var/htdocs/index.html\n  /usr/games/fortune > /var/htdocs/index.html\n  sleep 10\ndone\nThen, in the same directory, create a file called Dockerfile containing the following:\nFROM ubuntu:latest\nRUN apt-get update ; apt-get -y install fortune\nADD fortuneloop.sh /bin/fortuneloop.sh\nENTRYPOINT /bin/fortuneloop.sh\nThe image is based on the ubuntu:latest image, which doesn’t include the fortune\nbinary by default. That’s why in the second line of the Dockerfile you install it with\napt-get. After that, you add the fortuneloop.sh script to the image’s /bin folder.\nIn the last line of the Dockerfile, you specify that the \nfortuneloop.sh script should\nbe executed when the image is run.\nAfter preparing both files, build and upload the image to Docker Hub with the following\ntwo commands (replace \nluksa with your own Docker Hub user ID):\n$ docker build -t luksa/fortune .\n$ docker push luksa/fortune\nListing 6.1   A pod with two containers sharing the same volume: fortune-pod.yaml\n \n\n165Using volumes to share data between containers\n  - image: luksa/fortune                   \n    name: html-generator                   \n    volumeMounts:                          \n    - name: html                           \n      mountPath: /var/htdocs               \n  - image: nginx:alpine                   \n    name: web-server                      \n    volumeMounts:                         \n    - name: html                          \n      mountPath: /usr/share/nginx/html    \n      readOnly: true                      \n    ports:\n    - containerPort: 80\n      protocol: TCP\n  volumes:                 \n  - name: html             \n    emptyDir: {}           \nThe  pod  contains  two  containers  and  a  single  volume  that’s  mounted  in  both  of\nthem, yet at different paths. When the \nhtml-generator container starts, it starts writ-\ning the output of the \nfortune command to the /var/htdocs/index.html file every 10\nseconds. Because the volume is mounted at /var/htdocs, the index.html file is writ-\nten to the volume instead of the container’s top layer. As soon as the \nweb-server con-\ntainer starts, it starts serving whatever HTML files are in the /usr/share/nginx/html\ndirectory (this is the default directory Nginx serves files from). Because you mounted\nthe volume in that exact location, Nginx will serve the index.html file written there\nby the container running the fortune loop. The end effect is that a client sending an\nHTTP  request  to  the  pod  on  port  80  will  receive  the  current  fortune  message  as\nthe response. \nSEEING THE POD IN ACTION\nTo see the fortune message, you need to enable access to the pod. You’ll do that by\nforwarding a port from your local machine to the pod:\n$ kubectl port-forward fortune 8080:80\nForwarding from 127.0.0.1:8080 -> 80\nForwarding from [::1]:8080 -> 80\nNOTEAs an exercise, you can also expose the pod through a service instead\nof using port forwarding.\nNow  you  can  access  the  Nginx  server  through  port  8080  of  your  local  machine.  Use\ncurl to do that:\n$ curl http://localhost:8080\nBeware of a tall blond man with one black shoe.\nIf  you  wait  a  few  seconds  and  send  another  request,  you  should  receive  a  different\nmessage. By combining two containers, you created a simple app to see how a volume\ncan glue together two containers and enhance what each of them does.\nThe first container is called html-generator \nand runs the luksa/fortune image.\nThe volume called html is mounted \nat /var/htdocs in the container.\nThe second container is called web-server \nand runs the nginx:alpine image.\nThe same volume as above is \nmounted at /usr/share/nginx/html \nas read-only.\nA single emptyDir volume \ncalled html that’s mounted \nin the two containers above\n \n\n166CHAPTER 6Volumes: attaching disk storage to containers\nSPECIFYING THE MEDIUM TO USE FOR THE EMPTYDIR\nThe emptyDir  you  used  as  the  volume  was  created  on  the  actual  disk  of  the  worker\nnode hosting your pod, so its performance depends on the type of the node’s disks.\nBut you can tell Kubernetes to create the \nemptyDir on a tmpfs filesystem (in memory\ninstead of on disk). To do this, set the \nemptyDir’s medium to Memory like this:\nvolumes:\n  - name: html\n    emptyDir:\n      medium: Memory    \nAn emptyDir  volume  is  the  simplest  type  of  volume,  but  other  types  build  upon  it.\nAfter the empty directory is created, they populate it with data. One such volume type\nis the \ngitRepo volume type, which we’ll introduce next.\n6.2.2Using a Git repository as the starting point for a volume \nA gitRepo  volume  is  basically  an  emptyDir volume that gets populated by cloning a\nGit  repository  and  checking  out  a  specific  revision  when  the  pod  is  starting  up  (but\nbefore its containers are created). Figure 6.3 shows how this unfolds.\nNOTEAfter the gitRepo volume is created, it isn’t kept in sync with the repo\nit’s referencing. The files in the volume will not be updated when you push\nadditional commits to the Git repository. However, if your pod is managed by\na ReplicationController, deleting the pod will result in a new pod being cre-\nated and this new pod’s volume will then contain the latest commits. \nFor example, you can use a Git repository to store static HTML files of your website\nand create a pod containing a web server container and a \ngitRepo volume. Every time\nthe pod is created, it pulls the latest version of your website and starts serving it. The\nThis emptyDir’s \nfiles should be \nstored in memory.\nPod\nContainer\nUser\ngitRepo\nvolume\n1. User (or a replication\ncontroller) creates pod\nwith gitRepo volume\n2. Kubernetes creates\nan empty directory and\nclones the specified Git\nrepository into it\n3. The pod’s container is started\n(with the volume mounted at\nthe mount path)\nRepository\nFigure 6.3   A gitRepo volume is an emptyDir volume initially populated with the contents of a \nGit repository.\n \n\n167Using volumes to share data between containers\nonly drawback to this is that you need to delete the pod every time you push changes\nto the \ngitRepo and want to start serving the new version of the website. \n Let’s do this right now. It’s not that different from what you did before. \nRUNNING A WEB SERVER POD SERVING FILES FROM A CLONED GIT REPOSITORY\nBefore you create your pod, you’ll need an actual Git repository with HTML files in it.\nI’ve created a repo on GitHub at https://github.com/luksa/kubia-website-example.git.\nYou’ll need to fork it (create your own copy of the repo on GitHub) so you can push\nchanges to it later. \n Once you’ve created your fork, you can move on to creating the pod. This time,\nyou’ll only need a single Nginx container and a single \ngitRepo volume in the pod (be\nsure to point the \ngitRepo volume to your own fork of my repository), as shown in the\nfollowing listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gitrepo-volume-pod\nspec:\n  containers:\n  - image: nginx:alpine\n    name: web-server\n    volumeMounts:\n    - name: html\n      mountPath: /usr/share/nginx/html\n      readOnly: true\n    ports:\n    - containerPort: 80\n      protocol: TCP\n  volumes:\n  - name: html\n    gitRepo:                     \n      repository: https://github.com/luksa/kubia-website-example.git   \n      revision: master                     \n      directory: .      \nWhen you create the pod, the volume is first initialized as an empty directory and then\nthe specified Git repository is cloned into it. If you hadn’t set the directory to \n. (dot),\nthe repository would have been cloned into the kubia-website-example subdirectory,\nwhich isn’t what you want. You want the repo to be cloned into the root directory of\nyour volume. Along with the repository, you also specified you want Kubernetes to\ncheck out whatever revision the master branch is pointing to at the time the volume\nis created. \n With the pod running, you can try hitting it through port forwarding, a service, or by\nexecuting the \ncurl command from within the pod (or any other pod inside the cluster). \nListing 6.2   A pod using a gitRepo volume: gitrepo-volume-pod.yaml\nYou’re creating a \ngitRepo volume.\nThe volume will clone\nthis Git repository.\nThe master branch \nwill be checked out.\nYou want the repo to \nbe cloned into the root \ndir of the volume.\n \n\n168CHAPTER 6Volumes: attaching disk storage to containers\nCONFIRMING THE FILES AREN’T KEPT IN SYNC WITH THE GIT REPO\nNow  you’ll  make  changes  to  the  index.html  file  in  your  GitHub  repository.  If  you\ndon’t use Git locally, you can edit the file on GitHub directly—click on the file in your\nGitHub repository to open it and then click on the pencil icon to start editing it.\nChange the text and then commit the changes by clicking the button at the bottom.\n The master branch of the Git repository now includes the changes you made to the\nHTML  file.  These  changes  will  not  be  visible  on  your  Nginx  web  server  yet,  because\nthe \ngitRepo volume isn’t kept in sync with the Git repository. You can confirm this by\nhitting the pod again. \n  To  see  the  new  version  of  the  website,  you  need  to  delete  the  pod  and  create\nit again. Instead of having to delete the pod every time you make changes, you could\nrun an additional process, which keeps your volume in sync with the Git repository.\nI won’t explain in detail how to do this. Instead, try doing this yourself as an exer-\ncise, but here are a few pointers.\nINTRODUCING SIDECAR CONTAINERS\nThe Git sync process shouldn’t run in the same container as the Nginx web server, but\nin a second container: a sidecar container. A sidecar container is a container that aug-\nments the operation of the main container of the pod. You add a sidecar to a pod so\nyou can use an existing container image instead of cramming additional logic into the\nmain app’s code, which would make it overly complex and less reusable. \n  To  find  an  existing  container  image,  which  keeps  a  local  directory  synchronized\nwith a Git repository, go to Docker Hub and search for “git sync.” You’ll find many\nimages that do that. Then use the image in a new container in the pod from the previ-\nous  example,  mount  the  pod’s  existing  \ngitRepo  volume  in  the  new  container,  and\nconfigure the Git sync container to keep the files in sync with your Git repo. If you set\neverything up correctly, you should see that the files the web server is serving are kept\nin sync with your GitHub repo. \nNOTEAn example in chapter 18 includes using a Git sync container like the\none explained here, so you can wait until you reach chapter 18 and follow the\nstep-by-step instructions then instead of doing this exercise on your own now. \nUSING A GITREPO VOLUME WITH PRIVATE GIT REPOSITORIES\nThere’s  one  other  reason  for  having  to  resort  to  Git  sync  sidecar  containers.  We\nhaven’t talked about whether you can use a \ngitRepo volume with a private Git repo. It\nturns out you can’t. The current consensus among Kubernetes developers is to keep\nthe \ngitRepo volume simple and not add any support for cloning private repositories\nthrough  the  SSH  protocol,  because  that  would  require  adding  additional  config\noptions to the \ngitRepo volume. \n If you want to clone a private Git repo into your container, you should use a git-\nsync sidecar or a similar method instead of a \ngitRepo volume.\n \n\n169Accessing files on the worker node’s filesystem\nWRAPPING UP THE GITREPO VOLUME\nA gitRepo  volume,  like  the  emptyDir  volume,  is  basically  a  dedicated  directory  cre-\nated specifically for, and used exclusively by, the pod that contains the volume. When\nthe pod is deleted, the volume and its contents are deleted. Other types of volumes,\nhowever, don’t create a new directory, but instead mount an existing external direc-\ntory  into  the  pod’s  container’s  filesystem.  The  contents  of  that  volume  can  survive\nmultiple pod instantiations. We’ll learn about those types of volumes next.\n6.3Accessing files on the worker node’s filesystem\nMost  pods should be oblivious of their host node, so they shouldn’t access any files on\nthe node’s filesystem. But certain system-level pods (remember, these will usually be\nmanaged by a DaemonSet) do need to either read the node’s files or use the node’s\nfilesystem to access the node’s devices through the filesystem. Kubernetes makes this\npossible through a \nhostPath volume. \n6.3.1Introducing the hostPath volume\nA hostPath volume points to a specific file or directory on the node’s filesystem (see\nfigure 6.4). Pods running on the same node and using the same path in their \nhost-\nPath\n volume see the same files.\nhostPath  volumes  are  the  first  type  of  persistent  storage  we’re  introducing,  because\nboth  the  \ngitRepo  and  emptyDir  volumes’  contents  get  deleted  when  a  pod  is  torn\ndown, whereas a \nhostPath volume’s contents don’t. If a pod is deleted and the next\npod uses a \nhostPath volume pointing to the same path on the host, the new pod will\nsee whatever was left behind by the previous pod, but only if it’s scheduled to the same\nnode as the first pod.\nNode 1\nPod\nhostPath\nvolume\nPod\nhostPath\nvolume\nNode 2\nPod\nhostPath\nvolume\n/some/path/on/host/some/path/on/host\nFigure 6.4   A hostPath volume mounts a file or directory on the worker node into \nthe container’s filesystem.\n \n\n170CHAPTER 6Volumes: attaching disk storage to containers\n  If  you’re  thinking  of  using  a  hostPath  volume  as  the  place  to  store  a  database’s\ndata  directory,  think  again.  Because  the  volume’s  contents  are  stored  on  a  specific\nnode’s filesystem, when the database pod gets rescheduled to another node, it will no\nlonger see the data. This explains why it’s not a good idea to use a \nhostPath volume\nfor regular pods, because it makes the pod sensitive to what node it’s scheduled to.\n6.3.2Examining system pods that use hostPath volumes\nLet’s see how a hostPath volume can be used properly. Instead of creating a new pod,\nlet’s see if any existing system-wide pods are already using this type of volume. As you\nmay remember from one of the previous chapters, several such pods are running in\nthe \nkube-system namespace. Let’s list them again:\n$ kubectl get pod s --namespace kube-system\nNAME                          READY     STATUS    RESTARTS   AGE\nfluentd-kubia-4ebc2f1e-9a3e   1/1       Running   1          4d\nfluentd-kubia-4ebc2f1e-e2vz   1/1       Running   1          31d\n...\nPick the first one and see what kinds of volumes it uses (shown in the following listing).\n$ kubectl describe po fluentd-kubia-4ebc2f1e-9a3e --namespace kube-system\nName:           fluentd-cloud-logging-gke-kubia-default-pool-4ebc2f1e-9a3e\nNamespace:      kube-system\n...\nVolumes:\n  varlog:\n    Type:       HostPath (bare host directory volume)\n    Path:       /var/log\n  varlibdockercontainers:\n    Type:       HostPath (bare host directory volume)\n    Path:       /var/lib/docker/containers\nTIPIf you’re using Minikube, try the kube-addon-manager-minikube pod.\nAha! The pod uses two \nhostPath volumes to gain access to the node’s /var/log and\nthe /var/lib/docker/containers directories. You’d think you were lucky to find a pod\nusing a \nhostPath volume on the first try, but not really (at least not on GKE). Check\nthe other pods, and you’ll see most use this type of volume either to access the node’s\nlog files, kubeconfig (the Kubernetes config file), or the CA certificates.\n If you inspect the other pods, you’ll see none of them uses the \nhostPath volume\nfor storing their own data. They all use it to get access to the node’s data. But as we’ll\nsee later in the chapter, \nhostPath volumes are often used for trying out persistent stor-\nage  in  single-node  clusters,  such  as  the  one  created  by  Minikube.  Read  on  to  learn\nabout the types of volumes you should use for storing persistent data properly even in\na multi-node cluster.\nListing 6.3    A pod using hostPath volumes to access the node’s logs\n \n\n171Using persistent storage\nTIPRemember to use hostPath volumes only if you need to read or write sys-\ntem files on the node. Never use them to persist data across pods. \n6.4Using persistent storage\nWhen  an  application  running  in  a  pod  needs  to  persist  data  to  disk  and  have  that\nsame data available even when the pod is rescheduled to another node, you can’t use\nany of the volume types we’ve mentioned so far. Because this data needs to be accessi-\nble from any cluster node, it must be stored on some type of network-attached stor-\nage (NAS).\n To learn about volumes that allow persisting data, you’ll create a pod that will run\nthe MongoDB document-oriented NoSQL database. Running a database pod without\na  volume  or  with  a  non-persistent  volume  doesn’t  make  sense,  except  for  testing\npurposes, so you’ll add an appropriate type of volume to the pod and mount it in the\nMongoDB container. \n6.4.1Using a GCE Persistent Disk in a pod volume\nIf  you’ve  been  running  these  examples  on  Google  Kubernetes  Engine,  which  runs\nyour  cluster  nodes  on  Google  Compute  Engine  (GCE),  you’ll  use  a  GCE  Persistent\nDisk as your underlying storage mechanism. \n In the early versions, Kubernetes didn’t provision the underlying storage automati-\ncally—you had to do that manually. Automatic provisioning is now possible, and you’ll\nlearn  about  it  later  in  the  chapter,  but  first,  you’ll  start  by  provisioning  the  storage\nmanually. It will give you a chance to learn exactly what’s going on underneath. \nCREATING A GCE PERSISTENT DISK\nYou’ll start by creating the GCE persistent disk first. You need to create it in the same\nzone  as  your  Kubernetes  cluster.  If  you  don’t  remember  what  zone  you  created  the\ncluster in, you can see it by listing your Kubernetes clusters with the \ngcloud command\nlike this:\n$ gcloud container clusters list\nNAME   ZONE            MASTER_VERSION  MASTER_IP       ...\nkubia  europe-west1-b  1.2.5           104.155.84.137  ...\nThis shows you’ve created your cluster in zone europe-west1-b, so you need to create\nthe GCE persistent disk in the same zone as well. You create the disk like this:\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b mongodb\nWARNING: You have selected a disk size of under [200GB]. This may result in \npoor I/O performance. For more information, see: \nhttps://developers.google.com/compute/docs/disks#pdperformance.\nCreated [https://www.googleapis.com/compute/v1/projects/rapid-pivot-\n136513/zones/europe-west1-b/disks/mongodb].\nNAME     ZONE            SIZE_GB  TYPE         STATUS\nmongodb  europe-west1-b  1        pd-standard  READY\n \n\n172CHAPTER 6Volumes: attaching disk storage to containers\nThis  command  creates  a  1  GiB  large  GCE  persistent  disk  called  mongodb.  You  can\nignore the warning about the disk size, because you don’t care about the disk’s perfor-\nmance for the tests you’re about to run.\nCREATING A POD USING A GCEPERSISTENTDISK VOLUME\nNow that you have your physical storage properly set up, you can use it in a volume\ninside your MongoDB pod. You’re going to prepare the YAML for the pod, which is\nshown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mongodb \nspec:\n  volumes:\n  - name: mongodb-data          \n    gcePersistentDisk:           \n      pdName: mongodb            \n      fsType: ext4             \n  containers:\n  - image: mongo\n    name: mongodb\n    volumeMounts:                \n    - name: mongodb-data         \n      mountPath: /data/db      \n    ports:\n    - containerPort: 27017\n      protocol: TCP\nNOTEIf you’re using Minikube, you can’t use a GCE Persistent Disk, but you\ncan  deploy  \nmongodb-pod-hostpath.yaml,  which  uses  a  hostPath  volume\ninstead of a GCE PD.\nThe  pod  contains  a  single  container  and  a  single  volume  backed  by  the  GCE  Per-\nsistent  Disk  you’ve  created  (as  shown  in  figure  6.5).  You’re  mounting  the  volume\ninside the container at /data/db, because that’s where MongoDB stores its data.\nListing 6.4   A pod using a gcePersistentDisk volume: mongodb-pod-gcepd.yaml\nThe name\nof the\nvolume\n(also\nreferenced\nwhen\nmounting\nthe volume)\nThe type of the volume \nis a GCE Persistent Disk.\nThe name of the persistent \ndisk must match the actual \nPD you created earlier.\nThe filesystem type is EXT4 \n(a type of Linux filesystem).\nThe path where MongoDB \nstores its data\nPod: mongodb\nContainer: mongodb\nvolumeMounts:\nname: mongodb-data\nmountPath: /data/db\ngcePersistentDisk:\npdName: mongodb\nGCE\nPersistent Disk:\nmongodb\nVolume:\nmongodb\nFigure 6.5   A pod with a single container running MongoDB, which mounts a volume referencing an \nexternal GCE Persistent Disk\n \n\n173Using persistent storage\nWRITING DATA TO THE PERSISTENT STORAGE BY ADDING DOCUMENTS TO YOUR MONGODB DATABASE\nNow that you’ve created the pod and the container has been started, you can run the\nMongoDB shell inside the container and use it to write some data to the data store.\n You’ll run the shell as shown in the following listing.\n$ kubectl exec -it mongodb mongo\nMongoDB shell version: 3.2.8\nconnecting to: mongodb://127.0.0.1:27017\nWelcome to the MongoDB shell.\nFor interactive help, type \"help\".\nFor more comprehensive documentation, see\n    http://docs.mongodb.org/\nQuestions? Try the support group\n    http://groups.google.com/group/mongodb-user\n...\n> \nMongoDB allows storing JSON documents, so you’ll store one to see if it’s stored per-\nsistently and can be retrieved after the pod is re-created. Insert a new JSON document\nwith the following commands: \n> use mystore\nswitched to db mystore\n> db.foo.insert({name:'foo'})\nWriteResult({ \"nInserted\" : 1 })\nYou’ve inserted a simple JSON document with a single property (name: ’foo’). Now,\nuse the \nfind() command to see the document you inserted:\n> db.foo.find()\n{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\nThere it is. The document should be stored in your GCE persistent disk now. \nRE-CREATING THE POD AND VERIFYING THAT IT CAN READ THE DATA PERSISTED BY THE PREVIOUS POD\nYou can now exit the mongodb shell (type exit and press Enter), and then delete the\npod and recreate it:\n$ kubectl delete pod mongodb\npod \"mongodb\" deleted\n$ kubectl create -f mongodb-pod-gcepd.yaml\npod \"mongodb\" created\nThe  new  pod  uses  the  exact  same  GCE  persistent  disk  as  the  previous  pod,  so  the\nMongoDB container running inside it should see the exact same data, even if the pod\nis scheduled to a different node.\nTIPYou can see what node a pod is scheduled to by running kubectl get po\n-o\n wide.\nListing 6.5   Entering the MongoDB shell inside the mongodb pod\n \n\n174CHAPTER 6Volumes: attaching disk storage to containers\nOnce the container is up, you can again run the MongoDB shell and check to see if the\ndocument you stored earlier can still be retrieved, as shown in the following listing.\n$ kubectl exec -it mongodb mongo\nMongoDB shell version: 3.2.8\nconnecting to: mongodb://127.0.0.1:27017\nWelcome to the MongoDB shell.\n...\n> use mystore\nswitched to db mystore\n> db.foo.find()\n{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\nAs expected, the data is still there, even though you deleted the pod and re-created it.\nThis  confirms  you  can  use  a  GCE  persistent  disk  to  persist  data  across  multiple  pod\ninstances. \n You’re done playing with the MongoDB pod, so go ahead and delete it again, but\nhold off on deleting the underlying GCE persistent disk. You’ll use it again later in\nthe chapter.\n6.4.2Using other types of volumes with underlying persistent storage\nThe reason you created the GCE Persistent Disk volume is because your Kubernetes\ncluster runs on Google Kubernetes Engine. When you run your cluster elsewhere, you\nshould use other types of volumes, depending on the underlying infrastructure.\n If your Kubernetes cluster is running on Amazon’s AWS EC2, for example, you can\nuse an \nawsElasticBlockStore volume to provide persistent storage for your pods. If\nyour  cluster  runs  on  Microsoft  Azure,  you  can  use  the  \nazureFile  or  the  azureDisk\nvolume. We won’t go into detail on how to do that here, but it’s virtually the same as in\nthe  previous  example.  First,  you  need  to  create  the  actual  underlying  storage,  and\nthen set the appropriate properties in the volume definition.\nUSING AN AWS ELASTIC BLOCK STORE VOLUME\nFor  example,  to  use  an  AWS  elastic  block  store  instead  of  the  GCE  Persistent  Disk,\nyou’d only need to change the volume definition as shown in the following listing (see\nthose lines printed in bold).\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mongodb \nspec:\n  volumes:                       \n  - name: mongodb-data           \n    awsElasticBlockStore:          \nListing 6.6   Retrieving MongoDB’s persisted data in a new pod\nListing 6.7   A pod using an awsElasticBlockStore volume: mongodb-pod-aws.yaml\nUsing awsElasticBlockStore \ninstead of gcePersistentDisk\n \n\n175Using persistent storage\n      volumeId: my-volume          \n      fsType: ext4       \n  containers:\n  - ...\nUSING AN NFS VOLUME\nIf your cluster is running on your own set of servers, you have a vast array of other sup-\nported  options  for  mounting  external  storage  inside  your  volume.  For  example,  to\nmount  a  simple  NFS  share,  you  only  need  to  specify  the  NFS  server  and  the  path\nexported by the server, as shown in the following listing.\n  volumes:                       \n  - name: mongodb-data           \n    nfs:                     \n      server: 1.2.3.4         \n      path: /some/path     \nUSING OTHER STORAGE TECHNOLOGIES\nOther supported options include iscsi for mounting an ISCSI disk resource, glusterfs\nfor a GlusterFS mount, rbd for a RADOS Block Device, flexVolume, cinder, cephfs,\nflocker, fc (Fibre Channel), and others. You don’t need to know all of them if you’re\nnot  using  them.  They’re  mentioned  here  to  show  you  that  Kubernetes  supports  a\nbroad  range  of  storage  technologies  and  you  can  use  whichever  you  prefer  and  are\nused to.\n To see details on what properties you need to set for each of these volume types,\nyou  can  either  turn  to  the  Kubernetes  API  definitions  in  the  Kubernetes  API  refer-\nence or look up the information through \nkubectl explain, as shown in chapter 3. If\nyou’re already familiar with a particular storage technology, using the \nexplain com-\nmand should allow you to easily figure out how to mount a volume of the proper type\nand use it in your pods.\n But does a developer need to know all this stuff? Should a developer, when creat-\ning  a  pod,  have  to  deal  with  infrastructure-related  storage  details,  or  should  that  be\nleft to the cluster administrator? \n  Having  a  pod’s  volumes  refer  to  the  actual  underlying  infrastructure  isn’t  what\nKubernetes  is  about,  is  it?  For  example,  for  a  developer  to  have  to  specify  the  host-\nname of the NFS server feels wrong. And that’s not even the worst thing about it. \n  Including  this  type  of  infrastructure-related  information  into  a  pod  definition\nmeans  the  pod  definition  is  pretty  much  tied  to  a  specific  Kubernetes  cluster.  You\ncan’t use the same pod definition in another one. That’s why using volumes like this\nisn’t the best way to attach persistent storage to your pods. You’ll learn how to improve\non this in the next section.\nListing 6.8   A pod using an nfs volume: mongodb-pod-nfs.yaml\nSpecify the ID of the EBS \nvolume you created.\nThe filesystem type \nis EXT4 as before.\nThis volume is backed \nby an NFS share.\nThe IP of the \nNFS server\nThe path exported \nby the server\n \n\n176CHAPTER 6Volumes: attaching disk storage to containers\n6.5Decoupling pods from the underlying storage technology\nAll the persistent volume types we’ve explored so far have required the developer of the\npod to have knowledge of the actual network storage infrastructure available in the clus-\nter. For example, to create a NFS-backed volume, the developer has to know the actual\nserver the NFS export is located on. This is against the basic idea of Kubernetes, which\naims to hide the actual infrastructure from both the application and its developer, leav-\ning them free from worrying about the specifics of the infrastructure and making apps\nportable across a wide array of cloud providers and on-premises datacenters.\n  Ideally,  a  developer  deploying  their  apps  on  Kubernetes  should  never  have  to\nknow what kind of storage technology is used underneath, the same way they don’t\nhave to know what type of physical servers are being used to run their pods. Infrastruc-\nture-related dealings should be the sole domain of the cluster administrator.\n When a developer needs a certain amount of persistent storage for their applica-\ntion, they can request it from Kubernetes, the same way they can request CPU, mem-\nory, and other resources when creating a pod. The system administrator can configure\nthe cluster so it can give the apps what they request.\n6.5.1Introducing PersistentVolumes and PersistentVolumeClaims\nTo enable apps to request storage in a Kubernetes cluster without having to deal with\ninfrastructure  specifics,  two  new  resources  were  introduced.  They  are  Persistent-\nVolumes and PersistentVolumeClaims. The names may be a bit misleading, because as\nyou’ve  seen  in  the  previous  few  sections,  even  regular  Kubernetes  volumes  can  be\nused to store persistent data. \n Using a PersistentVolume inside a pod is a little more complex than using a regular\npod  volume,  so  let’s  illustrate  how  pods,  PersistentVolumeClaims,  PersistentVolumes,\nand the actual underlying storage relate to each other in figure 6.6.\nPod\nAdmin\nVolume\n1. Cluster admin sets up some type of\nnetwork storage (NFS export or similar)\n2. Admin then creates a PersistentVolume (PV)\nby posting a PV descriptor to the Kubernetes API\nNFS\nexport\nPersistent\nVolume\nUser\nPersistent\nVolumeClaim\n3. User creates a\nPersistentVolumeClaim (PVC)\n4. Kubernetes finds a PV of\nadequate size and access\nmode and binds the PVC\nto the PV\n5. User creates a\npod with a volume\nreferencing the PVC\nFigure 6.6   PersistentVolumes are provisioned by cluster admins and consumed by pods \nthrough PersistentVolumeClaims.\n \n\n177Decoupling pods from the underlying storage technology\nInstead  of  the  developer  adding  a  technology-specific  volume  to  their  pod,  it’s  the\ncluster  administrator  who  sets  up  the  underlying  storage  and  then  registers  it  in\nKubernetes  by  creating  a  PersistentVolume  resource  through  the  Kubernetes  API\nserver. When creating the PersistentVolume, the admin specifies its size and the access\nmodes it supports. \n When a cluster user needs to use persistent storage in one of their pods, they first\ncreate a PersistentVolumeClaim manifest, specifying the minimum size and the access\nmode they require. The user then submits the PersistentVolumeClaim manifest to the\nKubernetes  API  server,  and  Kubernetes  finds  the  appropriate  PersistentVolume  and\nbinds the volume to the claim. \n The PersistentVolumeClaim can then be used as one of the volumes inside a pod.\nOther users cannot use the same PersistentVolume until it has been released by delet-\ning the bound PersistentVolumeClaim.\n6.5.2Creating a PersistentVolume\nLet’s revisit the MongoDB example, but unlike before, you won’t reference the GCE\nPersistent  Disk  in  the  pod  directly.  Instead, you’ll first assume the role of a cluster\nadministrator and create a PersistentVolume backed by the GCE Persistent Disk. Then\nyou’ll assume the role of the application developer and first claim the PersistentVol-\nume and then use it inside your pod.\n In section 6.4.1 you set up the physical storage by provisioning the GCE Persistent\nDisk, so you don’t need to do that again. All you need to do is create the Persistent-\nVolume resource in Kubernetes by preparing the manifest shown in the following list-\ning and posting it to the API server.\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mongodb-pv\nspec:\n  capacity:                  \n    storage: 1Gi             \n  accessModes:                              \n  - ReadWriteOnce                           \n  - ReadOnlyMany                            \n  persistentVolumeReclaimPolicy: Retain    \n  gcePersistentDisk:                      \n    pdName: mongodb                       \n    fsType: ext4                          \nListing 6.9   A gcePersistentDisk PersistentVolume: mongodb-pv-gcepd.yaml\nDefining the \nPersistentVolume’s size\nIt can either be mounted by a single \nclient for reading and writing or by \nmultiple clients for reading only.\nAfter the claim is released, \nthe PersistentVolume \nshould be retained (not \nerased or deleted).\nThe PersistentVolume is \nbacked by the GCE Persistent \nDisk you created earlier.\n \n\n178CHAPTER 6Volumes: attaching disk storage to containers\nNOTEIf  you’re  using  Minikube,  create  the  PV  using  the  mongodb-pv-host-\npath.yaml file.\nWhen creating a PersistentVolume, the administrator needs to tell Kubernetes what its\ncapacity  is  and  whether  it  can  be  read  from  and/or  written  to  by  a  single  node  or  by\nmultiple nodes at the same time. They also need to tell Kubernetes what to do with the\nPersistentVolume when it’s released (when the PersistentVolumeClaim it’s bound to is\ndeleted). And last, but certainly not least, they need to specify the type, location, and\nother properties of the actual storage this PersistentVolume is backed by. If you look\nclosely, this last part is exactly the same as earlier, when you referenced the GCE Per-\nsistent Disk in the pod volume directly (shown again in the following listing).\nspec:\n  volumes:                       \n  - name: mongodb-data           \n    gcePersistentDisk:           \n      pdName: mongodb            \n      fsType: ext4               \n  ...\nAfter you create the PersistentVolume with the kubectl create command, it should\nbe ready to be claimed. See if it is by listing all PersistentVolumes:\n$ kubectl get pv\nNAME         CAPACITY   RECLAIMPOLICY   ACCESSMODES   STATUS      CLAIM\nmongodb-pv   1Gi        Retain          RWO,ROX       Available   \nNOTESeveral  columns  are  omitted.  Also,  pv  is  used  as  a  shorthand  for\npersistentvolume.\nAs expected, the PersistentVolume is shown as Available, because you haven’t yet cre-\nated the PersistentVolumeClaim. \nNOTEPersistentVolumes  don’t  belong  to  any  namespace  (see  figure  6.7).\nThey’re cluster-level resources like nodes.\nListing 6.10   Referencing a GCE PD in a pod’s volume\n \n\n179Decoupling pods from the underlying storage technology\n6.5.3Claiming a PersistentVolume by creating a \nPersistentVolumeClaim\nNow let’s lay down our admin hats and put our developer hats back on. Say you need\nto deploy a pod that requires persistent storage. You’ll use the PersistentVolume you\ncreated earlier. But you can’t use it directly in the pod. You need to claim it first.\n Claiming a PersistentVolume is a completely separate process from creating a pod,\nbecause you want the same PersistentVolumeClaim to stay available even if the pod is\nrescheduled (remember, rescheduling means the previous pod is deleted and a new\none is created). \nCREATING A PERSISTENTVOLUMECLAIM\nYou’ll create the claim now. You need to prepare a PersistentVolumeClaim manifest\nlike the one shown in the following listing and post it to the Kubernetes API through\nkubectl create.\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mongodb-pvc          \nListing 6.11   A PersistentVolumeClaim: mongodb-pvc.yaml\nPod(s)Pod(s)\nPersistent\nVolume\nPersistent\nVolume\nPersistent\nVolume\nPersistent\nVolume\n...\nUser A\nPersistent\nVolume\nClaim(s)\nPersistent\nVolume\nClaim(s)\nNamespace A\nUser B\nNamespace B\nNodeNodeNodeNodeNodeNode\nPersistent\nVolume\nFigure 6.7   PersistentVolumes, like cluster Nodes, don’t belong to any namespace, unlike pods and \nPersistentVolumeClaims.\nThe name of your claim—you’ll \nneed this later when using the \nclaim as the pod’s volume.\n \n\n180CHAPTER 6Volumes: attaching disk storage to containers\nspec:\n  resources:\n    requests:                \n      storage: 1Gi           \n  accessModes:              \n  - ReadWriteOnce           \n  storageClassName: \"\"     \nAs soon as you create the claim, Kubernetes finds the appropriate PersistentVolume\nand  binds  it  to  the  claim.  The  PersistentVolume’s  capacity  must  be  large  enough  to\naccommodate what the claim requests. Additionally, the volume’s access modes must\ninclude the access modes requested by the claim. In your case, the claim requests 1 GiB\nof storage and a \nReadWriteOnce access mode. The PersistentVolume you created ear-\nlier matches those two requirements so it is bound to your claim. You can see this by\ninspecting the claim.\nLISTING PERSISTENTVOLUMECLAIMS\nList all PersistentVolumeClaims to see the state of your PVC:\n$ kubectl get pvc\nNAME          STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE\nmongodb-pvc   Bound     mongodb-pv   1Gi        RWO,ROX       3s\nNOTEWe’re using pvc as a shorthand for persistentvolumeclaim.\nThe claim is shown as \nBound to PersistentVolume mongodb-pv. Note the abbreviations\nused for the access modes:\nRWO—ReadWriteOnce—Only  a  single  node  can  mount  the  volume  for  reading\nand writing.\nROX—ReadOnlyMany—Multiple nodes can mount the volume for reading.\nRWX—ReadWriteMany—Multiple nodes can mount the volume for both reading\nand writing.\nNOTERWO, ROX, and RWX pertain to the number of worker nodes that can use\nthe volume at the same time, not to the number of pods!\nLISTING PERSISTENTVOLUMES\nYou can also see that the PersistentVolume is now Bound and no longer Available by\ninspecting it with \nkubectl get:\n$ kubectl get pv\nNAME         CAPACITY   ACCESSMODES   STATUS   CLAIM                 AGE\nmongodb-pv   1Gi        RWO,ROX       Bound    default/mongodb-pvc   1m\nThe PersistentVolume shows it’s bound to claim default/mongodb-pvc. The default\npart  is  the  namespace  the  claim  resides  in  (you  created  the  claim  in  the  default\nRequesting 1 GiB of storage\nYou want the storage to support a single \nclient (performing both reads and writes).\nYou’ll learn about this in the section \nabout dynamic provisioning.\n \n\n181Decoupling pods from the underlying storage technology\nnamespace).  We’ve  already  said  that  PersistentVolume  resources  are  cluster-scoped\nand thus cannot be created in a specific namespace, but PersistentVolumeClaims can\nonly be created in a specific namespace. They can then only be used by pods in the\nsame namespace.\n6.5.4Using a PersistentVolumeClaim in a pod\nThe  PersistentVolume  is  now  yours  to  use.  Nobody  else  can  claim  the  same  volume\nuntil  you  release  it.  To  use  it  inside  a  pod,  you  need  to  reference  the  Persistent-\nVolumeClaim by name inside the pod’s volume (yes, the PersistentVolumeClaim, not\nthe PersistentVolume directly!), as shown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mongodb \nspec:\n  containers:\n  - image: mongo\n    name: mongodb\n    volumeMounts:\n    - name: mongodb-data\n      mountPath: /data/db\n    ports:\n    - containerPort: 27017\n      protocol: TCP\n  volumes:\n  - name: mongodb-data\n    persistentVolumeClaim:       \n      claimName: mongodb-pvc     \nGo ahead and create the pod. Now, check to see if the pod is indeed using the same\nPersistentVolume and its underlying GCE PD. You should see the data you stored ear-\nlier by running the MongoDB shell again, as shown in the following listing.\n$ kubectl exec -it mongodb mongo\nMongoDB shell version: 3.2.8\nconnecting to: mongodb://127.0.0.1:27017\nWelcome to the MongoDB shell.\n...\n> use mystore\nswitched to db mystore\n> db.foo.find()\n{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\nAnd  there  it  is.  You‘re  able  to  retrieve  the  document  you  stored  into  MongoDB\npreviously.\nListing 6.12   A pod using a PersistentVolumeClaim volume: mongodb-pod-pvc.yaml\nListing 6.13   Retrieving MongoDB’s persisted data in the pod using the PVC and PV\nReferencing the PersistentVolumeClaim \nby name in the pod volume\n \n\n182CHAPTER 6Volumes: attaching disk storage to containers\n6.5.5Understanding the benefits of using PersistentVolumes and claims\nExamine  figure  6.8,  which  shows  both  ways  a  pod  can  use  a  GCE  Persistent  Disk—\ndirectly or through a PersistentVolume and claim.\nConsider how using this indirect method of obtaining storage from the infrastructure\nis  much  simpler  for  the  application  developer  (or  cluster  user).  Yes,  it  does  require\nthe additional steps of creating the PersistentVolume and the PersistentVolumeClaim,\nbut the developer doesn’t have to know anything about the actual storage technology\nused underneath. \n Additionally, the same pod and claim manifests can now be used on many different\nKubernetes clusters, because they don’t refer to anything infrastructure-specific. The\nclaim states, “I need x amount of storage and I need to be able to read and write to it\nby a single client at once,” and then the pod references the claim by name in one of\nits volumes.\nPod: mongodb\nContainer: mongodb\nvolumeMounts:\nname: mongodb-data\nmountPath: /data/db\ngcePersistentDisk:\npdName: mongodb\nGCE\nPersistent Disk:\nmongodb\nVolume:\nmongodb\nPod: mongodb\nContainer: mongodb\nvolumeMounts:\nname: mongodb-data\nmountPath: /data/db\npersistentVolumeClaim:\nclaimName: mongodb-pvc\ngcePersistentDisk:\npdName: mongodb\nGCE\nPersistent Disk:\nmongodb\nPersistentVolume:\nmongodb-pv\n(1 Gi, RWO, RWX)\nVolume:\nmongodb\nClaim lists\n1Gi and\nReadWriteOnce\naccess\nPersistentVolumeClaim:\nmongodb-pvc\nFigure 6.8   Using the GCE Persistent Disk directly or through a PVC and PV\n \n\n183Decoupling pods from the underlying storage technology\n6.5.6Recycling PersistentVolumes\nBefore you wrap up this section on PersistentVolumes, let’s do one last quick experi-\nment. Delete the pod and the PersistentVolumeClaim:\n$ kubectl delete pod mongodb\npod \"mongodb\" deleted\n$ kubectl delete pvc mongodb-pvc\npersistentvolumeclaim \"mongodb-pvc\" deleted\nWhat if you create the PersistentVolumeClaim again? Will it be bound to the Persistent-\nVolume or not? After you create the claim, what does \nkubectl get pvc show?\n$ kubectl get pvc\nNAME           STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE\nmongodb-pvc    Pending                                         13s\nThe claim’s status is shown as Pending. Interesting. When you created the claim ear-\nlier, it was immediately bound to the PersistentVolume, so why wasn’t it bound now?\nMaybe listing the PersistentVolumes can shed more light on this:\n$ kubectl get pv\nNAME        CAPACITY  ACCESSMODES  STATUS    CLAIM               REASON AGE\nmongodb-pv  1Gi       RWO,ROX      Released  default/mongodb-pvc        5m\nThe STATUS  column  shows  the  PersistentVolume  as  Released,  not  Available  like\nbefore. Because you’ve already used the volume, it may contain data and shouldn’t be\nbound to a completely new claim without giving the cluster admin a chance to clean it\nup.  Without  this,  a  new  pod  using  the  same  PersistentVolume  could  read  the  data\nstored there by the previous pod, even if the claim and pod were created in a different\nnamespace (and thus likely belong to a different cluster tenant).\nRECLAIMING PERSISTENTVOLUMES MANUALLY\nYou told Kubernetes you wanted your PersistentVolume to behave like this when you\ncreated  it—by  setting  its  \npersistentVolumeReclaimPolicy  to  Retain.  You  wanted\nKubernetes to retain the volume and its contents after it’s released from its claim. As\nfar  as  I’m  aware,  the  only  way  to  manually  recycle  the  PersistentVolume  to  make  it\navailable  again  is  to  delete  and  recreate  the  PersistentVolume  resource.  As  you  do\nthat, it’s your decision what to do with the files on the underlying storage: you can\neither delete them or leave them alone so they can be reused by the next  pod.\nRECLAIMING PERSISTENTVOLUMES AUTOMATICALLY\nTwo other possible reclaim policies exist: Recycle and Delete. The first one deletes\nthe volume’s contents and makes the volume available to be claimed again. This way,\nthe  PersistentVolume  can  be  reused  multiple  times  by  different  PersistentVolume-\nClaims and different pods, as you can see in figure 6.9.\n The \nDelete  policy,  on  the  other  hand,  deletes  the underlying storage. Note that\nthe \nRecycle option is currently not available for GCE Persistent Disks. This type of\n \n\n184CHAPTER 6Volumes: attaching disk storage to containers\nA  PersistentVolume  only  supports  the  Retain  or  Delete  policies.  Other  Persistent-\nVolume types may or may not support each of these options, so before creating your\nown  PersistentVolume,  be  sure  to  check  what  reclaim  policies  are  supported  for  the\nspecific underlying storage you’ll use in the volume.\nTIPYou  can  change  the  PersistentVolume  reclaim  policy  on  an  existing\nPersistentVolume.  For  example,  if  it’s  initially  set  to  \nDelete,  you  can  easily\nchange it to \nRetain to prevent losing valuable data.\n6.6Dynamic provisioning of PersistentVolumes\nYou’ve seen how using PersistentVolumes and PersistentVolumeClaims makes it easy\nto obtain persistent storage without the developer having to deal with the actual stor-\nage technology used underneath. But this still requires a cluster administrator to pro-\nvision  the  actual  storage  up  front.  Luckily,  Kubernetes  can  also  perform  this  job\nautomatically through dynamic provisioning of PersistentVolumes.\n The cluster admin, instead of creating PersistentVolumes, can deploy a Persistent-\nVolume provisioner and define one or more StorageClass objects to let users choose\nwhat type of PersistentVolume they want. The users can refer to the \nStorageClass in\ntheir  PersistentVolumeClaims  and  the  provisioner  will  take  that  into  account  when\nprovisioning the persistent storage. \nNOTESimilar to PersistentVolumes, StorageClass resources aren’t namespaced.\nKubernetes includes provisioners for the most popular cloud providers, so the admin-\nistrator doesn’t always need to deploy a provisioner. But if Kubernetes is deployed\non-premises, a custom provisioner needs to be deployed.\nPersistentVolume\nPersistentVolumeClaim 1\nPod 1Pod 2\nPersistentVolumeClaim 2\nPod 3\nPVC is deleted;\nPV is automatically\nrecycled and ready\nto be claimed and\nre-used again\nUser creates\nPersistentVolumeClaim\nPod 2\nunmounts\nPVC\nPod 2\nmounts\nPVC\nPod 1\nmounts\nPVC\nPod 1\nunmounts\nPVC\nAdmin deletes\nPersistentVolume\nAdmin creates\nPersistentVolume\nTime\nFigure 6.9   The lifespan of a PersistentVolume, PersistentVolumeClaims, and pods using them\n \n\n185Dynamic provisioning of PersistentVolumes\n Instead of the administrator pre-provisioning a bunch of PersistentVolumes, they\nneed to define one or two (or more) StorageClasses  and  let  the  system  create  a  new\nPersistentVolume  each  time  one  is  requested  through  a  PersistentVolumeClaim.  The\ngreat thing about this is that it’s impossible to run out of PersistentVolumes (obviously,\nyou can run out of storage space). \n6.6.1Defining the available storage types through StorageClass \nresources\nBefore a user can create a PersistentVolumeClaim, which will result in a new Persistent-\nVolume  being  provisioned,  an  admin  needs  to  create  one  or  more  StorageClass\nresources. Let’s look at an example of one in the following listing.\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast\nprovisioner: kubernetes.io/gce-pd       \nparameters:\n  type: pd-ssd                     \n  zone: europe-west1-b             \nNOTEIf using Minikube, deploy the file storageclass-fast-hostpath.yaml.\nThe  StorageClass  resource  specifies  which  provisioner  should  be  used  for  provision-\ning  the  PersistentVolume  when  a  PersistentVolumeClaim  requests  this  StorageClass.\nThe parameters defined in the StorageClass definition are passed to the provisioner\nand are specific to each provisioner plugin. \n  The  StorageClass  uses  the  Google  Compute  Engine  (GCE)  Persistent  Disk  (PD)\nprovisioner,  which  means  it  can  be  used  when  Kubernetes  is  running  in  GCE.  For\nother cloud providers, other provisioners need to be used.\n6.6.2Requesting the storage class in a PersistentVolumeClaim\nAfter the StorageClass resource is created, users can refer to the storage class by name\nin their PersistentVolumeClaims. \nCREATING A PVC DEFINITION REQUESTING A SPECIFIC STORAGE CLASS\nYou can modify your mongodb-pvc to use dynamic provisioning. The following listing\nshows the updated YAML definition of the PVC.\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mongodb-pvc \nListing 6.14   A StorageClass definition: storageclass-fast-gcepd.yaml\nListing 6.15   A PVC with dynamic provisioning: mongodb-pvc-dp.yaml\nThe volume plugin to \nuse for provisioning \nthe PersistentVolume\nThe parameters passed \nto the provisioner\n \n\n186CHAPTER 6Volumes: attaching disk storage to containers\nspec:\n  storageClassName: fast     \n  resources:\n    requests:\n      storage: 100Mi\n  accessModes:\n    - ReadWriteOnce\nApart  from  specifying  the  size  and  access  modes,  your  PersistentVolumeClaim  now\nalso  specifies  the  class  of  storage  you  want  to  use.  When  you  create  the  claim,  the\nPersistentVolume  is  created  by  the  provisioner  referenced  in  the  \nfast  StorageClass\nresource. The provisioner is used even if an existing manually provisioned Persistent-\nVolume matches the PersistentVolumeClaim. \nNOTEIf you reference a non-existing storage class in a PVC, the provisioning\nof  the  PV  will  fail  (you’ll  see  a  \nProvisioningFailed  event  when  you  use\nkubectl describe on the PVC).\nEXAMINING THE CREATED PVC AND THE DYNAMICALLY PROVISIONED PV\nNext you’ll create the PVC and then use kubectl get to see it:\n$ kubectl get pvc mongodb-pvc\nNAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS\nmongodb-pvc   Bound    pvc-1e6bc048   1Gi        RWO           fast \nThe VOLUME column shows the PersistentVolume that’s bound to this claim (the actual\nname is longer than what’s shown above). You can try listing PersistentVolumes now to\nsee that a new PV has indeed been created automatically:\n$ kubectl get pv\nNAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   \nmongodb-pv     1Gi       RWO,ROX      Retain         Released \npvc-1e6bc048   1Gi       RWO          Delete         Bound     fast\nNOTEOnly pertinent columns are shown.\nYou  can  see  the  dynamically  provisioned  PersistentVolume.  Its  capacity  and  access\nmodes are what you requested in the PVC. Its reclaim policy is \nDelete, which means\nthe PersistentVolume will be deleted when the PVC is deleted. Beside the PV, the pro-\nvisioner also provisioned the actual storage. Your \nfast StorageClass is configured to\nuse  the  \nkubernetes.io/gce-pd  provisioner,  which  provisions  GCE  Persistent  Disks.\nYou can see the disk with the following command:\n$ gcloud compute disks list\nNAME                          ZONE            SIZE_GB  TYPE         STATUS\ngke-kubia-dyn-pvc-1e6bc048    europe-west1-d  1        pd-ssd       READY\ngke-kubia-default-pool-71df   europe-west1-d  100      pd-standard  READY\ngke-kubia-default-pool-79cd   europe-west1-d  100      pd-standard  READY\ngke-kubia-default-pool-blc4   europe-west1-d  100      pd-standard  READY\nmongodb                       europe-west1-d  1        pd-standard  READY\nThis PVC requests the \ncustom storage class.\n \n\n187Dynamic provisioning of PersistentVolumes\nAs you can see, the first persistent disk’s name suggests it was provisioned dynamically\nand its type shows it’s an SSD, as specified in the storage class you created earlier. \nUNDERSTANDING HOW TO USE STORAGE CLASSES\nThe cluster admin can create multiple storage classes with different performance or\nother characteristics. The developer then decides which one is most appropriate for\neach claim they create. \n  The  nice  thing  about  StorageClasses  is  the  fact  that  claims  refer  to  them  by\nname. The PVC definitions are therefore portable across different clusters, as long\nas  the  StorageClass  names  are  the  same  across  all  of  them.  To  see  this  portability\nyourself, you can try running the same example on Minikube, if you’ve been using\nGKE  up  to  this  point.  As  a  cluster  admin,  you’ll  have  to  create  a  different  storage\nclass  (but  with  the  same  name).  The  storage  class  defined  in  the  storageclass-fast-\nhostpath.yaml file is tailor-made for use in Minikube. Then, once you deploy the stor-\nage class, you as a cluster user can deploy the exact same PVC manifest and the exact\nsame pod manifest as before. This shows how the pods and PVCs are portable across\ndifferent clusters.\n6.6.3Dynamic provisioning without specifying a storage class\nAs  we’ve  progressed  through  this  chapter,  attaching  persistent  storage  to  pods  has\nbecome ever simpler. The sections in this chapter reflect how provisioning of storage\nhas evolved from early Kubernetes versions to now. In this final section, we’ll look at\nthe latest and simplest way of attaching a PersistentVolume to a pod. \nLISTING STORAGE CLASSES\nWhen you created your custom storage class called fast, you didn’t check if any exist-\ning storage classes were already defined in your cluster. Why don’t you do that now?\nHere are the storage classes available in GKE:\n$ kubectl get sc\nNAME                 TYPE\nfast                 kubernetes.io/gce-pd\nstandard (default)   kubernetes.io/gce-pd\nNOTEWe’re using sc as shorthand for storageclass.\nBeside  the  \nfast  storage  class,  which  you  created  yourself,  a  standard  storage  class\nexists and is marked as default. You’ll learn what that means in a moment. Let’s list the\nstorage classes available in Minikube, so we can compare:\n$ kubectl get sc\nNAME                 TYPE\nfast                 k8s.io/minikube-hostpath\nstandard (default)   k8s.io/minikube-hostpath\nAgain, the fast storage class was created by you and a default standard storage class\nexists  here  as  well.  Comparing  the  \nTYPE  columns  in  the  two  listings,  you  see  GKE  is\n \n\n188CHAPTER 6Volumes: attaching disk storage to containers\nusing  the  kubernetes.io/gce-pd  provisioner,  whereas  Minikube  is  using  k8s.io/\nminikube-hostpath\n. \nEXAMINING THE DEFAULT STORAGE CLASS\nYou’re going to use kubectl get to see more info about the standard storage class in a\nGKE cluster, as shown in the following listing.\n$ kubectl get sc standard -o yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    storageclass.beta.kubernetes.io/is-default-class: \"true\"   \n  creationTimestamp: 2017-05-16T15:24:11Z\n  labels:\n    addonmanager.kubernetes.io/mode: EnsureExists\n    kubernetes.io/cluster-service: \"true\"\n  name: standard\n  resourceVersion: \"180\"\n  selfLink: /apis/storage.k8s.io/v1/storageclassesstandard\n  uid: b6498511-3a4b-11e7-ba2c-42010a840014\nparameters:                                    \n  type: pd-standard                            \nprovisioner: kubernetes.io/gce-pd      \nIf you look closely toward the top of the listing, the storage class definition includes an\nannotation,  which  makes  this  the  default  storage  class.  The  default  storage  class  is\nwhat’s used to dynamically provision a PersistentVolume if the PersistentVolumeClaim\ndoesn’t explicitly say which storage class to use. \nCREATING A PERSISTENTVOLUMECLAIM WITHOUT SPECIFYING A STORAGE CLASS\nYou  can  create  a  PVC  without  specifying  the  storageClassName  attribute  and  (on\nGoogle Kubernetes Engine) a GCE Persistent Disk of type \npd-standard will be provi-\nsioned for you. Try this by creating a claim from the YAML in the following listing.\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mongodb-pvc2\nspec:                        \n  resources:                 \n    requests:                \n      storage: 100Mi         \n  accessModes:               \n    - ReadWriteOnce          \nListing 6.16   The definition of the standard storage class on GKE\nListing 6.17   PVC with no storage class defined: mongodb-pvc-dp-nostorageclass.yaml\nThis annotation \nmarks the storage \nclass as default.\nThe type parameter is used by the provisioner \nto know what type of GCE PD to create.\nThe GCE Persistent Disk provisioner \nis used to provision PVs of this class.\nYou’re not specifying \nthe storageClassName \nattribute (unlike earlier \nexamples).\n \n\n189Dynamic provisioning of PersistentVolumes\nThis  PVC  definition  includes  only  the  storage  size  request  and  the  desired  access\nmodes,  but  no  storage  class.  When  you  create  the  PVC,  whatever  storage  class  is\nmarked as default will be used. You can confirm that’s the case:\n$ kubectl get pvc mongodb-pvc2\nNAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS\nmongodb-pvc2  Bound    pvc-95a5ec12   1Gi        RWO           standard\n$ kubectl get pv pvc-95a5ec12\nNAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   \npvc-95a5ec12   1Gi       RWO          Delete         Bound     standard\n$ gcloud compute disks list\nNAME                          ZONE            SIZE_GB  TYPE         STATUS\ngke-kubia-dyn-pvc-95a5ec12    europe-west1-d  1        pd-standard  READY\n...\nFORCING A PERSISTENTVOLUMECLAIM TO BE BOUND TO ONE OF THE PRE-PROVISIONED \nP\nERSISTENTVOLUMES\nThis finally brings us to why you set storageClassName to an empty string in listing 6.11\n(when  you  wanted  the  PVC  to  bind  to  the  PV  you’d  provisioned  manually).  Let  me\nrepeat the relevant lines of that PVC definition here:\nkind: PersistentVolumeClaim\nspec:\n  storageClassName: \"\"       \nIf you hadn’t set the storageClassName attribute to an empty string, the dynamic vol-\nume provisioner would have provisioned a new PersistentVolume, despite there being\nan appropriate pre-provisioned PersistentVolume. At that point, I wanted to demon-\nstrate how a claim gets bound to a manually pre-provisioned PersistentVolume. I didn’t\nwant the dynamic provisioner to interfere. \nTIPExplicitly set storageClassName to \"\" if you want the PVC to use a pre-\nprovisioned PersistentVolume.\nUNDERSTANDING THE COMPLETE PICTURE OF DYNAMIC PERSISTENTVOLUME PROVISIONING\nThis  brings  us  to  the  end  of  this  chapter.  To  summarize,  the  best  way  to  attach  per-\nsistent storage to a pod is to only create the PVC (with an explicitly specified \nstorage-\nClassName\n if necessary) and the pod (which refers to the PVC by name). Everything\nelse is taken care of by the dynamic PersistentVolume provisioner.\n  To  get  a  complete  picture  of  the  steps  involved  in  getting  a  dynamically  provi-\nsioned PersistentVolume, examine figure 6.10.\n \n \n \nSpecifying an empty string as the storage class \nname ensures the PVC binds to a pre-provisioned \nPV instead of dynamically provisioning a new one.\n \n\n190CHAPTER 6Volumes: attaching disk storage to containers\n6.7Summary\nThis chapter has shown you how volumes are used to provide either temporary or per-\nsistent storage to a pod’s containers. You’ve learned how to\nCreate  a  multi-container  pod  and  have  the  pod’s  containers  operate  on  the\nsame files by adding a volume to the pod and mounting it in each container\nUse the emptyDir volume to store temporary, non-persistent data\nUse the gitRepo volume to easily populate a directory with the contents of a Git\nrepository at pod startup\nUse the hostPath volume to access files from the host node\nMount external storage in a volume to persist pod data across pod restarts\nDecouple  the  pod  from  the  storage  infrastructure  by  using  PersistentVolumes\nand PersistentVolumeClaims\nHave  PersistentVolumes  of  the  desired  (or  the  default)  storage  class  dynami-\ncally provisioned for each PersistentVolumeClaim\nPrevent the dynamic provisioner from interfering when you want the Persistent-\nVolumeClaim to be bound to a pre-provisioned PersistentVolume\nIn the next chapter, you’ll see what mechanisms Kubernetes provides to deliver con-\nfiguration data, secret information, and metadata about the pod and container to the\nprocesses running inside a pod. This is done with the special types of volumes we’ve\nmentioned in this chapter, but not yet explored.\nPod\nAdmin\nVolume\n1. Cluster admin sets up a PersistentVolume\nprovisioner (if one’s not already deployed)\n2. Admin creates one or\nmore StorageClasses\nand marks one as the\ndefault (it may already\nexist)\nActual\nstorage\nPersistent\nVolume\nUser\nPersistent\nVolume\nprovisioner\nPersistent\nVolumeClaim\nStorage\nClass\n3. User creates a PVC referencing one of the\nStorageClasses (or none to use the default)\n6. User creates a pod with\na volume referencing the\nPVC by name\n4. Kubernetes looks up the\nStorageClass and the provisioner\nreferenced in it and asks the provisioner\nto provision a new PV based on the\nPVC’s requested access mode and\nstorage size and the parameters\nin the StorageClass\n5. Provisioner provisions the\nactual storage, creates\na PersistentVolume, and\nbinds it to the PVC\nFigure 6.10   The complete picture of dynamic provisioning of PersistentVolumes\n \n\n191\nConfigMaps and Secrets:\nconfiguring applications\nUp to now you haven’t had to pass any kind of configuration data to the apps you’ve\nrun in the exercises in this book. Because almost all apps require configuration (set-\ntings  that  differ  between  deployed  instances,  credentials  for  accessing  external  sys-\ntems, and so on), which shouldn’t be baked into the built app itself, let’s see how to\npass configuration options to your app when running it in Kubernetes.\n7.1Configuring containerized applications\nBefore we go over how to pass configuration data to apps running in Kubernetes,\nlet’s look at how containerized applications are usually configured.\n  If  you  skip  the  fact  that  you  can  bake  the  configuration  into  the  application\nitself, when starting development of a new app, you usually start off by having the\nThis chapter covers\nChanging the main process of a container\nPassing command-line options to the app\nSetting environment variables exposed to the app\nConfiguring apps through ConfigMaps\nPassing sensitive information through Secrets\n \n\n192CHAPTER 7ConfigMaps and Secrets: configuring applications\napp configured through command-line arguments. Then, as the list of configuration\noptions grows, you can move the configuration into a config file. \n Another way of passing configuration options to an application that’s widely popu-\nlar in containerized applications is through environment variables. Instead of having\nthe app read a config file or command-line arguments, the app looks up the value of a\ncertain environment variable. The official MySQL container image, for example, uses\nan environment variable called \nMYSQL_ROOT_PASSWORD for setting the password for the\nroot super-user account. \n But why are environment variables so popular in containers? Using configuration\nfiles inside Docker containers is a bit tricky, because you’d have to bake the config file\ninto  the  container  image  itself  or  mount  a  volume  containing  the  file  into  the  con-\ntainer.  Obviously,  baking  files  into  the  image  is  similar  to  hardcoding  configuration\ninto the source code of the application, because it requires you to rebuild the image\nevery time you want to change the config. Plus, everyone with access to the image can\nsee the config, including any information that should be kept secret, such as creden-\ntials or encryption keys. Using a volume is better, but still requires you to make sure\nthe file is written to the volume before the container is started. \n If you’ve read the previous chapter, you might think of using a \ngitRepo volume as\na configuration source. That’s not a bad idea, because it allows you to keep the config\nnicely versioned and enables you to easily rollback a config change if necessary. But a\nsimpler  way  allows  you  to  put  the  configuration  data  into  a  top-level  Kubernetes\nresource and store it and all the other resource definitions in the same Git repository\nor in any other file-based storage. The Kubernetes resource for storing configuration\ndata is called a ConfigMap. We’ll learn how to use it in this chapter.\n Regardless if you’re using a ConfigMap to store configuration data or not, you can\nconfigure your apps by\nPassing command-line arguments to containers\nSetting custom environment variables for each container\nMounting configuration files into containers through a special type of volume\nWe’ll go over all these options in the next few sections, but before we start, let’s look\nat  config  options  from  a  security  perspective.  Though  most  configuration  options\ndon’t contain any sensitive information, several can. These include credentials, pri-\nvate encryption keys, and similar data that needs to be kept secure. This type of infor-\nmation  needs  to  be  handled  with  special  care,  which  is  why  Kubernetes  offers\nanother type of first-class object called a Secret. We’ll learn about it in the last part of\nthis chapter.\n7.2Passing command-line arguments to containers\nIn  all  the  examples  so  far,  you’ve  created  containers  that  ran  the  default  command\ndefined  in  the  container  image,  but  Kubernetes  allows  overriding  the  command  as\npart  of  the  pod’s  container  definition  when  you  want  to  run  a  different  executable\n \n\n193Passing command-line arguments to containers\ninstead of the one specified in the image, or want to run it with a different set of com-\nmand-line arguments. We’ll look at how to do that now.\n7.2.1Defining the command and arguments in Docker\nThe first thing I need to explain is that the whole command that gets executed in the\ncontainer is composed of two parts: the command and the arguments. \nUNDERSTANDING ENTRYPOINT AND CMD\nIn a Dockerfile, two instructions define the two parts:\nENTRYPOINT defines the executable invoked when the container is started.\nCMD specifies the arguments that get passed to the ENTRYPOINT.\nAlthough you can use the \nCMD instruction to specify the command you want to execute\nwhen the image is run, the correct way is to do it through the \nENTRYPOINT instruction\nand to only specify the \nCMD if you want to define the default arguments. The image can\nthen be run without specifying any arguments\n$ docker run <image>\nor with additional arguments, which override whatever’s set under CMD in the Dockerfile:\n$ docker run <image> <arguments>\nUNDERSTANDING THE DIFFERENCE BETWEEN THE SHELL AND EXEC FORMS\nBut there’s more. Both instructions support two different forms:\nshell form—For example, ENTRYPOINT node app.js.\nexec form—For example, ENTRYPOINT [\"node\", \"app.js\"].\nThe difference is whether the specified command is invoked inside a shell or not. \n In the \nkubia image you created in chapter 2, you used the exec form of the ENTRY-\nPOINT\n instruction: \nENTRYPOINT [\"node\", \"app.js\"]\nThis runs the node process directly (not inside a shell), as you can see by listing the\nprocesses running inside the container:\n$ docker exec 4675d ps x\n  PID TTY      STAT   TIME COMMAND\n    1 ?        Ssl    0:00 node app.js\n   12 ?        Rs     0:00 ps x\nIf you’d used the shell form (ENTRYPOINT node app.js), these would have been the\ncontainer’s processes:\n$ docker exec -it e4bad ps x\n  PID TTY      STAT   TIME COMMAND\n    1 ?        Ss     0:00 /bin/sh -c node app.js\n \n\n194CHAPTER 7ConfigMaps and Secrets: configuring applications\n    7 ?        Sl     0:00 node app.js\n   13 ?        Rs+    0:00 ps x\nAs  you  can  see,  in  that  case,  the  main  process  (PID 1)  would  be  the  shell  process\ninstead  of  the  node  process.  The  node  process  (\nPID 7)  would  be  started  from  that\nshell. The \nshell process is unnecessary, which is why you should always use the exec\nform of the ENTRYPOINT instruction.\nMAKING THE INTERVAL CONFIGURABLE IN YOUR FORTUNE IMAGE\nLet’s modify your fortune script and image so the delay interval in the loop is configu-\nrable. You’ll add an \nINTERVAL variable and initialize it with the value of the first com-\nmand-line argument, as shown in the following listing.\n#!/bin/bash\ntrap \"exit\" SIGINT\nINTERVAL=$1\necho Configured to generate new fortune every $INTERVAL seconds\nmkdir -p /var/htdocs\nwhile :\ndo\n  echo $(date) Writing fortune to /var/htdocs/index.html\n  /usr/games/fortune > /var/htdocs/index.html\n  sleep $INTERVAL\ndone\nYou’ve added or modified the lines in bold font. Now, you’ll modify the Dockerfile so\nit uses the \nexec version of the ENTRYPOINT instruction and sets the default interval to\n10 seconds using the \nCMD instruction, as shown in the following listing.\nFROM ubuntu:latest\nRUN apt-get update ; apt-get -y install fortune\nADD fortuneloop.sh /bin/fortuneloop.sh\nENTRYPOINT [\"/bin/fortuneloop.sh\"]        \nCMD [\"10\"]                                \nYou can now build and push the image to Docker Hub. This time, you’ll tag the image\nas \nargs instead of latest:\n$ docker build -t docker.io/luksa/fortune:args .\n$ docker push docker.io/luksa/fortune:args\nYou can test the image by running it locally with Docker:\n$ docker run -it docker.io/luksa/fortune:args\nConfigured to generate new fortune every 10 seconds\nFri May 19 10:39:44 UTC 2017 Writing fortune to /var/htdocs/index.html\nListing 7.1   Fortune script with interval configurable through argument: fortune-args/\nfortuneloop.sh\nListing 7.2   Dockerfile for the updated fortune image: fortune-args/Dockerfile\nThe exec form of the \nENTRYPOINT instruction\nThe default argument \nfor the executable\n \n\n195Passing command-line arguments to containers\nNOTEYou can stop the script with Control+C.\nAnd you can override the default sleep interval by passing it as an argument:\n$ docker run -it docker.io/luksa/fortune:args 15\nConfigured to generate new fortune every 15 seconds\nNow that you’re sure your image honors the argument passed to it, let’s see how to use\nit in a pod.\n7.2.2Overriding the command and arguments in Kubernetes\nIn Kubernetes, when specifying a container, you can choose to override both ENTRY-\nPOINT\n and CMD. To do that, you set the properties command and args in the container\nspecification, as shown in the following listing.\nkind: Pod\nspec:\n  containers:\n  - image: some/image\n    command: [\"/bin/command\"]\n    args: [\"arg1\", \"arg2\", \"arg3\"]\nIn  most  cases,  you’ll  only  set  custom  arguments  and  rarely  override  the  command\n(except in general-purpose images such as \nbusybox, which doesn’t define an ENTRY-\nPOINT\n at all). \nNOTEThe command and args fields can’t be updated after the pod is created.\nThe two Dockerfile instructions and the equivalent pod spec fields are shown in table 7.1.\nRUNNING THE FORTUNE POD WITH A CUSTOM INTERVAL\nTo  run  the  fortune  pod  with  a  custom  delay  interval,  you’ll  copy  your  fortune-\npod.yaml into fortune-pod-args.yaml and modify it as shown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune2s        \nListing 7.3   A pod definition specifying a custom command and arguments\nTable 7.1   Specifying the executable and its arguments in Docker vs Kubernetes\nDockerKubernetesDescription\nENTRYPOINTcommandThe executable that’s executed inside the container\nCMDargsThe arguments passed to the executable\nListing 7.4   Passing an argument in the pod definition: fortune-pod-args.yaml\nYou changed the \npod’s name.\n \n\n196CHAPTER 7ConfigMaps and Secrets: configuring applications\nspec:\n  containers:\n  - image: luksa/fortune:args      \n    args: [\"2\"]                  \n    name: html-generator\n    volumeMounts:\n    - name: html\n      mountPath: /var/htdocs\n...\nYou added the args array to the container definition. Try creating this pod now. The\nvalues of the array will be passed to the container as command-line arguments when it\nis run. \n The array notation used in this listing is great if you have one argument or a few. If\nyou have several, you can also use the following notation:\n    args:\n    - foo\n    - bar\n    - \"15\"\nTIPYou  don’t  need  to  enclose  string  values  in  quotations  marks  (but  you\nmust enclose numbers). \nSpecifying arguments is one way of passing config\noptions  to  your  containers  through  command-\nline  arguments.  Next,  you’ll  see  how  to  do  it\nthrough environment variables.\n7.3Setting environment variables for \na container\nAs I’ve already mentioned, containerized appli-\ncations  often  use  environment  variables  as  a\nsource  of  configuration  options.  Kubernetes\nallows  you  to  specify  a  custom  list  of  environ-\nment variables for each container of a pod, as\nshown in figure 7.1. Although it would be use-\nful to also define environment variables at the\npod  level  and  have  them  be  inherited  by  its\ncontainers, no such option currently exists.\nNOTELike   the   container’s   command   and\narguments,  the  list  of  environment  variables\nalso cannot be updated after the pod is created.\nUsing fortune:args \ninstead of fortune:latest\nThis argument makes the \nscript generate a new fortune \nevery two seconds.\nPod\nContainer A\nEnvironment variables\nFOO=BAR\nABC=123\nContainer B\nEnvironment variables\nFOO=FOOBAR\nBAR=567\nFigure 7.1   Environment variables can \nbe set per container.\n \n\n197Setting environment variables for a container\nMAKING THE INTERVAL IN YOUR FORTUNE IMAGE CONFIGURABLE THROUGH AN ENVIRONMENT VARIABLE\nLet’s see how to modify your fortuneloop.sh script once again to allow it to be config-\nured from an environment variable, as shown in the following listing.\n#!/bin/bash\ntrap \"exit\" SIGINT\necho Configured to generate new fortune every $INTERVAL seconds\nmkdir -p /var/htdocs\nwhile :\ndo\n  echo $(date) Writing fortune to /var/htdocs/index.html\n  /usr/games/fortune > /var/htdocs/index.html\n  sleep $INTERVAL\ndone\nAll you had to do was remove the row where the INTERVAL variable is initialized. Because\nyour “app” is a simple bash script, you didn’t need to do anything else. If the app was\nwritten  in  Java  you’d  use  \nSystem.getenv(\"INTERVAL\"),  whereas  in  Node.JS  you’d  use\nprocess.env.INTERVAL, and in Python you’d use os.environ['INTERVAL'].\n7.3.1Specifying environment variables in a container definition\nAfter  building  the  new  image  (I’ve  tagged  it  as  luksa/fortune:env  this  time)  and\npushing it to Docker Hub, you can run it by creating a new pod, in which you pass the\nenvironment  variable  to  the  script  by  including  it  in  your  container  definition,  as\nshown in the following listing.\nkind: Pod\nspec:\n containers:\n - image: luksa/fortune:env\n   env:                        \n   - name: INTERVAL            \n     value: \"30\"               \n   name: html-generator\n...\nAs mentioned previously, you set the environment variable inside the container defini-\ntion, not at the pod level. \nNOTEDon’t  forget  that  in  each  container,  Kubernetes  also  automatically\nexposes environment variables for each service in the same namespace. These\nenvironment variables are basically auto-injected configuration.\nListing 7.5   Fortune script with interval configurable through env var: fortune-env/\nfortuneloop.sh\nListing 7.6   Defining an environment variable in a pod: fortune-pod-env.yaml\nAdding a single variable to \nthe environment variable list\n \n\n198CHAPTER 7ConfigMaps and Secrets: configuring applications\n7.3.2Referring to other environment variables in a variable’s value\nIn the previous example, you set a fixed value for the environment variable, but you\ncan also reference previously defined environment variables or any other existing vari-\nables by using the \n$(VAR) syntax. If you define two environment variables, the second\none can include the value of the first one as shown in the following listing.\nenv:\n- name: FIRST_VAR\n  value: \"foo\"\n- name: SECOND_VAR\n  value: \"$(FIRST_VAR)bar\"\nIn this case, the SECOND_VAR’s value will be \"foobar\". Similarly, both the command and\nargs  attributes  you  learned  about  in  section  7.2  can  also  refer  to  environment  vari-\nables like this. You’ll use this method in section 7.4.5.\n7.3.3Understanding the drawback of hardcoding environment \nvariables\nHaving  values  effectively  hardcoded  in  the  pod  definition  means  you  need  to  have\nseparate  pod  definitions  for  your  production  and  your  development  pods.  To  reuse\nthe  same  pod  definition  in  multiple  environments,  it  makes  sense  to  decouple  the\nconfiguration  from  the  pod  descriptor.  Luckily,  you  can  do  that  using  a  ConfigMap\nresource and using it as a source for environment variable values using the \nvalueFrom\ninstead of the value field. You’ll learn about this next. \n7.4Decoupling configuration with a ConfigMap\nThe  whole  point  of  an  app’s  configuration  is  to  keep  the  config  options  that  vary\nbetween environments, or change frequently, separate from the application’s source\ncode. If you think of a pod descriptor as source code for your app (and in microservices\narchitectures  that’s  what  it  really  is,  because  it  defines  how  to  compose  the  individual\ncomponents into a functioning system), it’s clear you should move the configuration\nout of the pod description.\n7.4.1Introducing ConfigMaps\nKubernetes  allows  separating  configuration  options  into  a  separate  object  called  a\nConfigMap, which is a map containing key/value pairs with the values ranging from\nshort literals to full config files. \n An application doesn’t need to read the ConfigMap directly or even know that it\nexists.  The  contents  of  the  map  are  instead  passed  to  containers  as  either  environ-\nment  variables  or  as  files  in  a  volume  (see  figure  7.2).  And  because  environment\nListing 7.7   Referring to an environment variable inside another one\n \n\n199Decoupling configuration with a ConfigMap\nvariables  can  be  referenced  in  command-line  arguments  using  the  $(ENV_VAR)  syn-\ntax, you can also pass ConfigMap entries to processes as command-line arguments.\nSure, the application can also read the contents of a ConfigMap directly through the\nKubernetes  REST  API  endpoint  if  needed,  but  unless  you  have  a  real  need  for  this,\nyou should keep your app Kubernetes-agnostic as much as possible.\n Regardless of how an app consumes a ConfigMap, having the config in a separate\nstandalone object like this allows you to keep multiple manifests for ConfigMaps with\nthe same name, each for a different environment (development, testing, QA, produc-\ntion, and so on). Because pods reference the ConfigMap by name, you can use a dif-\nferent config in each environment while using the same pod specification across all of\nthem (see figure 7.3).\nPod\nEnvironment variables\nConfigMap\nkey1=value1\nkey2=value2\n...\nconfigMap\nvolume\nFigure 7.2   Pods use ConfigMaps \nthrough environment variables and \nconfigMap volumes.\nConfigMap:\napp-config\nNamespace: development\n(contains\ndevelopment\nvalues)\nPod(s)\nConfigMaps created\nfrom different manifests\nPods created from the\nsame pod manifests\nNamespace: production\nConfigMap:\napp-config\n(contains\nproduction\nvalues)\nPod(s)\nFigure 7.3   Two different ConfigMaps with the same name used in different \nenvironments\n \n\n200CHAPTER 7ConfigMaps and Secrets: configuring applications\n7.4.2Creating a ConfigMap\nLet’s see how to use a ConfigMap in one of your pods. To start with the simplest exam-\nple, you’ll first create a map with a single key and use it to fill the \nINTERVAL environment\nvariable from your previous example. You’ll create the ConfigMap with the special\nkubectl create configmap  command  instead  of  posting  a  YAML  with  the  generic\nkubectl create -f command. \nUSING THE KUBECTL CREATE CONFIGMAP COMMAND\nYou can define the map’s entries by passing literals to the kubectl command or you\ncan create the ConfigMap from files stored on your disk. Use a simple literal first:\n$ kubectl create configmap fortune-config --from-literal=sleep-interval=25\nconfigmap \"fortune-config\" created\nNOTEConfigMap keys must be a valid DNS subdomain (they may only con-\ntain  alphanumeric  characters,  dashes,  underscores,  and  dots).  They  may\noptionally include a leading dot.\nThis creates a ConfigMap called \nfortune-config with the single-entry sleep-interval\n=25\n (figure 7.4).\nConfigMaps usually contain more than one entry. To create a ConfigMap with multi-\nple literal entries, you add multiple \n--from-literal arguments:\n$ kubectl create configmap myconfigmap\n➥  --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two\nLet’s inspect the YAML descriptor of the ConfigMap you created by using the kubectl\nget\n command, as shown in the following listing.\n$ kubectl get configmap fortune-config -o yaml\napiVersion: v1\ndata:\n  sleep-interval: \"25\"                      \nkind: ConfigMap                              \nmetadata:\n  creationTimestamp: 2016-08-11T20:31:08Z\n  name: fortune-config                      \n  namespace: default\n  resourceVersion: \"910025\"\n  selfLink: /api/v1/namespaces/default/configmaps/fortune-config\n  uid: 88c4167e-6002-11e6-a50d-42010af00237\nListing 7.8   A ConfigMap definition\nsleep-interval25\nConfigMap: fortune-config\nFigure 7.4   The fortune-config \nConfigMap containing a single entry\nThe single entry \nin this map\nThis descriptor \ndescribes a ConfigMap.\nThe name of this map \n(you’re referencing it \nby this name)\n \n\n201Decoupling configuration with a ConfigMap\nNothing extraordinary. You could easily have written this YAML yourself (you wouldn’t\nneed to specify anything but the name in the \nmetadata section, of course) and posted\nit to the Kubernetes API with the well-known\n$ kubectl create -f fortune-config.yaml\nCREATING A CONFIGMAP ENTRY FROM THE CONTENTS OF A FILE\nConfigMaps  can  also  store  coarse-grained  config  data,  such  as  complete  config  files.\nTo do this, the \nkubectl create configmap command also supports reading files from\ndisk and storing them as individual entries in the ConfigMap:\n$ kubectl create configmap my-config --from-file=config-file.conf\nWhen you run the previous command, kubectl looks for the file config-file.conf in\nthe directory you run \nkubectl in. It will then store the contents of the file under the\nkey \nconfig-file.conf in the ConfigMap (the filename is used as the map key), but\nyou can also specify a key manually like this:\n$ kubectl create configmap my-config --from-file=customkey=config-file.conf\nThis command will store the file’s contents under the key customkey. As with literals,\nyou can add multiple files by using the \n--from-file argument multiple times. \nCREATING A CONFIGMAP FROM FILES IN A DIRECTORY\nInstead of importing each file individually, you can even import all files from a file\ndirectory:\n$ kubectl create configmap my-config --from-file=/path/to/dir\nIn this case, kubectl will create an individual map entry for each file in the specified\ndirectory, but only for files whose name is a valid ConfigMap key. \nCOMBINING DIFFERENT OPTIONS\nWhen creating ConfigMaps, you can use a combination of all the options mentioned\nhere (note that these files aren’t included in the book’s code archive—you can create\nthem yourself if you’d like to try out the command):\n$ kubectl create configmap my-config  \n➥  --from-file=foo.json                  \n➥  --from-file=bar=foobar.conf              \n➥  --from-file=config-opts/               \n➥  --from-literal=some=thing    \nHere, you’ve created the ConfigMap from multiple sources: a whole directory, a file,\nanother file (but stored under a custom key instead of using the filename as the key),\nand a literal value. Figure 7.5 shows all these sources and the resulting ConfigMap.\nA single file\nA file stored under \na custom key\nA whole directory\nA literal value\n \n\n202CHAPTER 7ConfigMaps and Secrets: configuring applications\n7.4.3Passing a ConfigMap entry to a container as an environment \nvariable\nHow do you now get the values from this map into a pod’s container? You have three\noptions. Let’s start with the simplest—setting an environment variable. You’ll use the\nvalueFrom field I mentioned in section 7.3.3. The pod descriptor should look like\nthe following listing.\napiVersion: v1\nkind: Pod\nListing 7.9   Pod with env var from a config map: fortune-pod-env-configmap.yaml\nConfigMap: my-config\nKey\nfoo.json\nfoo.json\nValue\nbarabc\ndebugtrue\nrepeat100\nsomething\n{\nfoo: bar\nbaz: 5\n}\nconfig-opts directory\nLiteral\nsome=thing\n{\nfoo: bar\nbaz: 5\n}\n--from-file=foo.json\n--from-file=config-opts/\n--from-literal=some=thing\nfoobar.conf\nabc\ndebug\ntrue\nrepeat\n100\n--from-file=bar=foobar.conf\nFigure 7.5   Creating a ConfigMap from individual files, a directory, and a literal value\n \n\n203Decoupling configuration with a ConfigMap\nmetadata:\n  name: fortune-env-from-configmap\nspec:\n  containers:\n  - image: luksa/fortune:env\n    env:                             \n    - name: INTERVAL                 \n      valueFrom:                       \n        configMapKeyRef:               \n          name: fortune-config      \n          key: sleep-interval    \n...\nYou defined an environment variable called INTERVAL and set its value to whatever is\nstored in the \nfortune-config ConfigMap under the key sleep-interval. When the\nprocess  running  in  the  \nhtml-generator  container  reads  the  INTERVAL  environment\nvariable, it will see the value \n25 (shown in figure 7.6).\nREFERENCING NON-EXISTING CONFIGMAPS IN A POD\nYou might wonder what happens if the referenced ConfigMap doesn’t exist when you\ncreate the pod. Kubernetes schedules the pod normally and tries to run its containers.\nThe container referencing the non-existing ConfigMap will fail to start, but the other\ncontainer will start normally. If you then create the missing ConfigMap, the failed con-\ntainer is started without requiring you to recreate the pod.\nNOTEYou can also mark a reference to a ConfigMap as optional (by setting\nconfigMapKeyRef.optional: true). In that case, the container starts even if\nthe ConfigMap doesn’t exist.\nThis  example  shows  you  how  to  decouple  the  configuration  from  the  pod  specifica-\ntion. This allows you to keep all the configuration options closely together (even for\nmultiple pods) instead of having them splattered around the pod definition (or dupli-\ncated across multiple pod manifests). \nYou’re setting the environment \nvariable called INTERVAL.\nInstead of setting a fixed value, you're \ninitializing it from a ConfigMap key.\nThe name of the ConfigMap \nyou're referencing\nYou're setting the variable to whatever is\nstored under this key in the ConfigMap.\nConfigMap: fortune-config\nsleep-interval\n25\nPod\nContainer: web-server\nContainer: html-generator\nEnvironment variables\nINTERVAL=25\nfortuneloop.sh\nprocess\nFigure 7.6   Passing a ConfigMap entry as \nan environment variable to a container\n \n\n204CHAPTER 7ConfigMaps and Secrets: configuring applications\n7.4.4Passing all entries of a ConfigMap as environment variables \nat once\nWhen your ConfigMap contains more than just a few entries, it becomes tedious and\nerror-prone  to  create  environment  variables  from  each  entry  individually.  Luckily,\nKubernetes version 1.6 provides a way to expose all entries of a ConfigMap as environ-\nment variables. \n Imagine having a ConfigMap with three keys called \nFOO, BAR, and FOO-BAR. You can\nexpose  them  all  as  environment  variables  by  using  the  \nenvFrom  attribute,  instead  of\nenv the way you did in previous examples. The following listing shows an example.\nspec:\n  containers:\n  - image: some-image\n    envFrom:                \n    - prefix: CONFIG_             \n      configMapRef:              \n        name: my-config-map      \n...\nAs you can see, you can also specify a prefix for the environment variables (CONFIG_ in\nthis case). This results in the following two environment variables being present inside\nthe container: \nCONFIG_FOO and CONFIG_BAR. \nNOTEThe prefix is optional, so if you omit it the environment variables will\nhave the same name as the keys. \nDid you notice I said two variables, but earlier, I said the ConfigMap has three entries\n(\nFOO, BAR,  and  FOO-BAR)?  Why  is  there  no  environment  variable  for  the  FOO-BAR\nConfigMap entry?\n The reason is that \nCONFIG_FOO-BAR  isn’t  a  valid  environment  variable  name\nbecause it contains a dash. Kubernetes doesn’t convert the keys in any way (it doesn’t\nconvert dashes to underscores, for example). If a ConfigMap key isn’t in the proper\nformat, it skips the entry (but it does record an event informing you it skipped it).\n7.4.5Passing a ConfigMap entry as a command-line argument\nNow, let’s also look at how to pass values from a ConfigMap as arguments to the main\nprocess running in the container. You can’t reference ConfigMap entries directly in\nthe \npod.spec.containers.args field, but you can first initialize an environment vari-\nable from the ConfigMap entry and then refer to the variable inside the arguments as\nshown in figure 7.7.\n Listing 7.11 shows an example of how to do this in the YAML.\n \nListing 7.10   Pod with env vars from all entries of a ConfigMap\nUsing envFrom instead of env\nAll environment variables will \nbe prefixed with CONFIG_.\nReferencing the ConfigMap \ncalled my-config-map\n \n\n205Decoupling configuration with a ConfigMap\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune-args-from-configmap\nspec:\n  containers:\n  - image: luksa/fortune:args         \n    env:                               \n    - name: INTERVAL                   \n      valueFrom:                       \n        configMapKeyRef:               \n          name: fortune-config         \n          key: sleep-interval          \n    args: [\"$(INTERVAL)\"]      \n...\nYou defined the environment variable exactly as you did before, but then you used the\n$(ENV_VARIABLE_NAME) syntax to have Kubernetes inject the value of the variable into\nthe argument. \n7.4.6Using a configMap volume to expose ConfigMap entries as files\nPassing configuration options as environment variables or command-line arguments\nis usually used for short variable values. A ConfigMap, as you’ve seen, can also con-\ntain whole config files. When you want to expose those to the container, you can use\none  of  the  special  volume  types  I  mentioned  in  the  previous  chapter,  namely  a\nconfigMap volume.\n A \nconfigMap volume will expose each entry of the ConfigMap as a file. The pro-\ncess running in the container can obtain the entry’s value by reading the contents of\nthe file.\nListing 7.11   Using ConfigMap entries as arguments: fortune-pod-args-configmap.yaml\nConfigMap: fortune-config\nsleep-interval\n25\nPod\nContainer: web-server\nContainer: html-generator\nEnvironment variables\nINTERVAL=25\nfortuneloop.sh $(INTERVAL)\nFigure 7.7   Passing a ConfigMap entry as a command-line argument\nUsing the image that takes the \ninterval from the first argument, \nnot from an environment variable\nDefining the \nenvironment variable \nexactly as before\nReferencing the environment \nvariable in the argument\n \n\n206CHAPTER 7ConfigMaps and Secrets: configuring applications\n  Although  this  method  is  mostly  meant  for  passing  large  config  files  to  the  con-\ntainer, nothing prevents you from passing short single values this way. \nCREATING THE CONFIGMAP\nInstead of modifying your fortuneloop.sh script once again, you’ll now try a different\nexample. You’ll use a config file to configure the Nginx web server running inside the\nfortune pod’s web-server container. Let’s say you want your Nginx server to compress\nresponses  it  sends  to  the  client.  To  enable  compression,  the  config  file  for  Nginx\nneeds to look like the following listing.\nserver {\n  listen              80;\n  server_name         www.kubia-example.com;\n  gzip on;                                       \n  gzip_types text/plain application/xml;         \n  location / {\n    root   /usr/share/nginx/html;\n    index  index.html index.htm;\n  }\n}\nNow  delete  your  existing  fortune-config  ConfigMap  with  kubectl delete config-\nmap\n fortune-config, so that you can replace it with a new one, which will include the\nNginx config file. You’ll create the ConfigMap from files stored on your local disk. \n Create a new directory called configmap-files and store the Nginx config from the\nprevious  listing  into  configmap-files/my-nginx-config.conf.  To  make  the  ConfigMap\nalso contain the \nsleep-interval entry, add a plain text file called sleep-interval to the\nsame directory and store the number 25 in it (see figure 7.8).\nNow create a ConfigMap from all the files in the directory like this:\n$ kubectl create configmap fortune-config --from-file=configmap-files\nconfigmap \"fortune-config\" created\nListing 7.12   An Nginx config with enabled gzip compression: my-nginx-config.conf\nThis enables gzip compression \nfor plain text and XML files.\nconfigmap-files/\nmy-nginx-config.conf\nserver {\nlisten 80;\nserver_name www.kubia...\n...\n}\nsleep-interval\n25\nFigure 7.8   The contents of the \nconfigmap-files directory and its files\n \n\n207Decoupling configuration with a ConfigMap\nThe following listing shows what the YAML of this ConfigMap looks like.\n$ kubectl get configmap fortune-config -o yaml\napiVersion: v1\ndata:\n  my-nginx-config.conf: |                            \n    server {                                         \n      listen              80;                        \n      server_name         www.kubia-example.com;     \n      gzip on;                                       \n      gzip_types text/plain application/xml;         \n      location / {                                   \n        root   /usr/share/nginx/html;                \n        index  index.html index.htm;                 \n      }                                              \n    }                                                \n  sleep-interval: |         \n    25                      \nkind: ConfigMap\n...\nNOTEThe pipeline character after the colon in the first line of both entries\nsignals that a literal multi-line value follows.\nThe ConfigMap contains two entries, with keys corresponding to the actual names\nof the files they were created from. You’ll now use the ConfigMap in both of your\npod’s containers.\nUSING THE CONFIGMAP'S ENTRIES IN A VOLUME\nCreating a volume populated with the contents of a ConfigMap is as easy as creating\na  volume  that  references  the  ConfigMap  by  name  and  mounting  the  volume  in  a\ncontainer. You already learned how to create volumes and mount them, so the only\nthing  left  to  learn  is  how  to  initialize  the  volume  with  files  created  from  a  Config-\nMap’s entries.\n  Nginx  reads  its  config  file  from  /etc/nginx/nginx.conf.  The  Nginx  image\nalready contains this file with default configuration options, which you don’t want\nto  override,  so  you  don’t  want  to  replace  this  file  as  a  whole.  Luckily,  the  default\nconfig file automatically includes all .conf files in the /etc/nginx/conf.d/ subdirec-\ntory as well, so you should add your config file in there. Figure 7.9 shows what you\nwant to achieve.\n The pod descriptor is shown in listing 7.14 (the irrelevant parts are omitted, but\nyou’ll find the complete file in the code archive).\n \n \nListing 7.13   YAML definition of a config map created from a file\nThe entry holding the \nNginx config file’s \ncontents\nThe sleep-interval entry\n \n\n208CHAPTER 7ConfigMaps and Secrets: configuring applications\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune-configmap-volume\nspec:\n  containers:\n  - image: nginx:alpine\n    name: web-server\n    volumeMounts:\n    ...\n    - name: config\n      mountPath: /etc/nginx/conf.d      \n      readOnly: true\n    ...\n  volumes:\n  ...\n  - name: config              \n    configMap:                 \n      name: fortune-config     \n  ...\nThis  pod  definition  includes  a  volume,  which  references  your  fortune-config\nConfigMap.  You  mount  the  volume  into  the  /etc/nginx/conf.d  directory  to  make\nNginx use it. \nVERIFYING NGINX IS USING THE MOUNTED CONFIG FILE\nThe web server should now be configured to compress the responses it sends. You can\nverify this by enabling port-forwarding from localhost:8080 to the pod’s port 80 and\nchecking the server’s response with \ncurl, as shown in the following listing.\n \nListing 7.14   A pod with ConfigMap entries mounted as files: fortune-pod-configmap-\nvolume.yaml\nPod\nContainer: html-generator\nContainer: web-server\nFilesystem\n/\netc/\nnginx/\nconf.d/\nConfigMap: fortune-config\nmy-nginx-config.conf\nserver {\n...\n}\nVolume:\nconfig\nFigure 7.9   Passing ConfigMap entries to a pod as files in a volume\nYou’re mounting the \nconfigMap volume at \nthis location.\nThe volume refers to your \nfortune-config ConfigMap.\n \n\n209Decoupling configuration with a ConfigMap\n$ kubectl port-forward fortune-configmap-volume 8080:80 &\nForwarding from 127.0.0.1:8080 -> 80\nForwarding from [::1]:8080 -> 80\n$ curl -H \"Accept-Encoding: gzip\" -I localhost:8080\nHTTP/1.1 200 OK\nServer: nginx/1.11.1\nDate: Thu, 18 Aug 2016 11:52:57 GMT\nContent-Type: text/html\nLast-Modified: Thu, 18 Aug 2016 11:52:55 GMT\nConnection: keep-alive\nETag: W/\"57b5a197-37\"\nContent-Encoding: gzip           \nEXAMINING THE MOUNTED CONFIGMAP VOLUME’S CONTENTS\nThe  response  shows  you  achieved  what  you  wanted,  but  let’s  look  at  what’s  in  the\n/etc/nginx/conf.d directory now:\n$ kubectl exec fortune-configmap-volume -c web-server ls /etc/nginx/conf.d\nmy-nginx-config.conf\nsleep-interval\nBoth  entries  from  the  ConfigMap  have  been  added  as  files  to  the  directory.  The\nsleep-interval  entry  is  also  included,  although  it  has  no  business  being  there,\nbecause  it’s  only  meant  to  be  used  by  the  \nfortuneloop  container.  You  could  create\ntwo  different  ConfigMaps  and  use  one  to  configure  the  \nfortuneloop  container  and\nthe other one to configure the \nweb-server container. But somehow it feels wrong to\nuse  multiple  ConfigMaps  to  configure  containers  of  the  same  pod.  After  all,  having\ncontainers in the same pod implies that the containers are closely related and should\nprobably also be configured as a unit. \nEXPOSING CERTAIN CONFIGMAP ENTRIES IN THE VOLUME\nLuckily,  you  can  populate  a  configMap volume with only part of the ConfigMap’s\nentries—in  your  case,  only  the  \nmy-nginx-config.conf  entry.  This  won’t  affect  the\nfortuneloop container, because you’re passing the sleep-interval entry to it through\nan environment variable and not through the volume. \n To define which entries should be exposed as files in a \nconfigMap volume, use the\nvolume’s \nitems attribute as shown in the following listing.\n  volumes:\n  - name: config              \n    configMap:                                  \n      name: fortune-config                      \n      items:                       \n      - key: my-nginx-config.conf        \n        path: gzip.conf                  \nListing 7.15   Seeing if nginx responses have compression enabled\nListing 7.16   A pod with a specific ConfigMap entry mounted into a file directory: \nfortune-pod-configmap-volume-with-items.yaml\nThis shows the response \nis compressed.\nSelecting which entries to include \nin the volume by listing them\nYou want the entry \nunder this key included.\nThe entry’s value should \nbe stored in this file.\n \n\n210CHAPTER 7ConfigMaps and Secrets: configuring applications\nWhen specifying individual entries, you need to set the filename for each individual\nentry, along with the entry’s key. If you run the pod from the previous listing, the\n/etc/nginx/conf.d  directory  is  kept  nice  and  clean,  because  it  only  contains  the\ngzip.conf file and nothing else. \nUNDERSTANDING THAT MOUNTING A DIRECTORY HIDES EXISTING FILES IN THAT DIRECTORY\nThere’s one important thing to discuss at this point. In both this and in your previous\nexample, you mounted the volume as a directory, which means you’ve hidden any files\nthat are stored in the /etc/nginx/conf.d directory in the container image itself. \n This is generally what happens in Linux when you mount a filesystem into a non-\nempty directory. The directory then only contains the files from the mounted filesys-\ntem,  whereas  the  original  files  in  that  directory  are  inaccessible  for  as  long  as  the\nfilesystem is mounted. \n In your case, this has no terrible side effects, but imagine mounting a volume to\nthe /etc directory, which usually contains many important files. This would most likely\nbreak the whole container, because all of the original files that should be in the /etc\ndirectory would no longer be there. If you need to add a file to a directory like /etc,\nyou can’t use this method at all.\nMOUNTING INDIVIDUAL CONFIGMAP ENTRIES AS FILES WITHOUT HIDING OTHER FILES IN THE DIRECTORY\nNaturally, you’re now wondering how to add individual files from a ConfigMap into\nan existing directory without hiding existing files stored in it. An additional \nsubPath\nproperty on the volumeMount allows you to mount either a single file or a single direc-\ntory from the volume instead of mounting the whole volume. Perhaps this is easier to\nexplain visually (see figure 7.10).\n Say you have a \nconfigMap volume containing a myconfig.conf file, which you want\nto  add  to  the  /etc  directory  as  someconfig.conf.  You  can  use  the  \nsubPath  property  to\nmount it there without affecting any other files in that directory. The relevant part of\nthe pod definition is shown in the following listing.\nPod\nContainer\nFilesystem\n/\netc/\nsomeconfig.conf\nexistingfile1\nexistingfile2\nConfigMap: app-config\nmyconfig.conf\nContents\nof the file\nanother-fileContents\nof the file\nconfigMap\nvolume\nmyconfig.conf\nanother-file\nexistingfile1\nand existingfile2\naren’t hidden.\nOnly myconfig.conf is mounted\ninto the container (yet under a\ndifferent filename).\nanother-file isn’t\nmounted into the\ncontainer.\nFigure 7.10   Mounting a single file from a volume\n \n\n211Decoupling configuration with a ConfigMap\nspec:\n  containers:\n  - image: some/image\n    volumeMounts:\n    - name: myvolume\n      mountPath: /etc/someconfig.conf     \n      subPath: myconfig.conf            \nThe subPath  property  can  be  used  when  mounting  any  kind  of  volume.  Instead  of\nmounting the whole volume, you can mount part of it. But this method of mounting\nindividual  files  has  a  relatively  big  deficiency  related  to  updating  files.  You’ll  learn\nmore about this in the following section, but first, let’s finish talking about the initial\nstate of a \nconfigMap volume by saying a few words about file permissions.\nSETTING THE FILE PERMISSIONS FOR FILES IN A CONFIGMAP VOLUME\nBy default, the permissions on all files in a configMap volume are set to 644 (-rw-r—r--).\nYou can change this by setting the \ndefaultMode property in the volume spec, as shown\nin the following listing.\n  volumes:\n  - name: config\n    configMap:\n      name: fortune-config\n      defaultMode: \"6600\"       \nAlthough  ConfigMaps  should  be  used  for  non-sensitive  configuration  data,  you  may\nwant  to  make  the  file  readable  and  writable  only  to  the  user  and  group  the  file  is\nowned by, as the example in the previous listing shows. \n7.4.7Updating an app’s config without having to restart the app\nWe’ve said that one of the drawbacks of using environment variables or command-line\narguments as a configuration source is the inability to update them while the pro-\ncess  is  running.  Using  a  ConfigMap  and  exposing  it  through  a  volume  brings  the\nability to update the configuration without having to recreate the pod or even restart\nthe container. \n  When  you  update  a  ConfigMap,  the  files  in  all  the  volumes  referencing  it  are\nupdated. It’s then up to the process to detect that they’ve been changed and reload\nthem. But Kubernetes will most likely eventually also support sending a signal to the\ncontainer after updating the files.\nWARNINGBe aware that as I’m writing this, it takes a surprisingly long time\nfor the files to be updated after you update the ConfigMap (it can take up to\none whole minute).\nListing 7.17   A pod with a specific config map entry mounted into a specific file\nListing 7.18   Setting file permissions: fortune-pod-configmap-volume-defaultMode.yaml \nYou’re mounting into \na file, not a directory.\nInstead of mounting the whole \nvolume, you’re only mounting \nthe myconfig.conf entry.\nThis sets the permissions \nfor all files to -rw-rw------.\n \n\n212CHAPTER 7ConfigMaps and Secrets: configuring applications\nEDITING A CONFIGMAP\nLet’s see how you can change a ConfigMap and have the process running in the pod\nreload the files exposed in the \nconfigMap volume. You’ll modify the Nginx config file\nfrom  your  previous  example  and  make  Nginx  use  the  new  config  without  restarting\nthe  pod.  Try  switching  gzip  compression  off  by  editing  the  \nfortune-config  Config-\nMap with \nkubectl edit:\n$ kubectl edit configmap fortune-config\nOnce your editor opens, change the gzip on line to gzip off, save the file, and then\nclose the editor. The ConfigMap is then updated, and soon afterward, the actual file\nin the volume is updated as well. You can confirm this by printing the contents of the\nfile with \nkubectl exec:\n$ kubectl exec fortune-configmap-volume -c web-server\n➥  cat /etc/nginx/conf.d/my-nginx-config.conf\nIf  you  don’t  see  the  update  yet,  wait  a  while  and  try  again.  It  takes  a  while  for  the\nfiles  to  get  updated.  Eventually,  you’ll  see  the  change  in  the  config  file,  but  you’ll\nfind this has no effect on Nginx, because it doesn’t watch the files and reload them\nautomatically. \nSIGNALING NGINX TO RELOAD THE CONFIG\nNginx will continue to compress its responses until you tell it to reload its config files,\nwhich you can do with the following command:\n$ kubectl exec fortune-configmap-volume -c web-server -- nginx -s reload\nNow, if you try hitting the server again with curl, you should see the response is no\nlonger  compressed  (it  no  longer  contains  the  \nContent-Encoding: gzip  header).\nYou’ve effectively changed the app’s config without having to restart the container or\nrecreate the pod. \nUNDERSTANDING HOW THE FILES ARE UPDATED ATOMICALLY\nYou may wonder what happens if an app can detect config file changes on its own and\nreloads them before Kubernetes has finished updating all the files in the \nconfigMap\nvolume. Luckily, this can’t happen, because all the files are updated atomically, which\nmeans all updates occur at once. Kubernetes achieves this by using symbolic links. If\nyou list all the files in the mounted \nconfigMap volume, you’ll see something like the\nfollowing listing.\n$ kubectl exec -it fortune-configmap-volume -c web-server -- ls -lA \n➥  /etc/nginx/conf.d\ntotal 4\ndrwxr-xr-x  ... 12:15 ..4984_09_04_12_15_06.865837643\nListing 7.19   Files in a mounted configMap volume\n \n\n213Using Secrets to pass sensitive data to containers\nlrwxrwxrwx  ... 12:15 ..data -> ..4984_09_04_12_15_06.865837643\nlrwxrwxrwx  ... 12:15 my-nginx-config.conf -> ..data/my-nginx-config.conf\nlrwxrwxrwx  ... 12:15 sleep-interval -> ..data/sleep-interval\nAs you can see, the files in the mounted configMap volume are symbolic links point-\ning to files in the \n..data dir. The ..data dir is also a symbolic link pointing to a direc-\ntory  called  \n..4984_09_04_something.  When  the  ConfigMap  is  updated,  Kubernetes\ncreates a new directory like this, writes all the files to it, and then re-links the \n..data\nsymbolic link to the new directory, effectively changing all files at once.\nUNDERSTANDING THAT FILES MOUNTED INTO EXISTING DIRECTORIES DON’T GET UPDATED\nOne big caveat relates to updating ConfigMap-backed volumes. If you’ve mounted a\nsingle file in the container instead of the whole volume, the file will not be updated!\nAt least, this is true at the time of writing this chapter. \n For now, if you need to add an individual file and have it updated when you update\nits source ConfigMap, one workaround is to mount the whole volume into a different\ndirectory and then create a symbolic link pointing to the file in question. The sym-\nlink  can  either  be  created  in  the  container  image  itself,  or  you  could  create  the\nsymlink when the container starts.\nUNDERSTANDING THE CONSEQUENCES OF UPDATING A CONFIGMAP\nOne of the most important features of containers is their immutability, which allows\nus to be certain that no differences exist between multiple running containers created\nfrom the same image, so is it wrong to bypass this immutability by modifying a Config-\nMap used by running containers? \n The main problem occurs when the app doesn’t support reloading its configura-\ntion.  This  results  in  different  running  instances  being  configured  differently—those\npods that are created after the ConfigMap is changed will use the new config, whereas\nthe old pods will still use the old one. And this isn’t limited to new pods. If a pod’s con-\ntainer is restarted (for whatever reason), the new process will also see the new config.\nTherefore,  if  the  app  doesn’t  reload  its  config  automatically,  modifying  an  existing\nConfigMap (while pods are using it) may not be a good idea. \n If the app does support reloading, modifying the ConfigMap usually isn’t such a\nbig  deal,  but  you  do  need  to  be  aware  that  because  files  in  the  ConfigMap  volumes\naren’t updated synchronously across all running instances, the files in individual pods\nmay be out of sync for up to a whole minute.\n7.5Using Secrets to pass sensitive data to containers\nAll  the  information  you’ve  passed  to  your  containers  so  far  is  regular,  non-sensitive\nconfiguration  data  that  doesn’t  need  to  be  kept  secure.  But  as  we  mentioned  at  the\nstart of the chapter, the config usually also includes sensitive information, such as cre-\ndentials and private encryption keys, which need to be kept secure.\n \n\n214CHAPTER 7ConfigMaps and Secrets: configuring applications\n7.5.1Introducing Secrets\nTo store and distribute such information, Kubernetes provides a separate object called\na  Secret.  Secrets  are  much  like  ConfigMaps—they’re  also  maps  that  hold  key-value\npairs. They can be used the same way as a ConfigMap. You can\nPass Secret entries to the container as environment variables\nExpose Secret entries as files in a volume\nKubernetes helps keep your Secrets safe by making sure each Secret is only distributed\nto  the  nodes  that  run  the  pods  that  need  access  to  the  Secret.  Also,  on  the  nodes\nthemselves, Secrets are always stored in memory and never written to physical storage,\nwhich would require wiping the disks after deleting the Secrets from them. \n  On  the  master  node  itself  (more  specifically  in  etcd),  Secrets  used  to  be  stored  in\nunencrypted form, which meant the master node needs to be secured to keep the sensi-\ntive data stored in Secrets secure. This didn’t  only  include  keeping  the  etcd  storage\nsecure, but also preventing unauthorized users from using the API server, because any-\none who can create pods can mount the Secret into the pod and gain access to the sen-\nsitive  data  through  it.  From  Kubernetes  version  1.7,  etcd  stores  Secrets  in  encrypted\nform, making the system much more secure. Because of this, it’s imperative you prop-\nerly choose when to use a Secret or a ConfigMap. Choosing between them is simple:\nUse a ConfigMap to store non-sensitive, plain configuration data.\nUse a Secret to store any data that is sensitive in nature and needs to be kept\nunder  key.  If  a  config  file  includes  both  sensitive  and  not-sensitive  data,  you\nshould store the file in a Secret.\nYou already used Secrets in chapter 5, when you created a Secret to hold the TLS certifi-\ncate needed for the Ingress resource. Now you’ll explore Secrets in more detail.\n7.5.2Introducing the default token Secret\nYou’ll  start  learning  about  Secrets  by  examining  a  Secret  that’s  mounted  into  every\ncontainer you run. You may have noticed it when using \nkubectl describe on a pod.\nThe command’s output has always contained something like this:\nVolumes:\n  default-token-cfee9:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-cfee9\nEvery pod has a secret volume attached to it automatically. The volume in the previ-\nous \nkubectl describe output refers to a Secret called default-token-cfee9. Because\nSecrets  are  resources,  you  can  list  them  with  \nkubectl get secrets  and  find  the\ndefault-token Secret in that list. Let’s see:\n$ kubectl get secrets\nNAME                  TYPE                                  DATA      AGE\ndefault-token-cfee9   kubernetes.io/service-account-token   3         39d\n \n\n215Using Secrets to pass sensitive data to containers\nYou can also use kubectl describe to learn a bit more about it, as shown in the follow-\ning listing.\n$ kubectl describe secrets\nName:        default-token-cfee9\nNamespace:   default\nLabels:      <none>\nAnnotations: kubernetes.io/service-account.name=default\n             kubernetes.io/service-account.uid=cc04bb39-b53f-42010af00237\nType:        kubernetes.io/service-account-token\nData\n====\nca.crt:      1139 bytes                                   \nnamespace:   7 bytes                                      \ntoken:       eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...      \nYou can see that the Secret contains three entries—ca.crt, namespace, and token—\nwhich represent everything you need to securely talk to the Kubernetes API server\nfrom within your pods, should you need to do that. Although ideally you want your\napplication to be completely Kubernetes-agnostic, when there’s no alternative other\nthan to talk to Kubernetes directly, you’ll use the files provided through this \nsecret\nvolume. \n The \nkubectl describe pod command shows where the secret volume is mounted:\nMounts:\n  /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfee9\nNOTEBy default, the default-token Secret is mounted into every container,\nbut  you  can  disable  that  in  each  pod  by  setting  the  \nautomountService-\nAccountToken\n field in the pod spec to false or by setting it to false on the\nservice account the pod is using. (You’ll learn about service accounts later in\nthe book.)\nTo help you visualize where and how the default token Secret is mounted, see fig-\nure 7.11.\n  We’ve  said  Secrets  are  like  ConfigMaps,  so  because  this  Secret  contains  three\nentries, you can expect to see three files in the directory the \nsecret volume is mounted\ninto. You can check this easily with \nkubectl exec:\n$ kubectl exec mypod ls /var/run/secrets/kubernetes.io/serviceaccount/\nca.crt\nnamespace\ntoken\nYou’ll see how your app can use these files to access the API server in the next chapter.\nListing 7.20   Describing a Secret\nThis secret \ncontains three \nentries.\n \n\n216CHAPTER 7ConfigMaps and Secrets: configuring applications\n7.5.3Creating a Secret\nNow, you’ll create your own little Secret.  You’ll  improve  your  fortune-serving  Nginx\ncontainer by configuring it to also serve HTTPS traffic. For this, you need to create a\ncertificate and a private key. The private key needs to be kept secure, so you’ll put it\nand the certificate into a Secret.\n First, generate the certificate and private key files (do this on your local machine).\nYou can also use the files in the book’s code archive (the cert and key files are in the\nfortune-https directory):\n$ openssl genrsa -out https.key 2048\n$ openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj \n/CN=www.kubia-example.com\nNow,  to  help  better  demonstrate  a  few  things  about  Secrets,  create  an  additional\ndummy file called foo and make it contain the string \nbar. You’ll understand why you\nneed to do this in a moment or two:\n$ echo bar > foo\nNow you can use kubectl create secret to create a Secret from the three files:\n$ kubectl create secret generic fortune-https --from-file=https.key\n➥  --from-file=https.cert --from-file=foo\nsecret \"fortune-https\" created\nThis  isn’t  very  different  from  creating  ConfigMaps.  In  this  case,  you’re  creating  a\ngeneric  Secret  called  fortune-https  and  including  two  entries  in  it  (https.key  with\nthe  contents  of  the  https.key  file  and  likewise  for  the  https.cert  key/file).  As  you\nlearned earlier, you could also include the whole directory with \n--from-file=fortune-\nhttps\n instead of specifying each file individually.\nPod\nContainer\nFilesystem\n/\nvar/\nrun/\nsecrets/\nkubernetes.io/\nserviceaccount/\nDefault token Secret\nDefault token\nsecret\nvolume\nca.crt...\n...\n...\nnamespace\ntoken\nFigure 7.11   The default-token Secret is created automatically and a corresponding \nvolume is mounted in each pod automatically.\n \n\n217Using Secrets to pass sensitive data to containers\nNOTEYou’re creating a generic Secret, but you could also have created a tls\nSecret with the kubectl create secret tls command, as you did in chapter 5.\nThis would create the Secret with different entry names, though.\n7.5.4Comparing ConfigMaps and Secrets\nSecrets  and  ConfigMaps  have  a  pretty  big  difference.  This  is  what  drove  Kubernetes\ndevelopers to create ConfigMaps after Kubernetes had already supported Secrets for a\nwhile. The following listing shows the YAML of the Secret you created.\n$ kubectl get secret fortune-https -o yaml\napiVersion: v1\ndata:\n  foo: YmFyCg==\n  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...\n  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...\nkind: Secret\n...\nNow compare this to the YAML of the ConfigMap you created earlier, which is shown\nin the following listing.\n$ kubectl get configmap fortune-config -o yaml\napiVersion: v1\ndata:\n  my-nginx-config.conf: |\n    server {\n      ...\n    }\n  sleep-interval: |\n    25\nkind: ConfigMap\n...\nNotice the difference? The contents of a Secret’s entries are shown as Base64-encoded\nstrings,  whereas  those  of  a  ConfigMap  are  shown  in  clear  text.  This  initially  made\nworking  with  Secrets  in  YAML  and  JSON  manifests  a  bit  more  painful,  because  you\nhad to encode and decode them when setting and reading their entries. \nUSING SECRETS FOR BINARY DATA\nThe reason for using Base64 encoding is simple. A Secret’s entries can contain binary\nvalues, not only plain-text. Base64 encoding allows you to include the binary data in\nYAML or JSON, which are both plain-text formats. \nTIPYou can use Secrets even for non-sensitive binary data, but be aware that\nthe maximum size of a Secret is limited to 1MB.\nListing 7.21   A Secret’s YAML definition\nListing 7.22   A ConfigMap’s YAML definition\n \n\n218CHAPTER 7ConfigMaps and Secrets: configuring applications\nINTRODUCING THE STRINGDATA FIELD\nBecause not all sensitive data is in binary form, Kubernetes also allows setting a Secret’s\nvalues through the \nstringData field. The following listing shows how it’s used.\nkind: Secret\napiVersion: v1\nstringData:           \n  foo: plain text      \ndata:\n  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...\n  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...\nThe stringData  field  is  write-only  (note:  write-only,  not  read-only).  It  can  only  be\nused to set values. When you retrieve the Secret’s YAML with \nkubectl get -o yaml, the\nstringData field will not be shown. Instead, all entries you specified in the string-\nData\n field (such as the foo entry in the previous example) will be shown under data\nand will be Base64-encoded like all the other entries. \nREADING A SECRET’S ENTRY IN A POD\nWhen you expose the Secret to a container through a secret volume, the value of the\nSecret entry is decoded and written to the file in its actual form (regardless if it’s plain\ntext or binary). The same is also true when exposing the Secret entry through an envi-\nronment variable. In both cases, the app doesn’t need to decode it, but can read the\nfile’s contents or look up the environment variable value and use it directly.\n7.5.5Using the Secret in a pod\nWith your fortune-https Secret containing both the cert and key files, all you need to\ndo now is configure Nginx to use them. \nMODIFYING THE FORTUNE-CONFIG CONFIGMAP TO ENABLE HTTPS\nFor this, you need to modify the config file again by editing the ConfigMap:\n$ kubectl edit configmap fortune-config\nAfter the text editor opens, modify the part that defines the contents of the my-nginx-\nconfig.conf\n entry so it looks like the following listing.\n...\ndata:\n  my-nginx-config.conf: |\n    server {\n      listen              80;\n      listen              443 ssl;\n      server_name         www.kubia-example.com;\nListing 7.23   Adding plain text entries to a Secret using the stringData field\nListing 7.24   Modifying the fortune-config ConfigMap’s data\nThe stringData can be used \nfor non-binary Secret data.\nSee, “plain text” is not Base64-encoded.\n \n\n219Using Secrets to pass sensitive data to containers\n      ssl_certificate     certs/https.cert;           \n      ssl_certificate_key certs/https.key;            \n      ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;\n      ssl_ciphers         HIGH:!aNULL:!MD5;\n      location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n      }\n    }\n  sleep-interval: |\n...\nThis configures the server to read the certificate and key files from /etc/nginx/certs,\nso you’ll need to mount the \nsecret volume there. \nMOUNTING THE FORTUNE-HTTPS SECRET IN A POD\nNext,  you’ll  create  a  new  fortune-https  pod  and  mount  the  secret  volume  holding\nthe certificate and key into the proper location in the \nweb-server container, as shown\nin the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune-https\nspec:\n  containers:\n  - image: luksa/fortune:env\n    name: html-generator\n    env:\n    - name: INTERVAL\n      valueFrom: \n        configMapKeyRef:\n          name: fortune-config\n          key: sleep-interval\n    volumeMounts:\n    - name: html\n      mountPath: /var/htdocs\n  - image: nginx:alpine\n    name: web-server\n    volumeMounts:\n    - name: html\n      mountPath: /usr/share/nginx/html\n      readOnly: true\n    - name: config\n      mountPath: /etc/nginx/conf.d\n      readOnly: true\n    - name: certs                         \n      mountPath: /etc/nginx/certs/        \n      readOnly: true                      \n    ports:\n    - containerPort: 80\nListing 7.25   YAML definition of the fortune-https pod: fortune-pod-https.yaml\nThe paths are \nrelative to /etc/nginx.\nYou configured Nginx to read the cert and \nkey file from /etc/nginx/certs, so you need \nto mount the Secret volume there.\n \n\n220CHAPTER 7ConfigMaps and Secrets: configuring applications\n    - containerPort: 443\n  volumes:\n  - name: html\n    emptyDir: {}\n  - name: config\n    configMap:\n      name: fortune-config\n      items:\n      - key: my-nginx-config.conf\n        path: https.conf\n  - name: certs                            \n    secret:                                \n      secretName: fortune-https            \nMuch  is  going  on  in  this  pod  descriptor,  so  let  me  help  you  visualize  it.  Figure  7.12\nshows the components defined in the YAML. The \ndefault-token Secret, volume, and\nvolume mount, which aren’t part of the YAML, but are added to your pod automati-\ncally, aren’t shown in the figure.\nNOTELike configMap  volumes,  secret  volumes  also  support  specifying  file\npermissions  for  the  files  exposed  in  the  volume  through  the  \ndefaultMode\nproperty.\nYou define the secret \nvolume here, referring to \nthe fortune-https Secret.\nContainer: web-server\nContainer: html-generator\nSecret: fortune-https\nDefault token Secret and volume not shown\nsecret\nvolume:\ncerts\nemptyDir\nvolume:\nhtml\nconfigMap\nvolume:\nconfig\nhttps.cert...\n...\n...\nhttps.key\nfoo\n/etc/nginx/conf.d/\n/etc/nginx/certs/\n/usr/share/nginx/html/\n/var/htdocs\nConfigMap: fortune-config\nmy-nginx-config.conf\nserver {\n...\n}\nPod\nEnvironment variables:\nINTERVAL=25\nsleep-interval25\nFigure 7.12   Combining a ConfigMap and a Secret to run your fortune-https pod\n \n\n221Using Secrets to pass sensitive data to containers\nTESTING WHETHER NGINX IS USING THE CERT AND KEY FROM THE SECRET\nOnce the pod is running, you can see if it’s serving HTTPS traffic by opening a port-\nforward  tunnel  to  the  pod’s  port  443  and  using  it  to  send  a  request  to  the  server\nwith \ncurl: \n$ kubectl port-forward fortune-https 8443:443 &\nForwarding from 127.0.0.1:8443 -> 443\nForwarding from [::1]:8443 -> 443\n$ curl https://localhost:8443 -k\nIf you configured the server properly, you should get a response. You can check the\nserver’s certificate to see if it matches the one you generated earlier. This can also be\ndone with \ncurl by turning on verbose logging using the -v option, as shown in the fol-\nlowing listing.\n$ curl https://localhost:8443 -k -v\n* About to connect() to localhost port 8443 (#0)\n*   Trying ::1...\n* Connected to localhost (::1) port 8443 (#0)\n* Initializing NSS with certpath: sql:/etc/pki/nssdb\n* skipping SSL peer certificate verification\n* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n* Server certificate:\n*   subject: CN=www.kubia-example.com          \n*   start date: aug 16 18:43:13 2016 GMT       \n*   expire date: aug 14 18:43:13 2026 GMT      \n*   common name: www.kubia-example.com         \n*   issuer: CN=www.kubia-example.com           \nUNDERSTANDING SECRET VOLUMES ARE STORED IN MEMORY\nYou successfully delivered your certificate and private key to your container by mount-\ning a \nsecret volume in its directory tree at /etc/nginx/certs. The secret volume uses\nan in-memory filesystem (tmpfs) for the Secret files. You can see this if you list mounts\nin the container:\n$ kubectl exec fortune-https -c web-server -- mount | grep certs\ntmpfs on /etc/nginx/certs type tmpfs (ro,relatime) \nBecause tmpfs is used, the sensitive data stored in the Secret is never written to disk,\nwhere it could be compromised. \nEXPOSING A SECRET’S ENTRIES THROUGH ENVIRONMENT VARIABLES\nInstead  of  using  a  volume,  you  could  also  have  exposed  individual  entries  from  the\nsecret as environment variables, the way you did with the sleep-interval entry from\nthe ConfigMap. For example, if you wanted to expose the \nfoo key from your Secret as\nenvironment variable \nFOO_SECRET, you’d add the snippet from the following listing to\nthe container definition.\nListing 7.26   Displaying the server certificate sent by Nginx\nThe certificate \nmatches the one you \ncreated and stored \nin the Secret.\n \n\n222CHAPTER 7ConfigMaps and Secrets: configuring applications\n    env:\n    - name: FOO_SECRET\n      valueFrom:                  \n        secretKeyRef:             \n          name: fortune-https    \n          key: foo           \nThis is almost exactly like when you set the INTERVAL environment variable, except\nthat this time you’re referring to a Secret by using \nsecretKeyRef instead of config-\nMapKeyRef\n, which is used to refer to a ConfigMap.\n Even though Kubernetes enables you to expose Secrets through environment vari-\nables, it may not be the best idea to use this feature. Applications usually dump envi-\nronment variables in error reports or even write them to the application log at startup,\nwhich may unintentionally expose them. Additionally, child processes inherit all the\nenvironment variables of the parent process, so if your app runs a third-party binary,\nyou have no way of knowing what happens with your secret data. \nTIPThink  twice  before  using  environment variables to pass your Secrets to\nyour container, because they may get exposed inadvertently. To be safe, always\nuse \nsecret volumes for exposing Secrets.\n7.5.6Understanding image pull Secrets\nYou’ve learned how to pass Secrets to your applications and use the data they contain.\nBut  sometimes  Kubernetes  itself  requires  you  to  pass  credentials  to  it—for  example,\nwhen  you’d  like  to  use  images  from  a  private  container  image  registry.  This  is  also\ndone through Secrets.\n Up to now all your container images have been stored on public image registries,\nwhich don’t require any special credentials to pull images from them. But most orga-\nnizations  don’t  want  their  images  to  be  available  to  everyone  and  thus  use  a  private\nimage registry. When deploying a pod, whose container images reside in a private reg-\nistry, Kubernetes needs to know the credentials required to pull the image. Let’s see\nhow to do that.\nUSING A PRIVATE IMAGE REPOSITORY ON DOCKER HUB\nDocker Hub, in addition to public image repositories, also allows you to create private\nrepositories. You can mark a repository as private by logging in at http://hub.docker\n.com with your web browser, finding the repository and checking a checkbox. \n To run a pod, which uses an image from the private repository, you need to do\ntwo things:\nCreate a Secret holding the credentials for the Docker registry.\nReference that Secret in the imagePullSecrets field of the pod manifest.\nListing 7.27   Exposing a Secret’s entry as an environment variable\nThe variable should be set \nfrom the entry of a Secret.\nThe name of the Secret \nholding the key\nThe key of the Secret \nto expose\n \n\n223Using Secrets to pass sensitive data to containers\nCREATING A SECRET FOR AUTHENTICATING WITH A DOCKER REGISTRY\nCreating a Secret holding the credentials for authenticating with a Docker registry\nisn’t that different from creating the generic Secret you created in section 7.5.3. You\nuse  the  same  \nkubectl create secret  command,  but  with  a  different  type  and\noptions:\n$ kubectl create secret docker-registry mydockerhubsecret \\\n  --docker-username=myusername --docker-password=mypassword \\ \n  --docker-email=my.email@provider.com\nRather than create a generic secret, you’re creating a docker-registry Secret called\nmydockerhubsecret.  You’re  specifying  your  Docker  Hub  username,  password,  and\nemail. If you inspect the contents of the newly created Secret with \nkubectl describe,\nyou’ll  see  that  it  includes  a  single  entry  called  \n.dockercfg.  This  is  equivalent  to  the\n.dockercfg file in your home directory, which is created by Docker when you run the\ndocker login command.\nUSING THE DOCKER-REGISTRY SECRET IN A POD DEFINITION\nTo  have  Kubernetes  use  the  Secret  when  pulling  images  from  your  private  Docker\nHub  repository,  all  you  need  to  do  is  specify  the  Secret’s  name  in  the  pod  spec,  as\nshown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: private-pod\nspec:\n  imagePullSecrets:                 \n  - name: mydockerhubsecret         \n  containers:\n  - image: username/private:tag\n    name: main\nIn the pod definition in the previous listing, you’re specifying the mydockerhubsecret\nSecret as one of the imagePullSecrets. I suggest you try this out yourself, because it’s\nlikely you’ll deal with private container images soon.\nNOT HAVING TO SPECIFY IMAGE PULL SECRETS ON EVERY POD\nGiven that people usually run many different pods in their systems, it makes you won-\nder if you need to add the same image pull Secrets to every pod. Luckily, that’s not the\ncase. In chapter 12 you’ll learn how image pull Secrets can be added to all your pods\nautomatically if you add the Secrets to a ServiceAccount.\nListing 7.28   A pod definition using an image pull Secret: pod-with-private-image.yaml\nThis enables pulling images \nfrom a private image registry.\n \n\n224CHAPTER 7ConfigMaps and Secrets: configuring applications\n7.6Summary\nThis  wraps  up  this  chapter  on  how  to  pass  configuration  data  to  containers.  You’ve\nlearned how to\nOverride the default command defined in a container image in the pod definition\nPass command-line arguments to the main container process\nSet environment variables for a container\nDecouple configuration from a pod specification and put it into a ConfigMap\nStore sensitive data in a Secret and deliver it securely to containers\nCreate a docker-registry Secret and use it to pull images from a private image\nregistry\nIn the next chapter, you’ll learn how to pass pod and container metadata to applica-\ntions  running  inside  them.  You’ll  also  see  how  the  default  token  Secret,  which  we\nlearned about in this chapter, is used to talk to the API server from within a pod. \n \n\n225\nAccessing pod metadata\nand other resources\nfrom applications\nApplications  often  need  information  about  the  environment  they’re  running  in,\nincluding  details  about  themselves  and  that  of  other  components  in  the  cluster.\nYou’ve  already  seen  how  Kubernetes  enables  service  discovery  through  environ-\nment  variables  or  DNS,  but  what  about  other  information?  In  this  chapter,  you’ll\nsee  how  certain  pod  and  container  metadata  can  be  passed  to  the  container  and\nhow easy it is for an app running inside a container to talk to the Kubernetes API\nserver to get information about the resources deployed in the cluster and even how\nto create or modify those resources.\nThis chapter covers\nUsing the Downward API to pass information into \ncontainers\nExploring the Kubernetes REST API\nLeaving authentication and server verification to \nkubectl proxy\nAccessing the API server from within a container\nUnderstanding the ambassador container pattern\nUsing Kubernetes client libraries\n \n\n226CHAPTER 8Accessing pod metadata and other resources from applications\n8.1Passing metadata through the Downward API\nIn the previous chapter you saw how you can pass configuration data to your appli-\ncations through environment variables or through \nconfigMap and secret volumes.\nThis  works  well  for  data  that  you  set  yourself  and  that  is  known  before  the  pod  is\nscheduled to a node and run there. But what about data that isn’t known up until\nthat point—such as the pod’s IP, the host node’s name, or even the pod’s own name\n(when the name is generated; for example, when the pod is created by a ReplicaSet\nor similar controller)? And what about data that’s already specified elsewhere, such\nas a pod’s labels and annotations? You don’t want to repeat the same information in\nmultiple places.\n Both these problems are solved by the Kubernetes Downward API. It allows you to\npass metadata about the pod and its environment through environment variables or\nfiles (in a \ndownwardAPI volume). Don’t be confused by the name. The Downward API\nisn’t like a REST endpoint that your app needs to hit so it can get the data. It’s a way of\nhaving environment variables or files populated with values from the pod’s specifica-\ntion or status, as shown in figure 8.1.\n8.1.1Understanding the available metadata\nThe  Downward  API  enables  you  to  expose  the  pod’s  own  metadata  to  the  processes\nrunning inside that pod. Currently, it allows you to pass the following information to\nyour containers:\nThe pod’s name\nThe pod’s IP address\nContainer: main\nEnvironment\nvariables\nAPI server\nUsed to initialize environment\nvariables and files in the\ndownwardAPI volume\nPod manifest\n- Metadata\n- Status\nPod\ndownwardAPI\nvolume\nApp process\nFigure 8.1   The Downward API exposes pod metadata through environment variables or files.\n \n\n227Passing metadata through the Downward API\nThe namespace the pod belongs to\nThe name of the node the pod is running on\nThe name of the service account the pod is running under\nThe CPU and memory requests for each container\nThe CPU and memory limits for each container\nThe pod’s labels\nThe pod’s annotations\nMost of the items in the list shouldn’t require further explanation, except perhaps the\nservice account and CPU/memory requests and limits, which we haven’t introduced\nyet. We’ll cover service accounts in detail in chapter 12. For now, all you need to know\nis that a service account is the account that the pod authenticates as when talking to\nthe  API  server.  CPU  and  memory  requests  and  limits  are  explained  in  chapter  14.\nThey’re  the  amount  of  CPU  and  memory  guaranteed  to  a  container  and  the  maxi-\nmum amount it can get.\n Most items in the list can be passed to containers either through environment vari-\nables  or  through  a  \ndownwardAPI  volume,  but  labels  and  annotations  can  only  be\nexposed  through  the  volume.  Part  of  the  data  can  be  acquired  by  other  means  (for\nexample, from the operating system directly), but the Downward API provides a sim-\npler alternative.\n Let’s look at an example to pass metadata to your containerized process.\n8.1.2Exposing metadata through environment variables\nFirst,  let’s  look  at  how  you  can  pass  the  pod’s  and  container’s  metadata  to  the  con-\ntainer  through  environment  variables.  You’ll  create  a  simple  single-container  pod\nfrom the following listing’s manifest.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: downward\nspec:\n  containers:\n  - name: main\n    image: busybox\n    command: [\"sleep\", \"9999999\"]\n    resources:\n      requests:\n        cpu: 15m\n        memory: 100Ki\n      limits:\n        cpu: 100m\n        memory: 4Mi\n    env:\n    - name: POD_NAME\nListing 8.1   Downward API used in environment variables: downward-api-env.yaml\n \n\n228CHAPTER 8Accessing pod metadata and other resources from applications\n      valueFrom:                            \n        fieldRef:                           \n          fieldPath: metadata.name          \n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: NODE_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    - name: SERVICE_ACCOUNT\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.serviceAccountName\n    - name: CONTAINER_CPU_REQUEST_MILLICORES\n      valueFrom:                                   \n        resourceFieldRef:                          \n          resource: requests.cpu                   \n          divisor: 1m                            \n    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES\n      valueFrom:\n        resourceFieldRef:\n          resource: limits.memory\n          divisor: 1Ki\nWhen your process runs, it can look up all the environment variables you defined in\nthe pod spec. Figure 8.2 shows the environment variables and the sources of their val-\nues.  The  pod’s  name,  IP,  and  namespace  will  be  exposed  through  the  \nPOD_NAME,\nPOD_IP,  and  POD_NAMESPACE  environment  variables,  respectively.  The  name  of  the\nnode  the  container  is  running  on  will  be  exposed  through  the  \nNODE_NAME  variable.\nThe  name  of  the  service  account  is  made  available  through  the  \nSERVICE_ACCOUNT\nenvironment  variable.  You’re  also  creating  two  environment  variables  that  will  hold\nthe amount of CPU requested for this container and the maximum amount of mem-\nory the container is allowed to consume.\n For environment variables exposing resource limits or requests, you specify a divi-\nsor. The actual value of the limit or the request will be divided by the divisor and the\nresult exposed through the environment variable. In the previous example, you’re set-\nting  the  divisor  for  CPU  requests  to  \n1m  (one  milli-core,  or  one  one-thousandth  of  a\nCPU  core).  Because  you’ve  set  the  CPU  request  to  \n15m,  the  environment  variable\nCONTAINER_CPU_REQUEST_MILLICORES will be set to 15. Likewise, you set the memory\nlimit to \n4Mi (4 mebibytes) and the divisor to 1Ki (1 Kibibyte), so the CONTAINER_MEMORY\n_LIMIT_KIBIBYTES\n environment variable will be set to 4096. \nInstead of specifying an absolute value, \nyou’re referencing the metadata.name \nfield from the pod manifest.\nA container’s CPU and memory \nrequests and limits are referenced \nby using resourceFieldRef instead \nof fieldRef.\nFor resource fields, you \ndefine a divisor to get the \nvalue in the unit you need.\n \n\n229Passing metadata through the Downward API\nThe divisor for CPU limits and requests can be either 1, which means one whole core,\nor \n1m, which is one millicore. The divisor for memory limits/requests can be 1 (byte),\n1k (kilobyte) or 1Ki (kibibyte), 1M (megabyte) or 1Mi (mebibyte), and so on.\n After creating the pod, you can use \nkubectl exec to see all these environment vari-\nables in your container, as shown in the following listing.\n$ kubectl exec downward env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=downward\nCONTAINER_MEMORY_LIMIT_KIBIBYTES=4096\nPOD_NAME=downward\nPOD_NAMESPACE=default\nPOD_IP=10.0.0.10\nNODE_NAME=gke-kubia-default-pool-32a2cac8-sgl7\nSERVICE_ACCOUNT=default\nCONTAINER_CPU_REQUEST_MILLICORES=15\nKUBERNETES_SERVICE_HOST=10.3.240.1\nKUBERNETES_SERVICE_PORT=443\n...\nListing 8.2   Environment variables in the downward pod\nPod manifest\nmetadata:\nname: downward\nnamespace: default\nspec:\nnodeName: minikube\nserviceAccountName: default\ncontainers:\n- name: main\nimage: busybox\ncommand: [\"sleep\", \"9999999\"]\nresources:\nrequests:\ncpu: 15m\nmemory: 100Ki\nlimits:\ncpu: 100m\nmemory: 4Mi\n...\nstatus:\npodIP: 172.17.0.4\n...\nPod: downward\nContainer: main\nEnvironment variables\nPOD_NAME=downward\nPOD_NAMESPACE=default\nPOD_IP=172.17.0.4\nNODE_NAME=minikube\nSERVICE_ACCOUNT=default\nCONTAINER_CPU_REQUEST_MILLICORES=15\nCONTAINER_MEMORY_LIMIT_KIBIBYTES=4096\ndivisor: 1m\ndivisor: 1Ki\nFigure 8.2   Pod metadata and attributes can be exposed to the pod through environment variables.\n \n\n230CHAPTER 8Accessing pod metadata and other resources from applications\nAll processes running inside the container can read those variables and use them how-\never they need. \n8.1.3Passing metadata through files in a downwardAPI volume\nIf you prefer to expose the metadata through files instead of environment variables,\nyou can define a \ndownwardAPI volume and mount it into your container. You must use\na \ndownwardAPI  volume  for  exposing  the  pod’s  labels  or  its  annotations,  because  nei-\nther can be exposed through environment variables. We’ll discuss why later.\n As with environment variables, you need to specify each metadata field explicitly if\nyou want to have it exposed to the process. Let’s see how to modify the previous exam-\nple to use a volume instead of environment variables, as shown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: downward\n  labels:                  \n    foo: bar               \n  annotations:             \n    key1: value1           \n    key2: |                \n      multi                \n      line                 \n      value                \nspec:\n  containers:\n  - name: main\n    image: busybox\n    command: [\"sleep\", \"9999999\"]\n    resources:\n      requests:\n        cpu: 15m\n        memory: 100Ki\n      limits:\n        cpu: 100m\n        memory: 4Mi\n    volumeMounts:                        \n    - name: downward                     \n      mountPath: /etc/downward           \n  volumes:\n  - name: downward                 \n    downwardAPI:                   \n      items:\n      - path: \"podName\"                     \n        fieldRef:                           \n          fieldPath: metadata.name          \n      - path: \"podNamespace\"\n        fieldRef:\n          fieldPath: metadata.namespace\nListing 8.3   Pod with a downwardAPI volume: downward-api-volume.yaml\nThese labels and \nannotations will be \nexposed through the \ndownwardAPI volume.\nYou’re mounting the \ndownward volume \nunder /etc/downward.\nYou’re defining a downwardAPI \nvolume with the name downward.\nThe pod’s name (from the metadata.name \nfield in the manifest) will be written to \nthe podName file.\n \n\n231Passing metadata through the Downward API\n      - path: \"labels\"                       \n        fieldRef:                            \n          fieldPath: metadata.labels         \n      - path: \"annotations\"                       \n        fieldRef:                                 \n          fieldPath: metadata.annotations         \n      - path: \"containerCpuRequestMilliCores\"\n        resourceFieldRef:\n          containerName: main\n          resource: requests.cpu\n          divisor: 1m\n      - path: \"containerMemoryLimitBytes\"\n        resourceFieldRef:\n          containerName: main\n          resource: limits.memory\n          divisor: 1\nInstead of passing the metadata through environment variables, you’re defining a vol-\nume called \ndownward and mounting it in your container under /etc/downward. The\nfiles this volume will contain are configured under the \ndownwardAPI.items attribute\nin the volume specification.\n Each item specifies the \npath (the filename) where the metadata should be written\nto and references either a pod-level field or a container resource field whose value you\nwant stored in the file (see figure 8.3).\nThe pod’s labels will be written \nto the /etc/downward/labels file.\nThe pod’s annotations will be \nwritten to the /etc/downward/\nannotations file.\ndownwardAPI volume\nPod manifest\nmetadata:\nname: downward\nnamespace: default\nlabels:\nfoo: bar\nannotations:\nkey1: value1\n...\nspec:\ncontainers:\n- name: main\nimage: busybox\ncommand: [\"sleep\", \"9999999\"]\nresources:\nrequests:\ncpu: 15m\nmemory: 100Ki\nlimits:\ncpu: 100m\nmemory: 4Mi\n...\n/podName\n/podNamespace\n/labels\n/annotations\n/containerCpuRequestMilliCores\n/containerMemoryLimitBytes\ndivisor: 1\ndivisor: 1m\nContainer: main\nPod: downward\nFilesystem\n/\netc/\ndownward/\nFigure 8.3   Using a downwardAPI volume to pass metadata to the container\n \n\n232CHAPTER 8Accessing pod metadata and other resources from applications\nDelete the previous pod and create a new one from the manifest in the previous list-\ning.  Then  look  at  the  contents  of  the  mounted  \ndownwardAPI  volume  directory.  You\nmounted the volume under /etc/downward/, so list the files in there, as shown in the\nfollowing listing.\n$ kubectl exec downward ls -lL /etc/downward\n-rw-r--r--   1 root   root   134 May 25 10:23 annotations\n-rw-r--r--   1 root   root     2 May 25 10:23 containerCpuRequestMilliCores\n-rw-r--r--   1 root   root     7 May 25 10:23 containerMemoryLimitBytes\n-rw-r--r--   1 root   root     9 May 25 10:23 labels\n-rw-r--r--   1 root   root     8 May 25 10:23 podName\n-rw-r--r--   1 root   root     7 May 25 10:23 podNamespace\nNOTEAs  with  the  configMap  and  secret  volumes,  you  can  change  the  file\npermissions through the \ndownwardAPI volume’s defaultMode property in the\npod spec.\nEach file corresponds to an item in the volume’s definition. The contents of files,\nwhich  correspond  to  the  same  metadata  fields  as  in  the  previous  example,  are  the\nsame as the values of environment variables you used before, so we won’t show them\nhere. But because you couldn’t expose labels and annotations through environment\nvariables  before,  examine  the  following  listing  for  the  contents  of  the  two  files  you\nexposed them in.\n$ kubectl exec downward cat /etc/downward/labels\nfoo=\"bar\"\n$ kubectl exec downward cat /etc/downward/annotations\nkey1=\"value1\"\nkey2=\"multi\\nline\\nvalue\\n\"\nkubernetes.io/config.seen=\"2016-11-28T14:27:45.664924282Z\"\nkubernetes.io/config.source=\"api\"\nAs you can see, each label/annotation is written in the key=value format on a sepa-\nrate line. Multi-line values are written to a single line with newline characters denoted\nwith \n\\n.\nUPDATING LABELS AND ANNOTATIONS\nYou may remember that labels and annotations can be modified while a pod is run-\nning.  As  you  might  expect,  when  they  change,  Kubernetes  updates  the  files  holding\nthem, allowing the pod to always see up-to-date data. This also explains why labels and\nannotations  can’t  be  exposed  through  environment  variables.  Because  environment\nvariable values can’t be updated afterward, if the labels or annotations of a pod were\nexposed through environment variables, there’s no way to expose the new values after\nthey’re modified.\nListing 8.4   Files in the downwardAPI volume\nListing 8.5   Displaying labels and annotations in the downwardAPI volume\n \n\n233Talking to the Kubernetes API server\nREFERRING TO CONTAINER-LEVEL METADATA IN THE VOLUME SPECIFICATION\nBefore we wrap up this section, we need to point out one thing. When exposing con-\ntainer-level  metadata,  such  as  a  container’s  resource  limit  or  requests  (done  using\nresourceFieldRef),  you  need  to  specify  the  name  of  the  container  whose  resource\nfield you’re referencing, as shown in the following listing.\nspec:\n  volumes:\n  - name: downward                       \n    downwardAPI:                         \n      items:\n      - path: \"containerCpuRequestMilliCores\"\n        resourceFieldRef:\n          containerName: main       \n          resource: requests.cpu\n          divisor: 1m\nThe reason for this becomes obvious if you consider that volumes are defined at the\npod  level,  not  at  the  container  level.  When  referring  to  a  container’s  resource  field\ninside a volume specification, you need to explicitly specify the name of the container\nyou’re referring to. This is true even for single-container pods. \n  Using  volumes  to  expose  a  container’s  resource  requests  and/or  limits  is  slightly\nmore complicated than using environment variables, but the benefit is that it allows\nyou  to  pass  one  container’s  resource  fields  to  a  different  container  if  needed  (but\nboth containers need to be in the same pod). With environment variables, a container\ncan only be passed its own resource limits and requests. \nUNDERSTANDING WHEN TO USE THE DOWNWARD API\nAs you’ve seen, using the Downward API isn’t complicated. It allows you to keep the\napplication Kubernetes-agnostic. This is especially useful when you’re dealing with an\nexisting  application  that  expects  certain  data  in  environment  variables.  The  Down-\nward  API  allows  you  to  expose  the  data  to  the  application  without  having  to  rewrite\nthe application or wrap it in a shell script, which collects the data and then exposes it\nthrough environment variables.\n But the metadata available through the Downward API is fairly limited. If you need\nmore, you’ll need to obtain it from the Kubernetes API server directly. You’ll learn\nhow to do that next.\n8.2Talking to the Kubernetes API server\nWe’ve seen how the Downward API provides a simple way to pass certain pod and con-\ntainer  metadata  to  the  process  running  inside  them.  It  only  exposes  the  pod’s  own\nmetadata and a subset of all of the pod’s data. But sometimes your app will need to\nknow more about other pods and even other resources defined in your cluster. The\nDownward API doesn’t help in those cases.\nListing 8.6   Referring to container-level metadata in a downwardAPI volume\nContainer name \nmust be specified\n \n\n234CHAPTER 8Accessing pod metadata and other resources from applications\n As you’ve seen throughout the book, information about services and pods can be\nobtained by looking at the service-related environment variables or through DNS. But\nwhen the app needs data about other resources or when it requires access to the most\nup-to-date information as possible, it needs to talk to the API server directly (as shown\nin figure 8.4).\nBefore you see how apps within pods can talk to the Kubernetes API server, let’s first\nexplore  the  server’s  REST  endpoints  from  your  local  machine,  so  you  can  see  what\ntalking to the API server looks like.\n8.2.1Exploring the Kubernetes REST API\nYou’ve learned about different Kubernetes resource types. But if you’re planning on\ndeveloping apps that talk to the Kubernetes API, you’ll want to know the API first. \n To do that, you can try hitting the API server directly. You can get its URL by run-\nning \nkubectl cluster-info:\n$ kubectl cluster-info\nKubernetes master is running at https://192.168.99.100:8443\nBecause the server uses HTTPS and requires authentication, it’s not simple to talk to\nit  directly.  You  can  try  accessing  it  with  \ncurl  and  using  curl’s --insecure  (or  -k)\noption to skip the server certificate check, but that doesn’t get you far:\n$ curl https://192.168.99.100:8443 -k\nUnauthorized\nLuckily,  rather  than  dealing  with  authentication  yourself,  you  can  talk  to  the  server\nthrough a proxy by running the \nkubectl proxy command. \nACCESSING THE API SERVER THROUGH KUBECTL PROXY \nThe kubectl proxy command runs a proxy server that accepts HTTP connections on\nyour local machine and proxies them to the API server while taking care of authenti-\ncation,  so  you  don’t  need  to  pass  the  authentication  token  in  every  request.  It  also\nmakes  sure  you’re  talking  to  the  actual  API  server  and  not  a  man  in  the  middle  (by\nverifying the server’s certificate on each request).\nContainer\nAPI server\nPod\nApp process\nAPI objects\nFigure 8.4   Talking to the API server \nfrom inside a pod to get information \nabout other API objects\n \n\n235Talking to the Kubernetes API server\n Running the proxy is trivial. All you need to do is run the following command:\n$ kubectl proxy\nStarting to serve on 127.0.0.1:8001\nYou don’t need to pass in any other arguments, because kubectl already knows every-\nthing it needs (the API server URL, authorization token, and so on). As soon as it starts\nup, the proxy starts accepting connections on local port 8001. Let’s see if it works:\n$ curl localhost:8001\n{\n  \"paths\": [\n    \"/api\",\n    \"/api/v1\",\n    ...\nVoila! You sent the request to the proxy, it sent a request to the API server, and then\nthe proxy returned whatever the server returned. Now, let’s start exploring.\nEXPLORING THE KUBERNETES API THROUGH THE KUBECTL PROXY\nYou  can  continue  to  use  curl,  or  you  can  open  your  web  browser  and  point  it  to\nhttp://localhost:8001. Let’s examine what the API server returns when you hit its base\nURL more closely. The server responds with a list of paths, as shown in the follow-\ning listing.\n$ curl http://localhost:8001\n{\n  \"paths\": [\n    \"/api\",\n    \"/api/v1\",                  \n    \"/apis\",\n    \"/apis/apps\",\n    \"/apis/apps/v1beta1\",\n    ...\n    \"/apis/batch\",              \n    \"/apis/batch/v1\",           \n    \"/apis/batch/v2alpha1\",     \n    ...\nThese paths correspond to the API groups and versions you specify in your resource\ndefinitions when creating resources such as Pods, Services, and so on. \n You may recognize the \nbatch/v1 in the /apis/batch/v1 path as the API group and\nversion  of  the  Job  resources  you  learned  about  in  chapter  4.  Likewise,  the  \n/api/v1\ncorresponds to the apiVersion: v1 you refer to in the common resources you created\n(Pods,  Services,  ReplicationControllers,  and  so  on).  The  most  common  resource\ntypes, which were introduced in the earliest versions of Kubernetes, don’t belong to\nListing 8.7   Listing the API server’s REST endpoints: http://localhost:8001\nMost resource types \ncan be found here.\nThe batch API \ngroup and its \ntwo versions\n \n\n236CHAPTER 8Accessing pod metadata and other resources from applications\nany  specific  group,  because  Kubernetes  initially  didn’t  even  use  the  concept  of  API\ngroups; they were introduced later. \nNOTEThese initial resource types without an API group are now considered\nto belong to the core API group.\nEXPLORING THE BATCH API GROUP’S REST ENDPOINT\nLet’s  explore  the  Job  resource  API.  You’ll  start  by  looking  at  what’s  behind  the\n/apis/batch path (you’ll omit the version for now), as shown in the following listing.\n$ curl http://localhost:8001/apis/batch\n{\n  \"kind\": \"APIGroup\",\n  \"apiVersion\": \"v1\",\n  \"name\": \"batch\",\n  \"versions\": [\n    {\n      \"groupVersion\": \"batch/v1\",             \n      \"version\": \"v1\"                         \n    },\n    {\n      \"groupVersion\": \"batch/v2alpha1\",       \n      \"version\": \"v2alpha1\"                   \n    }\n  ],\n  \"preferredVersion\": {                    \n    \"groupVersion\": \"batch/v1\",            \n    \"version\": \"v1\"                        \n  },\n  \"serverAddressByClientCIDRs\": null\n}\nThe response shows a description of the batch API group, including the available ver-\nsions  and  the  preferred  version  clients  should  use.  Let’s  continue  and  see  what’s\nbehind the \n/apis/batch/v1 path. It’s shown in the following listing.\n$ curl http://localhost:8001/apis/batch/v1\n{\n  \"kind\": \"APIResourceList\",              \n  \"apiVersion\": \"v1\",\n  \"groupVersion\": \"batch/v1\",             \n  \"resources\": [                          \n    {\n      \"name\": \"jobs\",             \n      \"namespaced\": true,         \n      \"kind\": \"Job\",              \nListing 8.8   Listing endpoints under /apis/batch: http://localhost:8001/apis/batch\nListing 8.9   Resource types in batch/v1: http://localhost:8001/apis/batch/v1\nThe batch API \ngroup contains \ntwo versions.\nClients should use the \nv1 version instead of \nv2alpha1.\nThis is a list of API resources \nin the batch/v1 API group.\nHere’s an array holding \nall the resource types \nin this group.\nThis describes the \nJob resource, which \nis namespaced.\n \n\n237Talking to the Kubernetes API server\n      \"verbs\": [                 \n        \"create\",                \n        \"delete\",                \n        \"deletecollection\",      \n        \"get\",                   \n        \"list\",                  \n        \"patch\",                 \n        \"update\",                \n        \"watch\"                  \n      ]\n    },\n    {\n      \"name\": \"jobs/status\",            \n      \"namespaced\": true,                  \n      \"kind\": \"Job\",\n      \"verbs\": [             \n        \"get\",               \n        \"patch\",             \n        \"update\"             \n      ]\n    }\n  ]\n}\nAs you can see, the API server returns a list of resource types and REST endpoints in\nthe \nbatch/v1 API group. One of those is the Job resource. In addition to the name of\nthe  resource  and  the  associated  \nkind,  the  API  server  also  includes  information  on\nwhether the resource is \nnamespaced or not, its short name (if it has one; Jobs don’t),\nand a list of \nverbs you can use with the resource. \n The returned list describes the REST resources exposed in the API server. The\n\"name\": \"jobs\" line tells you that the API contains the /apis/batch/v1/jobs end-\npoint.  The  \n\"verbs\"  array  says  you  can  retrieve,  update,  and  delete  Job  resources\nthrough  that  endpoint.  For  certain  resources,  additional  API  endpoints  are  also\nexposed (such as the \njobs/status path, which allows modifying only the status of\naJob).\nLISTING ALL JOB INSTANCES IN THE CLUSTER\nTo  get  a  list  of  Jobs  in  your  cluster,  perform  a  GET  request  on  path  /apis/batch/\nv1/jobs\n, as shown in the following listing.\n$ curl http://localhost:8001/apis/batch/v1/jobs\n{\n  \"kind\": \"JobList\",\n  \"apiVersion\": \"batch/v1\",\n  \"metadata\": {\n    \"selfLink\": \"/apis/batch/v1/jobs\",\n    \"resourceVersion\": \"225162\"\n  },\nListing 8.10   List of Jobs: http://localhost:8001/apis/batch/v1/jobs\nHere are the verbs that can be used \nwith this resource (you can create \nJobs; delete individual ones or a \ncollection of them; and retrieve, \nwatch, and update them).\nResources also have a \nspecial REST endpoint for \nmodifying their status.\nThe status can be \nretrieved, patched, \nor updated.\n \n\n238CHAPTER 8Accessing pod metadata and other resources from applications\n  \"items\": [\n    {\n      \"metadata\": {\n        \"name\": \"my-job\",\n        \"namespace\": \"default\",\n        ...\nYou probably have no Job resources deployed in your cluster, so the items array will be\nempty. You can try deploying the Job in Chapter08/my-job.yaml and hitting the REST\nendpoint again to get the same output as in listing 8.10.\nRETRIEVING A SPECIFIC JOB INSTANCE BY NAME\nThe  previous  endpoint  returned  a  list  of  all Jobs across all namespaces. To get back\nonly  one  specific  Job,  you  need  to  specify  its  name  and  namespace  in  the  URL.  To\nretrieve  the  Job  shown  in  the  previous  listing  (\nname: my-job; namespace: default),\nyou need to request the following path: \n/apis/batch/v1/namespaces/default/jobs/\nmy-job\n, as shown in the following listing.\n$ curl http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job\n{\n  \"kind\": \"Job\",\n  \"apiVersion\": \"batch/v1\",\n  \"metadata\": {\n    \"name\": \"my-job\",\n    \"namespace\": \"default\",\n    ...\nAs you can see, you get back the complete JSON definition of the my-job Job resource,\nexactly like you do if you run:\n$ kubectl get job my-job -o json\nYou’ve seen that you can browse the Kubernetes REST API server without using any\nspecial tools, but to fully explore the REST API and interact with it, a better option is\ndescribed at the end of this chapter. For now, exploring it with \ncurl like this is enough\nto make you understand how an application running in a pod talks to Kubernetes. \n8.2.2Talking to the API server from within a pod\nYou’ve  learned  how  to  talk  to  the  API  server  from  your  local  machine,  using  the\nkubectl proxy. Now, let’s see how to talk to it from within a pod, where you (usually)\ndon’t have \nkubectl. Therefore, to talk to the API server from inside a pod, you need\nto take care of three things:\nFind the location of the API server.\nMake sure you’re talking to the API server and not something impersonating it.\nAuthenticate with the server; otherwise it won’t let you see or do anything.\nListing 8.11   Retrieving a resource in a specific namespace by name\n \n\n239Talking to the Kubernetes API server\nYou’ll see how this is done in the next three sections. \nRUNNING A POD TO TRY OUT COMMUNICATION WITH THE API SERVER\nThe first thing you need is a pod from which to talk to the API server. You’ll run a pod\nthat does nothing (it runs the \nsleep command in its only container), and then run a\nshell in the container with \nkubectl exec. Then you’ll try to access the API server from\nwithin that shell using \ncurl.\n Therefore, you need to use a container image that contains the \ncurl binary. If you\nsearch for such an image on, say, Docker Hub, you’ll find the \ntutum/curl image, so\nuse  it  (you  can  also  use  any  other  existing  image  containing  the  \ncurl  binary  or  you\ncan build your own). The pod definition is shown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: curl\nspec:\n  containers:\n  - name: main\n    image: tutum/curl                \n    command: [\"sleep\", \"9999999\"]    \nAfter creating the pod, run kubectl exec to run a bash shell inside its container:\n$ kubectl exec -it curl bash\nroot@curl:/#\nYou’re now ready to talk to the API server.\nFINDING THE API SERVER’S ADDRESS\nFirst,  you  need  to  find  the  IP  and  port  of  the  Kubernetes  API  server.  This  is  easy,\nbecause  a  Service  called  \nkubernetes  is  automatically  exposed  in  the  default  name-\nspace  and  configured  to  point  to  the  API  server.  You  may  remember  seeing  it  every\ntime you listed services with \nkubectl get svc:\n$ kubectl get svc\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   10.0.0.1     <none>        443/TCP   46d\nAnd  you’ll  remember  from  chapter  5  that  environment  variables  are  configured  for\neach service. You can get both the IP address and the port of the API server by looking\nup  the  \nKUBERNETES_SERVICE_HOST  and  KUBERNETES_SERVICE_PORT  variables  (inside\nthe container):\nroot@curl:/# env | grep KUBERNETES_SERVICE\nKUBERNETES_SERVICE_PORT=443\nKUBERNETES_SERVICE_HOST=10.0.0.1\nKUBERNETES_SERVICE_PORT_HTTPS=443\nListing 8.12   A pod for trying out communication with the API server: curl.yaml\nUsing the tutum/curl image, \nbecause you need curl \navailable in the container\nYou’re running the sleep \ncommand with a long delay to \nkeep your container running.\n \n\n240CHAPTER 8Accessing pod metadata and other resources from applications\nYou  may  also  remember  that  each  service  also  gets  a  DNS  entry,  so  you  don’t  even\nneed  to  look  up  the  environment  variables,  but  instead  simply  point  \ncurl  to\nhttps://kubernetes. To be fair, if you don’t know which port the service is available at,\nyou  also  either  need  to  look  up  the  environment  variables  or  perform  a  DNS  SRV\nrecord lookup to get the service’s actual port number. \n The environment variables shown previously say that the API server is listening on\nport  443,  which  is  the  default  port  for  HTTPS,  so  try  hitting  the  server  through\nHTTPS:\nroot@curl:/# curl https://kubernetes\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\n...\nIf you'd like to turn off curl's verification of the certificate, use\n  the -k (or --insecure) option.\nAlthough  the  simplest  way  to  get  around  this  is  to  use  the  proposed  -k  option  (and\nthis is what you’d normally use when playing with the API server manually), let’s look\nat  the  longer  (and  correct)  route.  Instead  of  blindly  trusting  that  the  server  you’re\nconnecting to is the authentic API server, you’ll verify its identity by having \ncurl check\nits certificate. \nTIPNever  skip  checking  the  server’s  certificate  in  an  actual  application.\nDoing so could make your app expose its authentication token to an attacker\nusing a man-in-the-middle attack.\nVERIFYING THE SERVER’S IDENTITY\nIn  the  previous  chapter,  while  discussing  Secrets,  we  looked  at  an  automatically  cre-\nated  Secret  called  \ndefault-token-xyz,  which  is  mounted  into  each  container  at\n/var/run/secrets/kubernetes.io/serviceaccount/. Let’s see the contents of that Secret\nagain, by listing files in that directory:\nroot@curl:/# ls /var/run/secrets/kubernetes.io/serviceaccount/                                                                                                                                                                \nca.crt    namespace    token\nThe  Secret  has  three  entries  (and  therefore  three  files  in  the  Secret  volume).  Right\nnow, we’ll focus on the ca.crt file, which holds the certificate of the certificate author-\nity (CA) used to sign the Kubernetes API server’s certificate. To verify you’re talking to\nthe API server, you need to check if the server’s certificate is signed by the CA. \ncurl\nallows you to specify the CA certificate with the --cacert option, so try hitting the API\nserver again:\nroot@curl:/# curl --cacert /var/run/secrets/kubernetes.io/serviceaccount\n             \n➥ /ca.crt https://kubernetes\nUnauthorized\nNOTEYou may see a longer error description than “Unauthorized.”\n \n\n241Talking to the Kubernetes API server\nOkay, you’ve made progress. curl verified the server’s identity because its certificate\nwas signed by the CA you trust. As the \nUnauthorized response suggests, you still need\nto  take  care  of  authentication.  You’ll  do  that  in  a  moment,  but  first  let’s  see  how  to\nmake life easier by setting the \nCURL_CA_BUNDLE  environment  variable,  so  you  don’t\nneed to specify \n--cacert every time you run curl:\nroot@curl:/# export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/\n             \n➥ serviceaccount/ca.crt\nYou can now hit the API server without using --cacert:\nroot@curl:/# curl https://kubernetes\nUnauthorized\nThis is much nicer now. Your client (curl) trusts the API server now, but the API\nserver  itself  says  you’re  not  authorized  to  access  it,  because  it  doesn’t  know  who\nyou are.\nAUTHENTICATING WITH THE API SERVER\nYou  need  to  authenticate  with  the  server,  so  it  allows  you  to  read  and  even  update\nand/or delete the API objects deployed in the cluster. To authenticate, you need an\nauthentication token. Luckily, the token is provided through the default-token Secret\nmentioned  previously,  and  is  stored  in  the  \ntoken  file  in  the  secret  volume.  As  the\nSecret’s name suggests, that’s the primary purpose of the Secret. \n You’re going to use the token to access the API server. First, load the token into an\nenvironment variable:\nroot@curl:/# TOKEN=$(cat /var/run/secrets/kubernetes.io/\n             \n➥ serviceaccount/token)\nThe token is now stored in the TOKEN environment variable. You can use it when send-\ning requests to the API server, as shown in the following listing.\nroot@curl:/# curl -H \"Authorization: Bearer $TOKEN\" https://kubernetes\n{\n  \"paths\": [\n    \"/api\",\n    \"/api/v1\",\n    \"/apis\",\n    \"/apis/apps\",\n    \"/apis/apps/v1beta1\",\n    \"/apis/authorization.k8s.io\",    \n    ...\n    \"/ui/\",\n    \"/version\"\n  ]\n}\nListing 8.13   Getting a proper response from the API server\n \n\n242CHAPTER 8Accessing pod metadata and other resources from applications\nAs you can see, you passed the token inside the Authorization HTTP header in the\nrequest.  The  API  server  recognized  the  token  as  authentic  and  returned  a  proper\nresponse. You can now explore all the resources in your cluster, the way you did a few\nsections ago. \n For example, you could list all the pods in the same namespace. But first you need\nto know what namespace the \ncurl pod is running in.\nGETTING THE NAMESPACE THE POD IS RUNNING IN\nIn  the  first  part  of  this  chapter,  you  saw  how  to  pass  the  namespace  to  the  pod\nthrough  the  Downward  API.  But  if  you’re  paying  attention,  you  probably  noticed\nyour \nsecret  volume  also  contains  a  file  called  namespace.  It  contains  the  name-\nspace the pod is running in, so you can read the file instead of having to explicitly\npass  the  namespace  to  your  pod  through  an  environment  variable.  Load  the  con-\ntents of the file into the NS environment variable and then list all the pods, as shown\nin the following listing.\nroot@curl:/# NS=$(cat /var/run/secrets/kubernetes.io/\n             \n➥ serviceaccount/namespace)           \nroot@curl:/# curl -H \"Authorization: Bearer $TOKEN\"\n             \n➥ https://kubernetes/api/v1/namespaces/$NS/pods\n{\n  \"kind\": \"PodList\",\n  \"apiVersion\": \"v1\",\n  ...\nAnd there you go. By using the three files in the mounted secret volume directory,\nyou listed all the pods running in the same namespace as your pod. In the same man-\nner, you could also retrieve other API objects and even update them by sending \nPUT or\nPATCH instead of simple GET requests. \nDisabling role-based access control (RBAC)\nIf you’re using a Kubernetes cluster with RBAC enabled, the service account may not\nbe authorized to access (parts of) the API server. You’ll learn about service accounts\nand  RBAC  in  chapter  12.  For  now,  the  simplest  way  to  allow  you  to  query  the  API\nserver is to work around RBAC by running the following command:\n$ kubectl create clusterrolebinding permissive-binding \\\n  --clusterrole=cluster-admin \\\n  --group=system:serviceaccounts\nThis gives all service accounts (we could also say all pods) cluster-admin privileges,\nallowing  them  to  do  whatever  they  want.  Obviously,  doing  this  is  dangerous  and\nshould never be done on production clusters. For test purposes, it’s fine.\nListing 8.14   Listing pods in the pod’s own namespace\n \n\n243Talking to the Kubernetes API server\nRECAPPING HOW PODS TALK TO KUBERNETES\nLet’s recap how an app running inside a pod can access the Kubernetes API properly:\nThe app should verify whether the API server’s certificate is signed by the certif-\nicate authority, whose certificate is in the ca.crt file. \nThe app should authenticate itself by sending the Authorization header with\nthe bearer token from the \ntoken file. \nThe namespace file should be used to pass the namespace to the API server when\nperforming CRUD operations on API objects inside the pod’s namespace.\nDEFINITIONCRUD  stands  for  Create,  Read,  Update,  and  Delete.  The  corre-\nsponding HTTP methods are \nPOST, GET, PATCH/PUT, and DELETE, respectively.\nAll three aspects of pod to API server communication are displayed in figure 8.5.\n8.2.3Simplifying API server communication with ambassador \ncontainers\nDealing  with  HTTPS,  certificates,  and  authentication  tokens  sometimes  seems  too\ncomplicated  to  developers.  I’ve  seen  developers  disable  validation  of  server  certifi-\ncates on way too many occasions (and I’ll admit to doing it myself a few times). Luck-\nily, you can make the communication much simpler while keeping it secure. \nAPI server\nGET /api/v1/namespaces/<namespace>/pods\nAuthorization: Bearer <token>\nPod\nContainer\nFilesystemApp\n/\nvar/\nrun/\nsecrets/\nkubernetes.io/\nserviceaccount/\nDefault token secret volume\nca.crttokennamespace\nServer\ncertificate\nValidate\ncertificate\nFigure 8.5   Using the files from the default-token Secret to talk to the API server\n \n\n244CHAPTER 8Accessing pod metadata and other resources from applications\n Remember the kubectl proxy command we mentioned in section 8.2.1? You ran\nthe command on your local machine to make it easier to access the API server. Instead\nof  sending  requests  to  the  API  server  directly,  you  sent  them  to  the  proxy  and  let  it\ntake care of authentication, encryption, and server verification. The same method can\nbe used inside your pods, as well.\nINTRODUCING THE AMBASSADOR CONTAINER PATTERN\nImagine  having  an  application  that  (among  other  things)  needs  to  query  the  API\nserver. Instead of it talking to the API server directly, as you did in the previous sec-\ntion, you can run \nkubectl proxy in an ambassador container alongside the main con-\ntainer and communicate with the API server through it. \n Instead of talking to the API server directly, the app in the main container can con-\nnect  to  the  ambassador  through  HTTP  (instead  of  HTTPS)  and  let  the  ambassador\nproxy handle the HTTPS connection to the API server, taking care of security trans-\nparently (see figure 8.6). It does this by using the files from the default token’s \nsecret\nvolume.\nBecause all containers in a pod share the same loopback network interface, your app\ncan access the proxy through a port on localhost.\nRUNNING THE CURL POD WITH AN ADDITIONAL AMBASSADOR CONTAINER\nTo  see  the  ambassador  container  pattern  in  action,  you’ll  create  a  new  pod  like  the\ncurl pod you created earlier, but this time, instead of running a single container in\nthe  pod,  you’ll  run  an  additional  ambassador  container  based  on  a  general-purpose\nkubectl-proxy container image I’ve created and pushed to Docker Hub. You’ll find\nthe  Dockerfile  for  the  image  in  the  code  archive (in /Chapter08/kubectl-proxy/) if\nyou want to build it yourself.\n The pod’s manifest is shown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: curl-with-ambassador\nspec:\n  containers:\n  - name: main\nListing 8.15   A pod with an ambassador container: curl-with-ambassador.yaml\nContainer:\nmain\nContainer:\nambassador\nHTTPHTTPS\nAPI server\nPod\nFigure 8.6   Using an ambassador to connect to the API server\n \n\n245Talking to the Kubernetes API server\n    image: tutum/curl\n    command: [\"sleep\", \"9999999\"]\n  - name: ambassador                         \n    image: luksa/kubectl-proxy:1.6.2         \nThe pod spec is almost the same as before, but with a different pod name and an addi-\ntional container. Run the pod and then enter the main container with\n$ kubectl exec -it curl-with-ambassador -c main bash\nroot@curl-with-ambassador:/#\nYour  pod  now  has  two  containers,  and  you  want  to  run  bash  in  the  main  container,\nhence  the  \n-c main  option.  You  don’t  need  to  specify  the  container  explicitly  if  you\nwant to run the command in the pod’s first container. But if you want to run a com-\nmand inside any other container, you do need to specify the container’s name using\nthe \n-c option.\nTALKING TO THE API SERVER THROUGH THE AMBASSADOR\nNext  you’ll  try  connecting  to  the  API  server  through  the  ambassador  container.  By\ndefault, \nkubectl proxy  binds  to  port  8001,  and  because  both  containers  in  the  pod\nshare the same network interfaces, including loopback, you can point \ncurl to local-\nhost:8001\n, as shown in the following listing.\nroot@curl-with-ambassador:/# curl localhost:8001\n{\n  \"paths\": [\n    \"/api\",\n    ...\n  ]\n}\nSuccess! The output printed by curl is the same response you saw earlier, but this time\nyou didn’t need to deal with authentication tokens and server certificates. \n To get a clear picture of what exactly happened, refer to figure 8.7. \ncurl sent the\nplain HTTP request (without any authentication headers) to the proxy running inside\nthe  ambassador  container,  and  then  the  proxy  sent  an  HTTPS  request  to  the  API\nserver,  handling  the  client  authentication  by  sending  the  token  and  checking  the\nserver’s identity by validating its certificate.\n This is a great example of how an ambassador container can be used to hide the\ncomplexities  of  connecting  to  an  external  service  and  simplify  the  app  running  in\nthe main container. The ambassador container is reusable across many different apps,\nregardless of what language the main app is written in. The downside is that an addi-\ntional process is running and consuming additional resources.\nListing 8.16   Accessing the API server through the ambassador container\nThe ambassador container, \nrunning the kubectl-proxy image\n \n\n246CHAPTER 8Accessing pod metadata and other resources from applications\n8.2.4Using client libraries to talk to the API server\nIf your app only needs to perform a few simple operations on the API server, you can\noften use a regular HTTP client library and perform simple HTTP requests, especially\nif you take advantage of the \nkubectl-proxy ambassador container the way you did in\nthe  previous  example.  But  if  you  plan  on  doing  more  than  simple  API  requests,  it’s\nbetter to use one of the existing Kubernetes API client libraries.\nUSING EXISTING CLIENT LIBRARIES\nCurrently,  two  Kubernetes  API  client  libraries  exist  that  are  supported  by  the  API\nMachinery special interest group (SIG):\nGolang client—https://github.com/kubernetes/client-go\nPython—https://github.com/kubernetes-incubator/client-python\nNOTEThe  Kubernetes  community  has  a  number  of  Special  Interest  Groups\n(SIGs)  and  Working  Groups  that  focus  on  specific  parts  of  the  Kubernetes\necosystem. You’ll find a list of them at https://github.com/kubernetes/com-\nmunity/blob/master/sig-list.md.\nIn addition to the two officially supported libraries, here’s a list of user-contributed cli-\nent libraries for many other languages:\nJava client by Fabric8—https://github.com/fabric8io/kubernetes-client\nJava client by Amdatu—https://bitbucket.org/amdatulabs/amdatu-kubernetes\nNode.js client by tenxcloud—https://github.com/tenxcloud/node-kubernetes-client\nNode.js client by GoDaddy—https://github.com/godaddy/kubernetes-client\nPHP—https://github.com/devstub/kubernetes-api-php-client\nAnother PHP client—https://github.com/maclof/kubernetes-client\nContainer: main\nAPI server\nsleepcurl\nContainer: ambassador\nkubectl proxy\nPort 8001\nGET http://localhost:8001\nGET https://kubernetes:443\nAuthorization: Bearer <token>\nPod\nFigure 8.7   Offloading encryption, authentication, and server verification to kubectl proxy in an \nambassador container \n \n\n247Talking to the Kubernetes API server\nRuby—https://github.com/Ch00k/kubr\nAnother Ruby client—https://github.com/abonas/kubeclient\nClojure—https://github.com/yanatan16/clj-kubernetes-api\nScala—https://github.com/doriordan/skuber\nPerl—https://metacpan.org/pod/Net::Kubernetes\nThese libraries usually support HTTPS and take care of authentication, so you won’t\nneed to use the ambassador container. \nAN EXAMPLE OF INTERACTING WITH KUBERNETES WITH THE FABRIC8 JAVA CLIENT\nTo give you a sense of how client libraries enable you to talk to the API server, the fol-\nlowing listing shows an example of how to list services in a Java app using the Fabric8\nKubernetes client.\nimport java.util.Arrays;\nimport io.fabric8.kubernetes.api.model.Pod;\nimport io.fabric8.kubernetes.api.model.PodList;\nimport io.fabric8.kubernetes.client.DefaultKubernetesClient;\nimport io.fabric8.kubernetes.client.KubernetesClient;\npublic class Test {\n  public static void main(String[] args) throws Exception {\n    KubernetesClient client = new DefaultKubernetesClient();\n    // list pods in the default namespace\n    PodList pods = client.pods().inNamespace(\"default\").list();\n    pods.getItems().stream()\n      .forEach(s -> System.out.println(\"Found pod: \" +\n               s.getMetadata().getName()));\n    // create a pod\n    System.out.println(\"Creating a pod\");\n    Pod pod = client.pods().inNamespace(\"default\")\n      .createNew()\n      .withNewMetadata()\n        .withName(\"programmatically-created-pod\")\n      .endMetadata()\n      .withNewSpec()\n        .addNewContainer()\n          .withName(\"main\")\n          .withImage(\"busybox\")\n          .withCommand(Arrays.asList(\"sleep\", \"99999\"))\n        .endContainer()\n      .endSpec()\n      .done();\n    System.out.println(\"Created pod: \" + pod);\n    // edit the pod (add a label to it)\n    client.pods().inNamespace(\"default\")\n      .withName(\"programmatically-created-pod\")\n      .edit()\n      .editMetadata()\nListing 8.17   Listing, creating, updating, and deleting pods with the Fabric8 Java client\n \n\n248CHAPTER 8Accessing pod metadata and other resources from applications\n        .addToLabels(\"foo\", \"bar\")\n      .endMetadata()\n      .done();\n    System.out.println(\"Added label foo=bar to pod\");\n    System.out.println(\"Waiting 1 minute before deleting pod...\");\n    Thread.sleep(60000);\n    // delete the pod\n    client.pods().inNamespace(\"default\")\n      .withName(\"programmatically-created-pod\")\n      .delete();\n    System.out.println(\"Deleted the pod\");\n  }\n}\nThe code should be self-explanatory, especially because the Fabric8 client exposes\na  nice,  fluent  Domain-Specific-Language  (DSL)  API,  which  is  easy  to  read  and\nunderstand.\nBUILDING YOUR OWN LIBRARY WITH SWAGGER AND OPENAPI\nIf  no  client  is  available  for  your  programming  language  of  choice,  you  can  use  the\nSwagger API framework to generate the client library and documentation. The Kuber-\nnetes API server exposes Swagger API definitions at /swaggerapi and OpenAPI spec at\n/swagger.json. \n To find out more about the Swagger framework, visit the website at http://swagger.io.\nEXPLORING THE API WITH SWAGGER UI\nEarlier in the chapter I said I’d point you to a better way of exploring the REST API\ninstead of hitting the REST endpoints with \ncurl. Swagger, which I mentioned in the\nprevious section, is not just a tool for specifying an API, but also provides a web UI for\nexploring  REST  APIs  if  they  expose  the  Swagger  API  definitions.  The  better  way  of\nexploring REST APIs is through this UI.\n  Kubernetes  not  only  exposes  the  Swagger  API,  but  it  also  has  Swagger  UI  inte-\ngrated  into  the  API  server,  though  it’s  not  enabled  by  default.  You  can  enable  it  by\nrunning the API server with the \n--enable-swagger-ui=true option.\nTIPIf you’re using Minikube, you can enable Swagger UI when starting the\ncluster: \nminikube start --extra-config=apiserver.Features.Enable-\nSwaggerUI=true\nAfter you enable the UI, you can open it in your browser by pointing it to:\nhttp(s)://<api server>:<port>/swagger-ui\nI urge you to give Swagger UI a try. It not only allows you to browse the Kubernetes\nAPI, but also interact with it (you can \nPOST JSON resource manifests, PATCH resources,\nor \nDELETE them, for example). \n \n\n249Summary\n8.3Summary\nAfter reading this chapter, you now know how your app, running inside a pod, can get\ndata about itself, other pods, and other components deployed in the cluster. You’ve\nlearned\nHow a pod’s name, namespace, and other metadata can be exposed to the pro-\ncess either through environment variables or files in a \ndownwardAPI volume\nHow CPU and memory requests and limits are passed to your app in any unit\nthe app requires\nHow  a  pod can  use  downwardAPI  volumes  to  get  up-to-date  metadata,  which\nmay change during the lifetime of the pod\n (such as labels and annotations) \nHow you can browse the Kubernetes REST API through kubectl proxy\nHow pods can find the API server’s location through environment variables or\nDNS, similar to any other Service defined in Kubernetes\nHow  an  application  running  in  a  pod can  verify  that  it’s  talking  to  the  API\nserver and how it can authenticate itself\nHow using an ambassador container can make talking to the API server from\nwithin an app much simpler\nHow client libraries can get you interacting with Kubernetes in minutes\nIn this chapter, you learned how to talk to the API server, so the next step is learning\nmore about how it works. You’ll do that in chapter 11, but before we dive into such\ndetails,  you  still  need  to  learn  about  two  other  Kubernetes  resources—Deployments\nand StatefulSets. They’re explained in the next two chapters.\n \n\n250\nDeployments: updating\napplications declaratively\nYou now know how to package your app components into containers, group them\ninto  pods,  provide  them  with  temporary  or  permanent  storage,  pass  both  secret\nand non-secret config data to them, and allow pods to find and talk to each other.\nYou  know  how  to  run  a  full-fledged  system  composed  of  independently  running\nsmaller components—microservices, if you will. Is there anything else? \n Eventually, you’re going to want to update your app. This chapter covers how to\nupdate apps running in a Kubernetes cluster and how Kubernetes helps you move\ntoward a true zero-downtime update process. Although this can be achieved using\nonly ReplicationControllers or ReplicaSets, Kubernetes also provides a Deployment\nThis chapter covers\nReplacing pods with newer versions\nUpdating managed pods\nUpdating pods declaratively using Deployment \nresources\nPerforming rolling updates\nAutomatically blocking rollouts of bad versions\nControlling the rate of the rollout\nReverting pods to a previous version\n \n\n251Updating applications running in pods\nresource that sits on top of ReplicaSets and enables declarative application updates. If\nyou’re not completely sure what that means, keep reading—it’s not as complicated as\nit sounds.\n9.1Updating applications running in pods\nLet’s start off with a simple example. Imagine having a set of pod instances providing a\nservice to other pods and/or external clients. After reading this book up to this point,\nyou  likely  recognize  that  these  pods  are  backed  by  a  ReplicationController  or  a\nReplicaSet. A Service also exists through which clients (apps running in other pods or\nexternal clients) access the pods. This is how a basic application looks in Kubernetes\n(shown in figure 9.1).\nInitially, the pods run the first version of your application—let’s suppose its image is\ntagged  as  \nv1.  You  then  develop  a  newer  version  of  the  app  and  push  it  to  an  image\nrepository as a new image, tagged as \nv2. You’d next like to replace all the pods with\nthis  new  version.  Because  you  can’t  change  an  existing  pod’s  image  after  the  pod  is\ncreated, you need to remove the old pods and replace them with new ones running\nthe new image. \n You have two ways of updating all those pods. You can do one of the following:\nDelete all existing pods first and then start the new ones.\nStart new ones and, once they’re up, delete the old ones. You can do this either\nby  adding  all  the  new  pods  and  then  deleting  all  the  old  ones  at  once,  or\nsequentially, by adding new pods and removing old ones gradually.\nBoth these strategies have their benefits and drawbacks. The first option would lead to\na  short  period  of  time  when  your  application  is  unavailable.  The  second  option\nrequires your app to handle running two versions of the app at the same time. If your\napp stores data in a data store, the new version shouldn’t modify the data schema or\nthe data in such a way that breaks the previous version.\nReplicationController\nor ReplicaSet\nClientsService\nPodPodPod\nFigure 9.1   The basic outline of an \napplication running in Kubernetes\n \n\n252CHAPTER 9Deployments: updating applications declaratively\n How do you perform these two update methods in Kubernetes? First, let’s look at\nhow  to  do  this  manually;  then,  once  you  know  what’s  involved  in  the  process,  you’ll\nlearn how to have Kubernetes perform the update automatically.\n9.1.1Deleting old pods and replacing them with new ones\nYou already know how to get a ReplicationController to replace all its pod instances\nwith  pods  running  a  new  version.  You  probably  remember  the  pod  template  of  a\nReplicationController  can  be  updated  at  any  time.  When  the  ReplicationController\ncreates new instances, it uses the updated pod template to create them.\n  If  you  have  a  ReplicationController  managing  a  set  of  \nv1  pods,  you  can  easily\nreplace them by modifying the pod template so it refers to version \nv2 of the image and\nthen  deleting  the  old  pod  instances.  The  ReplicationController  will  notice  that  no\npods  match  its  label  selector  and  it  will  spin  up  new  instances.  The  whole  process  is\nshown in figure 9.2.\nThis is the simplest way to update a set of pods, if you can accept the short downtime\nbetween the time the old pods are deleted and new ones are started.\n9.1.2Spinning up new pods and then deleting the old ones\nIf you don’t want to see any downtime and your app supports running multiple ver-\nsions at once, you can turn the process around and first spin up all the new pods and\nPod template\nchanged\nv  pods deleted1\nmanually\nReplicationController\nService\nPod: v1Pod: v1\nPod\ntemplate: v2\nReplicationController\nPod\ntemplate: v2\nPod: v1\nService\nPod: v2Pod: v2Pod: v2\nReplicationController\nService\nPod: v1Pod: v1\nPod\ntemplate: v1\nPod: v1\nReplicationController\nService\nPod: v1Pod: v1Pod: v1\nPod\ntemplate: v2\nShort period of\ndowntime here\nv2 pods created by\nReplicationController\nFigure 9.2   Updating pods by changing a ReplicationController’s pod template and deleting old Pods\n \n\n253Updating applications running in pods\nonly  then  delete  the  old  ones.  This  will  require  more  hardware  resources,  because\nyou’ll have double the number of pods running at the same time for a short while. \n  This  is  a  slightly  more  complex  method  compared  to  the  previous  one,  but  you\nshould be able to do it by combining what  you’ve  learned  about  ReplicationControl-\nlers and Services so far.\nSWITCHING FROM THE OLD TO THE NEW VERSION AT ONCE\nPods are usually fronted by a Service. It’s possible to have the Service front only the\ninitial version of the pods while you bring up the pods running the new version. Then,\nonce all the new pods are up, you can change the Service’s label selector and have the\nService switch over to the new pods, as shown in figure 9.3. This is called a blue-green\ndeployment. After switching over, and once you’re sure the new version functions cor-\nrectly, you’re free to delete the old pods by deleting the old ReplicationController.\nNOTEYou can change a Service’s pod selector with the kubectl set selec-\ntor\n command.\nPERFORMING A ROLLING UPDATE\nInstead  of  bringing  up  all  the  new  pods  and  deleting  the  old  pods  at  once,  you  can\nalso perform a rolling update, which replaces pods step by step. You do this by slowly\nscaling down the previous ReplicationController and scaling up the new one. In this\ncase, you’ll want the Service’s pod selector to include both the old and the new pods,\nso it directs requests toward both sets of pods. See figure 9.4.\n Doing a rolling update manually is laborious and error-prone. Depending on the\nnumber  of  replicas,  you’d  need  to  run  a  dozen  or  more  commands  in  the  proper\norder to perform the update process. Luckily, Kubernetes allows you to perform the\nrolling update with a single command. You’ll learn how in the next section.\nServiceService\nReplicationController:\nv1\nPod: v1Pod: v1\nPod\ntemplate: v1\nPod: v1\nReplicationController:\nv2\nPod\ntemplate: v2\nPod: v2Pod: v2Pod: v2\nReplicationController:\nv1\nPod: v1Pod: v1\nPod\ntemplate: v1\nPod: v1\nReplicationController:\nv2\nPod\ntemplate: v2\nPod: v2Pod: v2Pod: v2\nFigure 9.3   Switching a Service from the old pods to the new ones\n \n\n254CHAPTER 9Deployments: updating applications declaratively\n9.2Performing an automatic rolling update with a \nReplicationController\nInstead of performing rolling updates using ReplicationControllers manually, you can\nhave \nkubectl perform them. Using kubectl to perform the update makes the process\nmuch easier, but, as you’ll see later, this is now an outdated way of updating apps. Nev-\nertheless, we’ll walk through this option first, because it was historically the first way of\ndoing an automatic rolling update, and also allows us to discuss the process without\nintroducing too many additional concepts. \n9.2.1Running the initial version of the app\nObviously, before you can update an app, you need to have an app deployed. You’re\ngoing to use a slightly modified version of the kubia NodeJS app you created in chap-\nter 2 as your initial version. In case you don’t remember what it does, it’s a simple web-\napp that returns the pod’s hostname in the HTTP response. \nCREATING THE V1 APP\nYou’ll change the app so it also returns its version number in the response, which will\nallow  you  to  distinguish  between  the  different  versions  you’re  about  to  build.  I’ve\nalready built and pushed the app image to Docker Hub under \nluksa/kubia:v1. The\nfollowing listing shows the app’s code.\nconst http = require('http');\nconst os = require('os');\nconsole.log(\"Kubia server starting...\");\nListing 9.1   The v1 version of our app: v1/app.js\nService\nPod: v1Pod: v1\nReplication\nController:\nv1\nv1\nReplication\nController:\nv2\nPod: v2\nService\nPod: v2Pod: v2Pod: v2\nService\nPod: v1Pod: v1Pod: v1\nService\nPod: v1Pod: v2Pod: v2\nv2\nReplication\nController:\nv1\nv1\nReplication\nController:\nv2\nv2\nReplication\nController:\nv1\nReplication\nController:\nv2\nv2\nReplication\nController:\nv1\nv1\nv1\nReplication\nController:\nv2\nv2\nFigure 9.4   A rolling update of pods using two ReplicationControllers\n \n\n255Performing an automatic rolling update with a ReplicationController\nvar handler = function(request, response) {\n  console.log(\"Received request from \" + request.connection.remoteAddress);\n  response.writeHead(200);\n  response.end(\"This is v1 running in pod \" + os.hostname() + \"\\n\");\n};\nvar www = http.createServer(handler);\nwww.listen(8080);\nRUNNING THE APP AND EXPOSING IT THROUGH A SERVICE USING A SINGLE YAML FILE\nTo run your app, you’ll create a ReplicationController and a LoadBalancer Service to\nenable  you  to  access  the  app  externally.  This  time,  rather  than  create  these  two\nresources separately, you’ll create a single YAML for both of them and post it to the\nKubernetes API with a single \nkubectl create command. A YAML manifest can con-\ntain  multiple  objects  delimited  with  a  line  containing  three  dashes,  as  shown  in  the\nfollowing listing.\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: kubia-v1\nspec:\n  replicas: 3\n  template:\n    metadata:\n      name: kubia\n      labels:                      \n        app: kubia                 \n    spec:\n      containers:\n      - image: luksa/kubia:v1     \n        name: nodejs\n---                         \napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia\nspec:\n  type: LoadBalancer\n  selector:                                        \n    app: kubia                                     \n  ports:\n  - port: 80\n    targetPort: 8080\nThe  YAML  defines  a  ReplicationController  called  kubia-v1  and  a  Service  called\nkubia. Go ahead and post the YAML to Kubernetes. After a while, your three v1 pods\nand the load balancer should all be running, so you can look up the Service’s external\nIP and start hitting the service with \ncurl, as shown in the following listing.\nListing 9.2   A YAML containing an RC and a Service: kubia-rc-and-service-v1.yaml\nThe Service fronts all \npods created by the \nReplicationController.\nYou’re creating a \nReplicationController for \npods running this image.\nYAML files can contain \nmultiple resource \ndefinitions separated by \na line with three dashes.\n \n\n256CHAPTER 9Deployments: updating applications declaratively\n$ kubectl get svc kubia\nNAME      CLUSTER-IP     EXTERNAL-IP       PORT(S)         AGE\nkubia     10.3.246.195   130.211.109.222   80:32143/TCP    5m\n$ while true; do curl http://130.211.109.222; done\nThis is v1 running in pod kubia-v1-qr192\nThis is v1 running in pod kubia-v1-kbtsk\nThis is v1 running in pod kubia-v1-qr192\nThis is v1 running in pod kubia-v1-2321o\n...\nNOTEIf you’re using Minikube or any other Kubernetes cluster where load\nbalancer  services  aren’t  supported,  you  can  use  the  Service’s  node  port  to\naccess the app. This was explained in chapter 5.\n9.2.2Performing a rolling update with kubectl\nNext you’ll create version 2 of the app. To keep things simple, all you’ll do is change\nthe response to say, “This is v2”:\n  response.end(\"This is v2 running in pod \" + os.hostname() + \"\\n\");\nThis  new  version  is  available  in  the  image  luksa/kubia:v2  on  Docker  Hub,  so  you\ndon’t need to build it yourself.\nListing 9.3   Getting the Service’s external IP and hitting the service in a loop with curl\nPushing updates to the same image tag\nModifying an app and pushing the changes to the same image tag isn’t a good idea,\nbut we all tend to do that during development. If you’re modifying the \nlatest tag,\nthat’s not a problem, but when you’re tagging an image with a different tag (for exam-\nple, tag \nv1 instead of latest), once the image is pulled by a worker node, the image\nwill  be  stored  on  the  node  and  not  pulled  again  when  a  new  pod  using  the  same\nimage is run (at least that’s the default policy for pulling images).\nThat means any changes you make to the image won’t be picked up if you push them\nto the same tag. If a new pod is scheduled to the same node, the Kubelet will run the\nold version of the image. On the other hand, nodes that haven’t run the old version\nwill pull and run the new image, so you might end up with two different versions of\nthe pod running. To make sure this doesn’t happen, you need to set the container’s\nimagePullPolicy property to Always. \nYou need to be aware that the default \nimagePullPolicy depends on the image tag.\nIf a container refers to the \nlatest tag (either explicitly or by not specifying the tag at\nall), \nimagePullPolicy defaults to Always, but if the container refers to any other\ntag, the policy defaults to \nIfNotPresent. \nWhen using a tag other than \nlatest, you need to set the imagePullPolicy properly\nif you make changes to an image without changing the tag. Or better yet, make sure\nyou always push changes to an image under a new tag.\n \n\n257Performing an automatic rolling update with a ReplicationController\nKeep the curl loop running and open another terminal, where you’ll get the rolling\nupdate started. To perform the update, you’ll run the \nkubectl rolling-update com-\nmand. All you need to do is tell it which ReplicationController you’re replacing, give a\nname  for  the  new  ReplicationController,  and  specify  the  new  image  you’d  like  to\nreplace the original one with. The following listing shows the full command for per-\nforming the rolling update.\n$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2\nCreated kubia-v2\nScaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep 3 \npods available, don't exceed 4 pods)\n...\nBecause you’re replacing ReplicationController kubia-v1 with one running version 2\nof  your  kubia  app,  you’d  like  the  new  ReplicationController  to  be  called  \nkubia-v2\nand use the luksa/kubia:v2 container image. \n When you run the command, a new ReplicationController called \nkubia-v2 is cre-\nated immediately. The state of the system at this point is shown in figure 9.5.\nThe new ReplicationController’s pod template references the \nluksa/kubia:v2 image\nand its initial desired replica count is set to 0\n, as you can see in the following listing.\n$ kubectl describe rc kubia-v2\nName:       kubia-v2\nNamespace:  default\nImage(s):   luksa/kubia:v2          \nSelector:   app=kubia,deployment=757d16a0f02f6a5c387f2b5edb62b155\nLabels:     app=kubia            \nReplicas:   0 current / 0 desired    \n...\nListing 9.4   Initiating a rolling-update of a ReplicationController using kubectl\nListing 9.5   Describing the new ReplicationController created by the rolling update\nPod: v1Pod: v1No v2 pods yetPod: v1\nReplicationController: kubia-v1\nImage:kubia/v1\nReplicas: 3\nReplicationController: kubia-v2\nImage:kubia/v2\nReplicas: 0\nFigure 9.5   The state of the system immediately after starting the rolling update\nThe new \nReplicationController \nrefers to the v2 image.\nInitially, the desired \nnumber of replicas is zero.\n \n\n258CHAPTER 9Deployments: updating applications declaratively\nUNDERSTANDING THE STEPS PERFORMED BY KUBECTL BEFORE THE ROLLING UPDATE COMMENCES\nkubectl  created  this  ReplicationController  by  copying  the  kubia-v1  controller  and\nchanging  the  image  in  its  pod  template.  If  you  look  closely  at  the  controller’s  label\nselector,  you’ll  notice  it  has  been  modified,  too.  It  includes  not  only  a  simple\napp=kubia label, but also an additional deployment label which the pods must have in\norder to be managed by this ReplicationController.\n You probably know this already, but this is necessary to avoid having both the new\nand the old ReplicationControllers operating on the same set of pods. But even if pods\ncreated by the new controller have the additional \ndeployment label in addition to the\napp=kubia label, doesn’t this mean they’ll be selected by the first ReplicationControl-\nler’s selector, because it’s set to \napp=kubia? \n Yes, that’s exactly what would happen, but there’s a catch. The rolling-update pro-\ncess has modified the selector of the first ReplicationController, as well:\n$ kubectl describe rc kubia-v1\nName:       kubia-v1\nNamespace:  default\nImage(s):   luksa/kubia:v1\nSelector:   app=kubia,deployment=3ddd307978b502a5b975ed4045ae4964-orig \nOkay, but doesn’t this mean the first controller now sees zero pods matching its selec-\ntor, because the three pods previously created by it contain only the \napp=kubia label?\nNo, because \nkubectl had also modified the labels of the live pods just before modify-\ning the ReplicationController’s selector:\n$ kubectl get po --show-labels\nNAME            READY  STATUS   RESTARTS  AGE  LABELS\nkubia-v1-m33mv  1/1    Running  0         2m   app=kubia,deployment=3ddd...\nkubia-v1-nmzw9  1/1    Running  0         2m   app=kubia,deployment=3ddd...\nkubia-v1-cdtey  1/1    Running  0         2m   app=kubia,deployment=3ddd...\nIf  this  is  getting  too  complicated,  examine  figure  9.6,  which  shows  the  pods,  their\nlabels, and the two ReplicationControllers, along with their pod selectors.\nReplicationController: kubia-v1\nReplicas: 3\nSelector:app=kubia,\ndeployment=3ddd...\nReplicationController: kubia-v2\nReplicas: 0\nSelector:app=kubia,\ndeployment=757d...\ndeployment: 3ddd...\napp: kubia\nPod: v1\ndeployment: 3ddd...\napp: kubia\nPod: v1\ndeployment: 3ddd...\napp: kubia\nPod: v1\nFigure 9.6   Detailed state of the old and new ReplicationControllers and pods at the start of a rolling \nupdate\n \n\n259Performing an automatic rolling update with a ReplicationController\nkubectl  had  to  do  all  this  before  even  starting  to  scale  anything  up  or  down.  Now\nimagine doing the rolling update manually. It’s easy to see yourself making a mistake\nhere  and  possibly  having  the  ReplicationController  kill  off  all  your  pods—pods  that\nare actively serving your production clients!\nREPLACING OLD PODS WITH NEW ONES BY SCALING THE TWO REPLICATIONCONTROLLERS\nAfter  setting  up  all  this,  kubectl  starts  replacing  pods  by  first  scaling  up  the  new\ncontroller  to  1.  The  controller  thus  creates  the  first  \nv2  pod.  kubectl  then  scales\ndown the old ReplicationController by 1. This is shown in the next two lines printed\nby \nkubectl:\nScaling kubia-v2 up to 1\nScaling kubia-v1 down to 2\nBecause the Service is targeting all pods with the app=kubia label, you should start see-\ning your \ncurl requests redirected to the new v2 pod every few loop iterations:\nThis is v2 running in pod kubia-v2-nmzw9      \nThis is v1 running in pod kubia-v1-kbtsk\nThis is v1 running in pod kubia-v1-2321o\nThis is v2 running in pod kubia-v2-nmzw9      \n...\nFigure 9.7 shows the current state of the system.\nAs \nkubectl continues with the rolling update, you start seeing a progressively bigger\npercentage of requests hitting \nv2 pods, as the update process deletes more of the v1\npods and replaces them with those running your new image. Eventually, the original\nRequests hitting the pod \nrunning the new version\nReplicationController: kubia-v1\nReplicas: 2\nSelector:app=kubia,\ndeployment=3ddd...\nReplicationController: kubia-v2\nReplicas: 1\nSelector:app=kubia,\ndeployment=757d...\ndeployment: 3ddd...\napp: kubia\nPod: v1\ndeployment: 3ddd...\napp: kubia\nPod: v1\ndeployment: 757d...\napp: kubia\nPod: v2\ncurl\nService\nSelector:app=kubia\nFigure 9.7   The Service is redirecting requests to both the old and new pods during the \nrolling update.\n \n\n260CHAPTER 9Deployments: updating applications declaratively\nReplicationController  is  scaled  to  zero,  causing  the  last  v1  pod  to  be  deleted,  which\nmeans  the  Service  will  now  be  backed  by  \nv2  pods  only.  At  that  point,  kubectl  will\ndelete the original ReplicationController and the update process will be finished, as\nshown in the following listing.\n...\nScaling kubia-v2 up to 2\nScaling kubia-v1 down to 1\nScaling kubia-v2 up to 3\nScaling kubia-v1 down to 0\nUpdate succeeded. Deleting kubia-v1\nreplicationcontroller \"kubia-v1\" rolling updated to \"kubia-v2\"\nYou’re now left with only the kubia-v2 ReplicationController and three v2 pods. All\nthroughout  this  update  process,  you’ve  hit  your  service  and  gotten  a  response  every\ntime. You have, in fact, performed a rolling update with zero downtime. \n9.2.3Understanding why kubectl rolling-update is now obsolete\nAt  the  beginning  of  this  section,  I  mentioned  an  even  better  way  of  doing  updates\nthan through \nkubectl rolling-update. What’s so wrong with this process that a bet-\nter one had to be introduced? \n  Well,  for  starters,  I,  for  one,  don’t  like  Kubernetes  modifying  objects  I’ve  created.\nOkay,  it’s  perfectly  fine  for  the  scheduler  to  assign  a  node  to  my  pods  after  I  create\nthem,  but  Kubernetes  modifying  the  labels  of  my  pods  and  the  label  selectors  of  my\nReplicationController\ns  is  something  that  I  don’t  expect  and  could  cause  me  to  go\naround the office yelling at my colleagues, “Who’s been messing with my controllers!?!?” \n But even more importantly, if you’ve paid close attention to the words I’ve used,\nyou probably noticed that all this time I said explicitly that the \nkubectl client was the\none performing all these steps of the rolling update. \n You can see this by turning on verbose logging with the \n--v option when triggering\nthe rolling update:\n$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6\nTIPUsing the --v 6 option increases the logging level enough to let you see\nthe requests \nkubectl is sending to the API server.\nUsing this option, \nkubectl will print out each HTTP request it sends to the Kuberne-\ntes API server. You’ll see PUT requests to\n/api/v1/namespaces/default/replicationcontrollers/kubia-v1\nwhich is the RESTful URL representing your kubia-v1 ReplicationController resource.\nThese  requests  are  the  ones  scaling  down  your  ReplicationController,  which  shows\nListing 9.6   The final steps performed by kubectl rolling-update\n \n\n261Using Deployments for updating apps declaratively\nthat the kubectl client is the one doing the scaling, instead of it being performed by\nthe Kubernetes master. \nTIPUse the verbose logging option when running other kubectl commands,\nto learn more about the communication between \nkubectl and the API server. \nBut why is it such a bad thing that the update process is being performed by the client\ninstead of on the server? Well, in your case, the update went smoothly, but what if you\nlost network connectivity while \nkubectl was performing the update? The update pro-\ncess would be interrupted mid-way. Pods and ReplicationControllers would end up in\nan intermediate state.\n Another reason why performing an update like this isn’t as good as it could be is\nbecause it’s imperative. Throughout this book, I’ve stressed how Kubernetes is about\nyou  telling  it  the  desired  state  of  the  system  and  having  Kubernetes  achieve  that\nstate on its own, by figuring out the best way to do it. This is how pods are deployed\nand  how  pods  are  scaled  up  and  down.  You  never  tell  Kubernetes  to  add  an  addi-\ntional  pod  or  remove  an  excess  one—you  change  the  number  of  desired  replicas\nand that’s it.\n  Similarly,  you  will  also  want  to  change  the  desired  image  tag  in  your  pod  defini-\ntions  and  have  Kubernetes  replace  the  pods  with  new  ones  running  the  new  image.\nThis  is  exactly  what  drove  the  introduction  of  a  new  resource  called  a  Deployment,\nwhich is now the preferred way of deploying applications in Kubernetes. \n9.3Using Deployments for updating apps declaratively\nA  Deployment  is  a  higher-level  resource  meant  for  deploying  applications  and\nupdating them declaratively, instead of doing it through a ReplicationController or\na ReplicaSet, which are both considered lower-level concepts.\n  When  you  create  a  Deployment,  a  ReplicaSet  resource  is  created  underneath\n(eventually more of them). As you may remember from chapter 4, ReplicaSets are a\nnew generation of ReplicationControllers, and should be used instead of them. Replica-\nSets  replicate  and  manage  pods,  as  well.  When  using  a  Deployment,  the  actual  pods\nare  created  and  managed  by  the  Deployment’s  ReplicaSets,  not  by  the  Deployment\ndirectly (the relationship is shown in figure 9.8).\nYou might wonder why you’d want to complicate things by introducing another object\non top of a ReplicationController or ReplicaSet, when they’re what suffices to keep a set\nof  pod  instances  running.  As  the  rolling  update  example  in  section  9.2  demonstrates,\nwhen updating the app, you need to introduce an additional ReplicationController and\nPodsReplicaSetDeployment\nFigure 9.8   A Deployment is backed \nby a ReplicaSet, which supervises the \ndeployment’s pods.\n \n\n262CHAPTER 9Deployments: updating applications declaratively\ncoordinate the two controllers to dance around each other without stepping on each\nother’s  toes.  You  need  something  coordinating  this  dance.  A  Deployment  resource\ntakes care of that (it’s not the Deployment resource itself, but the controller process\nrunning in the Kubernetes control plane that does that; but we’ll get to that in chap-\nter 11).\n  Using  a  Deployment  instead  of  the  lower-level  constructs  makes  updating  an  app\nmuch easier, because you’re defining the desired state through the single Deployment\nresource and letting Kubernetes take care of the rest, as you’ll see in the next few pages.\n9.3.1Creating a Deployment\nCreating  a  Deployment  isn’t  that  different  from  creating  a  ReplicationController.  A\nDeployment is also composed of a label selector, a desired replica count, and a pod\ntemplate.  In  addition  to  that,  it  also  contains  a  field,  which  specifies  a  deployment\nstrategy  that  defines  how  an  update  should  be  performed  when  the  Deployment\nresource is modified.  \nCREATING A DEPLOYMENT MANIFEST\nLet’s see how to use the kubia-v1 ReplicationController example from earlier in this\nchapter and modify it so it describes a Deployment instead of a ReplicationController.\nAs you’ll see, this requires only three trivial changes. The following listing shows the\nmodified YAML.\napiVersion: apps/v1beta1          \nkind: Deployment                  \nmetadata:\n  name: kubia          \nspec:\n  replicas: 3\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - image: luksa/kubia:v1\n        name: nodejs\nNOTEYou’ll find an older version of the Deployment resource in extensions/\nv1beta1\n, and a newer one in apps/v1beta2 with different required fields and\ndifferent defaults. Be aware that \nkubectl explain shows the older version.\nBecause the ReplicationController from before was managing a specific version of the\npods, you called it \nkubia-v1. A Deployment, on the other hand, is above that version\nstuff.  At  a  given  point  in  time,  the  Deployment  can  have  multiple  pod  versions  run-\nning under its wing, so its name shouldn’t reference the app version.\nListing 9.7   A Deployment definition: kubia-deployment-v1.yaml\nDeployments are in the apps \nAPI group, version v1beta1.\nYou’ve changed the kind \nfrom ReplicationController \nto Deployment.\nThere’s no need to include \nthe version in the name of \nthe Deployment.\n \n\n263Using Deployments for updating apps declaratively\nCREATING THE DEPLOYMENT RESOURCE\nBefore you create this Deployment, make sure you delete any ReplicationControllers\nand pods that are still running, but keep the \nkubia Service for now. You can use the\n--all switch to delete all those ReplicationControllers like this:\n$ kubectl delete rc --all\nYou’re now ready to create the Deployment: \n$ kubectl create -f kubia-deployment-v1.yaml --record\ndeployment \"kubia\" created\nTIPBe sure to include the --record command-line option when creating it.\nThis records the command in the revision history, which will be useful later.\nDISPLAYING THE STATUS OF THE DEPLOYMENT ROLLOUT\nYou can use the usual kubectl get deployment and the kubectl describe deployment\ncommands  to  see  details  of  the  Deployment,  but  let  me  point  you  to  an  additional\ncommand, which is made specifically for checking a Deployment’s status:\n$ kubectl rollout status deployment kubia\ndeployment kubia successfully rolled out\nAccording to this, the Deployment has been successfully rolled out, so you should see\nthe three pod replicas up and running. Let’s see:\n$ kubectl get po\nNAME                     READY     STATUS    RESTARTS   AGE\nkubia-1506449474-otnnh   1/1       Running   0          14s\nkubia-1506449474-vmn7s   1/1       Running   0          14s\nkubia-1506449474-xis6m   1/1       Running   0          14s\nUNDERSTANDING HOW DEPLOYMENTS CREATE REPLICASETS, WHICH THEN CREATE THE PODS\nTake note of the names of these pods. Earlier, when you used a ReplicationController\nto create pods, their names were composed of the name of the controller plus a ran-\ndomly  generated  string  (for  example,  \nkubia-v1-m33mv).  The  three  pods  created  by\nthe  Deployment  include  an  additional  numeric  value  in  the  middle  of  their  names.\nWhat is that exactly?\n The number corresponds to the hashed value of the pod template in the Deploy-\nment  and  the  ReplicaSet  managing  these  pods.  As  we  said  earlier,  a  Deployment\ndoesn’t manage pods directly. Instead, it creates ReplicaSets and leaves the managing\nto them, so let’s look at the ReplicaSet created by your Deployment:\n$ kubectl get replicasets\nNAME               DESIRED   CURRENT   AGE\nkubia-1506449474   3         3         10s\nThe ReplicaSet’s name also contains the hash value of its pod template. As you’ll see\nlater,  a  Deployment  creates  multiple  ReplicaSets—one  for  each  version  of  the  pod\n \n\n264CHAPTER 9Deployments: updating applications declaratively\ntemplate. Using the hash value of the pod template like this allows the Deployment\nto always use the same (possibly existing) ReplicaSet for a given version of the pod\ntemplate.\nACCESSING THE PODS THROUGH THE SERVICE\nWith the three replicas created by this ReplicaSet now running, you can use the Ser-\nvice you created a while ago to access them, because you made the new pods’ labels\nmatch the Service’s label selector. \n Up until this point, you probably haven’t seen a good-enough reason why you should\nuse Deployments over ReplicationControllers. Luckily, creating a Deployment also hasn’t\nbeen  any  harder  than  creating  a  ReplicationController.  Now,  you’ll  start  doing  things\nwith this Deployment, which will make it clear why Deployments are superior. This will\nbecome clear in the next few moments, when you see how updating the app through\na Deployment resource compares to updating it through a ReplicationController.\n9.3.2Updating a Deployment\nPreviously, when you ran your app using a ReplicationController, you had to explicitly\ntell Kubernetes to perform the update by running \nkubectl rolling-update. You even\nhad to specify the name for the new ReplicationController that should replace the old\none. Kubernetes replaced all the original pods with new ones and deleted the original\nReplicationController at the end of the process. During the process, you basically had\nto stay around, keeping your terminal open and waiting for \nkubectl to finish the roll-\ning update. \n  Now  compare  this  to  how  you’re  about  to  update  a  Deployment.  The  only  thing\nyou need to do is modify the pod template defined in the Deployment resource and\nKubernetes  will  take  all  the  steps  necessary  to  get  the  actual  system  state  to  what’s\ndefined in the resource. Similar to scaling a ReplicationController or ReplicaSet up or\ndown, all you need to do is reference a new image tag in the Deployment’s pod tem-\nplate  and  leave  it  to  Kubernetes  to  transform  your  system  so  it  matches  the  new\ndesired state.\nUNDERSTANDING THE AVAILABLE DEPLOYMENT STRATEGIES\nHow this new state should be achieved is governed by the deployment strategy config-\nured on the Deployment itself. The default strategy is to perform a rolling update (the\nstrategy  is  called  \nRollingUpdate).  The  alternative  is  the  Recreate  strategy,  which\ndeletes  all  the  old  pods  at  once  and  then  creates  new  ones,  similar  to  modifying  a\nReplicationController’s pod template and then deleting all the pods (we talked about\nthis in section 9.1.1).\n The \nRecreate strategy causes all old pods to be deleted before the new ones are\ncreated. Use this strategy when your application doesn’t support running multiple ver-\nsions  in  parallel  and  requires  the  old  version  to  be  stopped  completely  before  the\nnew one is started. This strategy does involve a short period of time when your app\nbecomes completely unavailable.\n \n\n265Using Deployments for updating apps declaratively\n  The  RollingUpdate  strategy,  on  the  other  hand,  removes  old  pods  one  by  one,\nwhile adding new ones at the same time, keeping the application available throughout\nthe  whole  process,  and  ensuring  there’s  no  drop  in  its  capacity  to  handle  requests.\nThis is the default strategy. The upper and lower limits for the number of pods above\nor below the desired replica count are configurable. You should use this strategy only\nwhen your app can handle running both the old and new version at the same time.\nSLOWING DOWN THE ROLLING UPDATE FOR DEMO PURPOSES\nIn the next exercise, you’ll use the RollingUpdate strategy, but you need to slow down\nthe  update  process  a  little,  so  you  can  see  that  the  update  is  indeed  performed  in  a\nrolling  fashion.  You  can  do  that  by  setting  the  \nminReadySeconds  attribute  on  the\nDeployment.  We’ll  explain  what  this  attribute  does  by  the  end  of  this  chapter.  For\nnow, set it to 10 seconds with the \nkubectl patch command.\n$ kubectl patch deployment kubia -p '{\"spec\": {\"minReadySeconds\": 10}}'\n\"kubia\" patched\nTIPThe kubectl patch command is useful for modifying a single property\nor a limited number of properties of a resource without having to edit its defi-\nnition in a text editor.\nYou  used  the  patch  command  to  change  the  spec  of  the  Deployment.  This  doesn’t\ncause  any  kind  of  update  to  the  pods,  because  you  didn’t  change  the  pod  template.\nChanging other Deployment properties, like the desired replica count or the deploy-\nment strategy, also doesn’t trigger a rollout, because it doesn’t affect the existing indi-\nvidual pods in any way.\nTRIGGERING THE ROLLING UPDATE\nIf you’d like to track the update process as it progresses, first run the curl loop again\nin another terminal to see what’s happening with the requests (don’t forget to replace\nthe IP with the actual external IP of your service):\n$ while true; do curl http://130.211.109.222; done\nTo trigger the actual rollout, you’ll change the image used in the single pod container\nto \nluksa/kubia:v2. Instead of editing the whole YAML of the Deployment object or\nusing  the  \npatch  command  to  change  the  image,  you’ll  use  the  kubectl set image\ncommand, which allows changing the image of any resource that contains a container\n(ReplicationControllers, ReplicaSets, Deployments, and so on). You’ll use it to modify\nyour Deployment like this:\n$ kubectl set image deployment kubia nodejs=luksa/kubia:v2\ndeployment \"kubia\" image updated\nWhen you execute this command, you’re updating the kubia Deployment’s pod tem-\nplate so the image used in its \nnodejs container is changed to luksa/kubia:v2 (from\n:v1). This is shown in figure 9.9.\n \n\n266CHAPTER 9Deployments: updating applications declaratively\nWays of modifying Deployments and other resources\nOver the course of this book, you’ve learned several ways how to modify an existing\nobject. Let’s list all of them together to refresh your memory.\nAll these methods are equivalent as far as Deployments go. What they do is change\nthe Deployment’s specification. This change then triggers the rollout process.\nImage registry\nPod template\nDeployment\nkubectl set image...\nluksa/kubia:v2\nContainer:\nnodejs\n:v1\n:v2\nImage registry\nPod template\nDeployment\nContainer:\nnodejs\n:v1:v2\nFigure 9.9   Updating a Deployment’s pod template to point to a new image\nTable 9.1   Modifying an existing resource in Kubernetes\nMethodWhat it does\nkubectl editOpens the object’s manifest in your default editor. After making \nchanges, saving the file, and exiting the editor, the object is updated.\nExample: kubectl edit deployment kubia\nkubectl patchModifies individual properties of an object.\nExample: kubectl patch deployment kubia -p '{\"spec\": \n{\"template\": {\"spec\": {\"containers\": [{\"name\": \n\"nodejs\", \"image\": \"luksa/kubia:v2\"}]}}}}'\nkubectl applyModifies the object by applying property values from a full YAML or \nJSON file. If the object specified in the YAML/JSON doesn’t exist yet, \nit’s created. The file needs to contain the full definition of the \nresource (it can’t include only the fields you want to update, as is the \ncase with kubectl patch).\nExample: kubectl apply -f kubia-deployment-v2.yaml\nkubectl replaceReplaces the object with a new one from a YAML/JSON file. In con-\ntrast to the apply command, this command requires the object to \nexist; otherwise it prints an error.\nExample: kubectl replace -f kubia-deployment-v2.yaml\nkubectl set imageChanges the container image defined in a Pod, ReplicationControl-\nler’s template, Deployment, DaemonSet, Job, or ReplicaSet.\nExample: kubectl set image deployment kubia \nnodejs=luksa/kubia:v2\n \n\n267Using Deployments for updating apps declaratively\nIf you’ve run the curl loop, you’ll see requests initially hitting only the v1 pods; then\nmore and more of them hit the v2 pod\ns until, finally, all of them hit only the remain-\ning \nv2 pods, after all v1 pods are deleted. This works much like the rolling update per-\nformed by \nkubectl.\nUNDERSTANDING THE AWESOMENESS OF DEPLOYMENTS\nLet’s think about what has happened. By changing the pod template in your Deploy-\nment resource, you’ve updated your app to a newer version—by changing a single\nfield! \n The controllers running as part of the Kubernetes control plane then performed\nthe update. The process wasn’t performed by the \nkubectl client, like it was when you\nused \nkubectl rolling-update. I don’t know about you, but I think that’s simpler than\nhaving  to  run  a  special  command  telling  Kubernetes what to do and then waiting\naround for the process to be completed.\nNOTEBe  aware  that  if  the  pod  template  in  the  Deployment  references  a\nConfigMap  (or  a  Secret),  modifying  the  ConfigMap  will  not  trigger  an\nupdate. One way to trigger an update when you need to modify an app’s con-\nfig is to create a new ConfigMap and modify the pod template so it references\nthe new ConfigMap.\nThe events that occurred below the Deployment’s surface during the update are simi-\nlar to what happened during the \nkubectl rolling-update. An additional ReplicaSet\nwas created and it was then scaled up slowly, while the previous ReplicaSet was scaled\ndown to zero (the initial and final states are shown in figure 9.10).\nYou can still see the old ReplicaSet next to the new one if you list them:\n$ kubectl get rs\nNAME               DESIRED   CURRENT   AGE\nkubia-1506449474   0         0         24m\nkubia-1581357123   3         3         23m\nPods: v1\nReplicaSet: v1\nReplicas: --\nBeforeAfter\nReplicaSet: v2\nReplicas: ++\nDeployment\nPods: v2\nReplicaSet: v1\nReplicaSet: v2\nDeployment\nFigure 9.10   A Deployment at the start and end of a rolling update\n \n\n268CHAPTER 9Deployments: updating applications declaratively\nSimilar  to  ReplicationControllers,  all  your  new  pods  are  now  managed  by  the  new\nReplicaSet. Unlike before, the old ReplicaSet is still there, whereas the old Replication-\nController was deleted at the end of the rolling-update process. You’ll soon see what\nthe purpose of this inactive ReplicaSet is. \n  But  you  shouldn’t  care  about  ReplicaSets  here,  because  you  didn’t  create  them\ndirectly. You created and operated only on the Deployment resource; the underlying\nReplicaSets are an implementation detail. You’ll agree that managing a single Deploy-\nment object is much easier compared to dealing with and keeping track of multiple\nReplicationControllers. \n Although this difference may not be so apparent when everything goes well with a\nrollout,  it  becomes  much  more  obvious  when  you  hit  a  problem  during  the  rollout\nprocess. Let’s simulate one problem right now.\n9.3.3Rolling back a deployment\nYou’re currently running version v2 of your image, so you’ll need to prepare version 3\nfirst. \nCREATING VERSION 3 OF YOUR APP\nIn  version  3,  you’ll  introduce  a  bug  that  makes  your  app  handle  only  the  first  four\nrequests  properly.  All  requests  from  the  fifth  request  onward  will  return  an  internal\nserver error (HTTP status code 500). You’ll simulate this by adding an \nif statement at\nthe beginning of the handler function. The following listing shows the new code, with\nall required changes shown in bold.\nconst http = require('http');\nconst os = require('os');\nvar requestCount = 0;\nconsole.log(\"Kubia server starting...\");\nvar handler = function(request, response) {\n  console.log(\"Received request from \" + request.connection.remoteAddress);\n  if (++requestCount >= 5) {\n    response.writeHead(500);\n    response.end(\"Some internal error has occurred! This is pod \" + \nos.hostname() + \"\\n\");\n    return;\n  }\n  response.writeHead(200);\n  response.end(\"This is v3 running in pod \" + os.hostname() + \"\\n\");\n};\nvar www = http.createServer(handler);\nwww.listen(8080); \nAs you can see, on the fifth and all subsequent requests, the code returns a 500 error\nwith the message “Some internal error has occurred...”\nListing 9.8   Version 3 of our app (a broken version): v3/app.js\n \n\n269Using Deployments for updating apps declaratively\nDEPLOYING VERSION 3\nI’ve made the v3 version of the image available as luksa/kubia:v3. You’ll deploy this\nnew version by changing the image in the Deployment specification again: \n$ kubectl set image deployment kubia nodejs=luksa/kubia:v3\ndeployment \"kubia\" image updated\nYou can follow the progress of the rollout with kubectl rollout status:\n$ kubectl rollout status deployment kubia\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 1 old replicas are pending termination...\ndeployment \"kubia\" successfully rolled out\nThe new version is now live. As the following listing shows, after a few requests, your\nweb clients start receiving errors.\n$ while true; do curl http://130.211.109.222; done\nThis is v3 running in pod kubia-1914148340-lalmx\nThis is v3 running in pod kubia-1914148340-bz35w\nThis is v3 running in pod kubia-1914148340-w0voh\n...\nThis is v3 running in pod kubia-1914148340-w0voh\nSome internal error has occurred! This is pod kubia-1914148340-bz35w\nThis is v3 running in pod kubia-1914148340-w0voh\nSome internal error has occurred! This is pod kubia-1914148340-lalmx\nThis is v3 running in pod kubia-1914148340-w0voh\nSome internal error has occurred! This is pod kubia-1914148340-lalmx\nSome internal error has occurred! This is pod kubia-1914148340-bz35w\nSome internal error has occurred! This is pod kubia-1914148340-w0voh\nUNDOING A ROLLOUT\nYou can’t have your users experiencing internal server errors, so you need to do some-\nthing about it fast. In section 9.3.6 you’ll see how to block bad rollouts automatically,\nbut  for  now,  let’s  see  what  you  can  do  about  your  bad  rollout  manually.  Luckily,\nDeployments make it easy to roll back to the previously deployed version by telling\nKubernetes to undo the last rollout of a Deployment:\n$ kubectl rollout undo deployment kubia\ndeployment \"kubia\" rolled back\nThis rolls the Deployment back to the previous revision. \nTIPThe undo command can also be used while the rollout process is still in\nprogress to essentially abort the rollout. Pods already created during the roll-\nout process are removed and replaced with the old ones again.\nListing 9.9   Hitting your broken version 3\n \n\n270CHAPTER 9Deployments: updating applications declaratively\nDISPLAYING A DEPLOYMENT’S ROLLOUT HISTORY\nRolling  back  a  rollout  is  possible  because  Deployments  keep  a  revision  history.  As\nyou’ll  see  later,  the  history  is  stored  in  the  underlying  ReplicaSets.  When  a  rollout\ncompletes, the old ReplicaSet isn’t deleted, and this enables rolling back to any revi-\nsion,  not  only  the  previous  one.  The  revision  history  can  be  displayed  with  the\nkubectl rollout history command:\n$ kubectl rollout history deployment kubia\ndeployments \"kubia\":\nREVISION    CHANGE-CAUSE\n2           kubectl set image deployment kubia nodejs=luksa/kubia:v2\n3           kubectl set image deployment kubia nodejs=luksa/kubia:v3\nRemember  the  --record  command-line  option  you  used  when  creating  the  Deploy-\nment? Without it, the \nCHANGE-CAUSE column in the revision history would be empty,\nmaking it much harder to figure out what’s behind each revision.\nROLLING BACK TO A SPECIFIC DEPLOYMENT REVISION\nYou  can  roll  back  to  a  specific  revision  by  specifying  the  revision  in  the  undo  com-\nmand. For example, if you want to roll back to the first version, you’d execute the fol-\nlowing command:\n$ kubectl rollout undo deployment kubia --to-revision=1\nRemember the inactive ReplicaSet left over when you modified the Deployment the\nfirst time? The ReplicaSet represents the first revision of your Deployment. All Replica-\nSets created by a Deployment represent the complete revision history, as shown in fig-\nure 9.11. Each ReplicaSet stores the complete information of the Deployment at that\nspecific revision, so you shouldn’t delete it manually. If you do, you’ll lose that specific\nrevision from the Deployment’s history, preventing you from rolling back to it.\nBut having old ReplicaSets cluttering your ReplicaSet list is not ideal, so the length of\nthe revision history is limited by the \nrevisionHistoryLimit property on the Deploy-\nment resource. It defaults to two, so normally only the current and the previous revision\nare  shown  in  the  history  (and  only  the  current  and  the  previous  ReplicaSet  are  pre-\nserved). Older ReplicaSets are deleted automatically. \nDeployment\nv1 ReplicaSet\nReplicaSet\nPods: v1\nReplicaSetReplicaSetReplicaSet\nRevision 2Revision 4Revision 3Revision 1\nRevision historyCurrent revision\nFigure 9.11   A Deployment’s ReplicaSets also act as its revision history.\n \n\n271Using Deployments for updating apps declaratively\nNOTEThe extensions/v1beta1 version of Deployments doesn’t have a default\nrevisionHistoryLimit, whereas the default in version apps/v1beta2 is 10.\n9.3.4Controlling the rate of the rollout\nWhen  you  performed  the  rollout  to  v3  and  tracked  its  progress  with  the  kubectl\nrollout\n status  command,  you  saw  that  first  a  new  pod  was  created,  and  when  it\nbecame available, one of the old pods was deleted and another new pod was created.\nThis continued until there were no old pods left. The way new pods are created and\nold ones are deleted is configurable through two additional properties of the rolling\nupdate strategy. \nINTRODUCING THE MAXSURGE AND MAXUNAVAILABLE PROPERTIES OF THE ROLLING UPDATE STRATEGY\nTwo properties affect how many pods are replaced at once during a Deployment’s roll-\ning  update.  They  are  \nmaxSurge  and  maxUnavailable and can be set as part of the\nrollingUpdate  sub-property  of  the  Deployment’s  strategy  attribute,  as  shown  in\nthe following listing.\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\nWhat these properties do is explained in table 9.2.\nBecause the desired replica count in your case was three, and both these properties\ndefault  to  25%,  \nmaxSurge  allowed  the  number  of  all  pods  to  reach  four,  and\nListing 9.10   Specifying parameters for the rollingUpdate strategy\nTable 9.2   Properties for configuring the rate of the rolling update\nPropertyWhat it does\nmaxSurgeDetermines how many pod instances you allow to exist above the desired replica \ncount configured on the Deployment. It defaults to 25%, so there can be at most \n25% more pod instances than the desired count. If the desired replica count is \nset to four, there will never be more than five pod instances running at the same \ntime during an update. When converting a percentage to an absolute number, \nthe number is rounded up. Instead of a percentage, the value can also be an \nabsolute value (for example, one or two additional pods can be allowed).\nmaxUnavailableDetermines how many pod instances can be unavailable relative to the desired \nreplica count during the update. It also defaults to 25%, so the number of avail-\nable pod instances must never fall below 75% of the desired replica count. Here, \nwhen converting a percentage to an absolute number, the number is rounded \ndown. If the desired replica count is set to four and the percentage is 25%, only \none pod can be unavailable. There will always be at least three pod instances \navailable to serve requests during the whole rollout. As with maxSurge, you can \nalso specify an absolute value instead of a percentage.\n \n\n272CHAPTER 9Deployments: updating applications declaratively\nmaxUnavailable disallowed having any unavailable pods (in other words, three pods\nhad to be available at all times). This is shown in figure 9.12.\nUNDERSTANDING THE MAXUNAVAILABLE PROPERTY\nThe extensions/v1beta1 version of Deployments uses different defaults—it sets both\nmaxSurge and maxUnavailable to 1 instead of 25%. In the case of three replicas, max-\nSurge\n is the same as before, but maxUnavailable  is  different  (1  instead  of  0).  This\nmakes the rollout process unwind a bit differently, as shown in figure 9.13.\nv1\nNumber\nof pods\n3\n4\n2\n1\nTime\nv1\n3 available\n1 unavailable\nCreate\none\nv2 pod\n4 available\n3 available\n1 unavailable\n4 available\n3 available\n1 unavailable\nmaxSurge= 1\nmaxUnavailable= 0\nDesired replica count = 3\n3 available\nv2\nv1v1v2v2\nv1v1\nv1\nv1\nv1\nv1\nv1v1\nv1\nv1v2v2v2v2\nv2v2v2\nv2\nv1\nv2\nv2v2v2\n4 available\nWait\nuntil\nit’s\navailable\nDelete\none v1\npod and\ncreate one\nv2 pod\nWait\nuntil\nit’s\navailable\nDelete\none v1\npod and\ncreate one\nv2 pod\nWait\nuntil\nit’s\navailable\nDelete\nlast\nv1 pod\nFigure 9.12   Rolling update of a Deployment with three replicas and default maxSurge and maxUnavailable \nv1\nNumber\nof pods\n3\n4\n2\n1\nTime\nv1\n2 available\n2 unavailable\n4 available\n2 available\n1 unavailable\n3 available\nmaxSurge= 1\nmaxUnavailable= 1\nDesired replica count = 3\nv1v1\nv1\nv1\nv1\nv2\nv2\nv2\nv2\nv2v2\nv2\nv2\nv2\nv2\nWait until\nboth are\navailable\nDelete\ntwo v1\npods and\ncreate one\nv2 pod\nDelete v1\npod and\ncreate two\nv2 pods\nWait\nuntil it’s\navailable\nFigure 9.13   Rolling update of a Deployment with the maxSurge=1 and maxUnavailable=1\n \n\n273Using Deployments for updating apps declaratively\nIn  this  case,  one  replica  can  be  unavailable,  so  if  the  desired  replica  count  is  three,\nonly  two  of  them  need  to  be  available.  That’s  why  the  rollout  process  immediately\ndeletes  one  pod  and  creates  two  new  ones.  This  ensures  two  pods  are  available  and\nthat  the  maximum  number  of  pods  isn’t  exceeded  (the  maximum  is  four  in  this\ncase—three plus one from \nmaxSurge). As soon as the two new pods are available, the\ntwo remaining old pods are deleted.\n This is a bit hard to grasp, especially since the \nmaxUnavailable property leads you\nto  believe  that  that’s  the  maximum  number  of  unavailable  pods  that  are  allowed.  If\nyou look at the previous figure closely, you’ll see two unavailable pods in the second\ncolumn even though \nmaxUnavailable is set to 1. \n It’s important to keep in mind that \nmaxUnavailable  is  relative  to  the  desired\nreplica  count.  If  the  replica  count  is  set  to  three  and  \nmaxUnavailable is set to one,\nthat  means  that  the  update  process  must  always keep at least two (3 minus 1) pods\navailable, while the number of pods that aren’t available can exceed one.\n9.3.5Pausing the rollout process\nAfter the bad experience with version 3 of your app, imagine you’ve now fixed the bug\nand pushed version 4 of your image. You’re a little apprehensive about rolling it out\nacross all your pods the way you did before. What you want is to run a single \nv4 pod\nnext to your existing \nv2 pods and see how it behaves with only a fraction of all your\nusers. Then, once you’re sure everything’s okay, you can replace all the old pods with\nnew ones. \n You could achieve this by running an additional pod either directly or through an\nadditional Deployment, ReplicationController, or ReplicaSet, but you do have another\noption  available  on  the  Deployment  itself.  A  Deployment  can  also  be  paused  during\nthe rollout process. This allows you to verify that everything is fine with the new ver-\nsion before proceeding with the rest of the rollout.\nPAUSING THE ROLLOUT\nI’ve prepared the v4 image, so go ahead and trigger the rollout by changing the image\nto \nluksa/kubia:v4, but then immediately (within a few seconds) pause the rollout:\n$ kubectl set image deployment kubia nodejs=luksa/kubia:v4\ndeployment \"kubia\" image updated\n$ kubectl rollout pause deployment kubia\ndeployment \"kubia\" paused\nA single new pod should have been created, but all original pods should also still be\nrunning. Once the new pod is up, a part of all requests to the service will be redirected\nto the new pod. This way, you’ve effectively run a canary release. A canary release is a\ntechnique for minimizing the risk of rolling out a bad version of an application and it\naffecting all your users. Instead of rolling out the new version to everyone, you replace\nonly one or a small number of old pods with new ones. This way only a small number\nof users will initially hit the new version. You can then verify whether the new version\n \n\n274CHAPTER 9Deployments: updating applications declaratively\nis working fine or not and then either continue the rollout across all remaining pods\nor roll back to the previous version. \nRESUMING THE ROLLOUT\nIn your case, by pausing the rollout process, only a small portion of client requests will\nhit your \nv4 pod, while most will still hit the v3 pods. Once you’re confident the new\nversion works as it should, you can resume the deployment to replace all the old pods\nwith new ones:\n$ kubectl rollout resume deployment kubia\ndeployment \"kubia\" resumed\nObviously,  having  to  pause  the  deployment  at  an  exact  point  in  the  rollout  process\nisn’t what you want to do. In the future, a new upgrade strategy may do that automati-\ncally, but currently, the proper way of performing a canary release is by using two dif-\nferent Deployments and scaling them appropriately. \nUSING THE PAUSE FEATURE TO PREVENT ROLLOUTS\nPausing a Deployment can also be used to prevent updates to the Deployment from\nkicking off the rollout process, allowing you to make multiple changes to the Deploy-\nment and starting the rollout only when you’re done making all the necessary changes.\nOnce  you’re  ready  for  changes  to  take  effect,  you  resume  the  Deployment  and  the\nrollout process will start.\nNOTEIf a Deployment is paused, the undo command won’t undo it until you\nresume the Deployment.\n9.3.6Blocking rollouts of bad versions\nBefore you conclude this chapter, we need to discuss one more property of the Deploy-\nment resource. Remember the \nminReadySeconds property you set on the Deployment\nat the beginning of section 9.3.2? You used it to slow down the rollout, so you could see\nit was indeed performing a rolling update and not replacing all the pods at once. The\nmain function of \nminReadySeconds is to prevent deploying malfunctioning versions, not\nslowing down a deployment for fun. \nUNDERSTANDING THE APPLICABILITY OF MINREADYSECONDS\nThe minReadySeconds  property  specifies  how  long  a  newly  created  pod  should  be\nready before the pod is treated as available. Until the pod is available, the rollout pro-\ncess  will  not  continue  (remember  the  \nmaxUnavailable  property?).  A  pod  is  ready\nwhen  readiness  probes  of  all  its  containers  return  a  success.  If  a  new  pod  isn’t  func-\ntioning  properly  and  its  readiness  probe  starts  failing  before  \nminReadySeconds  have\npassed, the rollout of the new version will effectively be blocked.\n  You  used  this  property  to  slow  down  your  rollout  process  by  having  Kubernetes\nwait  10  seconds  after  a  pod  was  ready  before  continuing  with  the  rollout.  Usually,\nyou’d set \nminReadySeconds to something much higher to make sure pods keep report-\ning they’re ready after they’ve already started receiving actual traffic. \n \n\n275Using Deployments for updating apps declaratively\n Although you should obviously test your pods both in a test and in a staging envi-\nronment  before  deploying  them  into  production,  using  \nminReadySeconds is like an\nairbag that saves your app from making a big mess after you’ve already let a buggy ver-\nsion slip into production. \n  With  a  properly  configured  readiness  probe  and  a  proper  \nminReadySeconds  set-\nting, Kubernetes would have prevented us from deploying the buggy \nv3 version ear-\nlier. Let me show you how.\nDEFINING A READINESS PROBE TO PREVENT OUR V3 VERSION FROM BEING ROLLED OUT FULLY\nYou’re going to deploy version v3 again, but this time, you’ll have the proper readi-\nness probe defined on the pod. Your Deployment is currently at version \nv4, so before\nyou start, roll back to version \nv2 again so you can pretend this is the first time you’re\nupgrading to \nv3. If you wish, you can go straight from v4 to v3, but the text that fol-\nlows assumes you returned to \nv2 first.\n Unlike before, where you only updated the image in the pod template, you’re now\nalso going to introduce a readiness probe for the container at the same time. Up until\nnow,  because  there  was  no  explicit  readiness  probe  defined,  the  container  and  the\npod were always considered ready, even if the app wasn’t truly ready or was returning\nerrors. There was no way for Kubernetes to know that the app was malfunctioning and\nshouldn’t be exposed to clients. \n  To  change  the  image  and  introduce  the  readiness  probe  at  once,  you’ll  use  the\nkubectl apply  command.  You’ll  use  the  following  YAML  to  update  the  deployment\n(you’ll  store  it  as  \nkubia-deployment-v3-with-readinesscheck.yaml),  as  shown  in\nthe following listing.\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: kubia\nspec:\n  replicas: 3\n  minReadySeconds: 10           \n  strategy:\n    rollingUpdate:\n      maxSurge: 1                  \n      maxUnavailable: 0         \n    type: RollingUpdate\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - image: luksa/kubia:v3\nListing 9.11   Deployment with a readiness probe: kubia-deployment-v3-with-\nreadinesscheck.yaml\nYou’re keeping \nminReadySeconds \nset to 10.\nYou’re keeping maxUnavailable \nset to 0 to make the deployment \nreplace pods one by one\n \n\n276CHAPTER 9Deployments: updating applications declaratively\n        name: nodejs\n        readinessProbe:\n          periodSeconds: 1       \n          httpGet:                  \n            path: /                 \n            port: 8080              \nUPDATING A DEPLOYMENT WITH KUBECTL APPLY\nTo update the Deployment this time, you’ll use kubectl apply like this:\n$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml \ndeployment \"kubia\" configured\nThe apply  command  updates  the  Deployment  with  everything  that’s  defined  in  the\nYAML file. It not only updates the image but also adds the readiness probe definition\nand anything else you’ve added or modified in the YAML. If the new YAML also con-\ntains the \nreplicas field, which doesn’t match the number of replicas on the existing\nDeployment,  the  apply  operation  will  also  scale  the  Deployment,  which  isn’t  usually\nwhat you want. \nTIPTo keep the desired replica count unchanged when updating a Deploy-\nment with \nkubectl apply, don’t include the replicas field in the YAML. \nRunning  the  \napply  command  will  kick  off  the  update  process,  which  you  can  again\nfollow with the \nrollout status command:\n$ kubectl rollout status deployment kubia\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\nBecause the status says one new pod has been created, your service should be hitting it\noccasionally, right? Let’s see:\n$ while true; do curl http://130.211.109.222; done\nThis is v2 running in pod kubia-1765119474-jvslk\nThis is v2 running in pod kubia-1765119474-jvslk\nThis is v2 running in pod kubia-1765119474-xk5g3\nThis is v2 running in pod kubia-1765119474-pmb26\nThis is v2 running in pod kubia-1765119474-pmb26\nThis is v2 running in pod kubia-1765119474-xk5g3\n...\nNope, you never hit the v3 pod. Why not? Is it even there? List the pods:\n$ kubectl get po\nNAME                     READY     STATUS    RESTARTS   AGE\nkubia-1163142519-7ws0i   0/1       Running   0          30s\nkubia-1765119474-jvslk   1/1       Running   0          9m\nkubia-1765119474-pmb26   1/1       Running   0          9m\nkubia-1765119474-xk5g3   1/1       Running   0          8m\nYou’re defining a readiness probe \nthat will be executed every second.\nThe readiness probe will \nperform an HTTP GET request \nagainst our container.\n \n\n277Using Deployments for updating apps declaratively\nAha! There’s your problem (or as you’ll learn soon, your blessing)! The pod is shown\nas not ready, but I guess you’ve been expecting that, right? What has happened?\nUNDERSTANDING HOW A READINESS PROBE PREVENTS BAD VERSIONS FROM BEING ROLLED OUT\nAs soon as your new pod starts, the readiness probe starts being hit every second (you\nset the probe’s interval to one second in the pod spec). On the fifth request the readi-\nness  probe  began  failing,  because  your  app  starts  returning  HTTP  status  code  500\nfrom the fifth request onward. \n As a result, the pod is removed as an endpoint from the service (see figure 9.14).\nBy  the  time  you  start  hitting  the  service  in  the  \ncurl  loop,  the  pod  has  already  been\nmarked  as  not  ready.  This  explains  why  you  never  hit  the  new  pod  with  \ncurl.  And\nthat’s  exactly  what  you  want,  because  you  don’t  want  clients  to  hit  a  pod  that’s  not\nfunctioning properly.\nBut  what  about  the  rollout  process?  The  \nrollout status  command  shows  only  one\nnew replica has started. Thankfully, the rollout process will not continue, because the\nnew pod will never become available. To be considered available, it needs to be ready\nfor at least 10 seconds. Until it’s available, the rollout process will not create any new\npods, and it also won’t remove any original pods because you’ve set the \nmaxUnavailable\nproperty to 0. \nService\ncurl\nPod: v2Pod: v2\nPod: v3\n(unhealthy)\nPod: v2\nReplicaSet: v2\nReplicas: 3\nDeployment\nReplicas: 3\nrollingUpdate:\nmaxSurge: 1\nmaxUnavailable: 0\nReplicaSet: v3\nReplicas: 1\nRequests are not forwarded\nto v3 pod because of failed\nreadiness probe\nFigure 9.14   Deployment blocked by a failing readiness probe in the new pod\n \n\n278CHAPTER 9Deployments: updating applications declaratively\n The fact that the deployment is stuck is a good thing, because if it had continued\nreplacing the old pods with the new ones, you’d end up with a completely non-working\nservice, like you did when you first rolled out version 3, when you weren’t using the\nreadiness  probe.  But  now,  with  the  readiness  probe  in  place,  there  was  virtually  no\nnegative impact on your users. A few users may have experienced the internal server\nerror, but that’s not as big of a problem as if the rollout had replaced all pods with the\nfaulty version 3.\nTIPIf  you  only  define  the  readiness  probe  without  setting  minReadySeconds\nproperly, new pods are considered available immediately when the first invo-\ncation  of  the  readiness  probe  succeeds.  If  the  readiness  probe  starts  failing\nshortly  after,  the  bad  version  is  rolled  out  across  all  pods.  Therefore,  you\nshould set \nminReadySeconds appropriately.\nCONFIGURING A DEADLINE FOR THE ROLLOUT\nBy default, after the rollout can’t make any progress in 10 minutes, it’s considered as\nfailed. If you use the \nkubectl describe deployment command, you’ll see it display a\nProgressDeadlineExceeded condition, as shown in the following listing.\n$ kubectl describe deploy kubia\nName:                   kubia\n...\nConditions:\n  Type          Status  Reason\n  ----          ------  ------\n  Available     True    MinimumReplicasAvailable\n  Progressing   False   ProgressDeadlineExceeded   \nThe time after which the Deployment is considered failed is configurable through the\nprogressDeadlineSeconds property in the Deployment spec.\nNOTEThe extensions/v1beta1 version of Deployments doesn’t set a deadline.\nABORTING A BAD ROLLOUT\nBecause the rollout will never continue, the only thing to do now is abort the rollout\nby undoing it:\n$ kubectl rollout undo deployment kubia\ndeployment \"kubia\" rolled back\nNOTEIn future versions, the rollout will be aborted automatically when the\ntime specified in \nprogressDeadlineSeconds is exceeded.\nListing 9.12   Seeing the conditions of a Deployment with kubectl describe\nThe Deployment \ntook too long to \nmake progress.\n \n\n279Summary\n9.4Summary\nThis  chapter  has  shown  you  how  to  make  your  life  easier  by  using  a  declarative\napproach  to  deploying  and  updating  applications  in  Kubernetes.  Now  that  you’ve\nread this chapter, you should know how to\nPerform a rolling update of pods managed by a ReplicationController\nCreate Deployments instead of lower-level ReplicationControllers or ReplicaSets\nUpdate your pods by editing the pod template in the Deployment specification\nRoll back a Deployment either to the previous revision or to any earlier revision\nstill listed in the revision history\nAbort a Deployment mid-way\nPause a Deployment to inspect how a single instance of the new version behaves\nin production before allowing additional pod instances to replace the old ones\nControl the rate of the rolling update through maxSurge and maxUnavailable\nproperties\nUse minReadySeconds and readiness probes to have the rollout of a faulty ver-\nsion blocked automatically\nIn addition to these Deployment-specific tasks, you also learned how to\nUse three dashes as a separator to define multiple resources in a single YAML file\nTurn on kubectl’s verbose logging to see exactly what it’s doing behind the\ncurtains\nYou now know how to deploy and manage sets of pods created from the same pod\ntemplate  and  thus  share  the  same  persistent  storage.  You  even  know  how  to  update\nthem declaratively. But what about running sets of pods, where each instance needs to\nuse its own persistent storage? We haven’t looked at that yet. That’s the subject of our\nnext chapter.\n \n\n280\nStatefulSets:\ndeploying replicated\nstateful applications\nYou  now  know  how  to  run  both  single-instance  and  replicated  stateless  pods,\nand  even  stateful  pods  utilizing  persistent  storage.  You  can  run  several  repli-\ncated web-server pod instances and you can run a single database pod instance\nthat uses persistent storage, provided either through plain pod volumes or through\nPersistentVolumes  bound  by  a  PersistentVolumeClaim.  But  can  you  employ  a\nReplicaSet to replicate the database pod?\nThis chapter covers\nDeploying stateful clustered applications\nProviding separate storage for each instance of \na replicated pod\nGuaranteeing a stable name and hostname for \npod replicas\nStarting and stopping pod replicas in a \npredictable order\nDiscovering peers through DNS SRV records\n \n\n281Replicating stateful pods\n10.1   Replicating stateful pods\nReplicaSets  create  multiple  pod  replicas  from  a  single  pod  template.  These  replicas\ndon’t differ from each other, apart from their name and IP address. If the pod tem-\nplate includes a volume, which refers to a specific PersistentVolumeClaim, all replicas\nof  the  ReplicaSet  will  use  the  exact  same  PersistentVolumeClaim  and  therefore  the\nsame PersistentVolume bound by the claim (shown in figure 10.1).\nBecause the reference to the claim is in the pod template, which is used to stamp out\nmultiple  pod  replicas,  you  can’t  make  each  replica  use  its  own  separate  Persistent-\nVolumeClaim. You can’t use a ReplicaSet to run a distributed data store, where each\ninstance needs its own separate storage—at least not by using a single ReplicaSet. To\nbe honest, none of the API objects you’ve seen so far make running such a data store\npossible. You need something else. \n10.1.1   Running multiple replicas with separate storage for each\nHow does one run multiple replicas of a pod and have each pod use its own storage\nvolume?  ReplicaSets  create  exact  copies  (replicas)  of  a  pod;  therefore  you  can’t  use\nthem for these types of pods. What can you use?\nCREATING PODS MANUALLY\nYou could create pods manually and have each of them use its own PersistentVolume-\nClaim, but because no ReplicaSet looks after them, you’d need to manage them man-\nually  and  recreate  them  when  they  disappear  (as  in  the  event  of  a  node  failure).\nTherefore, this isn’t a viable option.\nUSING ONE REPLICASET PER POD INSTANCE\nInstead of creating pods directly, you could create multiple ReplicaSets—one for each\npod with each ReplicaSet’s desired replica count set to one, and each ReplicaSet’s pod\ntemplate referencing a dedicated PersistentVolumeClaim (as shown in figure 10.2).\n Although this takes care of the automatic rescheduling in case of node failures or\naccidental  pod  deletions,  it’s  much  more  cumbersome  compared  to  having  a  single\nReplicaSet.  For  example,  think  about  how  you’d  scale  the  pods  in  that  case.  You\nPersistent\nVolume\nClaim\nPersistent\nVolume\nReplicaSet\nPod\nPod\nPod\nFigure 10.1   All pods from the same ReplicaSet always use the same \nPersistentVolumeClaim and PersistentVolume.\n \n\n282CHAPTER 10StatefulSets: deploying replicated stateful applications\ncouldn’t change the desired replica count—you’d have to create additional Replica-\nSets instead. \n Using multiple ReplicaSets is therefore not the best solution. But could you maybe\nuse a single ReplicaSet and have each pod instance keep its own persistent state, even\nthough they’re all using the same storage volume? \nUSING MULTIPLE DIRECTORIES IN THE SAME VOLUME\nA trick you can use is to have all pods use the same PersistentVolume, but then have a\nseparate file directory inside that volume for each pod (this is shown in figure 10.3).\nBecause you can’t configure pod replicas differently from a single pod template, you\ncan’t tell each instance what directory it should use, but you can make each instance\nautomatically select (and possibly also create) a data directory that isn’t being used\nby any other instance at that time. This solution does require coordination between\nthe instances, and isn’t easy to do correctly. It also makes the shared storage volume\nthe bottleneck.\n10.1.2   Providing a stable identity for each pod\nIn  addition  to  storage,  certain  clustered  applications  also  require  that  each  instance\nhas a long-lived stable identity. Pods can be killed from time to time and replaced with\nPVC A1PV A1\nReplicaSet A1\nPod A1-xyz\nPVC A2PV A2\nReplicaSet A2\nPod A2-xzy\nPVC A3PV A3\nReplicaSet A3\nPod A3-zyx\nFigure 10.2   Using one ReplicaSet for each pod instance\nPersistent\nVolume\nClaim\nPersistentVolume\nReplicaSet\nPod\nPod\nPod\nApp\nApp\nApp\n/data/1/\n/data/3/\n/data/2/\nFigure 10.3   Working around the shared storage problem by having the app \nin each pod use a different file directory \n \n\n283Replicating stateful pods\nnew  ones.  When  a  ReplicaSet  replaces  a  pod,  the  new  pod  is  a  completely  new  pod\nwith a new hostname and IP, although the data in its storage volume may be that of\nthe  killed  pod.  For  certain  apps,  starting  up  with  the  old  instance’s  data  but  with  a\ncompletely new network identity may cause problems.\n  Why  do  certain  apps  mandate  a  stable  network  identity?  This  requirement  is\nfairly common in distributed stateful applications. Certain apps require the adminis-\ntrator to list all the other cluster members and their IP addresses (or hostnames) in\neach  member’s  configuration  file.  But  in  Kubernetes,  every  time  a  pod  is  resched-\nuled,  the  new  pod  gets  both  a  new  hostname  and  a  new  IP  address,  so  the  whole\napplication cluster would have to be reconfigured every time one of its members is\nrescheduled. \nUSING A DEDICATED SERVICE FOR EACH POD INSTANCE\nA trick you can use to work around this problem is to provide a stable network address\nfor  cluster  members  by  creating  a  dedicated  Kubernetes  Service  for  each  individual\nmember. Because service IPs are stable, you can then point to each member through\nits service IP (rather than the pod IP) in the configuration. \n This is similar to creating a ReplicaSet for each member to provide them with indi-\nvidual storage, as described previously. Combining these two techniques results in the\nsetup shown in figure 10.4 (an additional service covering all the cluster members is\nalso shown, because you usually need one for clients of the cluster).\nThe solution is not only ugly, but it still doesn’t solve everything. The individual pods\ncan’t know which Service they are exposed through (and thus can’t know their stable\nIP), so they can’t self-register in other pods using that IP. \nPVC A1PV A1\nReplicaSet A1\nPod A1-xzy\nService A1\nService A\nPVC A2PV A2\nReplicaSet A2\nPod A2-xzy\nService A2\nPVC A3PV A3\nReplicaSet A3\nPod A3-zyx\nService A3\nFigure 10.4   Using one \nService and ReplicaSet per \npod to provide a stable \nnetwork address and an \nindividual volume for each \npod, respectively\n \n\n284CHAPTER 10StatefulSets: deploying replicated stateful applications\n Luckily, Kubernetes saves us from resorting to such complex solutions. The proper\nclean and simple way of running these special types of applications in Kubernetes is\nthrough a StatefulSet. \n10.2   Understanding StatefulSets\nInstead  of  using  a  ReplicaSet  to  run  these  types  of  pods,  you  create  a  StatefulSet\nresource, which is specifically tailored to applications where instances of the applica-\ntion must be treated as non-fungible individuals, with each one having a stable name\nand state. \n10.2.1   Comparing StatefulSets with ReplicaSets\nTo understand the purpose of StatefulSets, it’s best to compare them to ReplicaSets or\nReplicationControllers. But first let me explain them with a little analogy that’s widely\nused in the field.\nUNDERSTANDING STATEFUL PODS WITH THE PETS VS. CATTLE ANALOGY\nYou may have already heard of the pets vs. cattle analogy. If not, let me explain it. We\ncan treat our apps either as pets or as cattle. \nNOTEStatefulSets  were  initially  called  PetSets.  That  name  comes  from  the\npets vs. cattle analogy explained here.\nWe tend to treat our app instances as pets, where we give each instance a name and\ntake care of each instance individually. But it’s usually better to treat instances as cattle\nand not pay special attention to each individual instance. This makes it easy to replace\nunhealthy  instances  without  giving  it  a  second  thought,  similar  to  the  way  a  farmer\nreplaces unhealthy cattle. \n  Instances  of  a  stateless  app,  for  example, behave much like heads of cattle. It\ndoesn’t  matter  if  an  instance  dies—you  can  create  a  new  instance  and  people  won’t\nnotice the difference. \n On the other hand, with stateful apps, an app instance is more like a pet. When a\npet dies, you can’t go buy a new one and expect people not to notice. To replace a lost\npet, you need to find a new one that looks and behaves exactly like the old one. In the\ncase of apps, this means the new instance needs to have the same state and identity as\nthe old one.\nCOMPARING STATEFULSETS WITH REPLICASETS OR REPLICATIONCONTROLLERS\nPod replicas managed by a ReplicaSet or ReplicationController are much like cattle.\nBecause  they’re  mostly  stateless,  they  can  be  replaced  with  a  completely  new  pod\nreplica  at  any  time.  Stateful  pods  require  a  different  approach.  When  a  stateful  pod\ninstance dies (or the node it’s running on fails), the pod instance needs to be resur-\nrected on another node, but the new instance needs to get the same name, network\nidentity, and state as the one it’s replacing. This is what happens when the pods are\nmanaged through a StatefulSet. \n \n\n285Understanding StatefulSets\n A StatefulSet makes sure pods are rescheduled in such a way that they retain their\nidentity and state. It also allows you to easily scale the number of pets up and down. A\nStatefulSet,  like  a  ReplicaSet,  has  a  desired  replica  count  field  that  determines  how\nmany pets you want running at that time. Similar to ReplicaSets, pods are created from\na pod template specified as part of the StatefulSet (remember the cookie-cutter anal-\nogy?).  But  unlike  pods  created  by  ReplicaSets,  pods  created  by  the  StatefulSet  aren’t\nexact  replicas  of  each  other.  Each  can  have  its  own  set  of  volumes—in  other  words,\nstorage  (and  thus  persistent  state)—which  differentiates  it  from  its  peers.  Pet  pods\nalso have a predictable (and stable) identity instead of each new pod instance getting\na completely random one. \n10.2.2   Providing a stable network identity\nEach pod created by a StatefulSet is assigned an ordinal index (zero-based), which\nis then used to derive the pod’s name and hostname, and to attach stable storage to\nthe pod. The names of the pods are thus predictable, because each pod’s name is\nderived from the StatefulSet’s name and the ordinal index of the instance. Rather\nthan the pods having random names, they’re nicely organized, as shown in the next\nfigure.\nINTRODUCING THE GOVERNING SERVICE\nBut it’s not all about the pods having a predictable name and hostname. Unlike regu-\nlar pods, stateful pods sometimes need to be addressable by their hostname, whereas\nstateless  pods  usually  don’t.  After  all,  each  stateless  pod  is  like  any  other.  When  you\nneed one, you pick any one of them. But with stateful pods, you usually want to oper-\nate on a specific pod from the group, because they differ from each other (they hold\ndifferent state, for example). \n  For  this  reason,  a  StatefulSet  requires  you  to  create  a  corresponding  governing\nheadless  Service  that’s  used  to  provide  the  actual  network  identity  to  each  pod.\nThrough this Service, each pod gets its own DNS entry, so its peers and possibly other\nclients in the cluster can address the pod by its hostname. For example, if the govern-\ning Service belongs to the \ndefault namespace and is called foo, and one of the pods\nReplicaSet A\nPodA-fewrb\nPodA-jwqec\nPodA-dsfwx\nStatefulSet A\nPodA-1\nPodA-2\nPodA-0\nFigure 10.5   Pods created by a StatefulSet have predictable names (and hostnames), \nunlike those created by a ReplicaSet\n \n\n286CHAPTER 10StatefulSets: deploying replicated stateful applications\nis called A-0, you can reach the pod through its fully qualified domain name, which\nis \na-0.foo.default.svc.cluster.local. You can’t do that with pods managed by a\nReplicaSet.\n Additionally, you can also use DNS to look up all the StatefulSet’s pods’ names by\nlooking  up  SRV  records  for  the  \nfoo.default.svc.cluster.local  domain.  We’ll\nexplain SRV records in section 10.4 and learn how they’re used to discover members\nof a StatefulSet.\nREPLACING LOST PETS\nWhen a pod instance managed by a StatefulSet disappears (because the node the pod\nwas running on has failed, it was evicted from the node, or someone deleted the pod\nobject manually), the StatefulSet makes sure it’s replaced with a new instance—similar\nto how ReplicaSets do it. But in contrast to ReplicaSets, the replacement pod gets the\nsame name and hostname as the pod that has disappeared (this distinction between\nReplicaSets and StatefulSets is illustrated in figure 10.6).\nNode 1Node 2Node 1Node 2\nReplicaSet BReplicaSet B\nStatefulSet\nStatefulSet A\nPod A-0Pod A-1PodA-0PodA-0\nPod A-1\nNode 1 fails\nStatefulSet A\nNode 1Node 2Node 1Node 2\nReplicaSet\nNode 1 fails\nPod B-fdawrPod B-jkbde\nPodB-fdawr\nPodB-rsqkw\nPod B-jkbde\nFigure 10.6   A StatefulSet replaces a lost pod with a new one with the same identity, whereas a \nReplicaSet replaces it with a completely new unrelated pod.\n \n\n287Understanding StatefulSets\nThe  new  pod  isn’t  necessarily  scheduled  to  the  same  node,  but  as  you  learned  early\non, what node a pod runs on shouldn’t matter. This holds true even for stateful pods.\nEven if the pod is scheduled to a different node, it will still be available and reachable\nunder the same hostname as before. \nSCALING A STATEFULSET\nScaling the StatefulSet creates a new pod instance with the next unused ordinal index.\nIf you scale up from two to three instances, the new instance will get index 2 (the exist-\ning instances obviously have indexes 0 and 1). \n  The  nice  thing  about  scaling  down  a  StatefulSet  is  the  fact  that  you  always  know\nwhat pod will be removed. Again, this is also in contrast to scaling down a ReplicaSet,\nwhere  you  have  no  idea  what  instance  will  be  deleted,  and  you  can’t  even  specify\nwhich one you want removed first (but this feature may be introduced in the future).\nScaling down a StatefulSet always removes the instances with the highest ordinal index\nfirst (shown in figure 10.7). This makes the effects of a scale-down predictable.\nBecause  certain  stateful  applications  don’t  handle  rapid  scale-downs  nicely,  Stateful-\nSets scale down only one pod instance at a time. A distributed data store, for example,\nmay lose data if multiple nodes go down at the same time. For example, if a replicated\ndata  store  is  configured  to  store  two  copies  of  each  data  entry,  in  cases  where  two\nnodes go down at the same time, a data entry would be lost if it was stored on exactly\nthose two nodes. If the scale-down was sequential, the distributed data store has time\nto create an additional replica of the data entry somewhere else to replace the (single)\nlost copy.\n For this exact reason, StatefulSets also never permit scale-down operations if any of\nthe instances are unhealthy. If an instance is unhealthy, and you scale down by one at\nthe same time, you’ve effectively lost two cluster members at once.\n10.2.3   Providing stable dedicated storage to each stateful instance\nYou’ve  seen  how  StatefulSets  ensure  stateful  pods  have  a  stable  identity,  but  what\nabout storage? Each stateful pod instance needs to use its own storage, plus if a state-\nful pod is rescheduled (replaced with a new instance but with the same identity as\nbefore), the new instance must have the same storage attached to it. How do Stateful-\nSets achieve this?\nPod\nA-0\nPod\nA-1\nPod\nA-2\nStatefulSet A\nReplicas:3\nPod\nA-0\nPod\nA-1\nPod\nA-2\nStatefulSet A\nReplicas:2\nPod\nA-0\nPod\nA-1\nStatefulSet A\nReplicas:1\nScale downScale down\nFigure 10.7   Scaling down a StatefulSet always removes the pod with the highest ordinal index first.\n \n\n288CHAPTER 10StatefulSets: deploying replicated stateful applications\n Obviously, storage for stateful pods needs to be persistent and decoupled from\nthe pods. In chapter 6 you learned about PersistentVolumes and PersistentVolume-\nClaims,  which  allow  persistent  storage  to  be  attached  to  a  pod  by  referencing  the\nPersistentVolumeClaim in the pod by name. Because PersistentVolumeClaims map\nto PersistentVolumes one-to-one, each pod of a StatefulSet needs to reference a dif-\nferent  PersistentVolumeClaim  to  have  its  own  separate  PersistentVolume.  Because\nall pod instances are stamped from the same pod template, how can they each refer\nto a different PersistentVolumeClaim? And who creates these claims? Surely you’re\nnot expected to create as many PersistentVolumeClaims as the number of pods you\nplan to have in the StatefulSet upfront? Of course not.\nTEAMING UP POD TEMPLATES WITH VOLUME CLAIM TEMPLATES\nThe StatefulSet has to create the PersistentVolumeClaims as well, the same way it’s cre-\nating the pods. For this reason, a StatefulSet can also have one or more volume claim\ntemplates, which enable it to stamp out PersistentVolumeClaims along with each pod\ninstance (see figure 10.8).\nThe PersistentVolumes for the claims can either be provisioned up-front by an admin-\nistrator or just in time through dynamic provisioning of PersistentVolumes, as explained\nat the end of chapter 6. \nUNDERSTANDING THE CREATION AND DELETION OF PERSISTENTVOLUMECLAIMS\nScaling up a StatefulSet by one creates two or more API objects (the pod and one or\nmore PersistentVolumeClaims referenced by the pod). Scaling down, however, deletes\nonly the pod, leaving the claims alone. The reason for this is obvious, if you consider\nwhat happens when a claim is deleted. After a claim is deleted, the PersistentVolume it\nwas bound to gets recycled or deleted and its contents are lost. \n Because stateful pods are meant to run stateful applications, which implies that the\ndata they store in the volume is important, deleting the claim on scale-down of a Stateful-\nSet  could  be  catastrophic—especially  since  triggering  a  scale-down  is  as  simple  as\ndecreasing  the  \nreplicas  field  of  the  StatefulSet.  For  this  reason,  you’re  required  to\ndelete PersistentVolumeClaims manually to release the underlying PersistentVolume.\nPVC A-0PV\nPod A-0\nPVC A-1PV\nPod A-1\nPVC A-2PV\nPod A-2\nStatefulSet A\nPod\ntemplate\nVolume claim\ntemplate\nFigure 10.8   A StatefulSet creates both pods and PersistentVolumeClaims.\n \n\n289Understanding StatefulSets\nREATTACHING THE PERSISTENTVOLUMECLAIM TO THE NEW INSTANCE OF THE SAME POD\nThe  fact  that  the  PersistentVolumeClaim  remains  after  a  scale-down  means  a  subse-\nquent  scale-up  can  reattach  the  same  claim  along  with  the  bound  PersistentVolume\nand  its  contents  to  the  new  pod  instance  (shown  in  figure  10.9).  If  you  accidentally\nscale down a StatefulSet, you can undo the mistake by scaling up again and the new\npod will get the same persisted state again (as well as the same name).\n10.2.4   Understanding StatefulSet guarantees\nAs you’ve seen so far, StatefulSets behave differently from ReplicaSets or Replication-\nControllers.  But  this  doesn’t  end  with  the  pods  having  a  stable  identity  and  storage.\nStatefulSets also have different guarantees regarding their pods. \nUNDERSTANDING THE IMPLICATIONS OF STABLE IDENTITY AND STORAGE\nWhile regular, stateless pods are fungible, stateful pods aren’t. We’ve already seen how\na stateful pod is always replaced with an identical pod (one having the same name and\nhostname, using the same persistent storage, and so on). This happens when Kuber-\nnetes sees that the old pod is no longer there (for example, when you delete the pod\nmanually). \n But what if Kubernetes can’t be sure about  the  state  of  the  pod?  If  it  creates  a\nreplacement pod with the same identity, two instances of the app with the same iden-\ntity might be running in the system. The two would also be bound to the same storage,\nPod\nA-0\nPod\nA-1\nStatefulSet A\nReplicas:2\nScale\ndown\nScale\nup\nNew pod instance created\nwith same identity as before\nPVC is\nre-attached\nPVC\nA-0\nPV\nPVC\nA-1\nPV\nPod\nA-0\nStatefulSet A\nReplicas:1\nPVC\nA-0\nPV\nPVC\nA-1\nPV\nPod\nA-0\nPod has been deleted\nPod\nA-1\nStatefulSet A\nReplicas:2\nPVC\nA-0\nPV\nPVC\nA-1\nPVC has not\nbeen deleted\nPV\nFigure 10.9   StatefulSets don’t delete PersistentVolumeClaims when scaling down; then they \nreattach them when scaling back up.\n \n\n290CHAPTER 10StatefulSets: deploying replicated stateful applications\nso two processes with the same identity would be writing over the same files. With pods\nmanaged by a ReplicaSet, this isn’t a problem, because the apps are obviously made to\nwork on the same files. Also, ReplicaSets create pods with a randomly generated iden-\ntity, so there’s no way for two processes to run with the same identity. \nINTRODUCING STATEFULSET’S AT-MOST-ONE SEMANTICS\nKubernetes  must  thus  take  great  care  to  ensure  two  stateful  pod  instances  are  never\nrunning with the same identity and are bound to the same PersistentVolumeClaim. A\nStatefulSet must guarantee at-most-one semantics for stateful pod instances. \n  This  means  a  StatefulSet  must  be  absolutely  certain  that  a  pod  is  no  longer  run-\nning before it can create a replacement pod. This has a big effect on how node fail-\nures are handled. We’ll demonstrate this later in the chapter. Before we can do that,\nhowever, you need to create a StatefulSet and see how it behaves. You’ll also learn a\nfew more things about them along the way.\n10.3   Using a StatefulSet\nTo  properly  show  StatefulSets  in  action,  you’ll  build  your  own  little  clustered  data\nstore. Nothing fancy—more like a data store from the Stone Age. \n10.3.1   Creating the app and container image\nYou’ll use the kubia app you’ve used throughout the book as your starting point. You’ll\nexpand it so it allows you to store and retrieve a single data entry on each pod instance. \n The important parts of the source code of your data store are shown in the follow-\ning listing.\n...\nconst dataFile = \"/var/data/kubia.txt\";\n...\nvar handler = function(request, response) {\n  if (request.method == 'POST') {                \n    var file = fs.createWriteStream(dataFile);                     \n    file.on('open', function (fd) {                                \n      request.pipe(file);                                          \n      console.log(\"New data has been received and stored.\");       \n      response.writeHead(200);                                     \n      response.end(\"Data stored on pod \" + os.hostname() + \"\\n\");  \n    });\n  } else {                                       \n    var data = fileExists(dataFile)                                \n      ? fs.readFileSync(dataFile, 'utf8')                          \n      : \"No data posted yet\";                                      \n    response.writeHead(200);                                       \n    response.write(\"You've hit \" + os.hostname() + \"\\n\");          \n    response.end(\"Data stored on this pod: \" + data + \"\\n\");       \n  }\n};\nListing 10.1   A simple stateful app: kubia-pet-image/app.js\nOn POST \nrequests, store \nthe request’s \nbody into a \ndata file.\nOn GET (and all \nother types of) \nrequests, return \nyour hostname \nand the contents \nof the data file.\n \n\n291Using a StatefulSet\nvar www = http.createServer(handler);\nwww.listen(8080);\nWhenever the app receives a POST request, it writes the data it receives in the body of\nthe request to the file \n/var/data/kubia.txt. Upon a GET request, it returns the host-\nname and the stored data (contents of the file). Simple enough, right? This is the first\nversion  of  your  app.  It’s  not  clustered  yet,  but  it’s  enough  to  get  you  started.  You’ll\nexpand the app later in the chapter.\n The Dockerfile for building the container image is shown in the following listing\nand hasn’t changed from before.\nFROM node:7\nADD app.js /app.js\nENTRYPOINT [\"node\", \"app.js\"]\nGo ahead and build the image now, or use the one I pushed to docker.io/luksa/kubia-pet.\n10.3.2   Deploying the app through a StatefulSet\nTo deploy your app, you’ll need to create two (or three) different types of objects:\nPersistentVolumes for storing your data files (you’ll need to create these only if\nthe cluster doesn’t support dynamic provisioning of PersistentVolumes).\nA governing Service required by the StatefulSet.\nThe StatefulSet itself.\nFor  each  pod  instance,  the  StatefulSet  will  create  a  PersistentVolumeClaim  that  will\nbind to a PersistentVolume. If your cluster supports dynamic provisioning, you don’t\nneed to create any PersistentVolumes manually (you can skip the next section). If it\ndoesn’t, you’ll need to create them as explained in the next section. \nCREATING THE PERSISTENT VOLUMES\nYou’ll  need  three  PersistentVolumes,  because  you’ll  be  scaling  the  StatefulSet  up  to\nthree  replicas.  You  must  create  more  if  you  plan  on  scaling  the  StatefulSet  up  more\nthan that.\n If you’re using Minikube, deploy the PersistentVolumes defined in the Chapter06/\npersistent-volumes-hostpath.yaml file in the book’s code archive. \n  If  you’re  using  Google  Kubernetes  Engine,  you’ll  first  need  to  create  the  actual\nGCE Persistent Disks like this:\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-a\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-b\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-c\nNOTEMake  sure  to  create  the  disks  in  the  same  zone  that  your  nodes  are\nrunning in.\nListing 10.2   Dockerfile for the stateful app: kubia-pet-image/Dockerfile\n \n\n292CHAPTER 10StatefulSets: deploying replicated stateful applications\nThen  create  the  PersistentVolumes  from  the  persistent-volumes-gcepd.yaml  file,\nwhich is shown in the following listing.\nkind: List                     \napiVersion: v1\nitems:\n- apiVersion: v1\n  kind: PersistentVolume       \n  metadata:\n    name: pv-a                \n  spec:\n    capacity:\n      storage: 1Mi            \n    accessModes:\n      - ReadWriteOnce\n    persistentVolumeReclaimPolicy: Recycle     \n    gcePersistentDisk:         \n      pdName: pv-a             \n      fsType: nfs4                         \n- apiVersion: v1\n  kind: PersistentVolume\n  metadata:\n    name: pv-b\n ...\nNOTEIn  the  previous  chapter  you  specified  multiple  resources  in  the  same\nYAML by delimiting them with a three-dash line. Here you’re using a differ-\nent approach by defining a \nList object and listing the resources as items of\nthe object. Both methods are equivalent.\nThis manifest creates PersistentVolumes called \npv-a, pv-b, and pv-c. They use GCE Per-\nsistent Disks as the underlying storage mechanism, so they’re not appropriate for clus-\nters that aren’t running on Google Kubernetes Engine or Google Compute Engine. If\nyou’re running the cluster elsewhere, you must modify the PersistentVolume definition\nand use an appropriate volume type, such as NFS (Network File System), or similar.\nCREATING THE GOVERNING SERVICE\nAs explained earlier, before deploying a StatefulSet, you first need to create a headless\nService, which will be used to provide the network identity for your stateful pods. The\nfollowing listing shows the Service manifest.\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia       \nspec:\n  clusterIP: None    \nListing 10.3   Three PersistentVolumes: persistent-volumes-gcepd.yaml\nListing 10.4   Headless service to be used in the StatefulSet: kubia-service-headless.yaml\nFile describes a list \nof three persistent \nvolumes\nPersistent volumes’ names \nare pv-a, pv-b, and pv-c\nCapacity of each persistent \nvolume is 1 Mebibyte\nWhen the volume \nis released by the \nclaim, it’s recycled \nto be used again.\nThe volume uses a GCE \nPersistent Disk as the underlying \nstorage mechanism.\nName of the \nService\nThe StatefulSet’s governing \nService must be headless.\n \n\n293Using a StatefulSet\n  selector:           \n    app: kubia        \n  ports:\n  - name: http\n    port: 80\nYou’re setting the clusterIP field to None, which makes this a headless Service. It will\nenable peer discovery between your pods (you’ll need this later). Once you create the\nService, you can move on to creating the actual StatefulSet.\nCREATING THE STATEFULSET MANIFEST\nNow you can finally create the StatefulSet. The following listing shows the manifest.\napiVersion: apps/v1beta1\nkind: StatefulSet\nmetadata:\n  name: kubia\nspec:\n  serviceName: kubia\n  replicas: 2\n  template:\n    metadata:\n      labels:                  \n        app: kubia             \n    spec:\n      containers:\n      - name: kubia\n        image: luksa/kubia-pet\n        ports:\n        - name: http\n          containerPort: 8080\n        volumeMounts:\n        - name: data                  \n          mountPath: /var/data        \n  volumeClaimTemplates:\n  - metadata:                  \n      name: data               \n    spec:                      \n      resources:               \n        requests:              \n          storage: 1Mi         \n      accessModes:             \n      - ReadWriteOnce          \nThe StatefulSet manifest isn’t that different from ReplicaSet or Deployment manifests\nyou’ve created so far. What’s new is the \nvolumeClaimTemplates list. In it, you’re defin-\ning one volume claim template called \ndata, which will be used to create a Persistent-\nVolumeClaim for each pod. As you may remember from chapter 6, a pod references a\nclaim by including a \npersistentVolumeClaim volume in the manifest. In the previous\nListing 10.5   StatefulSet manifest: kubia-statefulset.yaml\nAll pods with the app=kubia \nlabel belong to this service.\nPods created by the StatefulSet \nwill have the app=kubia label.\nThe container inside the pod will \nmount the pvc volume at this path.\nThe PersistentVolumeClaims \nwill be created from this \ntemplate.\n \n\n294CHAPTER 10StatefulSets: deploying replicated stateful applications\npod template, you’ll find no such volume. The StatefulSet adds it to the pod specifica-\ntion automatically and configures the volume to be bound to the claim the StatefulSet\ncreated for the specific pod.\nCREATING THE STATEFULSET\nYou’ll create the StatefulSet now:\n$ kubectl create -f kubia-statefulset.yaml \nstatefulset \"kubia\" created\nNow, list your pods:\n$ kubectl get po\nNAME      READY     STATUS              RESTARTS   AGE\nkubia-0   0/1       ContainerCreating   0          1s\nNotice anything strange? Remember how a ReplicationController or a ReplicaSet cre-\nates  all  the  pod  instances  at  the  same  time?  Your  StatefulSet  is  configured  to  create\ntwo replicas, but it created a single pod. \n Don’t worry, nothing is wrong. The second pod will be created only after the first\none  is  up  and  ready.  StatefulSets  behave  this  way  because  certain  clustered  stateful\napps are sensitive to race conditions if two or more cluster members come up at the\nsame time, so it’s safer to bring each member up fully before continuing to bring up\nthe rest.\n List the pods again to see how the pod creation is progressing:\n$ kubectl get po\nNAME      READY     STATUS              RESTARTS   AGE\nkubia-0   1/1       Running             0          8s\nkubia-1   0/1       ContainerCreating   0          2s\nSee, the first pod is now running, and the second one has been created and is being\nstarted. \nEXAMINING THE GENERATED STATEFUL POD\nLet’s take a closer look at the first pod’s spec in the following listing to see how the\nStatefulSet has constructed the pod from the pod template and the PersistentVolume-\nClaim template.\n$ kubectl get po kubia-0 -o yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  ...\nspec:\n  containers:\n  - image: luksa/kubia-pet\n    ...\nListing 10.6   A stateful pod created by the StatefulSet\n \n\n295Using a StatefulSet\n    volumeMounts:\n    - mountPath: /var/data           \n      name: data                     \n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: default-token-r2m41\n      readOnly: true\n  ...\n  volumes:\n  - name: data                       \n    persistentVolumeClaim:           \n      claimName: data-kubia-0            \n  - name: default-token-r2m41\n    secret:\n      secretName: default-token-r2m41\nThe  PersistentVolumeClaim  template  was  used  to  create  the  PersistentVolumeClaim\nand the volume inside the pod, which refers to the created PersistentVolumeClaim. \nEXAMINING THE GENERATED PERSISTENTVOLUMECLAIMS\nNow list the generated PersistentVolumeClaims to confirm they were created:\n$ kubectl get pvc\nNAME           STATUS    VOLUME    CAPACITY   ACCESSMODES   AGE\ndata-kubia-0   Bound     pv-c      0                        37s\ndata-kubia-1   Bound     pv-a      0                        37s\nThe  names  of  the  generated  PersistentVolumeClaims  are  composed  of  the  name\ndefined in the \nvolumeClaimTemplate and the name of each pod. You can examine the\nclaims’ YAML to see that they match the template.\n10.3.3   Playing with your pods\nWith the nodes of your data store cluster now running, you can start exploring it. You\ncan’t communicate with your pods through the Service you created because it’s head-\nless. You’ll need to connect to individual pods directly (or create a regular Service, but\nthat wouldn’t allow you to talk to a specific pod).\n You’ve already seen ways to connect to a pod directly: by piggybacking on another\npod and running \ncurl inside it, by using port-forwarding, and so on. This time, you’ll\ntry another option. You’ll use the API server as a proxy to the pods. \nCOMMUNICATING WITH PODS THROUGH THE API SERVER\nOne useful feature of the API server is the ability to proxy connections directly to indi-\nvidual pods. If you want to perform requests against your \nkubia-0 pod, you hit the fol-\nlowing URL:\n<apiServerHost>:<port>/api/v1/namespaces/default/pods/kubia-0/proxy/<path>\nBecause the API server is secured, sending requests to pods through the API server is\ncumbersome (among other things, you need to pass the authorization token in each\nrequest).  Luckily,  in  chapter  8  you  learned  how  to  use  \nkubectl proxy  to  talk  to  the\nThe volume mount, as \nspecified in the manifest\nThe volume created \nby the StatefulSet\nThe claim referenced \nby this volume\n \n\n296CHAPTER 10StatefulSets: deploying replicated stateful applications\nAPI  server  without  having  to  deal  with  authentication  and  SSL  certificates.  Run  the\nproxy again:\n$ kubectl proxy\nStarting to serve on 127.0.0.1:8001\nNow, because you’ll be talking to the API server through the kubectl proxy, you’ll use\nlocalhost:8001 rather than the actual API server host and port. You’ll send a request to\nthe \nkubia-0 pod like this:\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\nYou've hit kubia-0\nData stored on this pod: No data posted yet\nThe response shows that the request was indeed received and handled by the app run-\nning in your pod \nkubia-0. \nNOTEIf you receive an empty response, make sure you haven’t left out that\nlast slash character at the end of the URL (or make sure \ncurl follows redirects\nby using its \n-L option). \nBecause  you’re  communicating  with  the  pod  through  the  API  server,  which  you’re\nconnecting  to  through  the  \nkubectl  proxy,  the  request  went  through  two  different\nproxies (the first was the \nkubectl proxy and the other was the API server, which prox-\nied the request to the pod). For a clearer picture, examine figure 10.10.\nThe  request  you  sent  to  the  pod  was  a  GET  request,  but  you  can  also  send  POST\nrequests through the API server. This is done by sending a POST request to the same\nproxy URL as the one you sent the GET request to. \n When your app receives a POST request, it stores whatever’s in the request body\ninto a local file. Send a POST request to the \nkubia-0 pod:\n$ curl -X POST -d \"Hey there! This greeting was submitted to kubia-0.\"\n➥ localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\nData stored on pod kubia-0\nkubectl proxycurl\nGET localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\nGET 192.168.99.100:8443/api/v1/namespaces/default/pods/kubia-0/proxy/\nAuthorization: Bearer <token>\nGET 172.17.0.3:8080/\nAPI server\nPod: kubia-0\n192.168.99.100\n172.17.0.3\nlocalhost\nFigure 10.10   Connecting to a pod through both the kubectl proxy and API server proxy\n \n\n297Using a StatefulSet\nThe data you sent should now be stored in that pod. Let’s see if it returns the stored\ndata when you perform a GET request again:\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\nYou've hit kubia-0\nData stored on this pod: Hey there! This greeting was submitted to kubia-0.\nOkay, so far so good. Now let’s see what the other cluster node (the kubia-1 pod)\nsays:\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/\nYou've hit kubia-1\nData stored on this pod: No data posted yet\nAs expected, each node has its own state. But is that state persisted? Let’s find out.\nDELETING A STATEFUL POD TO SEE IF THE RESCHEDULED POD IS REATTACHED TO THE SAME STORAGE\nYou’re going to delete the kubia-0 pod and wait for it to be rescheduled. Then you’ll\nsee if it’s still serving the same data as before:\n$ kubectl delete po kubia-0\npod \"kubia-0\" deleted\nIf you list the pods, you’ll see that the pod is terminating: \n$ kubectl get po\nNAME      READY     STATUS        RESTARTS   AGE\nkubia-0   1/1       Terminating   0          3m\nkubia-1   1/1       Running       0          3m\nAs soon as it terminates successfully, a new pod with the same name is created by the\nStatefulSet:\n$ kubectl get po\nNAME      READY     STATUS              RESTARTS   AGE\nkubia-0   0/1       ContainerCreating   0          6s\nkubia-1   1/1       Running             0          4m\n$ kubectl get po\nNAME      READY     STATUS    RESTARTS   AGE\nkubia-0   1/1       Running   0          9s\nkubia-1   1/1       Running   0          4m\nLet me remind you again that this new pod may be scheduled to any node in the clus-\nter, not necessarily the same node that the old pod was scheduled to. The old pod’s\nwhole identity (the name, hostname, and the storage) is effectively moved to the new\nnode (as shown in figure 10.11). If you’re using Minikube, you can’t see this because it\nonly runs a single node, but in a multi-node cluster, you may see the pod scheduled to\na different node than before.\n \n\n298CHAPTER 10StatefulSets: deploying replicated stateful applications\nWith the new pod now running, let’s check to see if it has the exact same identity as in\nits  previous  incarnation.  The  pod’s  name  is  the  same,  but  what  about  the  hostname\nand persistent data? You can ask the pod itself to confirm:\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\nYou've hit kubia-0\nData stored on this pod: Hey there! This greeting was submitted to kubia-0.\nThe pod’s response shows that both the hostname and the data are the same as before,\nconfirming that a StatefulSet always replaces a deleted pod with what’s effectively the\nexact same pod. \nSCALING A STATEFULSET\nScaling  down  a  StatefulSet  and  scaling  it  back  up  after  an  extended  time  period\nshould  be  no  different  than  deleting  a  pod  and  having  the  StatefulSet  recreate  it\nimmediately.  Remember  that  scaling  down  a  StatefulSet  only  deletes  the  pods,  but\nleaves the PersistentVolumeClaims untouched. I’ll let you try scaling down the State-\nfulSet yourself and confirm this behavior. \n  The  key  thing  to  remember  is  that  scaling  down  (and  up)  is  performed  gradu-\nally—similar  to  how  individual  pods  are  created  when  the  StatefulSet  is  created  ini-\ntially. When scaling down by more than one instance, the pod with the highest ordinal\nnumber is deleted first. Only after the pod terminates completely is the pod with the\nsecond highest ordinal number deleted. \nEXPOSING STATEFUL PODS THROUGH A REGULAR, NON-HEADLESS SERVICE\nBefore you move on to the last part of this chapter, you’re going to add a proper, non-\nheadless  Service  in  front  of  your  pods,  because  clients  usually  connect  to  the  pods\nthrough a Service rather than connecting directly.\nNode 1\nPod:kubia-0\nPod: kubia-1\nDelete kubia-0\nStorageStorage\nStorage\nPod: kubia-1\nStorage\nNode 1\nkubia-0 rescheduled\nNode 1\nNode 2Node 2Node 2\nStorage\nPod: kubia-1\nStorage\nPod:kubia-0\nFigure 10.11   A stateful pod may be rescheduled to a different node, but it retains the name, hostname, and storage.\n \n\n299Discovering peers in a StatefulSet\n You know how to create the Service by now, but in case you don’t, the following list-\ning shows the manifest.\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia-public\nspec:\n  selector:\n    app: kubia\n  ports:\n  - port: 80\n    targetPort: 8080\nBecause this isn’t an externally exposed Service (it’s a regular ClusterIP Service, not\na \nNodePort  or  a  LoadBalancer-type  Service),  you  can  only  access  it  from  inside  the\ncluster. You’ll need a pod to access it from, right? Not necessarily.\nCONNECTING TO CLUSTER-INTERNAL SERVICES THROUGH THE API SERVER\nInstead of using a piggyback pod to access the service from inside the cluster, you can\nuse  the  same  proxy  feature  provided  by  the  API  server  to  access  the  service  the  way\nyou’ve accessed individual pods.\n The URI path for proxy-ing requests to Services is formed like this:\n/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>\nTherefore, you can run curl on your local machine and access the service through the\nkubectl proxy like this (you ran kubectl proxy earlier and it should still be running):\n$ curl localhost:8001/api/v1/namespaces/default/services/kubia-\n➥ public/proxy/\nYou've hit kubia-1\nData stored on this pod: No data posted yet\nLikewise, clients (inside the cluster) can use the kubia-public service for storing to\nand reading data from your clustered data store. Of course, each request lands on a\nrandom  cluster  node,  so  you’ll  get  the  data from a random node each time. You’ll\nimprove this next.\n10.4   Discovering peers in a StatefulSet\nWe still need to cover one more important thing. An important requirement of clus-\ntered  apps  is  peer  discovery—the  ability  to  find  other  members  of  the  cluster.  Each\nmember of a StatefulSet needs to easily find all the other members. Sure, it could do\nthat by talking to the API server, but one of Kubernetes’ aims is to expose features that\nhelp keep applications completely Kubernetes-agnostic. Having apps talk to the Kuber-\nnetes API is therefore undesirable.\nListing 10.7   A regular Service for accessing the stateful pods: kubia-service-public.yaml\n \n\n300CHAPTER 10StatefulSets: deploying replicated stateful applications\n How can a pod discover its peers without talking to the API? Is there an existing,\nwell-known  technology  you  can  use  that  makes  this  possible?  How  about  the  Domain\nName System (DNS)? Depending on how much you know about DNS, you probably\nunderstand what an A, CNAME, or MX record is used for. Other lesser-known types of\nDNS records also exist. One of them is the SRV record.\nINTRODUCING SRV RECORDS\nSRV records are used to point to hostnames and ports of servers providing a specific\nservice. Kubernetes creates SRV records to point to the hostnames of the pods back-\ning a headless service. \n You’re going to list the SRV records for your stateful pods by running the \ndig DNS\nlookup tool inside a new temporary pod. This is the command you’ll use:\n$ kubectl run -it srvlookup --image=tutum/dnsutils --rm \n➥ --restart=Never -- dig SRV kubia.default.svc.cluster.local\nThe  command  runs  a  one-off  pod  (--restart=Never)  called  srvlookup,  which  is\nattached  to  the  console  (\n-it)  and  is  deleted  as  soon  as  it  terminates  (--rm).  The\npod runs a single container from the \ntutum/dnsutils image and runs the following\ncommand:\ndig SRV kubia.default.svc.cluster.local\nThe following listing shows what the command prints out.\n...\n;; ANSWER SECTION:\nk.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.\nk.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.\n;; ADDITIONAL SECTION:\nkubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4\nkubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6\n...\nNOTEI’ve had to shorten the actual name to get records to fit into a single\nline, so \nkubia.d.s.c.l is actually kubia.default.svc.cluster.local.\nThe \nANSWER SECTION shows two SRV records pointing to the two pods backing your head-\nless service. Each pod also gets its own \nA record, as shown in ADDITIONAL SECTION.\n For a pod to get a list of all the other pods of a StatefulSet, all you need to do is\nperform  an  SRV  DNS  lookup.  In  Node.js,  for  example,  the  lookup  is  performed\nlike this:\ndns.resolveSrv(\"kubia.default.svc.cluster.local\", callBackFunction);\nYou’ll use this command in your app to enable each pod to discover its peers.\nListing 10.8   Listing DNS SRV records of your headless Service\n \n\n301Discovering peers in a StatefulSet\nNOTEThe order of the returned SRV records is random, because they all have\nthe same priority. Don’t expect to always see \nkubia-0 listed before kubia-1.\n10.4.1   Implementing peer discovery through DNS\nYour  Stone  Age  data  store  isn’t  clustered  yet.  Each  data  store  node  runs  completely\nindependently  of  all  the  others—no  communication  exists  between  them.  You’ll  get\nthem talking to each other next.\n  Data  posted  by  clients  connecting  to  your  data  store  cluster  through  the  \nkubia-\npublic\n Service lands on a random cluster node. The cluster can store multiple data\nentries,  but  clients  currently  have  no  good  way  to  see  all  those  entries.  Because  ser-\nvices  forward  requests  to  pods  randomly,  a  client  would  need  to  perform  many\nrequests until it hit all the pods if it wanted to get the data from all the pods. \n  You  can  improve  this  by  having  the  node  respond  with  data  from  all  the  cluster\nnodes. To do this, the node needs to find all its peers. You’re going to use what you\nlearned about StatefulSets and SRV records to do this.\n  You’ll  modify  your  app’s  source  code  as  shown  in  the  following  listing  (the  full\nsource  is  available  in  the  book’s  code  archive;  the  listing  shows  only  the  important\nparts).\n...\nconst dns = require('dns');\nconst dataFile = \"/var/data/kubia.txt\";\nconst serviceName = \"kubia.default.svc.cluster.local\";\nconst port = 8080;\n...\nvar handler = function(request, response) {\n  if (request.method == 'POST') {\n    ...\n  } else {\n    response.writeHead(200);\n    if (request.url == '/data') {\n      var data = fileExists(dataFile) \n        ? fs.readFileSync(dataFile, 'utf8') \n        : \"No data posted yet\";\n      response.end(data);\n    } else {\n      response.write(\"You've hit \" + os.hostname() + \"\\n\");\n      response.write(\"Data stored in the cluster:\\n\");\n      dns.resolveSrv(serviceName, function (err, addresses) {    \n        if (err) {\n          response.end(\"Could not look up DNS SRV records: \" + err);\n          return;\n        }\n        var numResponses = 0;\n        if (addresses.length == 0) {\n          response.end(\"No peers discovered.\");\n        } else {\nListing 10.9   Discovering peers in a sample app: kubia-pet-peers-image/app.js\nThe app \nperforms a DNS \nlookup to obtain \nSRV records.\n \n\n302CHAPTER 10StatefulSets: deploying replicated stateful applications\n          addresses.forEach(function (item) {                   \n            var requestOptions = {\n              host: item.name, \n              port: port, \n              path: '/data'\n            };\n            httpGet(requestOptions, function (returnedData) {   \n              numResponses++;\n              response.write(\"- \" + item.name + \": \" + returnedData);\n              response.write(\"\\n\");\n              if (numResponses == addresses.length) {\n                response.end();\n              }\n            });\n          });\n        }\n      });\n    }\n  }\n};\n...\nFigure  10.12  shows  what  happens  when  a  GET  request  is  received  by  your  app.  The\nserver that receives the request first performs a lookup of SRV records for the head-\nless \nkubia service and then sends a GET request to each of the pods backing the ser-\nvice (even to itself, which obviously isn’t necessary, but I wanted to keep the code as\nsimple as possible). It then returns a list of all the nodes along with the data stored on\neach of them.\nThe container image containing this new version of the app is available at docker.io/\nluksa/kubia-pet-peers\n.\n10.4.2   Updating a StatefulSet\nYour StatefulSet is already running, so let’s see how to update its pod template so the\npods  use  the  new  image.  You’ll  also  set  the  replica  count  to  3  at  the  same  time.  To\nEach pod \npointed to by \nan SRV record is \nthen contacted \nto get its data.\ncurl\nDNS\n1.GET /\n4.GET /data\n5.GET /data\n2. SRV lookup\n6. Return collated data\nkubia-0kubia-1\nkubia-2\n3.GET /data\nFigure 10.12   The operation of your simplistic distributed data store\n \n\n303Discovering peers in a StatefulSet\nupdate the StatefulSet, use the kubectl edit command (the patch command would\nbe another option):\n$ kubectl edit statefulset kubia\nThis opens the StatefulSet definition in your default editor. In the definition, change\nspec.replicas  to  3  and  modify  the  spec.template.spec.containers.image  attri-\nbute so it points to the new image (\nluksa/kubia-pet-peers instead of luksa/kubia-\npet\n). Save the file and exit the editor to update the StatefulSet. Two replicas were\nrunning previously, so you should now see an additional replica called \nkubia-2 start-\ning. List the pods to confirm:\n$ kubectl get po\nNAME      READY     STATUS              RESTARTS   AGE\nkubia-0   1/1       Running             0          25m\nkubia-1   1/1       Running             0          26m\nkubia-2   0/1       ContainerCreating   0          4s\nThe new pod instance is running the new image. But what about the existing two rep-\nlicas? Judging from their age, they don’t seem to have been updated. This is expected,\nbecause initially, StatefulSets were more like ReplicaSets and not like Deployments,\nso  they  don’t  perform  a  rollout  when  the  template  is  modified.  You  need  to  delete\nthe replicas manually and the StatefulSet will bring them up again based on the new\ntemplate:\n$ kubectl delete po kubia-0 kubia-1\npod \"kubia-0\" deleted\npod \"kubia-1\" deleted\nNOTEStarting  from  Kubernetes  version  1.7,  StatefulSets  support  rolling\nupdates the same way Deployments and DaemonSets do. See the StatefulSet’s\nspec.updateStrategy field documentation using kubectl explain for more\ninformation.\n10.4.3   Trying out your clustered data store\nOnce the two pods are up, you can see if your shiny new Stone Age data store works as\nexpected. Post a few requests to the cluster, as shown in the following listing.\n$ curl -X POST -d \"The sun is shining\" \\\n➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/\nData stored on pod kubia-1\n$ curl -X POST -d \"The weather is sweet\" \\\n➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/\nData stored on pod kubia-0\nNow, read the stored data, as shown in the following listing.\nListing 10.10   Writing to the clustered data store through the service\n \n\n304CHAPTER 10StatefulSets: deploying replicated stateful applications\n$ curl localhost:8001/api/v1/namespaces/default/services\n➥ /kubia-public/proxy/\nYou've hit kubia-2\nData stored on each cluster node:\n- kubia-0.kubia.default.svc.cluster.local: The weather is sweet\n- kubia-1.kubia.default.svc.cluster.local: The sun is shining\n- kubia-2.kubia.default.svc.cluster.local: No data posted yet\nNice!  When  a  client  request  reaches  one  of  your  cluster  nodes,  it  discovers  all  its\npeers, gathers data from them, and sends all the data back to the client. Even if you\nscale the StatefulSet up or down, the pod servicing the client’s request can always find\nall the peers running at that time. \n  The  app  itself  isn’t  that  useful,  but  I  hope  you  found  it  a  fun  way  to  show  how\ninstances of a replicated stateful app can discover their peers and handle horizontal\nscaling with ease.\n10.5   Understanding how StatefulSets deal with node \nfailures\nIn  section  10.2.4  we  stated  that  Kubernetes  must  be  absolutely  sure  that  a  stateful\npod  is  no  longer  running  before  creating  its  replacement.  When  a  node  fails\nabruptly,  Kubernetes  can’t  know  the  state  of  the  node  or  its  pods.  It  can’t  know\nwhether the pods are no longer running, or if they still are and are possibly even still\nreachable, and it’s only the Kubelet that has stopped reporting the node’s state to\nthe master.\n  Because  a  StatefulSet  guarantees  that  there  will  never  be  two  pods  running  with\nthe same identity and storage, when a node appears to have failed, the StatefulSet can-\nnot and should not create a replacement pod until it knows for certain that the pod is\nno longer running. \n It can only know that when the cluster administrator  tells  it  so.  To  do  that,  the\nadmin needs to either delete the pod or delete the whole node (doing so then deletes\nall the pods scheduled to the node).\n  As  your  final  exercise  in  this  chapter,  you’ll  look  at  what  happens  to  StatefulSets\nand their pods when one of the cluster nodes gets disconnected from the network.\n10.5.1   Simulating a node’s disconnection from the network \nAs in chapter 4, you’ll simulate the node disconnecting from the network by shutting\ndown  the  node’s  \neth0  network  interface.  Because  this  example  requires  multiple\nnodes, you can’t run it on Minikube. You’ll use Google Kubernetes Engine instead.\nSHUTTING DOWN THE NODE’S NETWORK ADAPTER\nTo shut down a node’s eth0 interface, you need to ssh into one of the nodes like this:\n$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1\nListing 10.11   Reading from the data store\n \n\n305Understanding how StatefulSets deal with node failures\nThen, inside the node, run the following command:\n$ sudo ifconfig eth0 down\nYour ssh session will stop working, so you’ll need to open another terminal to continue.\nCHECKING THE NODE’S STATUS AS SEEN BY THE KUBERNETES MASTER\nWith  the  node’s  network  interface  down,  the  Kubelet  running  on  the  node  can  no\nlonger contact the Kubernetes API server and let it know that the node and all its pods\nare still running.\n After a while, the control plane will mark the node as \nNotReady. You can see this\nwhen listing nodes, as the following listing shows.\n$ kubectl get node\nNAME                                   STATUS     AGE       VERSION\ngke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2\ngke-kubia-default-pool-32a2cac8-m0g1   NotReady   16m       v1.6.2\ngke-kubia-default-pool-32a2cac8-sgl7   Ready      16m       v1.6.2\nBecause  the  control  plane  is  no  longer  getting  status  updates  from  the  node,  the\nstatus of all pods on that node is \nUnknown. This is shown in the pod list in the follow-\ning listing.\n$ kubectl get po\nNAME      READY     STATUS    RESTARTS   AGE\nkubia-0   1/1       Unknown   0          15m\nkubia-1   1/1       Running   0          14m\nkubia-2   1/1       Running   0          13m\nAs you can see, the kubia-0 pod’s status is no longer known because the pod was (and\nstill is) running on the node whose network interface you shut down.\nUNDERSTANDING WHAT HAPPENS TO PODS WHOSE STATUS IS UNKNOWN\nIf the node were to come back online and report its and its pod statuses again, the pod\nwould again be marked as \nRunning. But if the pod’s status remains unknown for more\nthan a few minutes (this time is configurable), the pod is automatically evicted from\nthe node. This is done by the master (the Kubernetes control plane). It evicts the pod\nby deleting the pod resource. \n When the Kubelet sees that the pod has been marked for deletion, it starts ter-\nminating the pod. In your case, the Kubelet can no longer reach the master (because\nyou  disconnected  the  node  from  the  network),  which  means  the  pod  will  keep\nrunning.\nListing 10.12   Observing a failed node’s status change to NotReady\nListing 10.13   Observing the pod’s status change after its node becomes NotReady\n \n\n306CHAPTER 10StatefulSets: deploying replicated stateful applications\n Let’s examine the current situation. Use kubectl describe to display details about\nthe \nkubia-0 pod, as shown in the following listing.\n$ kubectl describe po kubia-0\nName:        kubia-0\nNamespace:   default\nNode:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2\n...\nStatus:      Terminating (expires Tue, 23 May 2017 15:06:09 +0200)\nReason:      NodeLost\nMessage:     Node gke-kubia-default-pool-32a2cac8-m0g1 which was \n             running pod kubia-0 is unresponsive\nThe pod is shown as Terminating, with NodeLost listed as the reason for the termina-\ntion. The message says the node is considered lost because it’s unresponsive.\nNOTEWhat’s shown here is the control plane’s view of the world. In reality,\nthe pod’s container is still running perfectly fine. It isn’t terminating at all.\n10.5.2   Deleting the pod manually\nYou know the node isn’t coming back, but you need all three pods running to handle\nclients properly. You need to get the \nkubia-0 pod rescheduled to a healthy node. As\nmentioned earlier, you need to delete the node or the pod manually. \nDELETING THE POD IN THE USUAL WAY\nDelete the pod the way you’ve always deleted pods:\n$ kubectl delete po kubia-0\npod \"kubia-0\" deleted\nAll  done,  right?  By  deleting  the  pod,  the  StatefulSet  should  immediately  create  a\nreplacement  pod,  which  will  get  scheduled  to  one  of  the  remaining  nodes.  List  the\npods again to confirm: \n$ kubectl get po\nNAME      READY     STATUS    RESTARTS   AGE\nkubia-0   1/1       Unknown   0          15m\nkubia-1   1/1       Running   0          14m\nkubia-2   1/1       Running   0          13m\nThat’s strange. You deleted the pod a moment ago and kubectl said it had deleted it.\nWhy is the same pod still there? \nNOTEThe kubia-0 pod in the listing isn’t a new pod with the same name—\nthis  is  clear  by  looking  at  the  \nAGE column. If it were new, its age would be\nmerely a few seconds.\nListing 10.14   Displaying details of the pod with the unknown status\n \n\n307Summary\nUNDERSTANDING WHY THE POD ISN’T DELETED\nThe pod was marked for deletion even before you deleted it. That’s because the con-\ntrol plane itself already deleted it (in order to evict it from the node). \n  If  you  look  at  listing  10.14  again,  you’ll  see  that  the  pod’s  status  is  \nTerminating.\nThe pod was already marked for deletion earlier and will be removed as soon as the\nKubelet on its node notifies the API server that the pod’s containers have terminated.\nBecause the node’s network is down, this will never happen. \nFORCIBLY DELETING THE POD\nThe only thing you can do is tell the API server to delete the pod without waiting for\nthe Kubelet to confirm that the pod is no longer running. You do that like this:\n$ kubectl delete po kubia-0 --force --grace-period 0\nwarning: Immediate deletion does not wait for confirmation that the running \nresource has been terminated. The resource may continue to run on the \ncluster indefinitely.\npod \"kubia-0\" deleted\nYou  need  to  use  both  the  --force  and  --grace-period 0  options.  The  warning  dis-\nplayed by \nkubectl notifies you of what you did. If you list the pods again, you’ll finally\nsee a new \nkubia-0 pod created:\n$ kubectl get po\nNAME          READY     STATUS              RESTARTS   AGE\nkubia-0       0/1       ContainerCreating   0          8s\nkubia-1       1/1       Running             0          20m\nkubia-2       1/1       Running             0          19m\nWARNINGDon’t delete stateful pods forcibly unless you know the node is no\nlonger running or is unreachable (and will remain so forever). \nBefore  continuing,  you  may  want  to  bring  the  node  you  disconnected  back  online.\nYou can do that by restarting the node through the GCE web console or in a terminal\nby issuing the following command:\n$ gcloud compute instances reset <node name>\n10.6   Summary\nThis concludes the chapter on using StatefulSets to deploy stateful apps. This chapter\nhas shown you how to\nGive replicated pods individual storage\nProvide a stable identity to a pod\nCreate a StatefulSet and a corresponding headless governing Service\nScale and update a StatefulSet\nDiscover other members of the StatefulSet through DNS\n \n\n308CHAPTER 10StatefulSets: deploying replicated stateful applications\nConnect to other members through their host names\nForcibly delete stateful pods\nNow that you know the major building blocks you can use to have Kubernetes run and\nmanage your apps, we can look more closely at how it does that. In the next chapter,\nyou’ll learn about the individual components that control the Kubernetes cluster and\nkeep your apps running.\n \n\n309\nUnderstanding\nKubernetes internals\nBy reading this book up to this point, you’ve become familiar with what Kubernetes\nhas  to  offer  and  what  it  does.  But  so  far,  I’ve  intentionally  not  spent  much  time\nexplaining exactly how it does all this because, in my opinion, it makes no sense to\ngo into details of how a system works until you have a good understanding of what\nthe system does. That’s why we haven’t talked about exactly how a pod is scheduled\nor how the various controllers running inside the Controller Manager make deployed\nresources come to life. Because you now know most resources that can be deployed in\nKubernetes, it’s time to dive into how they’re implemented.\nThis chapter covers\nWhat components make up a Kubernetes cluster\nWhat each component does and how it does it\nHow creating a Deployment object results in a \nrunning pod\nWhat a running pod is\nHow the network between pods works\nHow Kubernetes Services work\nHow high-availability is achieved\n \n\n310CHAPTER 11Understanding Kubernetes internals\n11.1   Understanding the architecture\nBefore you look at how Kubernetes does what it does, let’s take a closer look at the\ncomponents that make up a Kubernetes cluster. In chapter 1, you saw that a Kuberne-\ntes cluster is split into two parts:\nThe Kubernetes Control Plane\nThe (worker) nodes\nLet’s look more closely at what these two parts do and what’s running inside them.\nCOMPONENTS OF THE CONTROL PLANE\nThe Control Plane is what controls and makes the whole cluster function. To refresh\nyour memory, the components that make up the Control Plane are\nThe etcd distributed persistent storage\nThe API server\nThe Scheduler\nThe Controller Manager\nThese components store and manage the state of the cluster, but they aren’t what runs\nthe application containers. \nCOMPONENTS RUNNING ON THE WORKER NODES\nThe  task  of  running  your  containers  is  up  to  the  components  running  on  each\nworker node:\nThe Kubelet\nThe Kubernetes Service Proxy (kube-proxy)\nThe Container Runtime (Docker, rkt, or others)\nADD-ON COMPONENTS\nBeside the Control Plane components and the components running on the nodes, a\nfew add-on components are required for the  cluster  to  provide  everything  discussed\nso far. This includes\nThe Kubernetes DNS server\nThe Dashboard\nAn Ingress controller\nHeapster, which we’ll talk about in chapter 14\nThe Container Network Interface network plugin (we’ll explain it later in this\nchapter)\n11.1.1   The distributed nature of Kubernetes components\nThe  previously  mentioned  components  all  run  as  individual  processes.  The  compo-\nnents and their inter-dependencies are shown in figure 11.1.\n \n\n311Understanding the architecture\nTo get all the features Kubernetes provides, all these components need to be running.\nBut several can also perform useful work individually without the other components.\nYou’ll see how as we examine each of them.\nHOW THESE COMPONENTS COMMUNICATE\nKubernetes  system  components  communicate  only  with  the  API  server.  They  don’t\ntalk to each other directly. The API server is the only component that communicates\nwith etcd. None of the other components communicate with etcd directly, but instead\nmodify the cluster state by talking to the API server.\n Connections between the API server and the other components are almost always\ninitiated by the components, as shown in figure 11.1. But the API server does connect\nto the Kubelet when you use \nkubectl to fetch logs, use kubectl attach to connect to\na running container, or use the \nkubectl port-forward command.\nNOTEThe kubectl attach command is similar to kubectl exec, but it attaches\nto  the  main  process  running  in  the  container  instead  of  running  an  addi-\ntional one.\nRUNNING MULTIPLE INSTANCES OF INDIVIDUAL COMPONENTS\nAlthough the components on the worker nodes all need to run on the same node,\nthe components of the Control Plane can easily be split across multiple servers. There\nChecking the status of the Control Plane components\nThe API server exposes an API resource called ComponentStatus, which shows the\nhealth status of each Control Plane component. You can list the components and\ntheir statuses with \nkubectl:\n$ kubectl get componentstatuses\nNAME                 STATUS    MESSAGE              ERROR\nscheduler            Healthy   ok\ncontroller-manager   Healthy   ok\netcd-0               Healthy   {\"health\": \"true\"}\nControl Plane (master node)Worker node(s)\netcd\nAPI server\nkube-proxy\nKubelet\nScheduler\nController\nManager\nController\nRuntime\nFigure 11.1   Kubernetes \ncomponents of the Control \nPlane and the worker nodes\n \n\n312CHAPTER 11Understanding Kubernetes internals\ncan be more than one instance of each Control Plane component running to ensure\nhigh availability. While multiple instances of etcd and API server can be active at the\nsame time and do perform their jobs in parallel, only a single instance of the Sched-\nuler and the Controller Manager may be active at a given time—with the others in\nstandby mode.\nHOW COMPONENTS ARE RUN\nThe Control Plane components, as well as kube-proxy, can either be deployed on the\nsystem directly or they can run as pods (as shown in listing 11.1). You may be surprised\nto hear this, but it will all make sense later when we talk about the Kubelet. \n  The  Kubelet  is  the  only  component  that  always  runs  as  a  regular  system  compo-\nnent, and it’s the Kubelet that then runs all the other components as pods. To run the\nControl Plane components as pods, the Kubelet is also deployed on the master. The\nnext  listing  shows  pods  in  the  \nkube-system  namespace  in  a  cluster  created  with\nkubeadm, which is explained in appendix B.\n$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName \n➥ --sort-by spec.nodeName -n kube-system\nPOD                              NODE\nkube-controller-manager-master   master      \nkube-dns-2334855451-37d9k        master      \netcd-master                      master      \nkube-apiserver-master            master      \nkube-scheduler-master            master      \nkube-flannel-ds-tgj9k            node1      \nkube-proxy-ny3xm                 node1      \nkube-flannel-ds-0eek8            node2      \nkube-proxy-sp362                 node2      \nkube-flannel-ds-r5yf4            node3      \nkube-proxy-og9ac                 node3      \nAs you can see in the listing, all the Control Plane components are running as pods on\nthe  master  node.  There  are  three  worker  nodes,  and  each  one  runs  the  kube-proxy\nand a Flannel pod, which provides the overlay network for the pods (we’ll talk about\nFlannel later). \nTIPAs shown in the listing, you can tell kubectl to display custom columns\nwith the \n-o custom-columns option and sort the resource list with --sort-by.\nNow, let’s look at each of the components up close, starting with the lowest level com-\nponent of the Control Plane—the persistent storage.\n11.1.2   How Kubernetes uses etcd\nAll  the  objects  you’ve  created  throughout  this  book—Pods,  ReplicationControllers,\nServices, Secrets, and so on—need to be stored somewhere in a persistent manner so\ntheir manifests survive API server restarts and failures. For this, Kubernetes uses etcd,\nListing 11.1   Kubernetes components running as pods\netcd, API server, Scheduler, \nController Manager, and \nthe DNS server are running \non the master.\nThe three nodes each run \na Kube Proxy pod and a \nFlannel networking pod.\n \n\n313Understanding the architecture\nwhich  is  a  fast,  distributed,  and  consistent  key-value  store.  Because  it’s  distributed,\nyou can run more than one etcd instance to provide both high availability and bet-\nter performance.\n  The  only  component  that  talks  to  etcd  directly  is  the  Kubernetes  API  server.  All\nother components read and write data to etcd indirectly through the API server. This\nbrings a few benefits, among them a more robust optimistic locking system as well as\nvalidation; and, by abstracting away the actual storage mechanism from all the other\ncomponents, it’s much simpler to replace it in the future. It’s worth emphasizing that\netcd is the only place Kubernetes stores cluster state and metadata.\nHOW RESOURCES ARE STORED IN ETCD\nAs I’m writing this, Kubernetes can use either etcd version 2 or version 3, but version 3\nis now recommended because of improved performance. etcd v2 stores keys in a hier-\narchical  key  space,  which  makes  key-value  pairs  similar  to  files  in  a  file  system.  Each\nkey in etcd is either a directory, which contains other keys, or is a regular key with a\ncorresponding value. etcd v3 doesn’t support directories, but because the key format\nremains  the  same  (keys  can  include  slashes),  you  can  still  think  of  them  as  being\ngrouped  into  directories.  Kubernetes  stores  all  its  data  in  etcd  under  /registry.  The\nfollowing listing shows a list of keys stored under /registry.\n$ etcdctl ls /registry\n/registry/configmaps\n/registry/daemonsets\n/registry/deployments\n/registry/events\n/registry/namespaces\n/registry/pods\n...\nAbout optimistic concurrency control\nOptimistic  concurrency  control  (sometimes  referred  to  as  optimistic  locking)  is  a\nmethod where instead of locking a piece of data and preventing it from being read or\nupdated while the lock is in place, the piece of data includes a version number. Every\ntime the data is updated, the version number increases. When updating the data, the\nversion number is checked to see if it has increased between the time the client read\nthe data and the time it submits the update. If this happens, the update is rejected\nand the client must re-read the new data and try to update it again. \nThe result is that when two clients try to update the same data entry, only the first\none succeeds.\nAll Kubernetes resources include a \nmetadata.resourceVersion field, which clients\nneed to pass back to the API server when updating an object. If the version doesn’t\nmatch the one stored in etcd, the API server rejects the update.\nListing 11.2   Top-level entries stored in etcd by Kubernetes\n \n\n314CHAPTER 11Understanding Kubernetes internals\nYou’ll recognize that these keys correspond to the resource types you learned about in\nthe previous chapters. \nNOTEIf you’re using v3 of the etcd API, you can’t use the ls command to see\nthe contents of a directory. Instead, you can list all keys that start with a given\nprefix with \netcdctl get /registry --prefix=true.\nThe following listing shows the contents of the /registry/pods directory.\n$ etcdctl ls /registry/pods\n/registry/pods/default\n/registry/pods/kube-system\nAs you can infer from the names, these two entries correspond to the default and the\nkube-system  namespaces,  which  means  pods  are  stored  per  namespace.  The  follow-\ning listing shows the entries in the /registry/pods/default directory.\n$ etcdctl ls /registry/pods/default\n/registry/pods/default/kubia-159041347-xk0vc\n/registry/pods/default/kubia-159041347-wt6ga\n/registry/pods/default/kubia-159041347-hp2o5\nEach entry corresponds to an individual pod. These aren’t directories, but key-value\nentries. The following listing shows what’s stored in one of them.\n$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga\n{\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\",\n\"generateName\":\"kubia-159041347-\",\"namespace\":\"default\",\"selfLink\":...\nYou’ll recognize that this is nothing other than a pod definition in JSON format. The\nAPI server stores the complete JSON representation of a resource in etcd. Because of\netcd’s hierarchical key space, you can think of all the stored resources as JSON files in\na filesystem. Simple, right?\nWARNINGPrior  to  Kubernetes  version  1.7,  the  JSON  manifest  of  a  Secret\nresource was also stored like this (it wasn’t encrypted). If someone got direct\naccess  to  etcd,  they  knew  all  your  Secrets.  From  version  1.7,  Secrets  are\nencrypted and thus stored much more securely.\nENSURING THE CONSISTENCY AND VALIDITY OF STORED OBJECTS\nRemember  Google’s  Borg  and  Omega  systems  mentioned  in  chapter  1,  which  are\nwhat Kubernetes is based on? Like Kubernetes, Omega also uses a centralized store to\nhold  the  state  of  the  cluster,  but  in  contrast,  multiple  Control  Plane  components\naccess the store directly. All these components need to make sure they all adhere to\nListing 11.3   Keys in the /registry/pods directory\nListing 11.4   etcd entries for pods in the default namespace\nListing 11.5   An etcd entry representing a pod\n \n\n315Understanding the architecture\nthe same optimistic locking mechanism to handle conflicts properly. A single compo-\nnent not adhering fully to the mechanism may lead to inconsistent data. \n  Kubernetes  improves  this  by  requiring  all  other  Control  Plane  components  to  go\nthrough the API server. This way updates to the cluster state are always consistent, because\nthe optimistic locking mechanism is implemented in a single place, so less chance exists,\nif any, of error. The API server also makes sure that the data written to the store is always\nvalid and that changes to the data are only performed by authorized clients. \nENSURING CONSISTENCY WHEN ETCD IS CLUSTERED\nFor ensuring high availability, you’ll usually run more than a single instance of etcd.\nMultiple  etcd  instances  will  need  to  remain  consistent.  Such  a  distributed  system\nneeds to reach a consensus on what the actual state is. etcd uses the RAFT consensus\nalgorithm to achieve this, which ensures that at any given moment, each node’s state is\neither what the majority of the nodes agrees is the current state or is one of the previ-\nously agreed upon states. \n Clients connecting to different nodes of an etcd cluster will either see the actual\ncurrent state or one of the states from the past (in Kubernetes, the only etcd client is\nthe API server, but there may be multiple instances). \n The consensus algorithm requires a majority (or quorum) for the cluster to progress\nto the next state. As a result, if the cluster splits into two disconnected groups of nodes,\nthe state in the two groups can never diverge, because to transition from the previous\nstate to the new one, there needs to be more than half of the nodes taking part in\nthe  state  change.  If  one  group  contains  the  majority  of  all  nodes,  the  other  one  obvi-\nously doesn’t. The first group can modify the cluster state, whereas the other one can’t.\nWhen  the  two  groups  reconnect,  the  second  group  can  catch  up  with  the  state  in  the\nfirst group (see figure 11.2).\nClients(s)Clients(s)Clients(s)\netcd-0\netcd-1etcd-2\nThe nodes know\nthere are three nodes\nin the etcd cluster.\netcd-0\netcd-1\nThese two nodes know\nthey still have quorum\nand can accept state\nchanges from clients.\netcd-2\nThis node knows it does\nnot have quorum and\nshould therefore not\nallow state changes.\nNetwork\nsplit\nFigure 11.2   In a split-brain scenario, only the side which still has the majority (quorum) accepts \nstate changes.\n \n\n316CHAPTER 11Understanding Kubernetes internals\nWHY THE NUMBER OF ETCD INSTANCES SHOULD BE AN ODD NUMBER\netcd is usually deployed with an odd number of instances. I’m sure you’d like to know\nwhy. Let’s compare having two vs. having one instance. Having two instances requires\nboth instances to be present to have a majority. If either of them fails, the etcd cluster\ncan’t transition to a new state because no majority exists. Having two instances is worse\nthan having only a single instance. By having two, the chance of the whole cluster fail-\ning has increased by 100%, compared to that of a single-node cluster failing. \n The same applies when comparing three vs. four etcd instances. With three instances,\none instance can fail and a majority (of two) still exists. With four instances, you need\nthree nodes for a majority (two aren’t enough). In both three- and four-instance clus-\nters,  only  a  single  instance  may  fail.  But  when  running  four  instances,  if  one  fails,  a\nhigher possibility exists of an additional instance of the three remaining instances fail-\ning (compared to a three-node cluster with one failed node and two remaining nodes).\n Usually, for large clusters, an etcd cluster of five or seven nodes is sufficient. It can\nhandle a two- or a three-node failure, respectively, which suffices in almost all situations. \n11.1.3   What the API server does\nThe  Kubernetes  API  server  is  the  central  component  used  by  all  other  components\nand by clients, such as \nkubectl. It provides a CRUD (Create, Read, Update, Delete)\ninterface  for  querying  and  modifying  the  cluster  state  over  a  RESTful  API.  It  stores\nthat state in etcd.\n In addition to providing a consistent way of storing objects in etcd, it also performs\nvalidation of those objects, so clients can’t store improperly configured objects (which\nthey could if they were writing to the store directly). Along with validation, it also han-\ndles optimistic locking, so changes to an object are never overridden by other clients\nin the event of concurrent updates.\n  One  of  the  API  server’s  clients  is  the  command-line  tool  \nkubectl  you’ve  been\nusing from the beginning of the book. When creating a resource from a JSON file, for\nexample, \nkubectl posts the file’s contents to the API server through an HTTP POST\nrequest.  Figure  11.3  shows  what  happens  inside  the  API  server  when  it  receives  the\nrequest. This is explained in more detail in the next few paragraphs.\nAPI server\netcd\nAuthentication\nplugin 1\nAuthentication\nplugin 2\nAuthentication\nplugin 3\nClient\n()kubectl\nHTTP POST\nrequest\nAuthorization\nplugin 1\nAuthorization\nplugin 2\nAuthorization\nplugin 3\nAdmission\ncontrol plugin 1\nAdmission\ncontrol plugin 2\nAdmission\ncontrol plugin 3\nResource\nvalidation\nFigure 11.3   The operation of the API server\n \n\n317Understanding the architecture\nAUTHENTICATING THE CLIENT WITH AUTHENTICATION PLUGINS\nFirst, the API server needs to authenticate the client sending the request. This is per-\nformed by one or more authentication plugins configured in the API server. The API\nserver  calls  these  plugins  in  turn,  until  one  of  them  determines  who  is  sending  the\nrequest. It does this by inspecting the HTTP request. \n Depending on the authentication method, the user can be extracted from the cli-\nent’s certificate or an HTTP header, such as \nAuthorization, which you used in chap-\nter 8. The plugin extracts the client’s username, user ID, and groups the user belongs\nto. This data is then used in the next stage, which is authorization.\nAUTHORIZING THE CLIENT WITH AUTHORIZATION PLUGINS\nBesides authentication plugins, the API server is also configured to use one or more\nauthorization plugins. Their job is to determine whether the authenticated user can\nperform the requested action on the requested resource. For example, when creating\npods, the API server consults all authorization plugins in turn, to determine whether\nthe user can create pods in the requested namespace. As soon as a plugin says the user\ncan perform the action, the API server progresses to the next stage.\nVALIDATING AND/OR MODIFYING THE RESOURCE IN THE REQUEST WITH ADMISSION CONTROL PLUGINS\nIf  the  request  is  trying  to  create,  modify,  or  delete  a  resource,  the  request  is  sent\nthrough Admission Control. Again, the server is configured with multiple Admission\nControl  plugins.  These  plugins  can  modify  the  resource  for  different  reasons.  They\nmay initialize fields missing from the resource specification to the configured default\nvalues  or  even  override  them.  They  may  even  modify  other  related  resources,  which\naren’t in the request, and can also reject a request for whatever reason. The resource\npasses through all Admission Control plugins.\nNOTEWhen  the  request  is  only  trying  to  read  data,  the  request  doesn’t  go\nthrough the Admission Control.\nExamples of Admission Control plugins include\nAlwaysPullImages—Overrides  the  pod’s  imagePullPolicy  to  Always,  forcing\nthe image to be pulled every time the pod is deployed.\nServiceAccount—Applies the default service account to pods that don’t specify\nit explicitly.\nNamespaceLifecycle—Prevents creation of pods in namespaces that are in the\nprocess of being deleted, as well as in non-existing namespaces.\nResourceQuota—Ensures  pods  in  a  certain  namespace  only  use  as  much  CPU\nand memory as has been allotted to the namespace. We’ll learn more about this\nin chapter 14.\nYou’ll find a list of additional Admission Control plugins in the Kubernetes documen-\ntation at https://kubernetes.io/docs/admin/admission-controllers/.\n \n\n318CHAPTER 11Understanding Kubernetes internals\nVALIDATING THE RESOURCE AND STORING IT PERSISTENTLY\nAfter letting the request pass through all the Admission Control plugins, the API server\nthen validates the object, stores it in etcd, and returns a response to the client.\n11.1.4   Understanding how the API server notifies clients of resource \nchanges\nThe API server doesn’t do anything else except what we’ve discussed. For example, it\ndoesn’t create pods when you create a ReplicaSet resource and it doesn’t manage the\nendpoints of a service. That’s what controllers in the Controller Manager do. \n  But  the  API  server  doesn’t  even  tell  these  controllers  what  to  do.  All  it  does  is\nenable  those  controllers  and  other  components  to  observe  changes  to  deployed\nresources. A Control Plane component can request to be notified when a resource is\ncreated, modified, or deleted. This enables the component to perform whatever task\nit needs in response to a change of the cluster metadata.\n  Clients  watch  for  changes  by  opening  an  HTTP  connection  to  the  API  server.\nThrough this connection, the client will then receive a stream of modifications to the\nwatched objects. Every time an object is updated, the server sends the new version of\nthe object to all connected clients watching the object. Figure 11.4 shows how clients\ncan watch for changes to pods and how a change to one of the pods is stored into etcd\nand then relayed to all clients watching pods at that moment.\nOne  of  the  API  server’s  clients  is  the  \nkubectl  tool,  which  also  supports  watching\nresources. For example, when deploying a pod, you don’t need to constantly poll the list\nof  pods  by  repeatedly  executing  \nkubectl get pods.  Instead,  you  can  use  the  --watch\nflag and be notified of each creation, modification, or deletion of a pod, as shown in\nthe following listing.\n$ kubectl get pods --watch\nNAME                    READY     STATUS              RESTARTS   AGE\nListing 11.6   Watching a pod being created and then deleted\nVarious\nclients\nkubectl\nAPI server\n1.GET /.../pods?watch=true\n2.POST /.../pods/pod-xyz\n5. Send updated object\nto all watchers\n3. Update object\nin etcd\n4. Modification\nnotification\netcd\nFigure 11.4   When an object is updated, the API server sends the updated object to all interested \nwatchers.\n \n\n319Understanding the architecture\nkubia-159041347-14j3i   0/1       Pending             0          0s\nkubia-159041347-14j3i   0/1       Pending             0          0s\nkubia-159041347-14j3i   0/1       ContainerCreating   0          1s\nkubia-159041347-14j3i   0/1       Running             0          3s\nkubia-159041347-14j3i   1/1       Running             0          5s\nkubia-159041347-14j3i   1/1       Terminating         0          9s\nkubia-159041347-14j3i   0/1       Terminating         0          17s\nkubia-159041347-14j3i   0/1       Terminating         0          17s\nkubia-159041347-14j3i   0/1       Terminating         0          17s\nYou can even have kubectl print out the whole YAML on each watch event like this:\n$ kubectl get pods -o yaml --watch\nThe watch mechanism is also used by the Scheduler, which is the next Control Plane\ncomponent you’re going to learn more about.\n11.1.5   Understanding the Scheduler\nYou’ve already learned that you don’t usually specify which cluster node a pod should\nrun on. This is left to the Scheduler. From afar, the operation of the Scheduler looks\nsimple. All it does is wait for newly created pods through the API server’s watch mech-\nanism and assign a node to each new pod that doesn’t already have the node set. \n The Scheduler doesn’t instruct the selected node (or the Kubelet running on that\nnode) to run the pod. All the Scheduler does is update the pod definition through the\nAPI server. The API server then notifies the Kubelet (again, through the watch mech-\nanism described previously) that the pod has been scheduled. As soon as the Kubelet\non the target node sees the pod has been scheduled to its node, it creates and runs the\npod’s containers.\n Although a coarse-grained view of the scheduling process seems trivial, the actual\ntask  of  selecting  the  best  node  for  the  pod  isn’t  that  simple.  Sure,  the  simplest\nScheduler could pick a random node and not care about the pods already running on\nthat node. On the other side of the spectrum, the Scheduler could use advanced tech-\nniques  such  as  machine  learning  to  anticipate  what  kind  of  pods  are  about  to  be\nscheduled in the next minutes or hours and schedule pods to maximize future hard-\nware  utilization  without  requiring  any  rescheduling  of  existing  pods.  Kubernetes’\ndefault Scheduler falls somewhere in between. \nUNDERSTANDING THE DEFAULT SCHEDULING ALGORITHM\nThe selection of a node can be broken down into two parts, as shown in figure 11.5:\nFiltering the list of all nodes to obtain a list of acceptable nodes the pod can be\nscheduled to.\nPrioritizing the acceptable nodes and choosing the best one. If multiple nodes\nhave the highest score, round-robin is used to ensure pods are deployed across\nall of them evenly.\n \n\n320CHAPTER 11Understanding Kubernetes internals\nFINDING ACCEPTABLE NODES\nTo  determine  which  nodes  are  acceptable  for  the  pod,  the  Scheduler  passes  each\nnode  through  a  list  of  configured  predicate  functions.  These  check  various  things\nsuch as\nCan the node fulfill the pod’s requests for hardware resources? You’ll learn how\nto specify them in chapter 14.\nIs the node running out of resources (is it reporting a memory or a disk pres-\nsure condition)? \nIf the pod requests to be scheduled to a specific node (by name), is this the node?\nDoes the node have a label that matches the node selector in the pod specifica-\ntion (if one is defined)?\nIf the pod requests to be bound to a specific host port (discussed in chapter 13),\nis that port already taken on this node or not? \nIf the pod requests a certain type of volume, can this volume be mounted for\nthis pod on this node, or is another pod on the node already using the same\nvolume?\nDoes the pod tolerate the taints of the node? Taints and tolerations are explained\nin chapter 16.\nDoes  the  pod  specify  node  and/or  pod  affinity  or  anti-affinity  rules?  If  yes,\nwould scheduling the pod to this node break those rules? This is also explained\nin chapter 16.\nAll these checks must pass for the node to be eligible to host the pod. After perform-\ning these checks on every node, the Scheduler ends up with a subset of the nodes. Any\nof these nodes could run the pod, because they have enough available resources for\nthe pod and conform to all requirements you’ve specified in the pod definition.\nSELECTING THE BEST NODE FOR THE POD\nEven though all these nodes are acceptable and can run the pod, several may be a\nbetter choice than others. Suppose you have a two-node cluster. Both nodes are eli-\ngible, but one is already running 10 pods, while the other, for whatever reason, isn’t\nrunning  any  pods  right  now.  It’s  obvious  the  Scheduler  should  favor  the  second\nnode in this case. \nNode 1Node 2\nNode 3Node 4\nNode 5...\nFind acceptable\nnodes\nNode 1Node 2\nNode 3Node 4\nNode 5...\nPrioritize nodes\nand select the\ntop one\nNode 3\nNode 1\nNode 4\nFigure 11.5   The Scheduler finds acceptable nodes for a pod and then selects the best node \nfor the pod.\n \n\n321Understanding the architecture\n Or is it? If these two nodes are provided by the cloud infrastructure, it may be bet-\nter to schedule the pod to the first node and relinquish the second node back to the\ncloud provider to save money. \nADVANCED SCHEDULING OF PODS\nConsider another example. Imagine having multiple replicas of a pod. Ideally, you’d\nwant them spread across as many nodes as possible instead of having them all sched-\nuled  to  a  single  one.  Failure  of  that  node  would  cause  the  service  backed  by  those\npods to become unavailable. But if the pods were spread across different nodes, a sin-\ngle node failure would barely leave a dent in the service’s capacity. \n Pods belonging to the same Service or ReplicaSet are spread across multiple nodes\nby default. It’s not guaranteed that this is always the case. But you can force pods to be\nspread  around  the  cluster  or  kept  close  together  by  defining  pod  affinity  and  anti-\naffinity rules, which are explained in chapter 16. \n  Even  these  two  simple  cases  show  how  complex  scheduling  can  be,  because  it\ndepends on a multitude of factors. Because of this, the Scheduler can either be config-\nured  to  suit  your  specific  needs  or  infrastructure  specifics,  or  it  can  even  be  replaced\nwith  a  custom  implementation  altogether.  You  could  also  run  a  Kubernetes  cluster\nwithout a Scheduler, but then you’d have to perform the scheduling manually.\nUSING MULTIPLE SCHEDULERS\nInstead of running a single Scheduler in the cluster, you can run multiple Schedulers.\nThen,  for  each  pod,  you  specify  the  Scheduler  that  should  schedule  this  particular\npod by setting the \nschedulerName property in the pod spec.\n Pods without this property set are scheduled using the default Scheduler, and so\nare pods with \nschedulerName set to default-scheduler. All other pods are ignored by\nthe default Scheduler, so they need to be scheduled either manually or by another\nScheduler watching for such pods. \n You can implement your own Schedulers and deploy them in the cluster, or you\ncan deploy an additional instance of Kubernetes’ Scheduler with different configura-\ntion options.\n11.1.6   Introducing the controllers running in the Controller Manager\nAs previously mentioned, the API server doesn’t do anything except store resources in\netcd  and  notify  clients  about  the  change.  The  Scheduler  only  assigns  a  node  to  the\npod, so you need other active components to make sure the actual state of the system\nconverges toward the desired state, as specified in the resources deployed through the\nAPI server. This work is done by controllers running inside the Controller Manager. \n The single Controller Manager process currently combines a multitude of control-\nlers performing various reconciliation tasks. Eventually those controllers will be split\nup  into  separate  processes,  enabling  you  to  replace  each  one  with  a  custom  imple-\nmentation if necessary. The list of these controllers includes the\nReplication Manager (a controller for ReplicationController resources)\nReplicaSet, DaemonSet, and Job controllers\n \n\n322CHAPTER 11Understanding Kubernetes internals\nDeployment controller\nStatefulSet controller\nNode controller\nService controller\nEndpoints controller\nNamespace controller\nPersistentVolume controller\nOthers\nWhat each of these controllers does should be evident from its name. From the list,\nyou can tell there’s a controller for almost  every  resource  you  can  create.  Resources\nare descriptions of what should be running in the cluster, whereas the controllers are\nthe active Kubernetes components that perform actual work as a result of the deployed\nresources.\nUNDERSTANDING WHAT CONTROLLERS DO AND HOW THEY DO IT\nControllers do many different things, but they all watch the API server for changes to\nresources (Deployments, Services, and so on) and perform operations for each change,\nwhether it’s a creation of a new object or an update or deletion of an existing object.\nMost of the time, these operations include creating other resources or updating the\nwatched resources themselves (to update the object’s \nstatus, for example).\n In general, controllers run a reconciliation loop, which reconciles the actual state\nwith  the  desired  state  (specified  in  the  resource’s  \nspec  section)  and  writes  the  new\nactual state to the resource’s \nstatus section. Controllers use the watch mechanism to\nbe  notified  of  changes,  but  because  using  watches  doesn’t  guarantee  the  controller\nwon’t  miss  an  event,  they  also  perform  a  re-list  operation  periodically  to  make  sure\nthey haven’t missed anything.\n Controllers never talk to each other directly. They don’t even know any other con-\ntrollers  exist.  Each  controller  connects  to  the  API  server  and,  through  the  watch\nmechanism  described  in  section  11.1.3,  asks  to  be  notified  when  a  change  occurs  in\nthe list of resources of any type the controller is responsible for. \n We’ll briefly look at what each of the controllers does, but if you’d like an in-depth\nview  of  what  they  do,  I  suggest  you  look  at  their  source  code  directly.  The  sidebar\nexplains how to get started.\nA few pointers on exploring the controllers’ source code\nIf you’re interested in seeing exactly how these controllers operate, I strongly encour-\nage you to browse through their source code. To make it easier, here are a few tips:\nThe source code for the controllers is available at https://github.com/kubernetes/\nkubernetes/blob/master/pkg/controller.\nEach controller usually has a constructor in which it creates an \nInformer, which is\nbasically a listener that gets called every time an API object gets updated. Usually,\n \n\n323Understanding the architecture\nTHE REPLICATION MANAGER\nThe controller that makes ReplicationController resources come to life is called the\nReplication Manager. We talked about how ReplicationControllers work in chapter 4.\nIt’s not the ReplicationControllers that do the actual work, but the Replication Man-\nager. Let’s quickly review what the controller does, because this will help you under-\nstand the rest of the controllers.\n In chapter 4, we said that the operation  of  a  ReplicationController  could  be\nthought of as an infinite loop, where in each iteration, the controller finds the num-\nber of pods matching its pod selector and compares the number to the desired replica\ncount. \n  Now  that  you  know  how  the  API  server  can  notify  clients  through  the  watch\nmechanism, it’s clear that the controller doesn’t poll the pods in every iteration, but\nis  instead  notified  by  the  watch  mechanism  of  each  change  that  may  affect  the\ndesired  replica  count  or  the  number  of  matched  pods  (see  figure  11.6).  Any  such\nchanges trigger the controller to recheck the desired vs. actual replica count and act\naccordingly.\n  You  already  know  that  when  too  few  pod  instances  are  running,  the  Replication-\nController runs additional instances. But it doesn’t actually run them itself. It creates\nan Informer listens for changes to a specific type of resource. Looking at the con-\nstructor will show you which resources the controller is watching.\nNext, go look for the \nworker() method. In it, you’ll find the method that gets invoked\neach time the controller needs to do something. The actual function is often stored\nin a field called \nsyncHandler or something similar. This field is also initialized in the\nconstructor, so that’s where you’ll find the name of the function that gets called. That\nfunction is the place where all the magic happens.\nController Manager\nWatches\nCreates and\ndeletes\nReplication\nManager\nAPI server\nReplicationController\nresources\nPod resources\nOther resources\nFigure 11.6   The Replication Manager watches for changes to API \nobjects.\n \n\n324CHAPTER 11Understanding Kubernetes internals\nnew  Pod  manifests,  posts  them  to  the  API  server,  and  lets  the  Scheduler  and  the\nKubelet do their job of scheduling and running the pod.\n  The  Replication  Manager  performs  its  work  by  manipulating  Pod  API  objects\nthrough the API server. This is how all controllers operate.\nTHE REPLICASET, THE DAEMONSET, AND THE JOB CONTROLLERS\nThe  ReplicaSet  controller  does  almost  the  same  thing  as  the  Replication  Manager\ndescribed  previously,  so  we  don’t  have  much  to  add  here.  The  DaemonSet  and  Job\ncontrollers are similar. They create Pod resources from the pod template defined in\ntheir respective resources. Like the Replication Manager, these controllers don’t run\nthe pods, but post Pod definitions to the API server, letting the Kubelet create their\ncontainers and run them.\nTHE DEPLOYMENT CONTROLLER\nThe Deployment controller takes care of keeping the actual state of a deployment in\nsync with the desired state specified in the corresponding Deployment API object. \n  The  Deployment  controller  performs  a  rollout  of  a  new  version  each  time  a\nDeployment object is modified (if the modification should affect the deployed pods).\nIt does this by creating a ReplicaSet and then appropriately scaling both the old and\nthe new ReplicaSet based on the strategy specified in the Deployment, until all the old\npods have been replaced with new ones. It doesn’t create any pods directly.\nTHE STATEFULSET CONTROLLER\nThe  StatefulSet  controller,  similarly  to  the  ReplicaSet  controller  and  other  related\ncontrollers, creates, manages, and deletes Pods according to the spec of a StatefulSet\nresource. But while those other controllers only manage Pods, the StatefulSet control-\nler also instantiates and manages PersistentVolumeClaims for each Pod instance.\nTHE NODE CONTROLLER\nThe Node controller manages the Node resources, which describe the cluster’s worker\nnodes. Among other things, a Node controller keeps the list of Node objects in sync\nwith the actual list of machines running in the cluster. It also monitors each node’s\nhealth and evicts pods from unreachable nodes.\n The Node controller isn’t the only component making changes to Node objects.\nThey’re  also  changed  by  the  Kubelet,  and  can  obviously  also  be  modified  by  users\nthrough REST API calls. \nTHE SERVICE CONTROLLER\nIn  chapter  5,  when  we  talked  about  Services,  you  learned  that  a  few  different  types\nexist. One of them was the \nLoadBalancer service, which requests a load balancer from\nthe  infrastructure  to  make  the  service  available  externally.  The  Service  controller  is\nthe  one  requesting  and  releasing  a  load  balancer  from  the  infrastructure,  when  a\nLoadBalancer-type Service is created or deleted.\n \n\n325Understanding the architecture\nTHE ENDPOINTS CONTROLLER\nYou’ll remember that Services aren’t linked directly to pods, but instead contain a list\nof endpoints (IPs and ports), which is created and updated either manually or auto-\nmatically  according  to  the  pod  selector  defined  on  the  Service.  The  Endpoints  con-\ntroller  is  the  active  component  that  keeps  the  endpoint  list  constantly  updated  with\nthe IPs and ports of pods matching the label selector.\n  As  figure  11.7  shows,  the  controller  watches  both  Services  and  Pods.  When\nServices are added or updated or Pods are added, updated, or deleted, it selects Pods\nmatching  the  Service’s  pod  selector  and  adds  their  IPs  and  ports  to  the  Endpoints\nresource.  Remember,  the  Endpoints  object  is  a  standalone  object,  so  the  controller\ncreates it if necessary. Likewise, it also deletes the Endpoints object when the Service is\ndeleted.\nTHE NAMESPACE CONTROLLER\nRemember namespaces (we talked about them in chapter 3)? Most resources belong\nto a specific namespace. When a Namespace resource is deleted, all the resources in\nthat  namespace  must  also  be  deleted.  This  is  what  the  Namespace  controller  does.\nWhen it’s notified of the deletion of a Namespace object, it deletes all the resources\nbelonging to the namespace through the API server\n. \nTHE PERSISTENTVOLUME CONTROLLER\nIn  chapter  6  you  learned  about  PersistentVolumes  and  PersistentVolumeClaims.\nOnce a user creates a PersistentVolumeClaim, Kubernetes must find an appropriate\nPersistentVolume and bind it to the claim. This is performed by the PersistentVolume\ncontroller. \n  When  a  PersistentVolumeClaim  pops  up,  the  controller  finds  the  best  match  for\nthe  claim  by  selecting  the  smallest  PersistentVolume  with  the  access  mode  matching\nthe one requested in the claim and the declared capacity above the capacity requested\nController Manager\nWatches\nCreates, modifies,\nand deletes\nEndpoints\ncontroller\nAPI server\nService resources\nPod resources\nEndpoints resources\nFigure 11.7   The Endpoints controller watches Service and Pod resources, \nand manages Endpoints.\n \n\n326CHAPTER 11Understanding Kubernetes internals\nin  the  claim.  It  does  this  by  keeping  an  ordered  list  of  PersistentVolumes  for  each\naccess mode by ascending capacity and returning the first volume from the list.\n  Then,  when  the  user  deletes  the  PersistentVolumeClaim,  the  volume  is  unbound\nand reclaimed according to the volume’s reclaim policy (left as is, deleted, or emptied).\nCONTROLLER WRAP-UP\nYou should now have a good feel for what each controller does and how controllers\nwork in general. Again, all these controllers operate on the API objects through the\nAPI  server.  They  don’t  communicate  with  the  Kubelets  directly  or  issue  any  kind  of\ninstructions to them. In fact, they don’t even know Kubelets exist. After a controller\nupdates  a  resource  in  the  API  server,  the  Kubelets  and  Kubernetes  Service  Proxies,\nalso oblivious of the controllers’ existence, perform their work, such as spinning up a\npod’s containers and attaching network storage to them, or in the case of services, set-\nting up the actual load balancing across pods. \n  The  Control  Plane  handles  one  part  of  the  operation  of  the  whole  system,  so  to\nfully understand how things unfold in a Kubernetes cluster, you also need to under-\nstand what the Kubelet and the Kubernetes Service Proxy do. We’ll learn that next.\n11.1.7   What the Kubelet does\nIn  contrast  to  all  the  controllers,  which  are  part  of  the  Kubernetes  Control  Plane  and\nrun on the master node(s), the Kubelet and the Service Proxy both run on the worker\nnodes, where the actual pods containers run. What does the Kubelet do exactly?\nUNDERSTANDING THE KUBELET’S JOB\nIn a nutshell, the Kubelet is the component responsible for everything running on a\nworker node. Its initial job is to register the node it’s running on by creating a Node\nresource in the API server. Then it needs to continuously monitor the API server for\nPods that have been scheduled to the node, and start the pod’s containers. It does this\nby telling the configured container runtime (which is Docker, CoreOS’ rkt, or some-\nthing else) to run a container from a specific container image. The Kubelet then con-\nstantly  monitors  running  containers  and  reports  their  status,  events,  and  resource\nconsumption to the API server. \n The Kubelet is also the component that runs the container liveness probes, restart-\ning containers when the probes fail. Lastly, it terminates containers when their Pod is\ndeleted from the API server and notifies the server that the pod has terminated.\nRUNNING STATIC PODS WITHOUT THE API SERVER\nAlthough the Kubelet talks to the Kubernetes API server and gets the pod manifests\nfrom there, it can also run pods based on pod manifest files in a specific local direc-\ntory as shown in figure 11.8. This feature is used to run the containerized versions of\nthe Control Plane components as pods, as you saw in the beginning of the chapter.\n Instead of running Kubernetes system components natively, you can put their pod\nmanifests into the Kubelet’s manifest directory and have the Kubelet run and manage\n \n\n327Understanding the architecture\nthem. You can also use the same method to run your custom system containers, but\ndoing it through a DaemonSet is the recommended method.\n11.1.8   The role of the Kubernetes Service Proxy\nBeside the Kubelet, every worker node also runs the kube-proxy, whose purpose is to\nmake sure clients can connect to the services you define through the Kubernetes API.\nThe kube-proxy makes sure connections to the service IP and port end up at one of\nthe pods backing that service (or other, non-pod service endpoints). When a service is\nbacked by more than one pod, the proxy performs load balancing across those pods. \nWHY IT’S CALLED A PROXY\nThe initial implementation of the kube-proxy was the userspace proxy. It used an\nactual  server  process  to  accept  connections  and  proxy  them  to  the  pods.  To  inter-\ncept  connections  destined  to  the  service  IPs,  the  proxy  configured  \niptables  rules\n(\niptables  is  the  tool  for  managing  the  Linux  kernel’s  packet  filtering  features)  to\nredirect the connections to the proxy server. A rough diagram of the \nuserspace proxy\nmode is shown in figure 11.9.\nContainer Runtime\n(Docker, rkt, ...)\nKubelet\nAPI server\nWorker node\nRuns, monitors,\nand manages\ncontainers\nPod resource\nContainer A\nContainer B\nContainer A\nContainer B\nContainer C\nPod manifest (file)\nLocal manifest directory\nContainer C\nFigure 11.8   The Kubelet runs pods based on pod specs from the API server and a local file directory.\nClient\nkube-proxy\nConfigures:iptables\nredirect through proxy server\niptables\nPod\nFigure 11.9   The userspace proxy mode\n \n\n328CHAPTER 11Understanding Kubernetes internals\nThe  kube-proxy  got  its  name  because  it  was  an  actual  proxy,  but  the  current,  much\nbetter  performing  implementation  only  uses  \niptables  rules  to  redirect  packets  to  a\nrandomly selected backend pod without passing them through an actual proxy server.\nThis mode is called the \niptables proxy mode and is shown in figure 11.10.\nThe major difference between these two modes is whether packets pass through the\nkube-proxy  and  must  be  handled  in  user  space,  or  whether  they’re  handled  only  by\nthe Kernel (in kernel space). This has a major impact on performance. \n  Another  smaller  difference  is  that  the  \nuserspace  proxy  mode  balanced  connec-\ntions  across  pods  in  a  true  round-robin  fashion,  while  the  \niptables  proxy  mode\ndoesn’t—it selects pods randomly. When only a few clients use a service, they may not\nbe spread evenly across pods. For example, if a service has two backing pods but only\nfive or so clients, don’t be surprised if you see four clients connect to pod A and only\none client connect to pod B. With a higher number of clients or pods, this problem\nisn’t so apparent.\n You’ll learn exactly how \niptables proxy mode works in section 11.5. \n11.1.9   Introducing Kubernetes add-ons\nWe’ve now discussed the core components that make a Kubernetes cluster work. But\nin  the  beginning  of  the  chapter,  we  also  listed  a  few  add-ons,  which  although  not\nalways required, enable features such as DNS lookup of Kubernetes services, exposing\nmultiple  HTTP  services  through  a  single  external  IP  address,  the  Kubernetes  web\ndashboard, and so on.\nHOW ADD-ONS ARE DEPLOYED\nThese  components  are  available  as  add-ons  and  are  deployed  as  pods  by  submitting\nYAML manifests to the API server, the way you’ve been doing throughout the book.\nSome of these components are deployed through a Deployment resource or a Repli-\ncationController resource, and some through a DaemonSet. \n  For  example,  as  I’m  writing  this,  in  Minikube,  the  Ingress  controller  and  the\ndashboard add-ons are deployed as ReplicationControllers, as shown in the follow-\ning listing.\n \nClient\nConfigures:iptables\nredirect straight to pod\n(no proxy server in-between)\niptables\nPod\nkube-proxy\nFigure 11.10   The iptables proxy mode\n \n\n329Understanding the architecture\n$ kubectl get rc -n kube-system\nNAME                       DESIRED   CURRENT   READY     AGE\ndefault-http-backend       1         1         1         6d\nkubernetes-dashboard       1         1         1         6d\nnginx-ingress-controller   1         1         1         6d\nThe DNS add-on is deployed as a Deployment, as shown in the following listing.\n$ kubectl get deploy -n kube-system\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nkube-dns   1         1         1            1           6d\nLet’s see how DNS and the Ingress controllers work.\nHOW THE DNS SERVER WORKS\nAll the pods in the cluster are configured to use the cluster’s internal DNS server by\ndefault.  This  allows  pods  to  easily  look up services by name or even the pod’s IP\naddresses in the case of headless services.\n The DNS server pod is exposed through the \nkube-dns service, allowing the pod to\nbe moved around the cluster, like any other pod. The service’s IP address is specified\nas the \nnameserver in the /etc/resolv.conf file inside every container deployed in the\ncluster. The \nkube-dns pod uses the API server’s watch mechanism to observe changes\nto Services and Endpoints and updates its DNS records with every change, allowing its\nclients  to  always  get  (fairly)  up-to-date  DNS  information.  I  say  fairly  because  during\nthe time between the update of the Service or Endpoints resource and the time the\nDNS pod receives the watch notification, the DNS records may be invalid.\nHOW (MOST) INGRESS CONTROLLERS WORK\nUnlike  the  DNS  add-on,  you’ll  find  a  few  different  implementations  of  Ingress  con-\ntrollers, but most of them work in the same way. An Ingress controller runs a reverse\nproxy  server  (like  Nginx,  for  example),  and  keeps  it  configured  according  to  the\nIngress, Service, and Endpoints resources defined in the cluster. The controller thus\nneeds to observe those resources (again, through the watch mechanism) and change\nthe proxy server’s config every time one of them changes. \n Although the Ingress resource’s definition points to a Service, Ingress controllers\nforward  traffic  to  the  service’s  pod  directly  instead  of  going  through  the  service  IP.\nThis affects the preservation of client IPs when external clients connect through the\nIngress controller, which makes them preferred over Services in certain use cases.\nUSING OTHER ADD-ONS\nYou’ve seen how both the DNS server and the Ingress controller add-ons are similar to\nthe controllers running in the Controller Manager, except that they also accept client\nconnections instead of only observing and modifying resources through the API server. \nListing 11.7   Add-ons deployed with ReplicationControllers in Minikube\nListing 11.8   The kube-dns Deployment \n \n\n330CHAPTER 11Understanding Kubernetes internals\n Other add-ons are similar. They all need to observe the cluster state and perform\nthe necessary actions when that changes. We’ll introduce a few other add-ons in this\nand the remaining chapters.\n11.1.10 Bringing it all together\nYou’ve now learned that the whole Kubernetes system is composed of relatively small,\nloosely  coupled  components  with  good  separation  of  concerns.  The  API  server,  the\nScheduler,  the  individual  controllers  running  inside  the  Controller  Manager,  the\nKubelet, and the kube-proxy all work together to keep the actual state of the system\nsynchronized with what you specify as the desired state. \n  For  example,  submitting  a  pod  manifest  to  the  API  server  triggers  a  coordinated\ndance  of  various  Kubernetes  components,  which  eventually  results  in  the  pod’s  con-\ntainers running. You’ll learn how this dance unfolds in the next section. \n11.2   How controllers cooperate\nYou  now  know  about  all  the  components  that  a  Kubernetes  cluster  is  comprised  of.\nNow, to solidify your understanding of how Kubernetes works, let’s go over what hap-\npens when a Pod resource is created. Because you normally don’t create Pods directly,\nyou’re  going  to  create  a  Deployment  resource  instead  and  see  everything  that  must\nhappen for the pod’s containers to be started.\n11.2.1   Understanding which components are involved\nEven  before  you  start  the  whole  process,  the  controllers,  the  Scheduler,  and  the\nKubelet  are  watching  the  API  server  for  changes  to  their  respective  resource  types.\nThis is shown in figure 11.11. The components depicted in the figure will each play a\npart in the process you’re about to trigger. The diagram doesn’t include etcd, because\nit’s  hidden  behind  the  API  server,  and  you  can  think  of  the  API  server  as  the  place\nwhere objects are stored.\nMaster node\nController Manager\nWatches\nDeployment\ncontroller\nScheduler\nReplicaSet\ncontroller\nAPI server\nDeployments\nPods\nReplicaSets\nWatches\nWatches\nNode X\nWatches\nDocker\nKubelet\nFigure 11.11   Kubernetes components watching API objects through the API server\n \n\n331How controllers cooperate\n11.2.2   The chain of events\nImagine you prepared the YAML file containing the Deployment manifest and you’re\nabout to submit it to Kubernetes through \nkubectl. kubectl sends the manifest to the\nKubernetes API server in an HTTP POST request. The API server validates the Deploy-\nment specification, stores it in etcd, and returns a response to \nkubectl. Now a chain\nof events starts to unfold, as shown in figure 11.12.\nTHE DEPLOYMENT CONTROLLER CREATES THE REPLICASET\nAll API server clients watching the list of Deployments through the API server’s watch\nmechanism are notified of the newly created Deployment resource immediately after\nit’s created. One of those clients is the Deployment controller, which, as we discussed\nearlier, is the active component responsible for handling Deployments. \n As you may remember from chapter 9, a Deployment is backed by one or more\nReplicaSets,  which  then  create  the  actual  pods.  As  a  new  Deployment  object  is\ndetected by the Deployment controller, it creates a ReplicaSet for the current speci-\nfication  of  the  Deployment.  This  involves  creating  a  new  ReplicaSet  resource\nthrough the Kubernetes API. The Deployment controller doesn’t deal with individ-\nual pods at all.\nMaster node\nController\nManager\n2. Notification\nthrough watch\n3. Creates\nReplicaSet\n4. Notification\n5. Creates pod\n6. Notification\nthrough watch\n7. Assigns pod to node\n1. Creates Deployment\nresource\nDeployment\ncontroller\nScheduler\nkubectl\nReplicaSet\ncontroller\nAPI server\nDeployment A\nDeployments\nReplicaSets\nPod A\nPods\nReplicaSet A\nNode X\n8. Notification\nthrough watch\n9. Tells Docker to\nrun containers\nDocker\n10. Runs\ncontainers\nContainer(s)\nKubelet\nFigure 11.12   The chain of events that unfolds when a Deployment resource is posted to the API server\n \n\n332CHAPTER 11Understanding Kubernetes internals\nTHE REPLICASET CONTROLLER CREATES THE POD RESOURCES\nThe newly created ReplicaSet is then picked up by the ReplicaSet controller, which\nwatches  for  creations,  modifications,  and  deletions  of  ReplicaSet  resources  in  the\nAPI server. The controller takes into consideration the replica count and pod selec-\ntor  defined  in  the  ReplicaSet  and  verifies  whether  enough  existing  Pods  match\nthe selector.\n  The  controller  then  creates  the  Pod  resources  based  on  the  pod  template  in  the\nReplicaSet (the pod template was copied over from the Deployment when the Deploy-\nment controller created the ReplicaSet). \nTHE SCHEDULER ASSIGNS A NODE TO THE NEWLY CREATED PODS\nThese newly created Pods are now stored in etcd, but they each still lack one import-\nant thing—they don’t have an associated node yet. Their \nnodeName attribute isn’t set.\nThe  Scheduler  watches  for  Pods  like  this,  and  when  it  encounters  one,  chooses  the\nbest  node  for  the  Pod  and  assigns  the  Pod  to  the  node.  The  Pod’s  definition  now\nincludes the name of the node it should be running on.\n Everything so far has been happening in the Kubernetes Control Plane. None of\nthe controllers that have taken part in this whole process have done anything tangible\nexcept update the resources through the API server. \nTHE KUBELET RUNS THE POD’S CONTAINERS\nThe  worker  nodes  haven’t  done  anything  up  to  this  point.  The  pod’s  containers\nhaven’t been started yet. The images for the pod’s containers haven’t even been down-\nloaded yet. \n But with the Pod now scheduled to a specific node, the Kubelet on that node can\nfinally get to work. The Kubelet, watching for changes to Pods on the API server, sees a\nnew Pod scheduled to its node, so it inspects the Pod definition and instructs Docker,\nor whatever container runtime it’s using, to start the pod’s containers. The container\nruntime then runs the containers.\n11.2.3   Observing cluster events\nBoth the Control Plane components and the Kubelet emit events to the API server as\nthey perform these actions. They do this by creating Event resources, which are like\nany  other  Kubernetes  resource.  You’ve  already  seen  events  pertaining  to  specific\nresources every time you used \nkubectl describe to inspect those resources, but you\ncan also retrieve events directly with \nkubectl get events.\n Maybe it’s me, but using \nkubectl get to inspect events is painful, because they’re\nnot  shown  in  proper  temporal  order.  Instead,  if  an  event  occurs  multiple  times,  the\nevent is displayed only once, showing when it was first seen, when it was last seen, and\nthe number of times it occurred. Luckily, watching events with the \n--watch option is\nmuch easier on the eyes and useful for seeing what’s happening in the cluster. \n The following listing shows the events emitted in the process described previously\n(some columns have been removed and the output is edited heavily to make it legible\nin the limited space on the page).\n \n\n333Understanding what a running pod is\n$ kubectl get events --watch\n    NAME             KIND         REASON              SOURCE \n... kubia            Deployment   ScalingReplicaSet   deployment-controller  \n                     \n➥ Scaled up replica set kubia-193 to 3\n... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  \n                     \n➥ Created pod: kubia-193-w7ll2\n... kubia-193-tpg6j  Pod          Scheduled           default-scheduler   \n                     \n➥ Successfully assigned kubia-193-tpg6j to node1\n... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  \n                     \n➥ Created pod: kubia-193-39590\n... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  \n                     \n➥ Created pod: kubia-193-tpg6j\n... kubia-193-39590  Pod          Scheduled           default-scheduler  \n                     \n➥ Successfully assigned kubia-193-39590 to node2\n... kubia-193-w7ll2  Pod          Scheduled           default-scheduler  \n                     \n➥ Successfully assigned kubia-193-w7ll2 to node2\n... kubia-193-tpg6j  Pod          Pulled              kubelet, node1  \n                     \n➥ Container image already present on machine\n... kubia-193-tpg6j  Pod          Created             kubelet, node1  \n                     \n➥ Created container with id 13da752\n... kubia-193-39590  Pod          Pulled              kubelet, node2  \n                     \n➥ Container image already present on machine\n... kubia-193-tpg6j  Pod          Started             kubelet, node1  \n                     \n➥ Started container with id 13da752\n... kubia-193-w7ll2  Pod          Pulled              kubelet, node2  \n                     \n➥ Container image already present on machine\n... kubia-193-39590  Pod          Created             kubelet, node2  \n                     \n➥ Created container with id 8850184\n...\nAs you can see, the SOURCE column shows the controller performing the action, and\nthe \nNAME and KIND columns show the resource the controller is acting on. The REASON\ncolumn  and  the  MESSAGE column (shown in every second line) give more details\nabout what the controller has done.\n11.3   Understanding what a running pod is\nWith the pod now running, let’s look more closely at what a running pod even is. If a\npod  contains  a  single  container,  do  you  think  that  the  Kubelet  just  runs  this  single\ncontainer, or is there more to it?\n You’ve run several pods throughout this book. If you’re the investigative type, you\nmay have already snuck a peek at what exactly Docker ran when you created a pod. If\nnot, let me explain what you’d see.\n Imagine you run a single container pod. Let’s say you create an Nginx pod:\n$ kubectl run nginx --image=nginx\ndeployment \"nginx\" created\nYou  can  now  ssh  into  the  worker  node  running  the  pod  and  inspect  the  list  of  run-\nning Docker containers. I’m using Minikube to test this out, so to \nssh into the single\nListing 11.9   Watching events emitted by the controllers\n \n\n334CHAPTER 11Understanding Kubernetes internals\nnode, I use minikube ssh. If you’re using GKE, you can ssh into a node with gcloud\ncompute\n ssh <node name>.\n Once you’re inside the node, you can list all the running containers with \ndocker\nps\n, as shown in the following listing.\ndocker@minikubeVM:~$ docker ps\nCONTAINER ID   IMAGE                  COMMAND                 CREATED\nc917a6f3c3f7   nginx                  \"nginx -g 'daemon off\"  4 seconds ago \n98b8bf797174   gcr.io/.../pause:3.0   \"/pause\"                7 seconds ago\n...\nNOTEI’ve  removed  irrelevant  information  from  the  previous  listing—this\nincludes both columns and rows. I’ve also removed all the other running con-\ntainers. If you’re trying this out yourself, pay attention to the two containers\nthat were created a few seconds ago. \nAs expected, you see the Nginx container, but also an additional container. Judging\nfrom  the  \nCOMMAND  column,  this  additional  container  isn’t  doing  anything  (the  con-\ntainer’s  command  is  \n\"pause\"). If you look closely, you’ll see that this container was\ncreated a few seconds before the Nginx container. What’s its role?\n  This  pause  container  is  the  container  that  holds  all  the  containers  of  a  pod\ntogether.  Remember  how  all  containers  of  a  pod  share  the  same  network  and  other\nLinux  namespaces?  The  pause  container  is  an  infrastructure  container  whose  sole\npurpose is to hold all these namespaces. All other user-defined containers of the pod\nthen use the namespaces of the pod infrastructure container (see figure 11.13).\nActual application containers may die and get restarted. When such a container starts\nup again, it needs to become part of the same Linux namespaces as before. The infra-\nstructure container makes this possible since its lifecycle is tied to that of the pod—the\ncontainer  runs  from  the  time  the  pod  is  scheduled  until  the  pod  is  deleted.  If  the\ninfrastructure pod is killed in the meantime, the Kubelet recreates it and all the pod’s\ncontainers.\nListing 11.10   Listing running Docker containers\nPod\nContainer A\nContainer A\nPod infrastructure\ncontainer\nContainer B\nContainer B\nUses Linux\nnamespaces from\nUses Linux\nnamespaces from\nFigure 11.13   A two-container pod results in three running containers \nsharing the same Linux namespaces.\n \n\n335Inter-pod networking\n11.4   Inter-pod networking\nBy now, you know that each pod gets its own unique IP address and can communicate\nwith  all  other  pods  through  a  flat,  NAT-less  network.  How  exactly  does  Kubernetes\nachieve this? In short, it doesn’t. The network is set up by the system administrator or\nby a Container Network Interface (CNI) plugin, not by Kubernetes itself. \n11.4.1   What the network must be like\nKubernetes  doesn’t  require  you  to  use  a  specific  networking  technology,  but  it  does\nmandate  that  the  pods  (or  to  be  more  precise,  their  containers)  can  communicate\nwith each other, regardless if they’re running on the same worker node or not. The\nnetwork the pods use to communicate must be such that the IP address a pod sees as\nits own is the exact same address that all other pods see as the IP address of the pod in\nquestion. \n Look at figure 11.14. When pod A connects to (sends a network packet to) pod B,\nthe source IP pod B sees must be the same IP that pod A sees as its own. There should\nbe no network address translation (NAT) performed in between—the packet sent by\npod A must reach pod B with both the source and destination address unchanged.\nThis is important, because it makes networking for applications running inside pods\nsimple  and  exactly  as  if  they  were  running  on  machines  connected  to  the  same  net-\nwork switch. The absence of NAT between pods enables applications running inside\nthem to self-register in other pods. \nNode 1\nPod A\nIP: 10.1.1.1\nsrcIP:10.1.1.1\ndstIP:10.1.2.1\nsrcIP:10.1.1.1\ndstIP:10.1.2.1\nPacket\nNode 2\nPod B\nIP: 10.1.2.1\nsrcIP:10.1.1.1\ndstIP:10.1.2.1\nPacket\nNetwork\nNo NAT (IPs\nare preserved)\nFigure 11.14   Kubernetes mandates pods are connected through a NAT-less \nnetwork.\n \n\n336CHAPTER 11Understanding Kubernetes internals\n For example, say you have a client pod X and pod Y, which provides a kind of noti-\nfication service to all pods that register with it. Pod X connects to pod Y and tells it,\n“Hey, I’m pod X, available at IP 1.2.3.4; please send updates to me at this IP address.”\nThe  pod  providing  the  service  can  connect  to  the  first  pod  by  using  the  received\nIP address. \n The requirement for NAT-less communication between pods also extends to pod-\nto-node  and  node-to-pod  communication.  But  when  a  pod  communicates  with  ser-\nvices out on the internet, the source IP of the packets the pod sends does need to be\nchanged,  because  the  pod’s  IP  is  private.  The  source  IP  of  outbound  packets  is\nchanged to the host worker node’s IP address.\n Building a proper Kubernetes cluster involves setting up the networking according\nto  these  requirements.  There  are  various  methods  and  technologies  available  to  do\nthis, each with its own benefits or drawbacks in a given scenario. Because of this, we’re\nnot  going  to  go  into  specific  technologies.  Instead,  let’s  explain  how  inter-pod  net-\nworking works in general. \n11.4.2   Diving deeper into how networking works\nIn section 11.3, we saw that a pod’s IP address and network namespace are set up and\nheld by the infrastructure container (the pause container). The pod’s containers then\nuse its network namespace. A pod’s network interface is thus whatever is set up in the\ninfrastructure container. Let’s see how the interface is created and how it’s connected\nto the interfaces in all the other pods. Look at figure 11.15. We’ll discuss it next.\nENABLING COMMUNICATION BETWEEN PODS ON THE SAME NODE\nBefore the infrastructure container is started, a virtual Ethernet interface pair (a veth\npair)  is  created  for  the  container.  One  interface  of  the  pair  remains  in  the  host’s\nnamespace  (you’ll  see  it  listed  as  \nvethXXX  when  you  run  ifconfig  on  the  node),\nwhereas  the  other  is  moved  into  the  container’s  network  namespace  and  renamed\neth0.  The  two  virtual  interfaces  are  like  two  ends  of  a  pipe  (or  like  two  network\ndevices connected by an Ethernet cable)—what goes in on one side comes out on the\nother, and vice-versa. \nNode\nPod A\neth0\n10.1.1.1\nveth123\nPod B\neth0\n10.1.1.2\nveth234\nBridge\n10.1.1.0/24\nThis is pod A’s\nveth pair.\nThis is pod B’s\nveth pair.\nFigure 11.15   Pods on a node are \nconnected to the same bridge through \nvirtual Ethernet interface pairs.\n \n\n337Inter-pod networking\n The interface in the host’s network namespace is attached to a network bridge that\nthe  container  runtime  is  configured  to  use.  The  \neth0  interface  in  the  container  is\nassigned an IP address from the bridge’s address range. Anything that an application\nrunning inside the container sends to the \neth0 network interface (the one in the con-\ntainer’s  namespace),  comes  out  at  the  other  veth  interface  in  the  host’s  namespace\nand is sent to the bridge. This means it can be received by any network interface that’s\nconnected to the bridge. \n  If  pod  A  sends  a  network  packet  to  pod  B,  the  packet  first  goes  through  pod  A’s\nveth pair to the bridge and then through pod B’s veth pair. All containers on a node\nare connected to the same bridge, which means they can all communicate with each\nother.  But  to  enable  communication  between  containers  running  on  different  nodes,\nthe bridges on those nodes need to be connected somehow. \nENABLING COMMUNICATION BETWEEN PODS ON DIFFERENT NODES\nYou  have  many  ways  to  connect  bridges  on  different  nodes.  This  can  be  done  with\noverlay or underlay networks or by regular layer 3 routing, which we’ll look at next.\n You know pod IP addresses must be unique across the whole cluster, so the bridges\nacross the nodes must use non-overlapping address ranges to prevent pods on differ-\nent nodes from getting the same IP. In the example shown in figure 11.16, the bridge\non  node  A  is  using  the  10.1.1.0/24  IP  range  and  the  bridge  on  node  B  is  using\n10.1.2.0/24, which ensures no IP address conflicts exist.\n Figure 11.16 shows that to enable communication between pods across two nodes\nwith plain layer 3 networking, the node’s physical network interface needs to be con-\nnected to the bridge as well. Routing tables on node A need to be configured so all\npackets  destined  for  10.1.2.0/24  are  routed  to  node  B,  whereas  node  B’s  routing\ntables need to be configured so packets sent to 10.1.1.0/24 are routed to node A.\n With this type of setup, when a packet is sent by a container on one of the nodes\nto a container on the other node, the packet first goes through the veth pair, then\nNode A\nPod A\nNetwork\neth0\n10.1.1.1\nveth123\nPod B\neth0\n10.1.1.2\nveth234\nBridge\n10.1.1.0/24\neth0\n10.100.0.1\nNode B\nPod C\neth0\n10.1.2.1\nveth345\nPod D\neth0\n10.1.2.2\nveth456\nBridge\n10.1.2.0/24\neth0\n10.100.0.2\nFigure 11.16   For pods on different nodes to communicate, the bridges need to be connected \nsomehow.\n \n\n338CHAPTER 11Understanding Kubernetes internals\nthrough  the  bridge  to  the  node’s  physical  adapter,  then  over  the  wire  to  the  other\nnode’s physical adapter, through the other node’s bridge, and finally through the veth\npair of the destination container.\n This works only when nodes are connected to the same network switch, without\nany  routers  in  between;  otherwise  those  routers  would  drop  the  packets  because\nthey refer to pod IPs, which are private. Sure, the routers in between could be con-\nfigured to route packets between the nodes, but this becomes increasingly difficult\nand error-prone as the number of routers between the nodes increases. Because of\nthis,  it’s  easier  to  use  a  Software  Defined  Network  (SDN),  which  makes  the  nodes\nappear  as  though  they’re  connected  to  the  same  network  switch,  regardless  of  the\nactual  underlying  network  topology,  no  matter  how  complex  it  is.  Packets  sent\nfrom the pod are encapsulated and sent over the network to the node running the\nother pod, where they are de-encapsulated and delivered to the pod in their origi-\nnal form.\n11.4.3   Introducing the Container Network Interface\nTo  make  it  easier  to  connect  containers  into  a  network,  a  project  called  Container\nNetwork Interface (CNI) was started. The CNI allows Kubernetes to be configured to\nuse any CNI plugin that’s out there. These plugins include\nCalico\nFlannel\nRomana\nWeave Net \nAnd others\nWe’re not going to go into the details of these plugins; if you want to learn more about\nthem, refer to https://kubernetes.io/docs/concepts/cluster-administration/addons/.\n Installing a network plugin isn’t difficult. You only need to deploy a YAML con-\ntaining a DaemonSet and a few other supporting resources. This YAML is provided\non each plugin’s project page. As you can imagine, the DaemonSet is used to deploy\na network agent on all cluster nodes. It then ties into the CNI interface on the node,\nbut  be  aware  that  the  Kubelet  needs  to  be  started  with  \n--network-plugin=cni  to\nuse CNI. \n11.5   How services are implemented\nIn chapter 5 you learned about Services, which allow exposing a set of pods at a long-\nlived, stable IP address and port. In order to focus on what Services are meant for and\nhow  they  can  be  used,  we  intentionally  didn’t  go  into  how  they  work.  But  to  truly\nunderstand Services and have a better feel for where to look when things don’t behave\nthe way you expect, you need to understand how they are implemented. \n \n\n339How services are implemented\n11.5.1   Introducing the kube-proxy\nEverything related to Services is handled by the kube-proxy process running on each\nnode.  Initially,  the  kube-proxy  was  an  actual  proxy  waiting  for  connections  and  for\neach  incoming  connection,  opening  a  new  connection  to  one  of  the  pods.  This  was\ncalled  the  \nuserspace proxy mode. Later, a better-performing iptables  proxy  mode\nreplaced it. This is now the default, but you can still configure Kubernetes to use the\nold mode if you want.\n Before we continue, let’s quickly review a few things about Services, which are rele-\nvant for understanding the next few paragraphs.\n  We’ve  learned  that  each  Service  gets  its  own  stable  IP  address  and  port.  Clients\n(usually  pods)  use  the  service  by  connecting  to  this  IP  address  and  port.  The  IP\naddress  is  virtual—it’s  not  assigned  to  any  network  interfaces  and  is  never  listed  as\neither the source or the destination IP address in a network packet when the packet\nleaves the node. A key detail of Services is that they consist of an IP and port pair (or\nmultiple IP and port pairs in the case of multi-port Services), so the service IP by itself\ndoesn’t represent anything. That’s why you can’t ping them. \n11.5.2   How kube-proxy uses iptables\nWhen  a  service  is  created  in  the  API  server,  the  virtual  IP  address  is  assigned  to  it\nimmediately. Soon afterward, the API server notifies all kube-proxy agents running on\nthe worker nodes that a new Service has been created. Then, each kube-proxy makes\nthat service addressable on the node it’s running on. It does this by setting up a few\niptables rules, which make sure each packet destined for the service IP/port pair is\nintercepted and its destination address modified, so the packet is redirected to one of\nthe pods backing the service. \n Besides watching the API server for changes to Services, kube-proxy also watches\nfor  changes  to  Endpoints  objects.  We  talked  about  them  in  chapter  5,  but  let  me\nrefresh your memory, as it’s easy to forget they even exist, because you rarely create\nthem manually. An Endpoints object holds the IP/port pairs of all the pods that back\nthe service (an IP/port pair can also point to something other than a pod). That’s\nwhy  the  kube-proxy  must  also  watch  all  Endpoints  objects.  After  all,  an  Endpoints\nobject  changes  every  time  a  new  backing  pod  is  created  or  deleted,  and  when  the\npod’s readiness status changes or the pod’s labels change and it falls in or out of scope\nof the service. \n Now let’s see how kube-proxy enables clients to connect to those pods through the\nService. This is shown in figure 11.17.\n The figure shows what the \nkube-proxy does and how a packet sent by a client pod\nreaches  one  of  the  pods  backing  the  Service.  Let’s  examine  what  happens  to  the\npacket when it’s sent by the client pod (pod A in the figure). \n The packet’s destination is initially set to the IP and port of the Service (in the\nexample,  the  Service  is  at  172.30.0.1:80).  Before  being  sent  to  the  network,  the\n \n\n340CHAPTER 11Understanding Kubernetes internals\npacket is first handled by node A’s kernel according to the iptables rules set up on\nthe node. \n The kernel checks if the packet matches any of those \niptables rules. One of them\nsays that if any packet has the destination IP equal to 172.30.0.1 and destination port\nequal to 80, the packet’s destination IP and port should be replaced with the IP and\nport of a randomly selected pod. \n  The  packet  in  the  example  matches  that  rule  and  so  its  destination  IP/port  is\nchanged. In the example, pod B2 was randomly selected, so the packet’s destination\nIP is changed to 10.1.2.1 (pod B2’s IP) and the port to 8080 (the target port specified\nin the Service spec). From here on, it’s exactly as if the client pod had sent the packet\nto pod B directly instead of through the service. \n  It’s  slightly  more  complicated  than  that,  but  that’s  the  most  important  part  you\nneed to understand.\n \nNode ANode B\nAPI server\nPod APod B1Pod B2Pod B3\nPacket X\nSource:\n10.1.1.1\nDestination:\n172.30.0.1:80\n10.1.2.1:8080\niptables\nService B\n172.30.0.1:80\nConfigures\niptables\nPacket X\nSource:\n10.1.1.1\nDestination:\n172.30.0.1:80\nkube-proxy\nEndpoints B\nPod A\nIP: 10.1.1.1\nPod B1\nIP: 10.1.1.2\nPod B2\nIP: 10.1.2.1\nPod B3\nIP: 10.1.2.2\nWatches for changes to\nservices and endpoints\nFigure 11.17   Network packets sent to a Service’s virtual IP/port pair are \nmodified and redirected to a randomly selected backend pod.\n \n\n341Running highly available clusters\n11.6   Running highly available clusters\nOne of the reasons for running apps inside Kubernetes is to keep them running with-\nout  interruption  with  no  or  limited  manual  intervention  in  case  of  infrastructure\nfailures. For running services without interruption it’s not only the apps that need to\nbe up all the time, but also the Kubernetes Control Plane components. We’ll look at\nwhat’s involved in achieving high availability next.\n11.6.1   Making your apps highly available\nWhen running apps in Kubernetes, the various controllers make sure your app keeps\nrunning smoothly and at the specified scale even when nodes fail. To ensure your app\nis  highly  available,  you  only  need  to  run  them  through  a  Deployment  resource  and\nconfigure  an  appropriate  number  of  replicas;  everything  else  is  taken  care  of  by\nKubernetes. \nRUNNING MULTIPLE INSTANCES TO REDUCE THE LIKELIHOOD OF DOWNTIME\nThis requires your apps to be horizontally scalable, but even if that’s not the case in\nyour  app,  you  should  still  use  a  Deployment  with  its  replica  count  set  to  one.  If  the\nreplica becomes unavailable, it will be replaced with a new one quickly, although that\ndoesn’t happen instantaneously. It takes time for all the involved controllers to notice\nthat  a  node  has  failed,  create  the  new  pod  replica,  and  start  the  pod’s  containers.\nThere will inevitably be a short period of downtime in between. \nUSING LEADER-ELECTION FOR NON-HORIZONTALLY SCALABLE APPS\nTo  avoid  the  downtime,  you  need  to  run  additional  inactive  replicas  along  with  the\nactive one and use a fast-acting lease or leader-election mechanism to make sure only\none is active. In case you’re unfamiliar with leader election, it’s a way for multiple app\ninstances running in a distributed environment to come to an agreement on which is\nthe  leader.  That  leader  is  either  the  only  one  performing  tasks,  while  all  others  are\nwaiting for the leader to fail and then becoming leaders themselves, or they can all be\nactive, with the leader being the only instance performing writes, while all the others\nare providing read-only access to their data, for example. This ensures two instances\nare never doing the same job, if that would lead to unpredictable system behavior due\nto race conditions.\n The mechanism doesn’t need to be incorporated into the app itself. You can use a\nsidecar  container  that  performs  all  the  leader-election  operations  and  signals  the\nmain container when it should become active. You’ll find an example of leader elec-\ntion in Kubernetes at https://github.com/kubernetes/contrib/tree/master/election.\n  Ensuring  your  apps  are  highly  available  is  relatively  simple,  because  Kubernetes\ntakes care of almost everything. But what if Kubernetes itself fails? What if the servers\nrunning the Kubernetes Control Plane components go down? How are those compo-\nnents made highly available?\n \n\n342CHAPTER 11Understanding Kubernetes internals\n11.6.2   Making Kubernetes Control Plane components highly available\nIn the beginning of this chapter, you learned about the few components that make up\na  Kubernetes  Control  Plane.  To  make  Kubernetes  highly  available,  you  need  to  run\nmultiple master nodes, which run multiple instances of the following components:\netcd, which is the distributed data store where all the API objects are kept\nAPI server\nController Manager, which is the process in which all the controllers run\nScheduler\nWithout going into the actual details of how to install and run these components, let’s\nsee what’s involved in making each of these components highly available. Figure 11.18\nshows an overview of a highly available cluster.\nRUNNING AN ETCD CLUSTER\nBecause etcd was designed as a distributed system, one of its key features is the ability\nto  run  multiple  etcd  instances,  so  making  it  highly  available  is  no  big  deal.  All  you\nneed to do is run it on an appropriate number of machines (three, five, or seven, as\nexplained earlier in the chapter) and make them aware of each other. You do this by\nincluding  the  list  of  all  the  other  instances  in  every  instance’s  configuration.  For\nexample, when starting an instance, you specify the IPs and ports where the other etcd\ninstances can be reached. \n etcd will replicate data across all its instances, so a failure of one of the nodes when\nrunning  a  three-machine  cluster  will  still  allow  the  cluster  to  accept  both  read  and\nwrite operations. To increase the fault tolerance to more than a single node, you need\nto run five or seven etcd nodes, which would allow the cluster to handle two or three\nNode 1\nKubelet\nNode 2\nKubelet\nNode 3\nKubelet\nNode 4\nKubelet\nNode 5\nKubelet\n...\nNode N\nKubelet\nLoad\nbalancer\nMaster 3\netcd\nAPI server\nScheduler\nController\nManager\n[standing-by][standing-by]\nMaster 2\netcd\nAPI server\nScheduler\nController\nManager\n[standing-by][standing-by]\nMaster 1\netcd\nAPI server\nScheduler\nController\nManager\n[active][active]\nFigure 11.18   A highly-available cluster with three master nodes\n \n\n343Running highly available clusters\nnode failures, respectively. Having more than seven etcd instances is almost never nec-\nessary and begins impacting performance.\nRUNNING MULTIPLE INSTANCES OF THE API SERVER\nMaking the API server highly available is even simpler. Because the API server is (almost\ncompletely) stateless (all the data is stored in etcd, but the API server does cache it), you\ncan run as many API servers as you need, and they don’t need to be aware of each other\nat all. Usually, one API server is collocated with every etcd instance. By doing this, the\netcd instances don’t need any kind of load balancer in front of them, because every API\nserver instance only talks to the local etcd instance. \n The API servers, on the other hand, do need to be fronted by a load balancer, so\nclients  (\nkubectl,  but  also  the  Controller  Manager,  Scheduler,  and  all  the  Kubelets)\nalways connect only to the healthy API server instances. \nENSURING HIGH AVAILABILITY OF THE CONTROLLERS AND THE SCHEDULER\nCompared  to  the  API  server,  where  multiple  replicas  can  run  simultaneously,  run-\nning  multiple  instances  of  the  Controller  Manager  or  the  Scheduler  isn’t  as  simple.\nBecause controllers and the Scheduler all actively watch the cluster state and act when\nit changes, possibly modifying the cluster state further (for example, when the desired\nreplica count on a ReplicaSet is increased by one, the ReplicaSet controller creates an\nadditional  pod),  running  multiple  instances  of  each  of  those  components  would\nresult in all of them performing the same action. They’d be racing each other, which\ncould cause undesired effects (creating two new pods instead of one, as mentioned in\nthe previous example).\n  For  this  reason,  when  running  multiple  instances  of  these  components,  only  one\ninstance may be active at any given time. Luckily, this is all taken care of by the compo-\nnents  themselves  (this  is  controlled  with  the  \n--leader-elect  option,  which  defaults  to\ntrue). Each individual component will only be active when it’s the elected leader. Only\nthe leader performs actual work, whereas all other instances are standing by and waiting\nfor the current leader to fail. When it does, the remaining instances elect a new leader,\nwhich then takes over the work. This mechanism ensures that two components are never\noperating at the same time and doing the same work (see figure 11.19).\nMaster 3\nScheduler\nController\nManager\n[standing-by][standing-by]\nMaster 1\nScheduler\nController\nManager\n[active][active]\nMaster 2\nScheduler\nController\nManager\n[standing-by][standing-by]\nOnly the controllers in\nthis Controller Manager\nare reacting to API\nresources being created,\nupdated, and deleted.\nThese Controller Managers\nand Schedulers aren’t doing\nanything except waiting to\nbecome leaders.\nOnly this Scheduler\nis scheduling pods.\nFigure 11.19   Only a single Controller Manager and a single Scheduler are active; others are standing by.\n \n\n344CHAPTER 11Understanding Kubernetes internals\nThe  Controller  Manager  and  Scheduler  can  run  collocated  with  the  API  server  and\netcd,  or  they  can  run  on  separate  machines.  When  collocated,  they  can  talk  to  the\nlocal API server directly; otherwise they connect to the API servers through the load\nbalancer.\nUNDERSTANDING THE LEADER ELECTION MECHANISM USED IN CONTROL PLANE COMPONENTS\nWhat I find most interesting here is that these components don’t need to talk to each\nother directly to elect a leader. The leader election mechanism works purely by creat-\ning a resource in the API server. And it’s not even a special kind of resource—the End-\npoints resource is used to achieve this (abused is probably a more appropriate term).\n  There’s  nothing  special  about  using  an  Endpoints  object  to  do  this.  It’s  used\nbecause  it  has  no  side  effects  as  long  as  no  Service  with  the  same  name  exists.  Any\nother  resource  could  be  used  (in  fact,  the  leader  election  mechanism  will  soon  use\nConfigMaps instead of Endpoints). \n  I’m  sure  you’re  interested  in  how  a  resource  can  be  used  for  this  purpose.  Let’s\ntake the Scheduler, for example. All instances of the Scheduler try to create (and later\nupdate)  an  Endpoints  resource  called  \nkube-scheduler.  You’ll  find  it  in  the  kube-\nsystem\n namespace, as the following listing shows.\n$ kubectl get endpoints kube-scheduler -n kube-system -o yaml\napiVersion: v1\nkind: Endpoints\nmetadata:\n  annotations:\n    control-plane.alpha.kubernetes.io/leader: '{\"holderIdentity\":\n      \n➥ \"minikube\",\"leaseDurationSeconds\":15,\"acquireTime\":\n      \n➥ \"2017-05-27T18:54:53Z\",\"renewTime\":\"2017-05-28T13:07:49Z\",\n      \n➥ \"leaderTransitions\":0}'\n  creationTimestamp: 2017-05-27T18:54:53Z\n  name: kube-scheduler\n  namespace: kube-system\n  resourceVersion: \"654059\"\n  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler\n  uid: f847bd14-430d-11e7-9720-080027f8fa4e\nsubsets: []\nThe control-plane.alpha.kubernetes.io/leader annotation is the important part.\nAs you can see, it contains a field called \nholderIdentity, which holds the name of the\ncurrent leader. The first instance that succeeds in putting its name there becomes\nthe leader. Instances race each other to do that, but there’s always only one winner.\n Remember the optimistic concurrency we explained earlier? That’s what ensures\nthat if multiple instances try to write their name into the resource only one of them\nsucceeds. Based on whether the write succeeded or not, each instance knows whether\nit is or it isn’t the leader. \n Once becoming the leader, it must periodically update the resource (every two sec-\nonds by default), so all other instances know that it’s still alive. When the leader fails,\nListing 11.11   The kube-scheduler Endpoints resource used for leader-election\n \n\n345Summary\nother instances see that the resource hasn’t been updated for a while, and try to become\nthe leader by writing their own name to the resource. Simple, right?\n11.7   Summary\nHopefully, this has been an interesting chapter that has improved your knowledge of\nthe inner workings of Kubernetes. This chapter has shown you\nWhat components make up a Kubernetes cluster and what each component is\nresponsible for\nHow  the  API  server,  Scheduler,  various  controllers  running  in  the  Controller\nManager, and the Kubelet work together to bring a pod to life\nHow the infrastructure container binds together all the containers of a pod\nHow  pods  communicate  with  other  pods  running  on  the  same  node  through\nthe network bridge, and how those bridges on different nodes are connected,\nso pods running on different nodes can talk to each other\nHow the kube-proxy performs load balancing across pods in the same service by\nconfiguring \niptables rules on the node\nHow multiple instances of each component of the Control Plane can be run to\nmake the cluster highly available\nNext, we’ll look at how to secure the API server and, by extension, the cluster as a whole.\n \n\n346\nSecuring the\nKubernetes API server\nIn  chapter  8  you  learned  how  applications  running  in  pods  can  talk  to  the  API\nserver  to  retrieve  or  change  the  state  of  resources  deployed  in  the  cluster.  To\nauthenticate with the API server, you used the ServiceAccount token mounted into\nthe pod. In this chapter, you’ll learn what ServiceAccounts are and how to config-\nure their permissions, as well as permissions for other subjects using the cluster. \n12.1   Understanding authentication\nIn the previous chapter, we said the API server can be configured with one or more\nauthentication  plugins  (and  the  same  is  true  for  authorization  plugins).  When  a\nrequest  is  received  by  the  API  server,  it  goes  through  the  list  of  authentication\nThis chapter covers\nUnderstanding authentication\nWhat ServiceAccounts are and why they’re used\nUnderstanding the role-based access control \n(RBAC) plugin\nUsing Roles and RoleBindings\nUsing ClusterRoles and ClusterRoleBindings\nUnderstanding the default roles and bindings\n \n\n347Understanding authentication\nplugins, so they can each examine the request and try to determine who’s sending the\nrequest.  The  first  plugin  that  can  extract  that  information  from  the  request  returns\nthe  username,  user  ID,  and  the  groups  the  client  belongs  to  back  to  the  API  server\ncore. The API server stops invoking the remaining authentication plugins and contin-\nues onto the authorization phase. \n Several authentication plugins are available. They obtain the identity of the client\nusing the following methods:\nFrom the client certificate\nFrom an authentication token passed in an HTTP header\nBasic HTTP authentication\nOthers\nThe authentication plugins are enabled through command-line options when starting\nthe API server. \n12.1.1   Users and groups\nAn  authentication  plugin  returns  the  username  and  group(s)  of  the  authenticated\nuser. Kubernetes doesn’t store that information anywhere; it uses it to verify whether\nthe user is authorized to perform an action or not.\nUNDERSTANDING USERS\nKubernetes distinguishes between two kinds of clients connecting to the API server:\nActual humans (users)\nPods (more specifically, applications running inside them)\nBoth these types of clients are authenticated using the aforementioned authentication\nplugins. Users are meant to be managed by an external system, such as a Single Sign\nOn (SSO) system, but the pods use a mechanism called service accounts, which are cre-\nated  and  stored  in  the  cluster  as  ServiceAccount  resources.  In  contrast,  no  resource\nrepresents user accounts, which means you can’t create, update, or delete users through\nthe API server. \n We won’t go into any details of how to manage users, but we will explore Service-\nAccounts  in  detail,  because  they’re  essential  for  running  pods.  For  more  informa-\ntion  on  how  to  configure  the  cluster  for  authentication  of  human  users,  cluster\nadministrators should refer to the Kubernetes Cluster Administrator guide at http://\nkubernetes.io/docs/admin.\nUNDERSTANDING GROUPS\nBoth human users and ServiceAccounts can belong to one or more groups. We’ve said\nthat the authentication plugin returns groups along with the username and user ID.\nGroups  are  used  to  grant  permissions  to  several  users  at  once,  instead  of  having  to\ngrant them to individual users. \n \n\n348CHAPTER 12Securing the Kubernetes API server\n  Groups  returned  by  the  plugin  are  nothing  but  strings,  representing  arbitrary\ngroup names, but built-in groups have special meaning:\nThe system:unauthenticated  group  is  used  for  requests  where  none  of  the\nauthentication plugins could authenticate the client.\nThe system:authenticated group is automatically assigned to a user who was\nauthenticated successfully.\nThe system:serviceaccounts  group  encompasses  all  ServiceAccounts  in  the\nsystem.\nThe system:serviceaccounts:<namespace>  includes  all  ServiceAccounts  in  a\nspecific namespace.\n12.1.2   Introducing ServiceAccounts\nLet’s  explore  ServiceAccounts  up  close.  You’ve  already  learned  that  the  API  server\nrequires clients to authenticate themselves before they’re allowed to perform opera-\ntions on the server. And you’ve already seen how pods can authenticate by sending the\ncontents  of  the  file\n/var/run/secrets/kubernetes.io/serviceaccount/token,  which\nis mounted into each container’s filesystem through a \nsecret volume.\n  But  what  exactly  does  that  file  represent?  Every  pod  is  associated  with  a  Service-\nAccount, which represents the identity of the app running in the pod. The token file\nholds the ServiceAccount’s authentication token. When an app uses this token to con-\nnect  to  the  API  server,  the  authentication  plugin  authenticates  the  ServiceAccount\nand  passes  the  ServiceAccount’s  username  back  to  the  API  server  core.  Service-\nAccount usernames are formatted like this:\nsystem:serviceaccount:<namespace>:<service account name>\nThe  API  server  passes  this  username  to  the  configured  authorization  plugins,  which\ndetermine whether the action the app is trying to perform is allowed to be performed\nby the ServiceAccount.\n ServiceAccounts are nothing more than a way for an application running inside a\npod to authenticate itself with the API server. As already mentioned, applications do\nthat by passing the ServiceAccount’s token in the request.\nUNDERSTANDING THE SERVICEACCOUNT RESOURCE\nServiceAccounts are resources just like Pods, Secrets, ConfigMaps, and so on, and are\nscoped  to  individual  namespaces.  A  default  ServiceAccount  is  automatically  created\nfor each namespace (that’s the one your pods have used all along). \n You can list ServiceAccounts like you do other resources:\n$ kubectl get sa\nNAME      SECRETS   AGE\ndefault   1         1d\nNOTEThe shorthand for serviceaccount is sa.\n \n\n349Understanding authentication\nAs you can see, the current namespace only contains the default ServiceAccount. Addi-\ntional ServiceAccounts can be added when required. Each pod is associated with exactly\none  ServiceAccount,  but  multiple  pods  can  use  the  same  ServiceAccount.  As  you  can\nsee in figure 12.1, a pod can only use a ServiceAccount from the same namespace.\nUNDERSTANDING HOW SERVICEACCOUNTS TIE INTO AUTHORIZATION\nYou can assign a ServiceAccount to a pod by specifying the account’s name in the pod\nmanifest. If you don’t assign it explicitly, the pod will use the default ServiceAccount\nin the namespace.\n  By  assigning  different  ServiceAccounts  to  pods,  you  can  control  which  resources\neach pod has access to. When a request bearing the authentication token is received\nby  the  API  server,  the  server  uses  the  token  to  authenticate  the  client  sending  the\nrequest and then determines whether or not the related ServiceAccount is allowed to\nperform  the  requested  operation.  The  API  server  obtains  this  information  from  the\nsystem-wide  authorization  plugin  configured  by  the  cluster  administrator.  One  of\nthe  available  authorization  plugins  is  the  role-based  access  control  (RBAC)  plugin,\nwhich  is  discussed  later  in  this  chapter.  From  Kubernetes  version  1.6  on,  the  RBAC\nplugin is the plugin most clusters should use.\n12.1.3   Creating ServiceAccounts\nWe’ve  said  every  namespace  contains  its  own  default  ServiceAccount,  but  additional\nones  can  be  created  if  necessary.  But  why  should  you  bother  with  creating  Service-\nAccounts instead of using the default one for all your pods? \n  The  obvious  reason  is  cluster  security.  Pods  that  don’t  need  to  read  any  cluster\nmetadata should run under a constrained account that doesn’t allow them to retrieve\nor modify any resources deployed in the cluster. Pods that need to retrieve resource\nmetadata should run under a ServiceAccount that only allows reading those objects’\nmetadata, whereas pods that need to modify those objects should run under their own\nServiceAccount allowing modifications of API objects. \nPod\nNamespace: foo\nService-\nAccount:\ndefault\nPodPod\nNamespace: baz\nPod\nNamespace: bar\nPodPod\nNot possible\nService-\nAccount:\ndefault\nAnother\nService-\nAccount\nMultiple pods using the\nsame ServiceAccount\nFigure 12.1   Each pod is associated with a single ServiceAccount in the pod’s namespace.\n \n\n350CHAPTER 12Securing the Kubernetes API server\n Let’s see how you can create additional ServiceAccounts, how they relate to Secrets,\nand how you can assign them to your pods.\nCREATING A SERVICEACCOUNT\nCreating a ServiceAccount is incredibly easy, thanks to the dedicated kubectl create\nserviceaccount\n command. Let’s create a new ServiceAccount called foo:\n$ kubectl create serviceaccount foo\nserviceaccount \"foo\" created\nNow,  you  can  inspect  the  ServiceAccount  with  the  describe  command,  as  shown  in\nthe following listing.\n$ kubectl describe sa foo\nName:               foo\nNamespace:          default\nLabels:             <none>\nImage pull secrets: <none>             \nMountable secrets:  foo-token-qzq7j    \nTokens:             foo-token-qzq7j    \nYou  can  see  that  a  custom  token  Secret  has  been  created  and  associated  with  the\nServiceAccount. If you look at the Secret’s data with \nkubectl describe secret foo-\ntoken-qzq7j\n, you’ll see it contains the same items (the CA certificate, namespace, and\ntoken)  as  the  default  ServiceAccount’s  token  does  (the  token  itself  will  obviously  be\ndifferent), as shown in the following listing.\n$ kubectl describe secret foo-token-qzq7j\n...\nca.crt:         1066 bytes\nnamespace:      7 bytes\ntoken:          eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\nNOTEYou’ve  probably  heard  of  JSON  Web  Tokens  (JWT).  The  authentica-\ntion tokens used in ServiceAccounts are JWT tokens.\nUNDERSTANDING A SERVICEACCOUNT’S MOUNTABLE SECRETS\nThe token is shown in the Mountable secrets list when you inspect a ServiceAccount\nwith \nkubectl describe.  Let  me  explain  what  that  list  represents.  In  chapter  7  you\nlearned  how  to  create  Secrets  and  mount  them  inside  a  pod.  By  default,  a  pod  can\nmount  any  Secret  it  wants.  But  the  pod’s  ServiceAccount  can  be  configured  to  only\nListing 12.1   Inspecting a ServiceAccount with kubectl describe\nListing 12.2   Inspecting the custom ServiceAccount’s Secret\nThese will be added \nautomatically to all pods \nusing this ServiceAccount.\nPods using this ServiceAccount \ncan only mount these Secrets if \nmountable Secrets are enforced.\nAuthentication token(s). \nThe first one is mounted \ninside the container.\n \n\n351Understanding authentication\nallow  the  pod  to  mount  Secrets  that  are  listed  as  mountable  Secrets  on  the  Service-\nAccount. To enable this feature, the ServiceAccount must contain the following anno-\ntation: \nkubernetes.io/enforce-mountable-secrets=\"true\". \n If the ServiceAccount is annotated with this annotation, any pods using it can mount\nonly the ServiceAccount’s mountable Secrets—they can’t use any other Secret.\nUNDERSTANDING A SERVICEACCOUNT’S IMAGE PULL SECRETS\nA ServiceAccount can also contain a list of image pull Secrets, which we examined in\nchapter 7. In case you don’t remember, they are Secrets that hold the credentials for\npulling container images from a private image repository. \n  The  following  listing  shows  an  example  of  a  ServiceAccount  definition,  which\nincludes the image pull Secret you created in chapter 7.\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\nimagePullSecrets:\n- name: my-dockerhub-secret\nA  ServiceAccount’s  image  pull  Secrets  behave  slightly  differently  than  its  mountable\nSecrets. Unlike mountable Secrets, they don’t determine which image pull Secrets a\npod  can  use,  but  which  ones  are  added  automatically  to  all  pods  using  the  Service-\nAccount. Adding image pull Secrets to a ServiceAccount saves you from having to add\nthem to each pod individually. \n12.1.4   Assigning a ServiceAccount to a pod\nAfter you create additional ServiceAccounts, you need to assign them to pods. This is\ndone  by  setting  the  name  of  the  ServiceAccount  in  the  \nspec.serviceAccountName\nfield in the pod definition. \nNOTEA pod’s ServiceAccount must be set when creating the pod. It can’t be\nchanged later. \nCREATING A POD WHICH USES A CUSTOM SERVICEACCOUNT\nIn chapter 8 you deployed a pod that ran a container based on the tutum/curl image\nand  an  ambassador  container  alongside  it.  You  used  it  to  explore  the  API  server’s\nREST  interface.  The  ambassador  container  ran  the  \nkubectl proxy  process,  which\nused the pod’s ServiceAccount’s token to authenticate with the API server. \n You can now modify the pod so it uses the \nfoo ServiceAccount you created minutes\nago. The next listing shows the pod definition.\n \n \nListing 12.3   ServiceAccount with an image pull Secret: sa-image-pull-secrets.yaml\n \n\n352CHAPTER 12Securing the Kubernetes API server\napiVersion: v1\nkind: Pod\nmetadata:\n  name: curl-custom-sa\nspec:\n  serviceAccountName: foo           \n  containers:\n  - name: main\n    image: tutum/curl\n    command: [\"sleep\", \"9999999\"]\n  - name: ambassador                  \n    image: luksa/kubectl-proxy:1.6.2\nTo confirm that the custom ServiceAccount’s token is mounted into the two contain-\ners, you can print the contents of the token as shown in the following listing.\n$ kubectl exec -it curl-custom-sa -c main \n➥ cat /var/run/secrets/kubernetes.io/serviceaccount/token\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\nYou can see the token is the one from the foo ServiceAccount by comparing the token\nstring in listing 12.5 with the one in listing 12.2. \nUSING THE CUSTOM SERVICEACCOUNT’S TOKEN TO TALK TO THE API SERVER\nLet’s see if you can talk to the API server using this token. As mentioned previously,\nthe  ambassador  container  uses  the  token  when  talking  to  the  server,  so  you  can  test\nthe  token  by  going  through  the  ambassador,  which  listens  on  \nlocalhost:8001,  as\nshown in the following listing.\n$ kubectl exec -it curl-custom-sa -c main curl localhost:8001/api/v1/pods\n{\n  \"kind\": \"PodList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"selfLink\": \"/api/v1/pods\",\n    \"resourceVersion\": \"433895\"\n  },\n  \"items\": [\n  ...\nOkay,  you  got  back  a  proper  response  from  the  server,  which  means  the  custom\nServiceAccount  is  allowed  to  list  pods.  This  may  be  because  your  cluster  doesn’t  use\nthe RBAC authorization plugin, or you gave all ServiceAccounts full permissions, as\ninstructed in chapter 8. \nListing 12.4   Pod using a non-default ServiceAccount: curl-custom-sa.yaml\nListing 12.5   Inspecting the token mounted into the pod’s container(s)\nListing 12.6   Talking to the API server with a custom ServiceAccount\nThis pod uses the \nfoo ServiceAccount \ninstead of the default.\n \n\n353Securing the cluster with role-based access control\n When your cluster isn’t using proper authorization, creating and using additional\nServiceAccounts  doesn’t  make  much  sense,  since  even  the  default  ServiceAccount  is\nallowed  to  do  anything.  The  only  reason  to  use  ServiceAccounts  in  that  case  is  to\nenforce  mountable  Secrets  or  to  provide  image  pull  Secrets  through  the  Service-\nAccount, as explained earlier. \n  But  creating  additional  ServiceAccounts  is  practically  a  must  when  you  use  the\nRBAC authorization plugin, which we’ll explore next.\n12.2   Securing the cluster with role-based access control\nStarting with Kubernetes version 1.6.0, cluster security was ramped up considerably. In\nearlier  versions,  if  you  managed  to  acquire  the  authentication  token  from  one  of  the\npods,  you  could  use  it  to  do  anything  you  want in the cluster. If you google around,\nyou’ll find demos showing how a path traversal (or directory traversal) attack (where clients\ncan retrieve files located outside of the web server’s web root directory) can be used to\nget the token and use it to run your malicious pods in an insecure Kubernetes cluster.\n But in version 1.8.0, the RBAC authorization plugin graduated to GA (General\nAvailability)  and  is  now  enabled  by  default  on  many  clusters  (for  example,  when\ndeploying a cluster with kubadm, as described in appendix B). RBAC prevents unau-\nthorized  users  from  viewing  or  modifying  the  cluster  state.  The  default  Service-\nAccount isn’t allowed to view cluster state, let alone modify it in any way, unless you\ngrant it additional privileges. To write apps that communicate with the Kubernetes\nAPI  server  (as  described  in  chapter  8),  you  need  to  understand  how  to  manage\nauthorization through RBAC-specific resources.\nNOTEIn  addition  to  RBAC,  Kubernetes  also  includes  other  authorization\nplugins,  such  as  the  Attribute-based  access  control  (ABAC)  plugin,  a  Web-\nHook  plugin  and  custom  plugin  implementations.  RBAC  is  the  standard,\nthough.\n12.2.1   Introducing the RBAC authorization plugin\nThe Kubernetes API server can be configured to use an authorization plugin to check\nwhether an action is allowed to be performed by the user requesting the action. Because\nthe  API  server  exposes  a  REST  interface,  users  perform  actions  by  sending  HTTP\nrequests  to  the  server.  Users  authenticate  themselves  by  including  credentials  in  the\nrequest (an authentication token, username and password, or a client certificate).\nUNDERSTANDING ACTIONS\nBut  what  actions  are  there?  As  you  know,  REST  clients  send  GET, POST, PUT, DELETE,\nand  other  types  of  HTTP  requests  to  specific  URL  paths,  which  represent  specific\nREST resources. In Kubernetes, those resources are Pods, Services, Secrets, and so on.\nHere are a few examples of actions in Kubernetes:\nGet Pods\nCreate Services\n \n\n354CHAPTER 12Securing the Kubernetes API server\nUpdate Secrets\nAnd so on\nThe verbs in those examples (get, create, update) map to HTTP methods (\nGET, POST,\nPUT)  performed  by  the  client  (the  complete  mapping  is  shown  in  table  12.1).  The\nnouns (Pods, Service, Secrets) obviously map to Kubernetes resources. \n  An  authorization  plugin  such  as  RBAC,  which  runs  inside  the  API  server,  deter-\nmines  whether  a  client  is  allowed  to  perform  the  requested  verb  on  the  requested\nresource or not.\nNOTEThe additional verb use is used for PodSecurityPolicy resources, which\nare explained in the next chapter.\nBesides  applying  security  permissions  to  whole  resource  types,  RBAC  rules  can  also\napply  to  specific  instances  of  a  resource  (for  example,  a  Service  called  \nmyservice).\nAnd  later  you’ll  see  that  permissions  can  also  apply  to  non-resource  URL  paths,\nbecause not every path the API server exposes maps to a resource (such as the \n/api\npath itself or the server health information at /healthz). \nUNDERSTANDING THE RBAC PLUGIN\nThe RBAC authorization plugin, as the name suggests, uses user roles as the key factor\nin determining whether the user may perform the action or not. A subject (which may\nbe a human, a ServiceAccount, or a group of users or ServiceAccounts) is associated\nwith one or more roles and each role is allowed to perform certain verbs on certain\nresources. \n If a user has multiple roles, they may do anything that any of their roles allows\nthem to do. If none of the user’s roles contains a permission to, for example, update\nSecrets,  the  API  server  will  prevent  the  user  from  performing  \nPUT  or  PATCH  requests\non Secrets.\n Managing authorization through the RBAC plugin is simple. It’s all done by creat-\ning four RBAC-specific Kubernetes resources, which we’ll look at next.\nTable 12.1   Mapping HTTP methods to authorization verbs\nHTTP methodVerb for single resourceVerb for collection\nGET, HEAD             get (and watch for watching)list (and watch)\nPOST                  createn/a\nPUT                   updaten/a\nPATCH                 patchn/a\nDELETE                delete                        deletecollection\n \n\n355Securing the cluster with role-based access control\n12.2.2   Introducing RBAC resources\nThe RBAC authorization rules are configured through four resources, which can be\ngrouped into two groups:\nRoles and ClusterRoles, which specify which verbs can be performed on which\nresources.\nRoleBindings and ClusterRoleBindings, which bind the above roles to specific\nusers, groups, or ServiceAccounts.\nRoles define what can be done, while bindings define who can do it (this is shown in\nfigure 12.2).\nThe  distinction  between  a  Role  and  a  ClusterRole,  or  between  a  RoleBinding  and  a\nClusterRoleBinding,  is  that  the  Role  and  RoleBinding  are  namespaced  resources,\nwhereas  the  ClusterRole  and  ClusterRoleBinding  are  cluster-level  resources  (not\nnamespaced). This is depicted in figure 12.3.\n As you can see from the figure, multiple RoleBindings can exist in a single name-\nspace (this is also true for Roles). Likewise, multiple ClusterRoleBindings and Cluster-\nRoles can be created. Another thing shown in the figure is that although RoleBindings\nare namespaced, they can also reference ClusterRoles, which aren’t. \n The best way to learn about these four resources and what their effects are is by try-\ning them out in a hands-on exercise. You’ll do that now.\n \n \n \n \nWhat?\nRole\nBinding\nSome\nresources\nOther\nresources\nRole\nDoesn’t allow\ndoing anything\nwith other resources\nUser A\nWho?\nAdmins group\nAllows users\nto access\nService-\nAccount:\nx\nFigure 12.2   Roles grant permissions, whereas RoleBindings bind Roles to subjects.\n \n\n356CHAPTER 12Securing the Kubernetes API server\nSETTING UP YOUR EXERCISE\nBefore you can explore how RBAC resources affect what you can do through the API\nserver, you need to make sure RBAC is enabled in your cluster. First, ensure you’re\nusing at least version 1.6 of Kubernetes and that the RBAC plugin is the only config-\nured  authorization  plugin.  There  can  be  multiple  plugins  enabled  in  parallel  and  if\none of them allows an action to be performed, the action is allowed.\nNOTEIf you’re using GKE 1.6 or 1.7, you need to explicitly disable legacy autho-\nrization by creating the cluster with the \n--no-enable-legacy-authorization\noption. If you’re using Minikube, you also may need to enable RBAC by start-\ning Minikube with \n--extra-config=apiserver.Authorization.Mode=RBAC\nIf you followed the instructions on how to disable RBAC in chapter 8, now’s the time\nto re-enable it by running the following command:\n$ kubectl delete clusterrolebinding permissive-binding\nTo try out RBAC, you’ll run a pod through which you’ll try to talk to the API server,\nthe way you did in chapter 8. But this time you’ll run two pods in different namespaces\nto see how per-namespace security behaves.\n In the examples in chapter 8, you ran two containers to demonstrate how an appli-\ncation in one container uses the other container to talk to the API server. This time,\nyou’ll  run  a  single  container  (based  on  the  \nkubectl-proxy  image)  and  use  kubectl\nexec\n to run curl inside that container directly. The proxy will take care of authentica-\ntion and HTTPS, so you can focus on the authorization aspect of API server security.\nNamespace C\nNamespaced\nresources\nCluster-level\nresources\nRoleBinding\nRoleBinding\nRole\nNamespace B\nNamespaced\nresources\nRoleBindingRole\nNamespace A\nNamespaced\nresources\nRoleBindingRole\nCluster scope (resources that aren’t namespaced)\nClusterRoleBindingClusterRole\nFigure 12.3   Roles and RoleBindings are namespaced; ClusterRoles and ClusterRoleBindings aren’t.\n \n\n357Securing the cluster with role-based access control\nCREATING THE NAMESPACES AND RUNNING THE PODS\nYou’re  going  to  create  one  pod  in  namespace  foo  and  the  other  one  in  namespace\nbar, as shown in the following listing.\n$ kubectl create ns foo\nnamespace \"foo\" created\n$ kubectl run test --image=luksa/kubectl-proxy -n foo\ndeployment \"test\" created\n$ kubectl create ns bar\nnamespace \"bar\" created\n$ kubectl run test --image=luksa/kubectl-proxy -n bar\ndeployment \"test\" created\nNow open two terminals and use kubectl exec to run a shell inside each of the two\npods (one in each terminal). For example, to run the shell in the pod in namespace\nfoo, first get the name of the pod:\n$ kubectl get po -n foo\nNAME                   READY     STATUS    RESTARTS   AGE\ntest-145485760-ttq36   1/1       Running   0          1m\nThen use the name in the kubectl exec command:\n$ kubectl exec -it test-145485760-ttq36 -n foo sh\n/ #\nDo the same in the other terminal, but for the pod in the bar namespace.\nLISTING SERVICES FROM YOUR PODS\nTo verify that RBAC is enabled and preventing the pod from reading cluster state, use\ncurl to list Services in the foo namespace:\n/ # curl localhost:8001/api/v1/namespaces/foo/services\nUser \"system:serviceaccount:foo:default\" cannot list services in the \nnamespace \"foo\".\nYou’re  connecting  to  localhost:8001,  which  is  where  the  kubectl proxy  process  is\nlistening (as explained in chapter 8). The process received your request and sent it to\nthe  API  server  while  authenticating  as  the  default  ServiceAccount  in  the  \nfoo  name-\nspace (as evident from the API server’s response). \n The API server responded that the ServiceAccount isn’t allowed to list Services in\nthe \nfoo namespace, even though the pod is running in that same namespace. You’re\nseeing RBAC in action. The default permissions for a ServiceAccount don’t allow it to\nlist  or  modify  any  resources.  Now,  let’s  learn  how  to  allow  the  ServiceAccount  to  do\nthat. First, you’ll need to create a Role resource.\nListing 12.7   Running test pods in different namespaces\n \n\n358CHAPTER 12Securing the Kubernetes API server\n12.2.3   Using Roles and RoleBindings\nA  Role  resource  defines  what  actions  can  be  taken  on  which  resources  (or,  as\nexplained earlier, which types of HTTP requests can be performed on which RESTful\nresources).  The  following  listing  defines  a  Role,  which  allows  users  to  \nget  and  list\nServices in the foo namespace.\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: foo            \n  name: service-reader\nrules:\n- apiGroups: [\"\"]            \n  verbs: [\"get\", \"list\"]     \n  resources: [\"services\"]   \nWARNINGThe plural form must be used when specifying resources.\nThis Role resource will be created in the \nfoo namespace. In chapter 8, you learned that\neach resource type belongs to an API group, which you specify in the \napiVersion field\n(along with the version) in the resource’s manifest. In a Role definition, you need to spec-\nify the \napiGroup for the resources listed in each rule included in the definition. If you’re\nallowing access to resources belonging to different API groups, you use multiple rules.\nNOTEIn the example, you’re allowing access to all Service resources, but you\ncould  also  limit  access  only  to  specific  Service  instances  by  specifying  their\nnames through an additional \nresourceNames field.\nFigure 12.4 shows the Role, its verbs and resources, and the namespace it will be cre-\nated in.\nListing 12.8   A definition of a Role: service-reader.yaml\nRoles are namespaced (if namespace is \nomitted, the current namespace is used).\nServices are resources in the core apiGroup, \nwhich has no name – hence the \"\".\nGetting individual Services (by name) \nand listing all of them is allowed.\nThis rule pertains to services \n(plural name must be used!).\nAllows getting\nAllows listing\nServices\nRole:\nservice-reader\nServices\nNamespace: fooNamespace: bar\nDoes not allow users to\nget or list Services in\nother namespaces\nFigure 12.4   The service-reader Role allows getting and listing Services in the foo namespace.\n \n\n359Securing the cluster with role-based access control\nCREATING A ROLE\nCreate the previous Role in the foo namespace now:\n$ kubectl create -f service-reader.yaml -n foo\nrole \"service-reader\" created\nNOTEThe -n option is shorthand for --namespace.\nNote that if you’re using GKE, the previous command may fail because you don’t have\ncluster-admin rights. To grant the rights, run the following command:\n$ kubectl create clusterrolebinding cluster-admin-binding \n➥ --clusterrole=cluster-admin --user=your.email@address.com\nInstead of creating the service-reader Role from a YAML file, you could also create\nit with the special \nkubectl create role command. Let’s use this method to create the\nRole in the \nbar namespace:\n$ kubectl create role service-reader --verb=get --verb=list \n➥ --resource=services -n bar\nrole \"service-reader\" created\nThese two Roles will allow you to list Services in the foo and bar namespaces from\nwithin your two pods (running in the \nfoo and bar namespace, respectively). But cre-\nating  the  two  Roles  isn’t  enough  (you  can  check  by  executing  the  \ncurl  command\nagain). You need to bind each of the Roles to the ServiceAccounts in their respec-\ntive namespaces. \nBINDING A ROLE TO A SERVICEACCOUNT\nA Role defines what actions can be performed, but it doesn’t specify who can perform\nthem. To do that, you must bind the Role to a subject, which can be a user, a Service-\nAccount, or a group (of users or ServiceAccounts).\n Binding Roles to subjects is achieved by creating a RoleBinding resource. To bind\nthe Role to the \ndefault ServiceAccount, run the following command:\n$ kubectl create rolebinding test --role=service-reader \n➥ --serviceaccount=foo:default -n foo\nrolebinding \"test\" created\nThe command should be self-explanatory. You’re creating a RoleBinding, which binds\nthe \nservice-reader Role to the default ServiceAccount in namespace foo. You’re cre-\nating the RoleBinding in namespace \nfoo. The RoleBinding and the referenced Service-\nAccount and Role are shown in figure 12.5.\nNOTETo bind a Role to a user instead of a ServiceAccount, use the --user\nargument to specify the username. To bind it to a group, use --group.\n \n\n360CHAPTER 12Securing the Kubernetes API server\nThe following listing shows the YAML of the RoleBinding you created.\n$ kubectl get rolebinding test -n foo -o yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: test\n  namespace: foo\n  ...\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role                         \n  name: service-reader               \nsubjects:\n- kind: ServiceAccount       \n  name: default              \n  namespace: foo             \nAs  you  can  see,  a  RoleBinding  always  references  a  single  Role  (as  evident  from  the\nroleRef property), but can bind the Role to multiple subjects (for example, one or\nmore ServiceAccounts and any number of users or groups). Because this RoleBinding\nbinds the Role to the ServiceAccount the pod in namespace \nfoo is running under, you\ncan now list Services from within that pod.\n/ # curl localhost:8001/api/v1/namespaces/foo/services\n{\n  \"kind\": \"ServiceList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"selfLink\": \"/api/v1/namespaces/foo/services\",\nListing 12.9   A RoleBinding referencing a Role\nListing 12.10   Getting Services from the API server\nNamespace: foo\nRole:\nservice-reader\nGet, list\nDefault ServiceAccount\nis allowed to get and list\nservices in this namespace\nServices\nRoleBinding:\ntest\nService-\nAccount:\ndefault\nFigure 12.5   The test RoleBinding binds the default ServiceAccount with the \nservice-reader Role.\nThis RoleBinding references \nthe service-reader Role.\nAnd binds it to the \ndefault ServiceAccount \nin the foo namespace.\n \n\n361Securing the cluster with role-based access control\n    \"resourceVersion\": \"24906\"\n  },\n  \"items\": []     \n}\nINCLUDING SERVICEACCOUNTS FROM OTHER NAMESPACES IN A ROLEBINDING\nThe pod in namespace bar can’t list the Services in its own namespace, and obviously\nalso  not  those  in  the  \nfoo  namespace.  But  you  can  edit  your  RoleBinding  in  the  foo\nnamespace  and  add  the  other  pod’s  ServiceAccount,  even  though  it’s  in  a  different\nnamespace. Run the following command:\n$ kubectl edit rolebinding test -n foo\nThen add the following lines to the list of subjects, as shown in the following listing.\nsubjects:\n- kind: ServiceAccount\n  name: default          \n  namespace: bar         \nNow you can also list Services in the foo namespace from inside the pod running in\nthe \nbar namespace. Run the same command as in listing 12.10, but do it in the other\nterminal, where you’re running the shell in the other pod.\n Before moving on to ClusterRoles and  ClusterRoleBindings,  let’s  summarize\nwhat  RBAC  resources  you  currently  have.  You  have  a  RoleBinding  in  namespace\nfoo,  which  references  the  service-reader  Role  (also  in  the  foo  namespace)  and\nbinds  the  \ndefault  ServiceAccounts  in  both  the  foo  and  the  bar  namespaces,  as\ndepicted in figure 12.6.\nListing 12.11   Referencing a ServiceAccount from another namespace\nThe list of items is empty, \nbecause no Services exist.\nYou’re referencing the default \nServiceAccount in the bar namespace.\nNamespace: foo\nRole:\nservice-reader\nGet, list\nBoth ServiceAccounts are\nallowed to get and list Services\nin the foo namespace\nServices\nNamespace: bar\nRoleBinding:\ntest\nService-\nAccount:\ndefault\nService-\nAccount:\ndefault\nFigure 12.6   A RoleBinding binding ServiceAccounts from different namespaces to the same Role.\n \n\n362CHAPTER 12Securing the Kubernetes API server\n12.2.4   Using ClusterRoles and ClusterRoleBindings\nRoles and RoleBindings are namespaced resources, meaning they reside in and apply\nto resources in a single namespace, but, as we saw, RoleBindings can refer to Service-\nAccounts from other namespaces, too. \n In addition to these namespaced resources, two cluster-level RBAC resources also\nexist:  ClusterRole  and  ClusterRoleBinding.  They’re  not  namespaced.  Let’s  see  why\nyou need them.\n A regular Role only allows access to resources in the same namespace the Role is\nin. If you want to allow someone access to resources across different namespaces, you\nhave to create a Role and RoleBinding in every one of those namespaces. If you want\nto extend this to all namespaces (this is something a cluster administrator would prob-\nably  need),  you  need  to  create  the  same  Role  and  RoleBinding  in  each  namespace.\nWhen  creating  an  additional  namespace,  you  have  to  remember  to  create  the  two\nresources there as well. \n As you’ve learned throughout the book, certain resources aren’t namespaced at\nall  (this  includes  Nodes,  PersistentVolumes,  Namespaces,  and  so  on).  We’ve  also\nmentioned the API server exposes some URL paths that don’t represent resources\n(\n/healthz for example). Regular Roles can’t grant access to those resources or non-\nresource URLs, but ClusterRoles can.\n  A  ClusterRole  is  a  cluster-level  resource  for  allowing  access  to  non-namespaced\nresources or non-resource URLs or used as a common role to be bound inside individ-\nual namespaces, saving you from having to redefine the same role in each of them.\nALLOWING ACCESS TO CLUSTER-LEVEL RESOURCES\nAs  mentioned,  a  ClusterRole  can  be  used  to  allow  access  to  cluster-level  resources.\nLet’s look at how to allow your pod to list PersistentVolumes in your cluster. First,\nyou’ll create a ClusterRole called \npv-reader:\n$ kubectl create clusterrole pv-reader --verb=get,list \n➥ --resource=persistentvolumes\nclusterrole \"pv-reader\" created\nThe ClusterRole’s YAML is shown in the following listing.\n$ kubectl get clusterrole pv-reader -o yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:                                       \n  name: pv-reader                               \n  resourceVersion: \"39932\"                      \n  selfLink: ...                                 \n  uid: e9ac1099-30e2-11e7-955c-080027e6b159     \nListing 12.12   A ClusterRole definition\nClusterRoles aren’t \nnamespaced, hence \nno namespace field.\n \n\n363Securing the cluster with role-based access control\nrules:\n- apiGroups:                      \n  - \"\"                            \n  resources:                      \n  - persistentvolumes             \n  verbs:                          \n  - get                           \n  - list                          \nBefore you bind this ClusterRole to your pod’s ServiceAccount, verify whether the pod\ncan  list  PersistentVolumes.  Run  the  following  command  in  the  first  terminal,  where\nyou’re running the shell inside the pod in the \nfoo namespace:\n/ # curl localhost:8001/api/v1/persistentvolumes\nUser \"system:serviceaccount:foo:default\" cannot list persistentvolumes at the \ncluster scope.\nNOTEThe  URL  contains  no  namespace,  because  PersistentVolumes  aren’t\nnamespaced. \nAs  expected,  the  default  ServiceAccount  can’t  list  PersistentVolumes.  You  need  to\nbind the ClusterRole to your ServiceAccount to allow it to do that. ClusterRoles can\nbe bound to subjects with regular RoleBindings, so you’ll create a RoleBinding now:\n$ kubectl create rolebinding pv-test --clusterrole=pv-reader \n➥ --serviceaccount=foo:default -n foo\nrolebinding \"pv-test\" created\nCan you list PersistentVolumes now?\n/ # curl localhost:8001/api/v1/persistentvolumes\nUser \"system:serviceaccount:foo:default\" cannot list persistentvolumes at the \ncluster scope.\nHmm, that’s strange. Let’s examine the RoleBinding’s YAML in the following listing.\nCan you tell what (if anything) is wrong with it?\n$ kubectl get rolebindings pv-test -o yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pv-test\n  namespace: foo\n  ...\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole              \n  name: pv-reader                \nListing 12.13   A RoleBinding referencing a ClusterRole\nIn this case, the \nrules are exactly \nlike those in a \nregular Role.\nThe binding references the \npv-reader ClusterRole.\n \n\n364CHAPTER 12Securing the Kubernetes API server\nsubjects:\n- kind: ServiceAccount          \n  name: default                 \n  namespace: foo                \nThe  YAML  looks  perfectly  fine.  You’re  referencing  the  correct  ClusterRole  and  the\ncorrect ServiceAccount, as shown in figure 12.7, so what’s wrong?\nAlthough you can create a RoleBinding and have it reference a ClusterRole when you\nwant to enable access to namespaced resources, you can’t use the same approach for\ncluster-level  (non-namespaced)  resources.  To  grant  access  to  cluster-level  resources,\nyou must always use a ClusterRoleBinding.\n  Luckily,  creating  a  ClusterRoleBinding  isn’t  that  different  from  creating  a  Role-\nBinding, but you’ll clean up and delete the RoleBinding first:\n$ kubectl delete rolebinding pv-test\nrolebinding \"pv-test\" deleted\nNow create the ClusterRoleBinding:\n$ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader \n➥ --serviceaccount=foo:default\nclusterrolebinding \"pv-test\" created\nAs you can see, you replaced rolebinding with clusterrolebinding in the command\nand didn’t (need to) specify the namespace. Figure 12.8 shows what you have now.\n Let’s see if you can list PersistentVolumes now:\n/ # curl localhost:8001/api/v1/persistentvolumes\n{\n  \"kind\": \"PersistentVolumeList\",\n  \"apiVersion\": \"v1\",\n...\nThe bound subject is the \ndefault ServiceAccount in \nthe foo namespace.\nNamespace: fooCluster-level resources\nClusterRole:\npv-reader\nGet, list\nPersistent\nVolumes\nRoleBinding:\npv-test\nDefault ServiceAccount\nis unable to get and list\nPersistentVolumes\nService-\nAccount:\ndefault\nFigure 12.7   A RoleBinding referencing a ClusterRole doesn’t grant access to cluster-\nlevel resources.\n \n\n365Securing the cluster with role-based access control\nYou  can!  It  turns  out  you  must  use  a  ClusterRole  and  a  ClusterRoleBinding  when\ngranting access to cluster-level resources.\nTIPRemember that a RoleBinding can’t grant access to cluster-level resources,\neven if it references a ClusterRoleBinding.\nALLOWING ACCESS TO NON-RESOURCE URLS\nWe’ve mentioned that the API server also exposes non-resource URLs. Access to these\nURLs must also be granted explicitly; otherwise the API server will reject the client’s\nrequest. Usually, this is done for you automatically through the \nsystem:discovery\nClusterRole  and  the  identically  named  ClusterRoleBinding,  which  appear  among\nother predefined ClusterRoles and ClusterRoleBindings (we’ll explore them in sec-\ntion 12.2.5). \n Let’s inspect the \nsystem:discovery ClusterRole shown in the following listing.\n$ kubectl get clusterrole system:discovery -o yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: system:discovery\n  ...\nrules:\n- nonResourceURLs:      \n  - /api                \n  - /api/*              \n  - /apis               \n  - /apis/*             \n  - /healthz            \n  - /swaggerapi         \n  - /swaggerapi/*       \n  - /version            \nListing 12.14   The default system:discovery ClusterRole\nNamespace: fooCluster-level resources\nClusterRole:\npv-reader\nGet, list\nPersistent\nVolumes\nClusterRoleBinding:\npv-test\nDefault ServiceAccount in\nfoo namespace is now allowed\nto get and list PersistentVolumes\nService-\nAccount:\ndefault\nFigure 12.8   A ClusterRoleBinding and ClusterRole must be used to grant access to cluster-\nlevel resources.\nInstead of referring \nto resources, this rule \nrefers to non-resource \nURLs.\n \n\n366CHAPTER 12Securing the Kubernetes API server\n  verbs:             \n  - get              \nYou can see this ClusterRole refers to URLs instead of resources (field nonResource-\nURLs\n is used instead of the resources field). The verbs field only allows the GET HTTP\nmethod to be used on these URLs.\nNOTEFor  non-resource  URLs,  plain  HTTP  verbs  such  as  post, put,  and\npatch are used instead of create or update. The verbs need to be specified in\nlowercase.\nAs  with  cluster-level  resources,  ClusterRoles  for  non-resource  URLs  must  be  bound\nwith  a  ClusterRoleBinding.  Binding  them  with  a  RoleBinding  won’t  have  any  effect.\nThe \nsystem:discovery  ClusterRole  has  a  corresponding  system:discovery  Cluster-\nRoleBinding, so let’s see what’s in it by examining the following listing.\n$ kubectl get clusterrolebinding system:discovery -o yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:discovery\n  ...\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole                           \n  name: system:discovery                      \nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group                                 \n  name: system:authenticated                  \n- apiGroup: rbac.authorization.k8s.io\n  kind: Group                                 \n  name: system:unauthenticated                \nThe YAML shows the ClusterRoleBinding refers to the system:discovery ClusterRole,\nas expected. It’s bound to two groups, \nsystem:authenticated and system:unauthenti-\ncated\n, which makes it bound to all users. This means absolutely everyone can access\nthe URLs listed in the ClusterRole. \nNOTEGroups  are  in  the  domain  of  the  authentication  plugin.  When  a\nrequest  is  received  by  the  API  server,  it  calls  the  authentication  plugin  to\nobtain the list of groups the user belongs to. This information is then used\nin authorization.\nYou can confirm this by accessing the \n/api URL path from inside the pod (through\nthe \nkubectl proxy, which means you’ll be authenticated as the pod’s ServiceAccount)\nListing 12.15   The default system:discovery ClusterRoleBinding\nOnly the HTTP GET method \nis allowed for these URLs.\nThis ClusterRoleBinding references \nthe system:discovery ClusterRole.\nIt binds the ClusterRole \nto all authenticated and \nunauthenticated users \n(that is, everyone).\n \n\n367Securing the cluster with role-based access control\nand from your local machine, without specifying any authentication tokens (making\nyou an unauthenticated user):\n$ curl https://$(minikube ip):8443/api -k\n{\n  \"kind\": \"APIVersions\",\n  \"versions\": [\n  ...\nYou’ve now used ClusterRoles and ClusterRoleBindings to grant access to cluster-level\nresources  and  non-resource  URLs.  Now  let’s  look  at  how  ClusterRoles  can  be  used\nwith namespaced RoleBindings to grant access to namespaced resources in the Role-\nBinding’s namespace.\nUSING CLUSTERROLES TO GRANT ACCESS TO RESOURCES IN SPECIFIC NAMESPACES\nClusterRoles  don’t  always  need  to  be  bound  with  cluster-level  ClusterRoleBindings.\nThey  can  also  be  bound  with  regular,  namespaced  RoleBindings.  You’ve  already\nstarted  looking  at  predefined  ClusterRoles,  so  let’s  look  at  another  one  called  \nview,\nwhich is shown in the following listing.\n$ kubectl get clusterrole view -o yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: view\n  ...\nrules:\n- apiGroups:\n  - \"\"\n  resources:                           \n  - configmaps                         \n  - endpoints                          \n  - persistentvolumeclaims             \n  - pods                               \n  - replicationcontrollers             \n  - replicationcontrollers/scale       \n  - serviceaccounts                    \n  - services                           \n  verbs:                \n  - get                 \n  - list                \n  - watch               \n...\nThis ClusterRole has many rules. Only the first one is shown in the listing. The rule\nallows getting, listing, and watching resources like ConfigMaps, Endpoints, Persistent-\nVolumeClaims,  and  so  on.  These  are  namespaced  resources,  even  though  you’re\nlooking  at  a  ClusterRole  (not  a  regular,  namespaced  Role).  What  exactly  does  this\nClusterRole do?\nListing 12.16   The default view ClusterRole\nThis rule applies to \nthese resources (note: \nthey’re all namespaced \nresources).\nAs the ClusterRole’s name \nsuggests, it only allows \nreading, not writing the \nresources listed. \n \n\n368CHAPTER 12Securing the Kubernetes API server\n It depends whether it’s bound with a ClusterRoleBinding or a RoleBinding (it can\nbe bound with either). If you create a ClusterRoleBinding and reference the Cluster-\nRole in it, the subjects listed in the binding can view the specified resources across all\nnamespaces. If, on the other hand, you create a RoleBinding, the subjects listed in the\nbinding can only view resources in the namespace of the RoleBinding. You’ll try both\noptions now.\n You’ll see how the two options affect your test pod’s ability to list pods. First, let’s\nsee what happens before any bindings are in place:\n/ # curl localhost:8001/api/v1/pods\nUser \"system:serviceaccount:foo:default\" cannot list pods at the cluster \nscope./ #\n/ # curl localhost:8001/api/v1/namespaces/foo/pods\nUser \"system:serviceaccount:foo:default\" cannot list pods in the namespace \n\"foo\".\nWith the first command, you’re trying to list pods across all namespaces. With the sec-\nond, you’re trying to list pods in the \nfoo namespace. The server doesn’t allow you to\ndo either.\n Now, let’s see what happens when you create a ClusterRoleBinding and bind it to\nthe pod’s ServiceAccount:\n$ kubectl create clusterrolebinding view-test --clusterrole=view \n➥ --serviceaccount=foo:default\nclusterrolebinding \"view-test\" created\nCan the pod now list pods in the foo namespace?\n/ # curl localhost:8001/api/v1/namespaces/foo/pods\n{\n  \"kind\": \"PodList\",\n  \"apiVersion\": \"v1\",\n  ...\nIt  can!  Because  you  created  a  ClusterRoleBinding,  it  applies  across  all  namespaces.\nThe pod in namespace \nfoo can list pods in the bar namespace as well:\n/ # curl localhost:8001/api/v1/namespaces/bar/pods\n{\n  \"kind\": \"PodList\",\n  \"apiVersion\": \"v1\",\n  ...\nOkay, the pod is allowed to list pods in a different namespace. It can also retrieve pods\nacross all namespaces by hitting the /api/v1/pods URL path:\n/ # curl localhost:8001/api/v1/pods\n{\n  \"kind\": \"PodList\",\n  \"apiVersion\": \"v1\",\n  ...\n \n\n369Securing the cluster with role-based access control\nAs expected, the pod can get a list of all the pods in the cluster. To summarize, com-\nbining  a  ClusterRoleBinding  with  a  ClusterRole  referring  to  namespaced  resources\nallows the pod to access namespaced resources in any namespace, as shown in fig-\nure 12.9.\nNow, let’s see what happens if you replace the ClusterRoleBinding with a RoleBinding.\nFirst, delete the ClusterRoleBinding:\n$ kubectl delete clusterrolebinding view-test\nclusterrolebinding \"view-test\" deleted\nNext create a RoleBinding instead. Because a RoleBinding is namespaced, you need\nto specify the namespace you want to create it in. Create it in the \nfoo namespace:\n$ kubectl create rolebinding view-test --clusterrole=view \n➥ --serviceaccount=foo:default -n foo\nrolebinding \"view-test\" created\nYou  now  have  a  RoleBinding  in  the  foo  namespace,  binding  the  default  Service-\nAccount  in  that  same  namespace  with  the  \nview  ClusterRole.  What  can  your  pod\naccess now?\n/ # curl localhost:8001/api/v1/namespaces/foo/pods\n{\n  \"kind\": \"PodList\",\n  \"apiVersion\": \"v1\",\n  ...\nNamespace: foo\nCluster-level\nresources\nNamespace: bar\nPodsPods\nDefault\nServiceAccount\nin foo namespace\nis allowed to\nview pods in\nany namespace\nClusterRole:\nview\nAllows getting,\nlisting, watching\nClusterRoleBinding:\nview-test\nPods,\nServices,\nEndpoints,\nConfigMaps,\n...\nService-\nAccount:\ndefault\nFigure 12.9   A ClusterRoleBinding and ClusterRole grants permission to resources across all \nnamespaces.\n \n\n370CHAPTER 12Securing the Kubernetes API server\n/ # curl localhost:8001/api/v1/namespaces/bar/pods\nUser \"system:serviceaccount:foo:default\" cannot list pods in the namespace \n\"bar\".\n/ # curl localhost:8001/api/v1/pods\nUser \"system:serviceaccount:foo:default\" cannot list pods at the cluster \nscope.\nAs you can see, your pod can list pods in the foo namespace, but not in any other spe-\ncific namespace or across all namespaces. This is visualized in figure 12.10.\nSUMMARIZING ROLE, CLUSTERROLE, ROLEBINDING, AND CLUSTERROLEBINDING COMBINATIONS\nWe’ve covered many different combinations and it may be hard for you to remember\nwhen to use each one. Let’s see if we can make sense of all these combinations by cat-\negorizing them per specific use case. Refer to table 12.2.\nTable 12.2   When to use specific combinations of role and binding types\nFor accessingRole type to useBinding type to use\nCluster-level resources (Nodes, PersistentVolumes, ...)ClusterRole    ClusterRoleBinding\nNon-resource URLs (/api, /healthz, ...)ClusterRole    ClusterRoleBinding\nNamespaced resources in any namespace (and \nacross all namespaces)\nClusterRole    ClusterRoleBinding\nNamespaced resources in a specific namespace (reus-\ning the same ClusterRole in multiple namespaces)\nClusterRole    RoleBinding\nNamespaced resources in a specific namespace \n(Role must be defined in each namespace)\nRole           RoleBinding\nNamespace: foo\nCluster-level resources\nNamespace: bar\nPodsPods\nClusterRole:\nview\nAllows getting,\nlisting, watching\nRoleBinding:\nview-test\nPods,\nServices,\nEndpoints,\nConfigMaps,\n...\nDefault ServiceAccount in\nfoo namespace is only allowed\nto view pods in namespace foo,\ndespite using a ClusterRole\nService-\nAccount:\ndefault\nFigure 12.10   A RoleBinding referring to a ClusterRole only grants access to resources inside the \nRoleBinding’s namespace.\n \n\n371Securing the cluster with role-based access control\nHopefully,  the  relationships  between  the  four  RBAC  resources  are  much  clearer\nnow.  Don’t  worry  if  you  still  feel  like  you  don’t  yet  grasp  everything.  Things  may\nclear up as we explore the pre-configured ClusterRoles and ClusterRoleBindings in\nthe next section.\n12.2.5   Understanding default ClusterRoles and ClusterRoleBindings\nKubernetes comes with a default set of ClusterRoles and ClusterRoleBindings, which\nare  updated  every  time  the  API  server  starts.  This  ensures  all  the  default  roles  and\nbindings are recreated if you mistakenly delete them or if a newer version of Kuberne-\ntes uses a different configuration of cluster roles and bindings.\n You can see the default cluster roles and bindings in the following listing.\n$ kubectl get clusterrolebindings\nNAME                                           AGE\ncluster-admin                                  1d\nsystem:basic-user                              1d\nsystem:controller:attachdetach-controller      1d\n...\nsystem:controller:ttl-controller               1d\nsystem:discovery                               1d\nsystem:kube-controller-manager                 1d\nsystem:kube-dns                                1d\nsystem:kube-scheduler                          1d\nsystem:node                                    1d\nsystem:node-proxier                            1d\n$ kubectl get clusterroles\nNAME                                           AGE\nadmin                                          1d\ncluster-admin                                  1d\nedit                                           1d\nsystem:auth-delegator                          1d\nsystem:basic-user                              1d\nsystem:controller:attachdetach-controller      1d\n...\nsystem:controller:ttl-controller               1d\nsystem:discovery                               1d\nsystem:heapster                                1d\nsystem:kube-aggregator                         1d\nsystem:kube-controller-manager                 1d\nsystem:kube-dns                                1d\nsystem:kube-scheduler                          1d\nsystem:node                                    1d\nsystem:node-bootstrapper                       1d\nsystem:node-problem-detector                   1d\nsystem:node-proxier                            1d\nsystem:persistent-volume-provisioner           1d\nview                                           1d\nListing 12.17   Listing all ClusterRoleBindings and ClusterRoles\n \n\n372CHAPTER 12Securing the Kubernetes API server\nThe most important roles are the view, edit, admin, and cluster-admin ClusterRoles.\nThey’re meant to be bound to ServiceAccounts used by user-defined pods.\nALLOWING READ-ONLY ACCESS TO RESOURCES WITH THE VIEW CLUSTERROLE\nYou already used the default view ClusterRole in the previous example. It allows read-\ning most resources in a namespace, except for Roles, RoleBindings, and Secrets. You’re\nprobably wondering, why not Secrets? Because one of those Secrets might include an\nauthentication  token  with  greater  privileges  than  those  defined  in  the  \nview  Cluster-\nRole  and  could  allow  the  user  to  masquerade  as  a  different  user  to  gain  additional\nprivileges (privilege escalation). \nALLOWING MODIFYING RESOURCES WITH THE EDIT CLUSTERROLE\nNext  is  the  edit  ClusterRole,  which  allows  you  to  modify  resources  in  a  namespace,\nbut also allows both reading and modifying Secrets. It doesn’t, however, allow viewing\nor modifying Roles or RoleBindings—again, this is to prevent privilege escalation.\nGRANTING FULL CONTROL OF A NAMESPACE WITH THE ADMIN CLUSTERROLE\nComplete  control  of  the  resources  in  a  namespace  is  granted  in  the  admin  Cluster-\nRole.  Subjects  with  this  ClusterRole  can  read  and  modify  any  resource  in  the  name-\nspace,  except  ResourceQuotas  (we’ll  learn  what  those  are  in  chapter  14)  and  the\nNamespace resource itself. The main difference between the \nedit and the admin Cluster-\nRoles is in the ability to view and modify Roles and RoleBindings in the namespace.\nNOTETo prevent privilege escalation, the API server only allows users to cre-\nate  and  update  Roles  if  they  already  have  all  the  permissions  listed  in  that\nRole (and for the same scope). \nALLOWING COMPLETE CONTROL WITH THE CLUSTER-ADMIN CLUSTERROLE \nComplete  control  of  the  Kubernetes  cluster  can  be  given  by  assigning  the  cluster-\nadmin\n ClusterRole to a subject. As you’ve seen before, the admin ClusterRole doesn’t\nallow  users  to  modify  the  namespace’s  ResourceQuota  objects  or  the  Namespace\nresource itself. If you want to allow a user to do that, you need to create a RoleBinding\nthat  references  the  \ncluster-admin  ClusterRole.  This  gives  the  user  included  in  the\nRoleBinding  complete  control  over  all  aspects  of  the  namespace  in  which  the  Role-\nBinding is created.\n  If  you’ve  paid  attention,  you  probably  already  know  how  to  give  users  complete\ncontrol  of  all  the  namespaces  in  the  cluster.  Yes,  by  referencing  the  \ncluster-admin\nClusterRole in a ClusterRoleBinding instead of a RoleBinding.\nUNDERSTANDING THE OTHER DEFAULT CLUSTERROLES\nThe list of default ClusterRoles includes a large number of other ClusterRoles, which\nstart with the \nsystem: prefix. These are meant to be used by the various Kubernetes\ncomponents. Among them, you’ll find roles such as \nsystem:kube-scheduler, which\nis obviously used by the Scheduler, \nsystem:node, which is used by the Kubelets, and\nso on. \n \n\n373Summary\n Although the Controller Manager runs as a single pod, each controller running\ninside  it  can  use  a  separate  ClusterRole  and  ClusterRoleBinding  (they’re  prefixed\nwith \nsystem: controller:). \n Each of these system ClusterRoles has a matching ClusterRoleBinding, which binds\nit  to  the  user  the  system  component  authenticates  as.  The  \nsystem:kube-scheduler\nClusterRoleBinding,  for  example,  assigns  the  identically  named  ClusterRole  to  the\nsystem:kube-scheduler user, which is the username the scheduler Authenticates as. \n12.2.6   Granting authorization permissions wisely\nBy default, the default ServiceAccount in a namespace has no permissions other than\nthose  of  an  unauthenticated  user  (as  you  may  remember  from  one  of  the  previous\nexamples, the \nsystem:discovery ClusterRole and associated binding allow anyone to\nmake GET requests on a few non-resource URLs). Therefore, pods, by default, can’t\neven view cluster state. It’s up to you to grant them appropriate permissions to do that. \n  Obviously,  giving  all  your  ServiceAccounts  the  \ncluster-admin  ClusterRole  is  a\nbad idea. As is always the case with security, it’s best to give everyone only the permis-\nsions  they  need  to  do  their  job  and  not  a  single  permission  more  (principle  of  least\nprivilege).\nCREATING SPECIFIC SERVICEACCOUNTS FOR EACH POD\nIt’s a good idea to create a specific ServiceAccount for each pod (or a set of pod rep-\nlicas)  and  then  associate  it  with  a  tailor-made  Role  (or  a  ClusterRole)  through  a\nRoleBinding  (not  a  ClusterRoleBinding,  because  that  would  give  the  pod  access  to\nresources in other namespaces, which is probably not what you want). \n If one of your pods (the application running  within  it)  only  needs  to  read  pods,\nwhile the other also needs to modify them, then create two different ServiceAccounts\nand make those pods use them by specifying the \nserviceAccountName property in the\npod spec, as you learned in the first part of this chapter. Don’t add all the necessary\npermissions required by both pods to the default ServiceAccount in the namespace. \nEXPECTING YOUR APPS TO BE COMPROMISED\nYour aim is to reduce the possibility of an intruder getting hold of your cluster. Today’s\ncomplex  apps  contain  many  vulnerabilities.  You  should  expect  unwanted  persons  to\neventually get their hands on the ServiceAccount’s authentication token, so you should\nalways constrain the ServiceAccount to prevent them from doing any real damage.\n12.3   Summary\nThis chapter has given you a foundation on how to secure the Kubernetes API server.\nYou learned the following:\nClients of the API server include both human users and applications running\nin pods.\nApplications in pods are associated with a ServiceAccount. \nBoth users and ServiceAccounts are associated with groups.\n \n\n374CHAPTER 12Securing the Kubernetes API server\nBy  default,  pods  run  under  the  default  ServiceAccount,  which  is  created  for\neach namespace automatically.\nAdditional ServiceAccounts can be created manually and associated with a pod.\nServiceAccounts can be configured to allow mounting only a constrained list of\nSecrets in a given pod.\nA ServiceAccount can also be used to attach image pull Secrets to pods, so you\ndon’t need to specify the Secrets in every pod.\nRoles and ClusterRoles define what actions can be performed on which resources.\nRoleBindings  and  ClusterRoleBindings  bind  Roles  and  ClusterRoles  to  users,\ngroups, and ServiceAccounts.\nEach cluster comes with default ClusterRoles and ClusterRoleBindings.\nIn the next chapter, you’ll learn how to protect the cluster nodes from pods and how\nto isolate pods from each other by securing the network.\n \n\n375\nSecuring cluster nodes\nand the network\nIn  the  previous  chapter,  we  talked  about  securing  the  API  server.  If  an  attacker\ngets access to the API server, they can run whatever they like by packaging their\ncode  into  a  container  image  and  running  it  in  a  pod.  But  can  they  do  any  real\ndamage?  Aren’t  containers  isolated  from  other  containers  and  from  the  node\nthey’re running on? \n Not necessarily. In this chapter, you’ll learn how to allow pods to access the\nresources of the node they’re running on. You’ll also learn how to configure the\ncluster  so  users  aren’t  able  to  do  whatever  they  want  with  their  pods.  Then,  in\nThis chapter covers\nUsing the node’s default Linux namespaces \nin pods\nRunning containers as different users\nRunning privileged containers\nAdding or dropping a container’s kernel \ncapabilities\nDefining security policies to limit what pods can do\nSecuring the pod network\n \n\n376CHAPTER 13Securing cluster nodes and the network\nthe last part of the chapter, you’ll also learn how to secure the network the pods use\nto communicate.\n13.1   Using the host node’s namespaces in a pod\nContainers  in  a  pod  usually  run  under  separate  Linux  namespaces,  which  isolate\ntheir processes from processes running in other containers or in the node’s default\nnamespaces. \n For example, we learned that each pod gets its own IP and port space, because it\nuses its own network namespace. Likewise, each pod has its own process tree, because\nit has its own PID namespace, and it also uses its own IPC namespace, allowing only\nprocesses in the same pod to communicate with each other through the Inter-Process\nCommunication mechanism (IPC).\n13.1.1   Using the node’s network namespace in a pod\nCertain pods (usually system pods) need to operate in the host’s default namespaces,\nallowing them to see and manipulate node-level resources and devices. For example, a\npod may need to use the node’s network adapters instead of its own virtual network\nadapters. This can be achieved by setting the \nhostNetwork property in the pod spec\nto \ntrue.\n In that case, the pod gets to use the node’s network interfaces instead of having its\nown set, as shown in figure 13.1. This means the pod doesn’t get its own IP address and\nif it runs a process that binds to a port, the process will be bound to the node’s port.\nYou can try running such a pod. The next listing shows an example pod manifest.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-host-network\nListing 13.1   A pod using the node’s network namespace: pod-with-host-network.yaml\nNode\nPod A\nPod’s own network\nnamespace\neth0lo\neth0docker0loeth1\nNode’s default network\nnamespace\nPod B\nhostNetwork: true\nFigure 13.1   A pod \nwith \nhostNetwork: \ntrue uses the node’s \nnetwork interfaces \ninstead of its own.\n \n\n377Using the host node’s namespaces in a pod\nspec:\n  hostNetwork: true              \n  containers:\n  - name: main\n    image: alpine\n    command: [\"/bin/sleep\", \"999999\"]\nAfter you run the pod, you can use the following command to see that it’s indeed using\nthe host’s network namespace (it sees all the host’s network adapters, for example).\n$ kubectl exec pod-with-host-network ifconfig\ndocker0   Link encap:Ethernet  HWaddr 02:42:14:08:23:47\n          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0\n          ...\neth0      Link encap:Ethernet  HWaddr 08:00:27:F8:FA:4E\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\n          ...\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          ...\nveth1178d4f Link encap:Ethernet  HWaddr 1E:03:8D:D6:E1:2C\n          inet6 addr: fe80::1c03:8dff:fed6:e12c/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n...\nWhen the Kubernetes Control Plane components are deployed as pods (such as when\nyou  deploy  your  cluster  with  \nkubeadm,  as  explained  in  appendix  B),  you’ll  find  that\nthose  pods  use  the  \nhostNetwork  option,  effectively  making  them  behave  as  if  they\nweren’t running inside a pod.\n13.1.2   Binding to a host port without using the host’s network \nnamespace\nA related feature allows pods to bind to a port in the node’s default namespace, but\nstill have their own network namespace. This is done by using the \nhostPort property\nin one of the container’s ports defined in the \nspec.containers.ports field.\n Don’t confuse pods using \nhostPort with pods exposed through a NodePort service.\nThey’re two different things, as explained in figure 13.2.\n The first thing you’ll notice in the figure is that when a pod is using a \nhostPort, a\nconnection to the node’s port is forwarded directly to the pod running on that node,\nwhereas  with  a  \nNodePort  service,  a  connection  to  the  node’s  port  is  forwarded  to  a\nrandomly selected pod (possibly on another node). The other difference is that with\npods  using  a  \nhostPort,  the  node’s  port  is  only  bound  on  nodes  that  run  such  pods,\nwhereas \nNodePort  services  bind  the  port  on  all  nodes,  even  on  those  that  don’t  run\nsuch a pod (as on node 3 in the figure).\nListing 13.2   Network interfaces in a pod using the host’s network namespace\nUsing the host node’s \nnetwork namespace\n \n\n378CHAPTER 13Securing cluster nodes and the network\nIt’s  important  to  understand  that  if  a  pod  is  using  a  specific  host  port,  only  one\ninstance of the pod can be scheduled to each node, because two processes can’t bind\nto the same host port. The Scheduler takes this into account when scheduling pods, so\nit doesn’t schedule multiple pods to the same node, as shown in figure 13.3. If you\nhave three nodes and want to deploy four pod replicas, only three will be scheduled\n(one pod will remain Pending).\nNode 1\nPod 1\nTwo pods using\nhostPort\nPort\n8080\nPort\n9000\nNode 2\nPod 2\nPort\n8080\nPort\n9000\nNode 3\nNode 1\nPod 1\nTwo pods under\nthe same\nNodePort\nservice\nPort\n8080\nNode 2\nPod 2\nPort\n8080\nNode 3\nPort\n88\nPort\n88\nPort\n88\nService\n()iptables\nService\n()iptables\nService\n()iptables\nFigure 13.2   Difference between pods using a hostPort and pods behind a NodePort service.\nNode 1\nPod 1\nPort\n8080\nHost\nport\n9000\nHost\nport\n9000\nPod 2\nPort\n8080\nNode 2\nPod 3\nPort\n8080\nHost\nport\n9000\nNode 3\nPod 4\nPort\n8080\nCannot be scheduled to the same\nnode, because the port is already bound\nOnly a single\nreplica per node\nFigure 13.3   If a host port is used, only a single pod instance can be scheduled to a node.\n \n\n379Using the host node’s namespaces in a pod\nLet’s see how to define the hostPort in a pod’s YAML definition. The following listing\nshows the YAML to run your \nkubia pod and bind it to the node’s port 9000.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-hostport\nspec:\n  containers:\n  - image: luksa/kubia\n    name: kubia\n    ports:\n    - containerPort: 8080    \n      hostPort: 9000        \n      protocol: TCP\nAfter you create this pod, you can access it through port 9000 of the node it’s sched-\nuled to. If you have multiple nodes, you’ll see you can’t access the pod through that\nport on the other nodes. \nNOTEIf you’re trying this on GKE, you need to configure the firewall prop-\nerly using \ngcloud compute firewall-rules, the way you did in chapter 5.\nThe \nhostPort  feature  is  primarily  used  for  exposing  system  services,  which  are\ndeployed to every node using DaemonSets. Initially, people also used it to ensure two\nreplicas of the same pod were never scheduled to the same node, but now you have a\nbetter way of achieving this—it’s explained in chapter 16.\n13.1.3   Using the node’s PID and IPC namespaces\nSimilar to the hostNetwork option are the hostPID and hostIPC pod spec properties.\nWhen  you  set  them  to  \ntrue,  the  pod’s  containers  will  use  the  node’s  PID  and  IPC\nnamespaces,  allowing  processes  running  in  the  containers  to  see  all  the  other  pro-\ncesses on the node or communicate with them through IPC, respectively. See the fol-\nlowing listing for an example.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-host-pid-and-ipc\nspec:\n  hostPID: true                    \n  hostIPC: true                     \n  containers:\n  - name: main\n    image: alpine\n    command: [\"/bin/sleep\", \"999999\"]\nListing 13.3   Binding a pod to a port in the node’s port space: kubia-hostport.yaml\nListing 13.4   Using the host’s PID and IPC namespaces: pod-with-host-pid-and-ipc.yaml\nThe container can be \nreached on port 8080 \nof the pod’s IP.\nIt can also be reached \non port 9000 of the \nnode it’s deployed on.\nYou want the pod to \nuse the host’s PID \nnamespace.\nYou also want the \npod to use the host’s \nIPC namespace.\n \n\n380CHAPTER 13Securing cluster nodes and the network\nYou’ll remember that pods usually see only their own processes, but if you run this pod\nand then list the processes from within its container, you’ll see all the processes run-\nning on the host node, not only the ones running in the container, as shown in the\nfollowing listing.\n$ kubectl exec pod-with-host-pid-and-ipc ps aux\nPID   USER     TIME   COMMAND\n    1 root       0:01 /usr/lib/systemd/systemd --switched-root --system ...\n    2 root       0:00 [kthreadd]\n    3 root       0:00 [ksoftirqd/0]\n    5 root       0:00 [kworker/0:0H]\n    6 root       0:00 [kworker/u2:0]\n    7 root       0:00 [migration/0]\n    8 root       0:00 [rcu_bh]\n    9 root       0:00 [rcu_sched]\n   10 root       0:00 [watchdog/0]\n...\nBy  setting  the  hostIPC  property  to  true,  processes  in  the  pod’s  containers  can  also\ncommunicate with all the other processes running on the node, through Inter-Process\nCommunication.\n13.2   Configuring the container’s security context\nBesides  allowing  the  pod  to  use  the  host’s  Linux  namespaces,  other  security-related\nfeatures can also be configured on the pod and its container through the \nsecurity-\nContext\n properties, which can be specified under the pod spec directly and inside the\nspec of individual containers.\nUNDERSTANDING WHAT’S CONFIGURABLE IN THE SECURITY CONTEXT\nConfiguring the security context allows you to do various things:\nSpecify the user (the user’s ID) under which the process in the container will run.\nPrevent the container from running as root (the default user a container runs\nas is usually defined in the container image itself, so you may want to prevent\ncontainers from running as root).\nRun the container in privileged mode, giving it full access to the node’s kernel.\nConfigure fine-grained privileges, by adding or dropping capabilities—in con-\ntrast  to  giving  the  container  all  possible  permissions  by  running  it  in  privi-\nleged mode.\nSet  SELinux  (Security  Enhanced  Linux) options to strongly lock down a\ncontainer.\nPrevent the process from writing to the container’s filesystem.\nWe’ll explore these options next. \nListing 13.5   Processes visible in a pod with hostPID: true\n \n\n381Configuring the container’s security context\nRUNNING A POD WITHOUT SPECIFYING A SECURITY CONTEXT\nFirst,  run  a  pod  with  the  default  security  context  options  (by  not  specifying  them  at\nall), so you can see how it behaves compared to pods with a custom security context:\n$ kubectl run pod-with-defaults --image alpine --restart Never \n➥  -- /bin/sleep 999999\npod \"pod-with-defaults\" created\nLet’s  see  what  user  and  group  ID  the  container  is  running  as,  and  which  groups  it\nbelongs to. You can see this by running the \nid command inside the container:\n$ kubectl exec pod-with-defaults id\nuid=0(root) gid=0(root) groups=0(root), 1(bin), 2(daemon), 3(sys), 4(adm), \n6(disk), 10(wheel), 11(floppy), 20(dialout), 26(tape), 27(video)\nThe container is running as user ID (uid) 0, which is root, and group ID (gid) 0 (also\nroot). It’s also a member of multiple other groups. \nNOTEWhat user the container runs as is specified in the container image. In\na  Dockerfile,  this  is  done  using  the  \nUSER  directive.  If  omitted,  the  container\nruns as root.\nNow, you’ll run a pod where the container runs as a different user.\n13.2.1   Running a container as a specific user\nTo run a pod under a different user ID than the one that’s baked into the container\nimage,  you’ll  need  to  set  the  pod’s  \nsecurityContext.runAsUser  property.  You’ll\nmake the container run as user \nguest, whose user ID in the alpine container image is\n405, as shown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-as-user-guest\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command: [\"/bin/sleep\", \"999999\"]\n    securityContext:\n      runAsUser: 405      \nNow,  to  see  the  effect  of  the  runAsUser  property,  run  the  id  command  in  this  new\npod, the way you did before:\n$ kubectl exec pod-as-user-guest id\nuid=405(guest) gid=100(users)\nListing 13.6   Running containers as a specific user: pod-as-user-guest.yaml\nYou need to specify a user ID, not \na username (id 405 corresponds \nto the guest user).\n \n\n382CHAPTER 13Securing cluster nodes and the network\nAs requested, the container is running as the guest user. \n13.2.2   Preventing a container from running as root\nWhat if you don’t care what user the container runs as, but you still want to prevent it\nfrom running as root? \n Imagine having a pod deployed with a container image that was built with a \nUSER\ndaemon\n directive in the Dockerfile, which makes the container run under the daemon\nuser.  What  if  an  attacker  gets  access  to  your  image  registry  and  pushes  a  different\nimage under the same tag? The attacker’s image is configured to run as the root user.\nWhen  Kubernetes  schedules  a  new  instance  of  your  pod,  the  Kubelet  will  download\nthe attacker’s image and run whatever code they put into it. \n Although containers are mostly isolated from the host system, running their pro-\ncesses as root is still considered a bad practice. For example, when a host directory is\nmounted  into  the  container,  if  the  process  running  in  the  container  is  running  as\nroot, it has full access to the mounted directory, whereas if it’s running as non-root,\nit won’t. \n To prevent the attack scenario described previously, you can specify that the pod’s\ncontainer needs to run as a non-root user, as shown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-run-as-non-root\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command: [\"/bin/sleep\", \"999999\"]\n    securityContext:                   \n      runAsNonRoot: true               \nIf you deploy this pod, it gets scheduled, but is not allowed to run:\n$ kubectl get po pod-run-as-non-root\nNAME                 READY  STATUS                                                  \npod-run-as-non-root  0/1    container has runAsNonRoot and image will run \n                            \n➥  as root\nNow, if anyone tampers with your container images, they won’t get far.\n13.2.3   Running pods in privileged mode\nSometimes pods need to do everything that the node they’re running on can do, such\nas  use  protected  system  devices  or  other  kernel  features,  which  aren’t  accessible  to\nregular containers. \nListing 13.7   Preventing containers from running as root: pod-run-as-non-root.yaml\nThis container will only \nbe allowed to run as a \nnon-root user.\n \n\n383Configuring the container’s security context\n An example of such a pod is the kube-proxy pod, which needs to modify the node’s\niptables rules to make services work, as was explained in chapter 11. If you follow the\ninstructions in appendix B and deploy a cluster with \nkubeadm, you’ll see every cluster\nnode runs a kube-proxy pod and you can examine its YAML specification to see all the\nspecial features it’s using. \n  To  get  full  access  to  the  node’s  kernel,  the  pod’s  container  runs  in  privileged\nmode. This is achieved by setting the \nprivileged property in the container’s security-\nContext\n property to true. You’ll create a privileged pod from the YAML in the follow-\ning listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-privileged\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command: [\"/bin/sleep\", \"999999\"]\n    securityContext:\n      privileged: true     \nGo ahead and deploy this pod, so you can compare it with the non-privileged pod you\nran earlier. \n If you’re familiar with Linux, you may know it has a special file directory called /dev,\nwhich contains device files for all the devices on the system. These aren’t regular files on\ndisk, but are special files used to communicate with devices. Let’s see what devices are\nvisible  in  the  non-privileged  container  you  deployed  earlier  (the  \npod-with-defaults\npod), by listing files in its /dev directory, as shown in the following listing.\n$ kubectl exec -it pod-with-defaults ls /dev\ncore             null             stderr           urandom\nfd               ptmx             stdin            zero\nfull             pts              stdout\nfuse             random           termination-log\nmqueue           shm              tty\nThe listing shows all the devices. The list is fairly short. Now, compare this with the fol-\nlowing listing, which shows the device files your privileged pod can see.\n$ kubectl exec -it pod-privileged ls /dev\nautofs              snd                 tty46\nbsg                 sr0                 tty47\nListing 13.8   A pod with a privileged container: pod-privileged.yaml\nListing 13.9   List of available devices in a non-privileged pod\nListing 13.10   List of available devices in a privileged pod\nThis container will \nrun in privileged \nmode\n \n\n384CHAPTER 13Securing cluster nodes and the network\nbtrfs-control       stderr              tty48\ncore                stdin               tty49\ncpu                 stdout              tty5\ncpu_dma_latency     termination-log     tty50\nfd                  tty                 tty51\nfull                tty0                tty52\nfuse                tty1                tty53\nhpet                tty10               tty54\nhwrng               tty11               tty55\n...                 ...                 ...\nI  haven’t  included  the  whole  list,  because  it’s  too  long  for  the  book,  but  it’s  evident\nthat the device list is much longer than before. In fact, the privileged container sees\nall the host node’s devices. This means it can use any device freely. \n For example, I had to use privileged mode like this when I wanted a pod running\non a Raspberry Pi to control LEDs connected it.\n13.2.4   Adding individual kernel capabilities to a container\nIn the previous section, you saw one way of giving a container unlimited power. In the\nold  days,  traditional  UNIX  implementations  only  distinguished  between  privileged\nand  unprivileged  processes,  but  for  many  years,  Linux  has  supported  a  much  more\nfine-grained permission system through kernel capabilities.\n  Instead  of  making  a  container  privileged  and  giving  it  unlimited  permissions,  a\nmuch safer method (from a security perspective) is to give it access only to the kernel\nfeatures it really requires. Kubernetes allows you to add capabilities to each container\nor drop part of them, which allows you to fine-tune the container’s permissions and\nlimit the impact of a potential intrusion by an attacker.\n For example, a container usually isn’t allowed to change the system time (the hard-\nware clock’s time). You can confirm this by trying to set the time in your \npod-with-\ndefaults\n pod:\n$ kubectl exec -it pod-with-defaults -- date +%T -s \"12:00:00\"\ndate: can't set date: Operation not permitted\nIf you want to allow the container to change the system time, you can add a capabil-\nity called \nCAP_SYS_TIME to the container’s capabilities list, as shown in the follow-\ning listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-add-settime-capability\nspec:\n  containers:\n  - name: main\n    image: alpine\nListing 13.11   Adding the CAP_SYS_TIME capability: pod-add-settime-capability.yaml\n \n\n385Configuring the container’s security context\n    command: [\"/bin/sleep\", \"999999\"]\n    securityContext:                     \n      capabilities:                      \n        add:                  \n        - SYS_TIME            \nNOTELinux  kernel  capabilities  are  usually  prefixed  with  CAP_.  But  when\nspecifying them in a pod spec, you must leave out the prefix.\nIf you run the same command in this new pod’s container, the system time is changed\nsuccessfully:\n$ kubectl exec -it pod-add-settime-capability -- date +%T -s \"12:00:00\"\n12:00:00\n$ kubectl exec -it pod-add-settime-capability -- date\nSun May  7 12:00:03 UTC 2017\nWARNINGIf  you  try  this  yourself,  be  aware  that  it  may  cause  your  worker\nnode to become unusable. In Minikube, although the system time was auto-\nmatically reset back by the Network Time Protocol (NTP) daemon, I had to\nreboot the VM to schedule new pods. \nYou can confirm the node’s time has been changed by checking the time on the node\nrunning the pod. In my case, I’m using Minikube, so I have only one node and I can\nget its time like this:\n$ minikube ssh date\nSun May  7 12:00:07 UTC 2017\nAdding capabilities like this is a much better way than giving a container full privileges\nwith \nprivileged: true. Admittedly, it does require you to know and understand what\neach capability does.\nTIPYou’ll find the list of Linux kernel capabilities in the Linux man pages.\n13.2.5   Dropping capabilities from a container\nYou’ve seen how to add capabilities, but you can also drop capabilities that may oth-\nerwise be available to the container. For example, the default capabilities given to a\ncontainer  include  the  \nCAP_CHOWN  capability,  which  allows  processes  to  change  the\nownership of files in the filesystem. \n  You  can  see  that’s  the  case  by  changing  the  ownership  of  the  /tmp  directory  in\nyour \npod-with-defaults pod to the guest user, for example:\n$ kubectl exec pod-with-defaults chown guest /tmp\n$ kubectl exec pod-with-defaults -- ls -la / | grep tmp\ndrwxrwxrwt    2 guest    root             6 May 25 15:18 tmp\nCapabilities are added or dropped \nunder the securityContext property.\nYou’re adding the \nSYS_TIME capability.\n \n\n386CHAPTER 13Securing cluster nodes and the network\nTo prevent the container from doing that, you need to drop the capability by listing it\nunder  the  container’s  \nsecurityContext.capabilities.drop  property,  as  shown  in\nthe following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-drop-chown-capability\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command: [\"/bin/sleep\", \"999999\"]\n    securityContext:\n      capabilities:\n        drop:                   \n        - CHOWN                 \nBy dropping the CHOWN capability, you’re not allowed to change the owner of the /tmp\ndirectory in this pod:\n$ kubectl exec pod-drop-chown-capability chown guest /tmp\nchown: /tmp: Operation not permitted\nYou’re  almost  done  exploring  the  container’s  security  context  options.  Let’s  look  at\none more.\n13.2.6   Preventing processes from writing to the container’s filesystem\nYou may want to prevent the processes running in the container from writing to the\ncontainer’s filesystem, and only allow them to write to mounted volumes. You’d want\nto do that mostly for security reasons. \n Let’s imagine you’re running a PHP application with a hidden vulnerability, allow-\ning  an  attacker  to  write  to  the  filesystem.  The  PHP  files  are  added  to  the  container\nimage at build time and are served from the container’s filesystem. Because of the vul-\nnerability, the attacker can modify those files and inject them with malicious code. \n These types of attacks can be thwarted by preventing the container from writing to\nits filesystem, where the app’s executable code is normally stored. This is done by set-\nting the container’s \nsecurityContext.readOnlyRootFilesystem property to true, as\nshown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-readonly-filesystem\nListing 13.12   Dropping a capability from a container: pod-drop-chown-capability.yaml\nListing 13.13   A container with a read-only filesystem: pod-with-readonly-filesystem.yaml\nYou’re not allowing this container \nto change file ownership.\n \n\n387Configuring the container’s security context\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command: [\"/bin/sleep\", \"999999\"]\n    securityContext:                      \n      readOnlyRootFilesystem: true        \n    volumeMounts:                      \n    - name: my-volume                  \n      mountPath: /volume               \n      readOnly: false                  \n  volumes:\n  - name: my-volume\n    emptyDir:\nWhen you deploy this pod, the container is running as root, which has write permis-\nsions to the \n/ directory, but trying to write a file there fails:\n$ kubectl exec -it pod-with-readonly-filesystem touch /new-file\ntouch: /new-file: Read-only file system\nOn the other hand, writing to the mounted volume is allowed:\n$ kubectl exec -it pod-with-readonly-filesystem touch /volume/newfile\n$ kubectl exec -it pod-with-readonly-filesystem -- ls -la /volume/newfile\n-rw-r--r--    1 root     root       0 May  7 19:11 /mountedVolume/newfile\nAs shown in the example, when you make the container’s filesystem read-only, you’ll\nprobably  want  to  mount  a  volume  in  every  directory  the  application  writes  to  (for\nexample, logs, on-disk caches, and so on).\nTIPTo  increase  security,  when  running  pods  in  production,  set  their  con-\ntainer’s \nreadOnlyRootFilesystem property to true.\nSETTING SECURITY CONTEXT OPTIONS AT THE POD LEVEL\nIn all these examples, you’ve set the security context of an individual container. Sev-\neral of these options can also be set at the pod level (through the \npod.spec.security-\nContext\n  property).  They  serve  as  a  default  for  all  the  pod’s  containers  but  can  be\noverridden at the container level. The pod-level security context also allows you to set\nadditional properties, which we’ll explain next.\n13.2.7   Sharing volumes when containers run as different users\nIn chapter 6, we explained how volumes are used to share data between the pod’s\ncontainers. You had no trouble writing files in one container and reading them in\nthe other. \n But this was only because both containers were running as root, giving them full\naccess  to  all  the  files  in  the  volume.  Now  imagine  using  the  \nrunAsUser  option  we\nexplained earlier. You may need to run the two containers as two different users (per-\nhaps you’re using two third-party container images, where each one runs its process\nThis container’s filesystem \ncan’t be written to...\n...but writing to /volume is \nallowed, becase a volume \nis mounted there.\n \n\n388CHAPTER 13Securing cluster nodes and the network\nunder its own specific user). If those two containers use a volume to share files, they\nmay not necessarily be able to read or write files of one another. \n That’s why Kubernetes allows you to specify supplemental groups for all the pods\nrunning  in  the  container,  allowing  them  to  share  files,  regardless  of  the  user  IDs\nthey’re running as. This is done using the following two properties:\nfsGroup\nsupplementalGroups\nWhat they do is best explained in an example, so let’s see how to use them in a pod\nand then see what their effect is. The next listing describes a pod with two containers\nsharing the same volume.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-shared-volume-fsgroup\nspec:\n  securityContext:                       \n    fsGroup: 555                         \n    supplementalGroups: [666, 777]       \n  containers:\n  - name: first\n    image: alpine\n    command: [\"/bin/sleep\", \"999999\"]\n    securityContext:                     \n      runAsUser: 1111                    \n    volumeMounts:                               \n    - name: shared-volume                       \n      mountPath: /volume\n      readOnly: false\n  - name: second\n    image: alpine\n    command: [\"/bin/sleep\", \"999999\"]\n    securityContext:                     \n      runAsUser: 2222                    \n    volumeMounts:                               \n    - name: shared-volume                       \n      mountPath: /volume\n      readOnly: false\n  volumes:                                      \n  - name: shared-volume                         \n    emptyDir:\nAfter you create this pod, run a shell in its first container and see what user and group\nIDs the container is running as:\n$ kubectl exec -it pod-with-shared-volume-fsgroup -c first sh\n/ $ id\nuid=1111 gid=0(root) groups=555,666,777\nListing 13.14fsGroup & supplementalGroups: pod-with-shared-volume-fsgroup.yaml\nThe fsGroup and supplementalGroups \nare defined in the security context at \nthe pod level.\nThe first container \nruns as user ID 1111.\nBoth containers \nuse the same \nvolume\nThe second\ncontainer\nruns as user\nID 2222.\n \n\n389Restricting the use of security-related features in pods\nThe id command shows the container is running with user ID 1111, as specified in the\npod definition. The effective group ID is \n0(root), but group IDs 555, 666, and 777 are\nalso associated with the user. \n In the pod definition, you set \nfsGroup to 555. Because of this, the mounted volume\nwill be owned by group ID \n555, as shown here:\n/ $ ls -l / | grep volume\ndrwxrwsrwx    2 root     555              6 May 29 12:23 volume\nIf  you  create  a  file  in  the  mounted  volume’s  directory,  the  file  is  owned  by  user  ID\n1111 (that’s the user ID the container is running as) and by group ID 555:\n/ $ echo foo > /volume/foo\n/ $ ls -l /volume\ntotal 4\n-rw-r--r--    1 1111     555              4 May 29 12:25 foo\nThis is different from how ownership is otherwise set up for newly created files. Usu-\nally, the user’s effective group ID, which is \n0 in your case, is used when a user creates\nfiles. You can see this by creating a file in the container’s filesystem instead of in the\nvolume:\n/ $ echo foo > /tmp/foo\n/ $ ls -l /tmp\ntotal 4\n-rw-r--r--    1 1111     root             4 May 29 12:41 foo\nAs you can see, the fsGroup security context property is used when the process cre-\nates  files  in  a  volume  (but  this  depends  on  the  volume  plugin  used),  whereas  the\nsupplementalGroups property defines a list of additional group IDs the user is asso-\nciated with. \n This concludes this section about the configuration of the container’s security con-\ntext. Next, we’ll see how a cluster administrator can restrict users from doing so.\n13.3   Restricting the use of security-related features in pods\nThe examples in the previous sections have shown how a person deploying pods can\ndo  whatever  they  want  on  any  cluster  node,  by  deploying  a  privileged  pod  to  the\nnode, for example. Obviously, a mechanism must prevent users from doing part or\nall of what’s been explained. The cluster admin can restrict the use of the previously\ndescribed  security-related  features  by  creating  one  or  more  PodSecurityPolicy\nresources.\n13.3.1   Introducing the PodSecurityPolicy resource\nPodSecurityPolicy is a cluster-level (non-namespaced) resource, which defines what\nsecurity-related features users can or can’t use in their pods. The job of upholding\nthe   policies   configured   in   PodSecurityPolicy   resources   is   performed   by   the\n \n\n390CHAPTER 13Securing cluster nodes and the network\nPodSecurityPolicy admission control plugin running in the API server (we explained\nadmission control plugins in chapter 11).\nNOTEThe  PodSecurityPolicy  admission  control  plugin  may  not  be  enabled\nin your cluster. Before running the following examples, ensure it’s enabled. If\nyou’re using Minikube, refer to the next sidebar.\nWhen someone posts a pod resource to the API server, the PodSecurityPolicy admis-\nsion  control  plugin  validates  the  pod  definition  against  the  configured  PodSecurity-\nPolicies.  If  the  pod  conforms  to  the  cluster’s  policies,  it’s  accepted  and  stored  into\netcd;  otherwise  it’s  rejected  immediately.  The  plugin  may  also  modify  the  pod\nresource according to defaults configured in the policy.\nUNDERSTANDING WHAT A PODSECURITYPOLICY CAN DO\nA PodSecurityPolicy resource defines things like the following:\nWhether a pod can use the host’s IPC, PID, or Network namespaces\nWhich host ports a pod can bind to\nWhat user IDs a container can run as\nWhether a pod with privileged containers can be created\nEnabling RBAC and PodSecurityPolicy admission control in Minikube\nI’m  using  Minikube  version  v0.19.0  to  run  these  examples.  That  version  doesn’t\nenable either the PodSecurityPolicy admission control plugin or RBAC authorization,\nwhich is required in part of the exercises. One exercise also requires authenticating\nas  a  different  user,  so  you’ll  also  need  to  enable  the  basic  authentication  plugin\nwhere users are defined in a file.\nTo run Minikube with all these plugins enabled, you may need to use this (or a similar)\ncommand, depending on the version you’re using: \n$ minikube start --extra-config apiserver.Authentication.PasswordFile.\n➥ BasicAuthFile=/etc/kubernetes/passwd --extra-config=apiserver.\n➥ Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRun\n➥ Options.AdmissionControl=NamespaceLifecycle,LimitRanger,Service\n➥ Account,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,\n➥ DefaultTolerationSeconds,PodSecurityPolicy\nThe API server won’t start up until you create the password file you specified in the\ncommand line options. This is how to create the file:\n$ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd\npassword,alice,1000,basic-user\npassword,bob,2000,privileged-user\nEOF\nYou’ll find a shell script that runs both commands in the book’s code archive in\nChapter13/minikube-with-rbac-and-psp-enabled.sh.\n \n\n391Restricting the use of security-related features in pods\nWhich kernel capabilities are allowed, which are added by default and which are\nalways dropped\nWhat SELinux labels a container can use\nWhether a container can use a writable root filesystem or not\nWhich filesystem groups the container can run as\nWhich volume types a pod can use\nIf you’ve read this chapter up to this point, everything but the last item in the previous\nlist should be familiar. The last item should also be fairly clear. \nEXAMINING A SAMPLE PODSECURITYPOLICY\nThe  following  listing  shows  a  sample  PodSecurityPolicy,  which  prevents  pods  from\nusing the host’s IPC, PID, and Network namespaces, and prevents running privileged\ncontainers and the use of most host ports (except ports from 10000-11000 and 13000-\n14000).  The  policy  doesn’t  set  any  constraints  on  what  users,  groups,  or  SELinux\ngroups the container can run as.\napiVersion: extensions/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: default\nspec:\n  hostIPC: false                 \n  hostPID: false                 \n  hostNetwork: false             \n  hostPorts:                         \n  - min: 10000                       \n    max: 11000                       \n  - min: 13000                       \n    max: 14000                       \n  privileged: false              \n  readOnlyRootFilesystem: true   \n  runAsUser:                      \n    rule: RunAsAny                \n  fsGroup:                        \n    rule: RunAsAny                \n  supplementalGroups:             \n    rule: RunAsAny                \n  seLinux:                      \n    rule: RunAsAny              \n  volumes:                  \n  - '*'                     \nMost of the options specified in the example should be self-explanatory, especially if\nyou’ve  read  the  previous  sections.  After  this  PodSecurityPolicy  resource  is  posted  to\nListing 13.15   An example PodSecurityPolicy: pod-security-policy.yaml\nContainers aren’t \nallowed to use the \nhost’s IPC, PID, or \nnetwork namespace.\nThey can only bind to host ports \n10000 to 11000 (inclusive) or \nhost ports 13000 to 14000.\nContainers cannot run \nin privileged mode.\nContainers are forced to run \nwith a read-only root filesystem.\nContainers can \nrun as any user \nand any group.\nThey can also use any \nSELinux groups they want.\nAll volume types can \nbe used in pods.\n \n\n392CHAPTER 13Securing cluster nodes and the network\nthe cluster, the API server will no longer allow you to deploy the privileged pod used\nearlier. For example\n$ kubectl create -f pod-privileged.yaml\nError from server (Forbidden): error when creating \"pod-privileged.yaml\":\npods \"pod-privileged\" is forbidden: unable to validate against any pod \nsecurity policy: [spec.containers[0].securityContext.privileged: Invalid \nvalue: true: Privileged containers are not allowed]\nLikewise, you can no longer deploy pods that want to use the host’s PID, IPC, or Net-\nwork namespace. Also, because you set \nreadOnlyRootFilesystem to true in the pol-\nicy, the container filesystems in all pods will be read-only (containers can only write\nto volumes).\n13.3.2   Understanding runAsUser, fsGroup, and supplementalGroups \npolicies\nThe  policy  in  the  previous  example  doesn’t  impose  any  limits  on  which  users  and\ngroups containers can run as, because you’ve used the \nRunAsAny rule for the runAs-\nUser\n, fsGroup,  and  supplementalGroups  fields.  If  you  want  to  constrain  the  list  of\nallowed user or group IDs, you change the rule to \nMustRunAs and specify the range of\nallowed IDs. \nUSING THE MUSTRUNAS RULE\nLet’s look at an example. To only allow containers to run as user ID 2 and constrain the\ndefault filesystem group and supplemental group IDs to be anything from \n2–10 or 20–\n30\n (all inclusive), you’d include the following snippet in the PodSecurityPolicy resource.\n  runAsUser:\n    rule: MustRunAs\n    ranges:\n    - min: 2                \n      max: 2                \n  fsGroup:\n    rule: MustRunAs\n    ranges:\n    - min: 2                \n      max: 10               \n    - min: 20               \n      max: 30               \n  supplementalGroups:\n    rule: MustRunAs\n    ranges:\n    - min: 2                \n      max: 10               \n    - min: 20               \n      max: 30               \nListing 13.16   Specifying IDs containers must run as: psp-must-run-as.yaml\nAdd a single range with min equal \nto max to set one specific ID.\nMultiple ranges are \nsupported—here, \ngroup IDs can be 2–10 \nor 20–30 (inclusive).\n \n\n393Restricting the use of security-related features in pods\nIf the pod spec tries to set either of those fields to a value outside of these ranges, the\npod will not be accepted by the API server. To try this, delete the previous PodSecurity-\nPolicy and create the new one from the psp-must-run-as.yaml file. \nNOTEChanging the policy has no effect on existing pods, because PodSecurity-\nPolicies are enforced only when creating or updating pods.\nDEPLOYING A POD WITH RUNASUSER OUTSIDE OF THE POLICY’S RANGE\nIf  you  try  deploying  the  pod-as-user-guest.yaml  file  from  earlier,  which  says  the  con-\ntainer should run as user ID \n405, the API server rejects the pod:\n$ kubectl create -f pod-as-user-guest.yaml\nError from server (Forbidden): error when creating \"pod-as-user-guest.yaml\"\n: pods \"pod-as-user-guest\" is forbidden: unable to validate against any pod \nsecurity policy: [securityContext.runAsUser: Invalid value: 405: UID on \ncontainer main does not match required range.  Found 405, allowed: [{2 2}]]\nOkay, that was obvious. But what happens if you deploy a pod without setting the runAs-\nUser\n property, but the user ID is baked into the container image (using the USER direc-\ntive in the Dockerfile)?\nDEPLOYING A POD WITH A CONTAINER IMAGE WITH AN OUT-OF-RANGE USER ID\nI’ve  created  an  alternative  image  for  the  Node.js  app  you’ve  used  throughout  the\nbook. The image is configured so that the container will run as user ID 5. The Docker-\nfile for the image is shown in the following listing.\nFROM node:7\nADD app.js /app.js\nUSER 5                         \nENTRYPOINT [\"node\", \"app.js\"]\nI pushed the image to Docker Hub as luksa/kubia-run-as-user-5. If I deploy a pod\nwith that image, the API server doesn’t reject it:\n$ kubectl run run-as-5 --image luksa/kubia-run-as-user-5 --restart Never\npod \"run-as-5\" created\nUnlike before, the API server accepted the pod and the Kubelet has run its container.\nLet’s see what user ID the container is running as:\n$ kubectl exec run-as-5 -- id\nuid=2(bin) gid=2(bin) groups=2(bin)\nAs you can see, the container is running as user ID 2, which is the ID you specified in\nthe  PodSecurityPolicy.  The  PodSecurityPolicy  can  be  used  to  override  the  user  ID\nhardcoded into a container image.\nListing 13.17   Dockerfile with a USER directive: kubia-run-as-user-5/Dockerfile\nContainers run from \nthis image will run \nas user ID 5.\n \n\n394CHAPTER 13Securing cluster nodes and the network\nUSING THE MUSTRUNASNONROOT RULE IN THE RUNASUSER FIELD\nFor  the  runAsUser  field  an  additional  rule  can  be  used:  MustRunAsNonRoot.  As  the\nname suggests, it prevents users from deploying containers that run as root. Either the\ncontainer  spec  must  specify  a  \nrunAsUser  field,  which  can’t  be  zero  (zero  is  the  root\nuser’s ID), or the container image itself must run as a non-zero user ID. We explained\nwhy this is a good thing earlier.\n13.3.3   Configuring allowed, default, and disallowed capabilities\nAs you learned, containers can run in privileged mode or not, and you can define a\nmore  fine-grained  permission  configuration  by  adding  or  dropping  Linux  kernel\ncapabilities in each container. Three fields influence which capabilities containers can\nor cannot use:\nallowedCapabilities\ndefaultAddCapabilities\nrequiredDropCapabilities\nWe’ll look at an example first, and then discuss what each of the three fields does. The\nfollowing listing shows a snippet of a PodSecurityPolicy resource defining three fields\nrelated to capabilities.\napiVersion: extensions/v1beta1 \nkind: PodSecurityPolicy\nspec:\n  allowedCapabilities:          \n  - SYS_TIME                    \n  defaultAddCapabilities:         \n  - CHOWN                         \n  requiredDropCapabilities:     \n  - SYS_ADMIN                   \n  - SYS_MODULE                  \n  ...\nNOTEThe SYS_ADMIN capability allows a range of administrative operations,\nand the \nSYS_MODULE capability allows loading and unloading of Linux kernel\nmodules.\nSPECIFYING WHICH CAPABILITIES CAN BE ADDED TO A CONTAINER\nThe allowedCapabilities field is used to specify which capabilities pod authors can\nadd in the \nsecurityContext.capabilities field in the container spec. In one of the\nprevious examples, you added the \nSYS_TIME capability to your container. If the Pod-\nSecurityPolicy admission control plugin had been enabled, you wouldn’t have been\nable to add that capability, unless it was specified in the PodSecurityPolicy as shown\nin listing 13.18.\nListing 13.18   Specifying capabilities in a PodSecurityPolicy: psp-capabilities.yaml\nAllow containers to \nadd the SYS_TIME \ncapability.\nAutomatically add the CHOWN \ncapability to every container.\nRequire containers to \ndrop the SYS_ADMIN and \nSYS_MODULE capabilities.\n \n\n395Restricting the use of security-related features in pods\nADDING CAPABILITIES TO ALL CONTAINERS\nAll  capabilities  listed  under  the  defaultAddCapabilities  field  will  be  added  to\nevery  deployed  pod’s  containers.  If  a  user  doesn’t  want  certain  containers  to  have\nthose capabilities, they need to explicitly drop them in the specs of those containers.\n The example in listing 13.18 enables the automatic addition of the \nCAP_CHOWN capa-\nbility to every container, thus allowing processes running in the container to change the\nownership of files in the container (with the \nchown command, for example).\nDROPPING CAPABILITIES FROM A CONTAINER\nThe final field in this example is requiredDropCapabilities. I must admit, this was a\nsomewhat strange name for me at first, but it’s not that complicated. The capabilities\nlisted in this field are dropped automatically from every container (the PodSecurity-\nPolicy  Admission  Control  plugin  will  add  them  to  every  container’s  \nsecurity-\nContext.capabilities.drop\n field). \n If a user tries to create a pod where they explicitly add one of the capabilities listed\nin the policy’s \nrequiredDropCapabilities field, the pod is rejected:\n$ kubectl create -f pod-add-sysadmin-capability.yaml\nError from server (Forbidden): error when creating \"pod-add-sysadmin-\ncapability.yaml\": pods \"pod-add-sysadmin-capability\" is forbidden: unable \nto validate against any pod security policy: [capabilities.add: Invalid \nvalue: \"SYS_ADMIN\": capability may not be added]\n13.3.4   Constraining the types of volumes pods can use\nThe last thing a PodSecurityPolicy resource can do is define which volume types users\ncan  add  to  their  pods.  At  the  minimum,  a  PodSecurityPolicy  should  allow  using  at\nleast  the  \nemptyDir, configMap, secret, downwardAPI,  and  the  persistentVolume-\nClaim\n volumes. The pertinent part of such a PodSecurityPolicy resource is shown in\nthe following listing.\nkind: PodSecurityPolicy\nspec:\n  volumes:\n  - emptyDir\n  - configMap\n  - secret\n  - downwardAPI\n  - persistentVolumeClaim\nIf  multiple  PodSecurityPolicy  resources  are  in  place,  pods  can  use  any  volume  type\ndefined in any of the policies (the union of all \nvolumes lists is used).\nListing 13.19   A PSP snippet allowing the use of only certain volume types: \npsp-volumes.yaml\n \n\n396CHAPTER 13Securing cluster nodes and the network\n13.3.5   Assigning different PodSecurityPolicies to different users \nand groups\nWe  mentioned  that  a  PodSecurityPolicy  is  a  cluster-level  resource,  which  means  it\ncan’t  be  stored  in  and  applied  to  a  specific  namespace.  Does  that  mean  it  always\napplies  across  all  namespaces?  No,  because  that  would  make  them  relatively  unus-\nable.  After  all,  system  pods  must  often  be  allowed  to  do  things  that  regular  pods\nshouldn’t.\n  Assigning  different  policies  to  different  users  is  done  through  the  RBAC  mecha-\nnism described in the previous chapter. The idea is to create as many policies as you\nneed and make them available to individual users or groups by creating ClusterRole\nresources  and  pointing  them  to  the  individual  policies  by  name.  By  binding  those\nClusterRoles  to  specific  users  or  groups  with  ClusterRoleBindings,  when  the  Pod-\nSecurityPolicy Admission Control plugin needs to decide whether to admit a pod defi-\nnition or not, it will only consider the policies accessible to the user creating the pod. \n You’ll see how to do this in the next exercise. You’ll start by creating an additional\nPodSecurityPolicy.\nCREATING A PODSECURITYPOLICY ALLOWING PRIVILEGED CONTAINERS TO BE DEPLOYED\nYou’ll create a special PodSecurityPolicy that will allow privileged users to create pods\nwith privileged containers. The following listing shows the policy’s definition.\napiVersion: extensions/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: privileged          \nspec:\n  privileged: true        \n  runAsUser:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  volumes:\n  - '*'\nAfter you post this policy to the API server, you have two policies in the cluster:\n$ kubectl get psp\nNAME         PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    ...  \ndefault      false   []     RunAsAny   RunAsAny    RunAsAny   ...\nprivileged   true    []     RunAsAny   RunAsAny    RunAsAny   ...\nNOTEThe shorthand for PodSecurityPolicy is psp.\nListing 13.20   A PodSecurityPolicy for privileged users: psp-privileged.yaml\nThe name of this \npolicy is \"privileged.”\nIt allows running \nprivileged containers.\n \n\n397Restricting the use of security-related features in pods\nAs you can see in the PRIV column, the default policy doesn’t allow running privi-\nleged  containers,  whereas  the  \nprivileged  policy  does.  Because  you’re  currently\nlogged in as a cluster-admin, you can see all the policies. When creating pods, if any\npolicy  allows  you  to  deploy  a  pod  with  certain  features,  the  API  server  will  accept\nyour pod.\n Now imagine two additional users are using your cluster: Alice and Bob. You want\nAlice  to  only  deploy  restricted  (non-privileged)  pods,  but  you  want  to  allow  Bob  to\nalso deploy privileged pod\ns. You do this by making sure Alice can only use the default\nPodSecurityPolicy, while allowing Bob to use both.\nUSING RBAC TO ASSIGN DIFFERENT PODSECURITYPOLICIES TO DIFFERENT USERS\nIn the previous chapter, you used RBAC to grant users access to only certain resource\ntypes, but I mentioned that access can be granted to specific resource instances by ref-\nerencing  them  by  name.  That’s  what  you’ll  use  to  make  users  use  different  Pod-\nSecurityPolicy resources.\n First, you’ll create two ClusterRoles, each allowing the use of one of the policies.\nYou’ll  call  the  first  one  \npsp-default  and  in  it  allow  the  use  of  the  default  Pod-\nSecurityPolicy resource. You can use \nkubectl create clusterrole to do that:\n$ kubectl create clusterrole psp-default --verb=use \n➥  --resource=podsecuritypolicies --resource-name=default\nclusterrole \"psp-default\" created\nNOTEYou’re using the special verb use instead of get, list, watch, or similar.\nAs you can see, you’re referring to a specific instance of a PodSecurityPolicy resource by\nusing  the  \n--resource-name  option.  Now,  create  another  ClusterRole  called  psp-\nprivileged\n, pointing to the privileged policy:\n$ kubectl create clusterrole psp-privileged --verb=use\n➥  --resource=podsecuritypolicies --resource-name=privileged\nclusterrole \"psp-privileged\" created\nNow, you need to bind these two policies to users. As you may remember from the pre-\nvious  chapter,  if  you’re  binding  a  ClusterRole  that  grants  access  to  cluster-level\nresources (which is what PodSecurityPolicy resources are), you need to use a Cluster-\nRoleBinding instead of a (namespaced) RoleBinding. \n You’re going to bind the \npsp-default ClusterRole to all authenticated users, not\nonly  to  Alice.  This  is  necessary  because  otherwise  no  one  could  create  any  pods,\nbecause  the  Admission  Control  plugin  would  complain  that  no  policy  is  in  place.\nAuthenticated  users  all  belong  to  the  \nsystem:authenticated  group,  so  you’ll  bind\nthe ClusterRole to the group:\n$ kubectl create clusterrolebinding psp-all-users \n➥ --clusterrole=psp-default --group=system:authenticated\nclusterrolebinding \"psp-all-users\" created\n \n\n398CHAPTER 13Securing cluster nodes and the network\nYou’ll bind the psp-privileged ClusterRole only to Bob:\n$ kubectl create clusterrolebinding psp-bob \n➥ --clusterrole=psp-privileged --user=bob\nclusterrolebinding \"psp-bob\" created\nAs  an  authenticated  user,  Alice  should  now  have  access  to  the  default  PodSecurity-\nPolicy, whereas Bob should have access to both the \ndefault and the privileged Pod-\nSecurityPolicies.  Alice  shouldn’t  be  able  to  create  privileged  pods,  whereas  Bob\nshould. Let’s see if that’s true.\nCREATING ADDITIONAL USERS FOR KUBECTL\nBut how do you authenticate as Alice or Bob instead of whatever you’re authenticated\nas currently? The book’s appendix A explains how \nkubectl can be used with multiple\nclusters, but also with multiple contexts. A context includes the user credentials used\nfor talking to a cluster. Turn to appendix A to find out more. Here we’ll show the bare\ncommands enabling you to use \nkubectl as Alice or Bob. \n  First,  you’ll  create  two  new  users  in  \nkubectl’s  config  with  the  following  two\ncommands:\n$ kubectl config set-credentials alice --username=alice --password=password\nUser \"alice\" set.\n$ kubectl config set-credentials bob --username=bob --password=password\nUser \"bob\" set.\nIt should be obvious what the commands do. Because you’re setting username and\npassword credentials, \nkubectl will use basic HTTP authentication for these two users\n(other authentication methods include tokens, client certificates, and so on).\nCREATING PODS AS A DIFFERENT USER\nYou can now try creating a privileged pod while authenticating as Alice. You can tell\nkubectl which user credentials to use by using the --user option:\n$ kubectl --user alice create -f pod-privileged.yaml\nError from server (Forbidden): error when creating \"pod-privileged.yaml\": \npods \"pod-privileged\" is forbidden: unable to validate against any pod \nsecurity policy: [spec.containers[0].securityContext.privileged: Invalid \nvalue: true: Privileged containers are not allowed]\nAs expected, the API server doesn’t allow Alice to create privileged pods. Now, let’s see\nif it allows Bob to do that:\n$ kubectl --user bob create -f pod-privileged.yaml\npod \"pod-privileged\" created\n \n\n399Isolating the pod network\nAnd  there  you  go.  You’ve  successfully  used  RBAC  to  make  the  Admission  Control\nplugin use different PodSecurityPolicy resources for different users.\n13.4   Isolating the pod network\nUp to now in this chapter, we’ve explored many security-related configuration options\nthat apply to individual pods and their containers. In the remainder of this chapter,\nwe’ll look at how the network between pods can be secured by limiting which pods can\ntalk to which pods.\n  Whether  this  is  configurable  or  not  depends  on  which  container  networking\nplugin is used in the cluster. If the networking plugin supports it, you can configure\nnetwork isolation by creating NetworkPolicy resources. \n A NetworkPolicy applies to pods that match its label selector and specifies either\nwhich  sources  can  access  the  matched  pods  or  which  destinations  can  be  accessed\nfrom the matched pods. This is configured through ingress and egress rules, respec-\ntively.  Both  types  of  rules  can  match  only  the  pods  that  match  a  pod  selector,  all\npods  in  a  namespace  whose  labels  match  a  namespace  selector,  or  a  network  IP\nblock specified using Classless Inter-Domain Routing (CIDR) notation (for example,\n192.168.1.0/24). \n We’ll look at both ingress and egress rules and all three matching options.\nNOTEIngress  rules  in  a  NetworkPolicy  have  nothing  to  do  with  the  Ingress\nresource discussed in chapter 5.\n13.4.1   Enabling network isolation in a namespace\nBy default, pods in a given namespace can be accessed by anyone. First, you’ll need\nto  change  that.  You’ll  create  a  \ndefault-deny  NetworkPolicy,  which  will  prevent  all\nclients  from  connecting  to  any  pod  in  your  namespace.  The  NetworkPolicy  defini-\ntion is shown in the following listing.\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector:        \nWhen you create this NetworkPolicy in a certain namespace, no one can connect to\nany pod in that namespace. \n \n \n \nListing 13.21   A default-deny NetworkPolicy: network-policy-default-deny.yaml\nEmpty pod selector \nmatches all pods in the \nsame namespace\n \n\n400CHAPTER 13Securing cluster nodes and the network\nNOTEThe CNI plugin or other type of networking solution used in the clus-\nter  must  support  NetworkPolicy,  or  else there will be no effect on inter-pod\nconnectivity.\n13.4.2   Allowing only some pods in the namespace to connect to \na server pod\nTo let clients connect to the pods in the namespace, you must now explicitly say who\ncan  connect  to  the  pods.  By  who  I  mean  which  pods.  Let’s  explore  how  to  do  this\nthrough an example. \n Imagine having a PostgreSQL database pod running in namespace \nfoo and a web-\nserver  pod  that  uses  the  database.  Other  pods  are  also  in  the  namespace,  and  you\ndon’t want to allow them to connect to the database. To secure the network, you need\nto create the NetworkPolicy resource shown in the following listing in the same name-\nspace as the database pod.\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: postgres-netpolicy\nspec:\n  podSelector:                     \n    matchLabels:                   \n      app: database                \n  ingress:                           \n  - from:                            \n    - podSelector:                   \n        matchLabels:                 \n          app: webserver             \n    ports:                     \n    - port: 5432               \nThe example NetworkPolicy allows pods with the app=webserver label to connect to\npods with the \napp=database label, and only on port 5432. Other pods can’t connect to\nthe database pods, and no one (not even the webserver pods) can connect to anything\nother than port 5432 of the database pods. This is shown in figure 13.4.\n Client pods usually connect to server pods through a Service instead of directly to\nthe pod, but that doesn’t change anything. The NetworkPolicy is enforced when con-\nnecting through a Service, as well.\n \n \n \n \nListing 13.22   A NetworkPolicy for the Postgres pod: network-policy-postgres.yaml\nThis policy secures \naccess to pods with \napp=database label.\nIt allows incoming connections \nonly from pods with the \napp=webserver label.\nConnections to this \nport are allowed.\n \n\n401Isolating the pod network\n13.4.3   Isolating the network between Kubernetes namespaces\nNow let’s look at another example, where multiple tenants are using the same Kuber-\nnetes  cluster.  Each  tenant  can  use  multiple  namespaces,  and  each  namespace  has  a\nlabel  specifying  the  tenant  it  belongs  to.  For  example,  one  of  those  tenants  is  Man-\nning. All their namespaces have been labeled with \ntenant: manning. In one of their\nnamespaces,  they  run  a  Shopping  Cart  microservice  that  needs  to  be  available  to  all\npods running in any of their namespaces. Obviously, they don’t want any other tenants\nto access their microservice.\n To secure their microservice, they create the NetworkPolicy resource shown in the\nfollowing listing.\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: shoppingcart-netpolicy\nspec:\n  podSelector:                       \n    matchLabels:                     \n      app: shopping-cart             \n  ingress:\n  - from:\n    - namespaceSelector:            \n        matchLabels:                \n          tenant: manning           \n    ports:\n    - port: 80\nListing 13.23   NetworkPolicy for the shopping cart pod(s): network-policy-cart.yaml\napp: database\nPod:\ndatabase\nPort\n5432\nPort\n9876\napp: webserver\nPod:\nwebserver\nPod selector:\napp=webserver\nPod selector:\napp=database\napp: webserver\nPod:\nwebserver\nOther pods\nNetworkPolicy: postgres-netpolicy\nFigure 13.4   A NetworkPolicy allowing only some pods to access other pods and only on a specific \nport\nThis policy applies to pods \nlabeled as microservice= \nshopping-cart.\nOnly pods running in namespaces \nlabeled as tenant=manning are \nallowed to access the microservice.\n \n\n402CHAPTER 13Securing cluster nodes and the network\nThis  NetworkPolicy  ensures  only  pods  running  in  namespaces  labeled  as  tenant:\nmanning\n can access their Shopping Cart microservice, as shown in figure 13.5.\nIf  the  shopping  cart  provider  also  wants  to  give  access  to  other  tenants  (perhaps  to\none of their partner companies), they can either create an additional NetworkPolicy\nresource or add an additional ingress rule to their existing NetworkPolicy.\nNOTEIn  a  multi-tenant  Kubernetes  cluster,  tenants  usually  can’t  add  labels\n(or annotations) to their namespaces themselves. If they could, they’d be able\nto circumvent the \nnamespaceSelector-based ingress rules.\n13.4.4   Isolating using CIDR notation\nInstead of specifying a pod- or namespace selector to define who can access the pods\ntargeted in the NetworkPolicy, you can also specify an IP block in CIDR notation. For\nexample, to allow the \nshopping-cart pods from the previous section to only be acces-\nsible  from  IPs  in  the  192.168.1.1  to  .255  range,  you’d  specify  the  ingress  rule  in  the\nnext listing.\n  ingress:\n  - from:\n    - ipBlock:                    \n        cidr: 192.168.1.0/24      \nListing 13.24   Specifying an IP block in an ingress rule: network-policy-cidr.yaml\napp: shopping-cart\nPod:\nshopping-cart\nPort\n80\nNamespace selector:\ntenant=manning\nPod selector:\napp=shopping-cart\nOther pods\nPods\nNetworkPolicy:\nshoppingcart-netpolicy\nNamespace: manningA\nNamespace: ecommerce-ltd\nOther namespaces\ntenant: manning\nPods\nNamespace: manningB\ntenant: manning\nFigure 13.5   A NetworkPolicy only allowing pods in namespaces matching a namespaceSelector to access a \nspecific pod.\nThis ingress rule only allows traffic from \nclients in the 192.168.1.0/24 IP block. \n \n\n403Summary\n13.4.5   Limiting the outbound traffic of a set of pods\nIn  all  previous  examples,  you’ve  been  limiting  the  inbound  traffic  to  the  pods  that\nmatch  the  NetworkPolicy’s  pod  selector  using  ingress  rules,  but  you  can  also  limit\ntheir outbound traffic through egress rules. An example is shown in the next listing.\nspec:\n  podSelector:               \n    matchLabels:             \n      app: webserver         \n  egress:               \n  - to:                       \n    - podSelector:            \n        matchLabels:          \n          app: database       \nThe  NetworkPolicy  in  the  previous  listing  allows  pods  that  have  the  app=webserver\nlabel to only access pods that have the app=database label and nothing else (neither\nother pods, nor any other IP, regardless of whether it’s internal or external to the\ncluster).\n13.5   Summary\nIn  this  chapter,  you  learned  about  securing  cluster  nodes  from  pods  and  pods  from\nother pods. You learned that\nPods can use the node’s Linux namespaces instead of using their own.\nContainers can be configured to run as a different user and/or group than the\none defined in the container image.\nContainers can also run in privileged mode, allowing them to access the node’s\ndevices that are otherwise not exposed to pods.\nContainers  can  be  run  as  read-only,  preventing  processes  from  writing  to  the\ncontainer’s filesystem (and only allowing them to write to mounted volumes).\nCluster-level PodSecurityPolicy resources can be created to prevent users from\ncreating pods that could compromise a node.\nPodSecurityPolicy  resources  can  be  associated  with  specific  users  using  RBAC’s\nClusterRoles and ClusterRoleBindings.\nNetworkPolicy  resources  are  used  to  limit  a  pod’s  inbound  and/or  outbound\ntraffic.\nIn the next chapter, you’ll learn how computational resources available to pods can be\nconstrained and how a pod’s quality of service is configured.\nListing 13.25   Using egress rules in a NetworkPolicy: network-policy-egress.yaml\nThis policy applies to pods with \nthe app=webserver label.\nIt limits\nthe pods’\noutbound\ntraffic.\nWebserver pods may only \nconnect to pods with the \napp=database label.\n \n\n404\nManaging pods’\n computational resources\nUp to now you’ve created pods without caring about how much CPU and memory\nthey’re  allowed  to  consume.  But  as  you’ll  see  in  this  chapter,  setting  both  how\nmuch a pod is expected to consume and the maximum amount it’s allowed to con-\nsume  is  a  vital  part  of  any  pod  definition.  Setting  these  two  sets  of  parameters\nmakes  sure  that  a  pod  takes  only  its  fair  share  of  the  resources  provided  by  the\nKubernetes cluster and also affects how pods are scheduled across the cluster.\nThis chapter covers\nRequesting CPU, memory, and other \ncomputational resources for containers\nSetting a hard limit for CPU and memory\nUnderstanding Quality of Service guarantees for \npods\nSetting default, min, and max resources for pods \nin a namespace\nLimiting the total amount of resources available \nin a namespace\n \n\n405Requesting resources for a pod’s containers\n14.1   Requesting resources for a pod’s containers\nWhen creating a pod, you can specify the amount of CPU and memory that a con-\ntainer  needs  (these  are  called  requests)  and  a  hard  limit  on  what  it  may  consume\n(known as limits). They’re specified for each container individually, not for the pod as\na whole. The pod’s resource requests and limits are the sum of the requests and lim-\nits of all its containers. \n14.1.1   Creating pods with resource requests\nLet’s look at an example pod manifest, which has the CPU and memory requests spec-\nified for its single container, as shown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: requests-pod\nspec:\n  containers:\n  - image: busybox\n    command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"]\n    name: main              \n    resources:              \n      requests:             \n        cpu: 200m          \n        memory: 10Mi    \nIn the pod manifest, your single container requires one-fifth of a CPU core (200 mil-\nlicores) to run properly. Five such pods/containers can run sufficiently fast on a single\nCPU core. \n When you don’t specify a request for CPU, you’re saying you don’t care how much\nCPU time the process running in your container is allotted. In the worst case, it may\nnot  get  any  CPU  time  at  all  (this  happens  when  a  heavy  demand  by  other  processes\nexists on the CPU). Although this may be fine for low-priority batch jobs, which aren’t\ntime-critical, it obviously isn’t appropriate for containers handling user requests.\n In the pod spec, you’re also requesting 10 mebibytes of memory for the container.\nBy  doing  that,  you’re  saying  that  you  expect  the  processes  running  inside  the  con-\ntainer to use at most 10 mebibytes of RAM. They might use less, but you’re not expect-\ning them to use more than that in normal circumstances. Later in this chapter you’ll\nsee what happens if they do.\n Now you’ll run the pod. When the pod starts, you can take a quick look at the pro-\ncess’ CPU consumption by running the \ntop command inside the container, as shown\nin the following listing.\nListing 14.1   A pod with resource requests: requests-pod.yaml\nYou’re specifying resource \nrequests for the main container.\nThe container requests 200 \nmillicores (that is, 1/5 of a \nsingle CPU core’s time).\nThe container also\nrequests 10 mebibytes\nof memory.\n \n\n406CHAPTER 14Managing pods’ computational resources\n$ kubectl exec -it requests-pod top\nMem: 1288116K used, 760368K free, 9196K shrd, 25748K buff, 814840K cached\nCPU:  9.1% usr 42.1% sys  0.0% nic 48.4% idle  0.0% io  0.0% irq  0.2% sirq\nLoad average: 0.79 0.52 0.29 2/481 10\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\n    1     0 root     R     1192  0.0   1 50.2 dd if /dev/zero of /dev/null\n    7     0 root     R     1200  0.0   0  0.0 top\nThe dd command you’re running in the container consumes as much CPU as it can,\nbut  it  only  runs  a  single  thread  so  it  can  only  use  a  single  core.  The  Minikube  VM,\nwhich is where this example is running, has two CPU cores allotted to it. That’s why\nthe process is shown consuming 50% of the whole CPU. \n Fifty percent of two cores is obviously one whole core, which means the container\nis using more than the 200 millicores you requested in the pod specification. This is\nexpected, because requests don’t limit the amount of CPU a container can use. You’d\nneed to specify a CPU limit to do that. You’ll try that later, but first, let’s see how spec-\nifying resource requests in a pod affects the scheduling of the pod.\n14.1.2   Understanding how resource requests affect scheduling\nBy specifying resource requests, you’re specifying the minimum amount of resources\nyour pod needs. This information is what the Scheduler uses when scheduling the pod\nto  a  node.  Each  node  has  a  certain  amount  of  CPU  and  memory  it  can  allocate  to\npods.  When  scheduling  a  pod,  the  Scheduler  will  only  consider  nodes  with  enough\nunallocated  resources  to  meet  the  pod’s  resource  requirements.  If  the  amount  of\nunallocated  CPU  or  memory  is  less  than  what  the  pod  requests,  Kubernetes  will  not\nschedule the pod to that node, because the node can’t provide the minimum amount\nrequired by the pod.\nUNDERSTANDING HOW THE SCHEDULER DETERMINES IF A POD CAN FIT ON A NODE\nWhat’s important and somewhat surprising here is that the Scheduler doesn’t look at\nhow much of each individual resource is being used at the exact time of scheduling\nbut  at  the  sum  of  resources  requested  by  the  existing  pods  deployed  on  the  node.\nEven though existing pods may be using less than what they’ve requested, scheduling\nanother pod based on actual resource consumption would break the guarantee given\nto the already deployed pods.\n This is visualized in figure 14.1. Three pods are deployed on the node. Together,\nthey’ve  requested  80%  of  the  node’s  CPU  and  60%  of  the  node’s  memory.  Pod  D,\nshown at the bottom right of the figure, cannot be scheduled onto the node because it\nrequests 25% of the CPU, which is more than the 20% of unallocated CPU. The fact\nthat the three pods are currently using only 70% of the CPU makes no difference.\nListing 14.2   Examining CPU and memory usage from within a container\n \n\n407Requesting resources for a pod’s containers\nUNDERSTANDING HOW THE SCHEDULER USES PODS’ REQUESTS WHEN SELECTING THE BEST NODE \nFOR A POD\nYou may remember from chapter 11 that the Scheduler first filters the list of nodes to\nexclude those that the pod can’t fit on and then prioritizes the remaining nodes per the\nconfigured  prioritization  functions.  Among  others,  two  prioritization  functions  rank\nnodes  based  on  the  amount  of  resources  requested:  \nLeastRequestedPriority  and\nMostRequestedPriority.  The  first  one  prefers  nodes  with  fewer  requested  resources\n(with a greater amount of unallocated resources), whereas the second one is the exact\nopposite—it prefers nodes that have the most requested resources (a smaller amount of\nunallocated CPU and memory). But, as we’ve discussed, they both consider the amount\nof requested resources, not the amount of resources actually consumed.\n The Scheduler is configured to use only one of those functions. You may wonder\nwhy anyone would want to use the \nMostRequestedPriority function. After all, if you\nhave a set of nodes, you usually want to spread CPU load evenly across them. However,\nthat’s  not  the  case  when  running  on  cloud  infrastructure,  where  you  can  add  and\nremove  nodes  whenever  necessary.  By  configuring  the  Scheduler  to  use  the  \nMost-\nRequestedPriority\n function, you guarantee that Kubernetes will use the smallest pos-\nsible number of nodes while still providing each pod with the amount of CPU/memory\nit requests. By keeping pods tightly packed, certain nodes are left vacant and can be\nremoved. Because you’re paying for individual nodes, this saves you money.\nINSPECTING A NODE’S CAPACITY\nLet’s  see  the  Scheduler  in  action.  You’ll  deploy  another  pod  with  four  times  the\namount of requested resources as before. But before you do that, let’s see your node’s\ncapacity.  Because  the  Scheduler  needs  to  know  how  much  CPU  and  memory  each\nnode has, the Kubelet reports this data to the API server, making it available through\nPod C\nNode\nPod AUnallocatedCPU requestsPod B\nPod ACurrently unusedCPU usagePod BPod C\n0%100%\nPod AMemory requestsPod BPod C\nPod AMemory usagePod BPod C\nCPU requests\nMemory requests\nUnallocated\nCurrently unused\nPod D\nPod D cannot be scheduled; its CPU\nrequests exceed unallocated CPU\nFigure 14.1   The Scheduler only cares about requests, not actual usage.\n \n\n408CHAPTER 14Managing pods’ computational resources\nthe Node resource. You can see it by using the kubectl describe command as in the\nfollowing listing.\n$ kubectl describe nodes\nName:       minikube\n...\nCapacity:                       \n  cpu:           2               \n  memory:        2048484Ki       \n  pods:          110             \nAllocatable:                       \n  cpu:           2                  \n  memory:        1946084Ki          \n  pods:          110                \n...\nThe output shows two sets of amounts related to the available resources on the node:\nthe node’s capacity and allocatable resources. The capacity represents the total resources\nof a node, which may not all be available to pods. Certain resources may be reserved\nfor Kubernetes and/or system components. The Scheduler bases its decisions only on\nthe allocatable resource amounts.\n In the previous example, the node called \nminikube runs in a VM with two cores\nand has no CPU reserved, making the whole CPU allocatable to pods. Therefore,\nthe  Scheduler  should  have  no  problem  scheduling  another  pod  requesting  800\nmillicores. \n Run the pod now. You can use the YAML file in the code archive, or run the pod\nwith the \nkubectl run command like this:\n$ kubectl run requests-pod-2 --image=busybox --restart Never\n➥ --requests='cpu=800m,memory=20Mi' -- dd if=/dev/zero of=/dev/null\npod \"requests-pod-2\" created\nLet’s see if it was scheduled:\n$ kubectl get po requests-pod-2\nNAME             READY     STATUS    RESTARTS   AGE\nrequests-pod-2   1/1       Running   0          3m\nOkay, the pod has been scheduled and is running. \nCREATING A POD THAT DOESN’T FIT ON ANY NODE\nYou now have two pods deployed, which together have requested a total of 1,000 mil-\nlicores or exactly 1 core. You should therefore have another 1,000 millicores available\nfor  additional  pods,  right?  You  can  deploy  another  pod  with  a  resource  request  of\n1,000 millicores. Use a similar command as before:\n$ kubectl run requests-pod-3 --image=busybox --restart Never\n➥ --requests='cpu=1,memory=20Mi' -- dd if=/dev/zero of=/dev/null\npod \"requests-pod-2\" created\nListing 14.3   A node’s capacity and allocatable resources\nThe overall capacity \nof the node\nThe resources \nallocatable to pods\n \n\n409Requesting resources for a pod’s containers\nNOTEThis  time  you’re  specifying  the  CPU  request  in  whole  cores  (cpu=1)\ninstead of millicores (\ncpu=1000m).\nSo far, so good. The pod has been accepted by the API server (you’ll remember from\nthe previous chapter that the API server can reject pods if they’re invalid in any way).\nNow, check if the pod is running:\n$ kubectl get po requests-pod-3\nNAME             READY     STATUS    RESTARTS   AGE\nrequests-pod-3   0/1       Pending   0          4m\nEven if you wait a while, the pod is still stuck at Pending. You can see more informa-\ntion  on  why  that’s  the  case  by  using  the  \nkubectl describe  command,  as  shown  in\nthe following listing.\n$ kubectl describe po requests-pod-3\nName:       requests-pod-3\nNamespace:  default\nNode:       /                    \n...\nConditions:\n  Type           Status\n  PodScheduled   False           \n...\nEvents:\n... Warning  FailedScheduling    No nodes are available      \n                                 that match all of the       \n                                 following predicates::      \n                                 Insufficient cpu (1).       \nThe output shows that the pod hasn’t been scheduled because it can’t fit on any node\ndue to insufficient CPU on your single node.  But  why  is  that?  The  sum  of  the  CPU\nrequests of all three pods equals 2,000 millicores or exactly two cores, which is exactly\nwhat your node can provide. What’s wrong?\nDETERMINING WHY A POD ISN’T BEING SCHEDULED\nYou can figure out why the pod isn’t being scheduled by inspecting the node resource.\nUse  the  \nkubectl describe node  command  again  and  examine  the  output  more\nclosely in the following listing.\n$ kubectl describe node\nName:                   minikube\n...\nNon-terminated Pods:    (7 in total)\n  Namespace    Name            CPU Requ.   CPU Lim.  Mem Req.    Mem Lim.\n  ---------    ----            ----------  --------  ---------   --------\n  default      requests-pod    200m (10%)  0 (0%)    10Mi (0%)   0 (0%)\nListing 14.4   Examining why a pod is stuck at Pending with kubectl describe pod\nListing 14.5   Inspecting allocated resources on a node with kubectl describe node\nNo node is \nassociated \nwith the pod.\nThe pod hasn’t \nbeen scheduled.\nScheduling has \nfailed because of \ninsufficient CPU.\n \n\n410CHAPTER 14Managing pods’ computational resources\n  default      requests-pod-2  800m (40%)  0 (0%)    20Mi (1%)   0 (0%)\n  kube-system  dflt-http-b...  10m (0%)    10m (0%)  20Mi (1%)   20Mi (1%)\n  kube-system  kube-addon-...  5m (0%)     0 (0%)    50Mi (2%)   0 (0%)\n  kube-system  kube-dns-26...  260m (13%)  0 (0%)    110Mi (5%)  170Mi (8%)\n  kube-system  kubernetes-...  0 (0%)      0 (0%)    0 (0%)      0 (0%)\n  kube-system  nginx-ingre...  0 (0%)      0 (0%)    0 (0%)      0 (0%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  CPU Requests  CPU Limits      Memory Requests Memory Limits\n  ------------  ----------      --------------- -------------\n  1275m (63%)   10m (0%)        210Mi (11%)     190Mi (9%)\nIf you look at the bottom left of the listing, you’ll see a total of 1,275 millicores have\nbeen  requested  by  the  running  pods,  which  is  275  millicores  more  than  what  you\nrequested  for  the  first  two  pods  you  deployed.  Something  is  eating  up  additional\nCPU resources. \n You can find the culprit in the list of pods in the previous listing. Three pods in the\nkube-system  namespace  have  explicitly  requested  CPU  resources.  Those  pods  plus\nyour  two  pods  leave  only  725  millicores  available  for  additional  pods.  Because  your\nthird pod requested 1,000 millicores, the Scheduler won’t schedule it to this node, as\nthat would make the node overcommitted. \nFREEING RESOURCES TO GET THE POD SCHEDULED\nThe pod will only be scheduled when an adequate amount of CPU is freed (when one\nof  the  first  two  pods  is  deleted,  for  example).  If  you  delete  your  second  pod,  the\nScheduler will be notified of the deletion (through the watch mechanism described in\nchapter  11)  and  will  schedule  your  third  pod  as  soon  as  the  second  pod  terminates.\nThis is shown in the following listing.\n$ kubectl delete po requests-pod-2\npod \"requests-pod-2\" deleted \n$ kubectl get po\nNAME             READY     STATUS        RESTARTS   AGE\nrequests-pod     1/1       Running       0          2h\nrequests-pod-2   1/1       Terminating   0          1h\nrequests-pod-3   0/1       Pending       0          1h\n$ kubectl get po\nNAME             READY     STATUS    RESTARTS   AGE\nrequests-pod     1/1       Running   0          2h\nrequests-pod-3   1/1       Running   0          1h\nIn  all  these  examples,  you’ve  specified  a  request  for  memory,  but  it  hasn’t  played  any\nrole in the scheduling because your node has more than enough allocatable memory to\naccommodate all your pods’ requests. Both CPU and memory requests are treated the\nsame way by the Scheduler, but in contrast to memory requests, a pod’s CPU requests\nalso play a role elsewhere—while the pod is running. You’ll learn about this next.\nListing 14.6   Pod is scheduled after deleting another pod\n \n\n411Requesting resources for a pod’s containers\n14.1.3   Understanding how CPU requests affect CPU time sharing\nYou  now  have  two  pods  running  in  your  cluster  (you  can  disregard  the  system  pods\nright  now,  because  they’re  mostly  idle).  One  has  requested  200  millicores  and  the\nother one five times as much. At the beginning of the chapter, we said Kubernetes dis-\ntinguishes between resource requests and limits. You haven’t defined any limits yet, so\nthe  two  pods  are  in  no  way  limited  when  it  comes  to  how  much  CPU  they  can  each\nconsume. If the process inside each pod consumes as much CPU time as it can, how\nmuch CPU time does each pod get? \n  The  CPU  requests  don’t  only  affect  scheduling—they  also  determine  how  the\nremaining  (unused)  CPU  time  is  distributed  between  pods.  Because  your  first  pod\nrequested 200 millicores of CPU and the other one 1,000 millicores, any unused CPU\nwill be split among the two pods in a 1 to 5 ratio, as shown in figure 14.2. If both pods\nconsume  as  much  CPU  as  they  can,  the  first  pod  will  get  one  sixth  or  16.7%  of  the\nCPU time and the other one the remaining five sixths or 83.3%.\nBut if one container wants to use up as much CPU as it can, while the other one is sit-\nting idle at a given moment, the first container will be allowed to use the whole CPU\ntime (minus the small amount of time used by the second container, if any). After all,\nit makes sense to use all the available CPU if no one else is using it, right? As soon as\nthe second container needs CPU time, it will get it and the first container will be throt-\ntled back.\n14.1.4   Defining and requesting custom resources\nKubernetes also allows you to add your own custom resources to a node and request\nthem  in  the  pod’s  resource  requests.  Initially  these  were  known  as  Opaque  Integer\nResources, but were replaced with Extended Resources in Kubernetes version 1.8.\nPod A:\n200 m\nCPU\nrequests\nPod B: 1000 m800 m available\nCPU\nusage\n2000 m1000 m0 m\nPod A and B requests\nare in 1:5 ratio.\nAvailable CPU time is\ndistributed in same ratio.\nPod B: 1667 m\n133 m\n(1/6)\n667 m\n(5/6)\nPod A:\n333 m\nFigure 14.2   Unused CPU time is distributed to containers based on their CPU requests.\n \n\n412CHAPTER 14Managing pods’ computational resources\n  First,  you  obviously  need  to  make  Kubernetes  aware  of  your  custom  resource  by\nadding  it  to  the  Node  object’s  \ncapacity  field.  This  can  be  done  by  performing  a\nPATCH HTTP request. The resource name can be anything, such as example.org/my-\nresource\n,  as  long  as  it  doesn’t  start  with  the  kubernetes.io  domain.  The  quantity\nmust be an integer (for example, you can’t set it to 100 millis, because 0.1 isn’t an inte-\nger; but you can set it to 1000m or 2000m or, simply, 1 or 2). The value will be copied\nfrom the \ncapacity to the allocatable field automatically.\n Then, when creating pods, you specify the same resource name and the requested\nquantity under the \nresources.requests field in the container spec or with --requests\nwhen using kubectl run like you did in previous examples. The Scheduler will make\nsure the pod is only deployed to a node that has the requested amount of the custom\nresource  available.  Every  deployed  pod  obviously  reduces  the  number  of  allocatable\nunits of the resource.\n An example of a custom resource could be the number of GPU units available on the\nnode. Pods requiring the use of a GPU specify that in their requests. The Scheduler then\nmakes sure the pod is only scheduled to nodes with at least one GPU still unallocated.\n14.2   Limiting resources available to a container\nSetting resource requests for containers in a pod ensures each container gets the min-\nimum  amount  of  resources  it  needs.  Now  let’s  see  the  other  side  of  the  coin—the\nmaximum amount the container will be allowed to consume. \n14.2.1   Setting a hard limit for the amount of resources a container can use\nWe’ve seen how containers are allowed to use up all the CPU if all the other processes\nare sitting idle. But you may want to prevent certain containers from using up more\nthan a specific amount of CPU. And you’ll always want to limit the amount of memory\na container can consume. \n CPU is a compressible resource, which means the amount used by a container can\nbe throttled without affecting the process running in the container in an adverse way.\nMemory is obviously different—it’s incompressible. Once a process is given a chunk of\nmemory,  that  memory  can’t  be  taken  away  from  it  until  it’s  released  by  the  process\nitself. That’s why you need to limit the maximum amount of memory a container can\nbe given. \n Without limiting memory, a container (or a pod) running on a worker node may\neat  up  all  the  available  memory  and  affect  all  other  pods  on  the  node  and  any  new\npods  scheduled  to  the  node  (remember  that  new  pods  are  scheduled  to  the  node\nbased on the memory requests and not actual memory usage). A single malfunction-\ning or malicious pod can practically make the whole node unusable.\nCREATING A POD WITH RESOURCE LIMITS\nTo prevent this from happening, Kubernetes allows you to specify resource limits for\nevery container (along with, and virtually in the same way as, resource requests). The\nfollowing listing shows an example pod manifest with resource limits.\n \n\n413Limiting resources available to a container\napiVersion: v1\nkind: Pod\nmetadata:\n  name: limited-pod\nspec:\n  containers:\n  - image: busybox\n    command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"]\n    name: main\n    resources:            \n      limits:             \n        cpu: 1             \n        memory: 20Mi       \nThis pod’s container has resource limits configured for both CPU and memory. The\nprocess  or  processes  running  inside  the  container  will  not  be  allowed  to  consume\nmore than 1 CPU core and 20 mebibytes of memory. \nNOTEBecause  you  haven’t  specified  any  resource  requests,  they’ll  be  set  to\nthe same values as the resource limits.\nOVERCOMMITTING LIMITS\nUnlike resource requests, resource limits aren’t constrained by the node’s allocatable\nresource amounts. The sum of all limits of all the pods on a node is allowed to exceed\n100% of the node’s capacity (figure 14.3). Restated, resource limits can be overcom-\nmitted. This has an important consequence—when 100% of the node’s resources are\nused up, certain containers will need to be killed.\nYou’ll see how Kubernetes decides which containers to kill in section 14.3, but individ-\nual  containers  can  be  killed  even  if  they  try  to  use  more  than  their  resource  limits\nspecify. You’ll learn more about this next.\nListing 14.7   A pod with a hard limit on CPU and memory: limited-pod.yaml\nSpecifying resource \nlimits for the container\nThis container will be \nallowed to use at \nmost 1 CPU core.\nThe container will be\nallowed to use up to 20\nmebibytes of memory.\nNode\n0%136%100%\nPod AMemory requestsPod BPod C\nPod AMemory limitsPod B\nUnallocated\nPod C\nFigure 14.3   The sum of resource limits of all pods on a node can exceed 100% of the node’s \ncapacity.\n \n\n414CHAPTER 14Managing pods’ computational resources\n14.2.2   Exceeding the limits\nWhat happens when a process running in a container tries to use a greater amount of\nresources than it’s allowed to? \n You’ve already learned that CPU is a compressible resource, and it’s only natural\nfor a process to want to consume all of the CPU time when not waiting for an I/O\noperation. As you’ve learned, a process’ CPU usage is throttled, so when a CPU\nlimit is set for a container, the process isn’t given more CPU time than the config-\nured limit. \n  With  memory,  it’s  different.  When  a  process  tries  to  allocate  memory  over  its\nlimit, the process is killed (it’s said the container is \nOOMKilled, where OOM stands\nfor  Out  Of  Memory).  If  the  pod’s  restart  policy  is  set  to  \nAlways or OnFailure,  the\nprocess is restarted immediately, so you may not even notice it getting killed. But if it\nkeeps going over the memory limit and getting killed, Kubernetes will begin restart-\ning it with increasing delays between restarts. You’ll see a \nCrashLoopBackOff status\nin that case:\n$ kubectl get po\nNAME        READY     STATUS             RESTARTS   AGE\nmemoryhog   0/1       CrashLoopBackOff   3          1m\nThe CrashLoopBackOff status doesn’t mean the Kubelet has given up. It means that\nafter each crash, the Kubelet is increasing the time period before restarting the con-\ntainer. After the first crash, it restarts the container immediately and then, if it crashes\nagain,  waits  for  10  seconds  before  restarting  it  again.  On  subsequent  crashes,  this\ndelay is then increased exponentially to 20, 40, 80, and 160 seconds, and finally lim-\nited  to  300  seconds.  Once  the  interval  hits  the  300-second  limit,  the  Kubelet  keeps\nrestarting  the  container  indefinitely  every  five  minutes  until  the  pod  either  stops\ncrashing or is deleted. \n To examine why the container crashed, you can check the pod’s log and/or use\nthe \nkubectl describe pod command, as shown in the following listing.\n$ kubectl describe pod\nName:       memoryhog\n...\nContainers:\n  main:\n    ...\n    State:          Terminated          \n      Reason:       OOMKilled           \n      Exit Code:    137\n      Started:      Tue, 27 Dec 2016 14:55:53 +0100\n      Finished:     Tue, 27 Dec 2016 14:55:58 +0100\n    Last State:     Terminated            \n      Reason:       OOMKilled             \n      Exit Code:    137\nListing 14.8   Inspecting why a container terminated with kubectl describe pod\nThe current container was \nkilled because it was out \nof memory (OOM).\nThe previous container \nwas also killed because \nit was  OOM\n \n\n415Limiting resources available to a container\n      Started:      Tue, 27 Dec 2016 14:55:37 +0100\n      Finished:     Tue, 27 Dec 2016 14:55:50 +0100\n    Ready:          False\n...\nThe OOMKilled status tells you that the container was killed because it was out of mem-\nory.  In  the  previous  listing,  the  container  went  over  its  memory  limit  and  was  killed\nimmediately. \n It’s important not to set memory limits too low if you don’t want your container to\nbe killed. But containers can get \nOOMKilled even if they aren’t over their limit. You’ll\nsee why in section 14.3.2, but first, let’s discuss something that catches most users off-\nguard the first time they start specifying limits for their containers.\n14.2.3   Understanding how apps in containers see limits\nIf you haven’t deployed the pod from listing 14.7, deploy it now:\n$ kubectl create -f limited-pod.yaml\npod \"limited-pod\" created\nNow, run the top command in the container, the way you did at the beginning of the\nchapter. The command’s output is shown in the following listing.\n$ kubectl exec -it limited-pod top\nMem: 1450980K used, 597504K free, 22012K shrd, 65876K buff, 857552K cached\nCPU: 10.0% usr 40.0% sys  0.0% nic 50.0% idle  0.0% io  0.0% irq  0.0% sirq\nLoad average: 0.17 1.19 2.47 4/503 10\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\n    1     0 root     R     1192  0.0   1 49.9 dd if /dev/zero of /dev/null\n    5     0 root     R     1196  0.0   0  0.0 top\nFirst, let me remind you that the pod’s CPU limit is set to 1 core and its memory limit\nis set to 20 MiB. Now, examine the output of the \ntop command closely. Is there any-\nthing that strikes you as odd?\n Look at the amount of used and free memory. Those numbers are nowhere near\nthe 20 MiB you set as the limit for the container. Similarly, you set the CPU limit to\none  core  and  it  seems  like  the  main  process  is  using  only  50%  of  the  available  CPU\ntime, even though the \ndd command, when used like you’re using it, usually uses all the\nCPU it has available. What’s going on?\nUNDERSTANDING THAT CONTAINERS ALWAYS SEE THE NODE’S MEMORY, NOT THE CONTAINER’S\nThe top  command  shows  the  memory  amounts  of  the  whole  node  the  container  is\nrunning on. Even though you set a limit on how much memory is available to a con-\ntainer, the container will not be aware of this limit. \nListing 14.9   Running the top command in a CPU- and memory-limited container\n \n\n416CHAPTER 14Managing pods’ computational resources\n  This  has  an  unfortunate  effect  on  any  application  that  looks  up  the  amount  of\nmemory available on the system and uses that information to decide how much mem-\nory it wants to reserve. \n The problem is visible when running Java apps, especially if you don’t specify the\nmaximum heap size for the Java Virtual Machine with the \n-Xmx option. In that case,\nthe JVM will set the maximum heap size based on the host’s total memory instead of\nthe memory available to the container. When you run your containerized Java apps in\na Kubernetes cluster on your laptop, the problem doesn’t manifest itself, because the\ndifference between the memory limits you set for the pod and the total memory avail-\nable on your laptop is not that great. \n But when you deploy your pod onto a production system, where nodes have much\nmore physical memory, the JVM may go over the container’s memory limit you config-\nured and will be \nOOMKilled. \n And if you think setting the \n-Xmx  option  properly  solves  the  issue,  you’re  wrong,\nunfortunately. The \n-Xmx option only constrains the heap size, but does nothing about\nthe JVM’s off-heap memory. Luckily, new versions of Java alleviate that problem by tak-\ning the configured container limits into account.\nUNDERSTANDING THAT CONTAINERS ALSO SEE ALL THE NODE’S CPU CORES\nExactly  like  with  memory,  containers  will  also  see  all  the  node’s  CPUs,  regardless  of\nthe CPU limits configured for the container. Setting a CPU limit to one core doesn’t\nmagically only expose only one CPU core to the container. All the CPU limit does is\nconstrain the amount of CPU time the container can use. \n A container with a one-core CPU limit running on a 64-core CPU will get 1/64th\nof the overall CPU time. And even though its limit is set to one core, the container’s\nprocesses will not run on only one core. At different points in time, its code may be\nexecuted on different cores.\n Nothing is wrong with this, right? While that’s generally the case, at least one sce-\nnario exists where this situation is catastrophic.\n  Certain  applications  look  up  the  number  of CPUs on the system to decide how\nmany worker threads they should run. Again, such an app will run fine on a develop-\nment laptop, but when deployed on a node with a much bigger number of cores, it’s\ngoing to spin up too many threads, all competing for the (possibly) limited CPU time.\nAlso, each thread requires additional memory, causing the apps memory usage to sky-\nrocket. \n You may want to use the Downward API to pass the CPU limit to the container and\nuse it instead of relying on the number of CPUs your app can see on the system. You\ncan also tap into the cgroups system directly to get the configured CPU limit by read-\ning the following files:\n/sys/fs/cgroup/cpu/cpu.cfs_quota_us\n/sys/fs/cgroup/cpu/cpu.cfs_period_us\n \n\n417Understanding pod QoS classes\n14.3   Understanding pod QoS classes\nWe’ve  already  mentioned  that  resource  limits  can  be  overcommitted  and  that  a\nnode can’t necessarily provide all its pods the amount of resources specified in their\nresource limits. \n Imagine having two pods, where pod A is using, let’s say, 90% of the node’s mem-\nory and then pod B suddenly requires more memory than what it had been using up\nto  that  point  and  the  node  can’t  provide  the  required  amount  of  memory.  Which\ncontainer should be killed? Should it be pod B, because its request for memory can’t\nbe satisfied, or should pod A be killed to free up memory, so it can be provided to\npod B? \n Obviously, it depends. Kubernetes can’t make a proper decision on its own. You\nneed a way to specify which pods have priority in such cases. Kubernetes does this by\ncategorizing pods into three Quality of Service (QoS) classes:\nBestEffort (the lowest priority)\nBurstable\nGuaranteed (the highest)\n14.3.1   Defining the QoS class for a pod\nYou might expect these classes to be assignable to pods through a separate field in the\nmanifest, but they aren’t. The QoS class is derived from the combination of resource\nrequests and limits for the pod’s containers. Here’s how.\nASSIGNING A POD TO THE BESTEFFORT CLASS\nThe lowest priority QoS class is the BestEffort class. It’s assigned to pods that don’t\nhave any requests or limits set at all (in any of their containers). This is the QoS class\nthat has been assigned to all the pods you created in previous chapters. Containers\nrunning  in  these  pods  have  had  no  resource  guarantees  whatsoever.  In  the  worst\ncase,  they  may  get  almost  no  CPU  time  at  all  and  will  be  the  first  ones  killed  when\nmemory  needs  to  be  freed  for  other  pods.  But  because  a  \nBestEffort  pod  has  no\nmemory  limits  set,  its  containers  may  use  as  much  memory  as  they  want,  if  enough\nmemory is available.\nASSIGNING A POD TO THE GUARANTEED CLASS\nOn the other end of the spectrum is the Guaranteed QoS class. This class is given to\npods whose containers’ requests are equal to the limits for all resources. For a pod’s\nclass to be \nGuaranteed, three things need to be true:\nRequests and limits need to be set for both CPU and memory.\nThey need to be set for each container.\nThey need to be equal (the limit needs to match the request for each resource\nin each container).\nBecause  a  container’s  resource  requests,  if  not  set  explicitly,  default  to  the  limits,\nspecifying the limits for all resources (for each container in the pod) is enough for\n \n\n418CHAPTER 14Managing pods’ computational resources\nthe  pod  to  be  Guaranteed.  Containers  in  those  pods  get  the  requested  amount  of\nresources, but cannot consume additional ones (because their limits are no higher\nthan their requests). \nASSIGNING THE BURSTABLE QOS CLASS TO A POD\nIn  between  BestEffort  and  Guaranteed  is  the  Burstable  QoS  class.  All  other  pods\nfall  into  this  class.  This  includes  single-container  pods  where  the  container’s  limits\ndon’t  match  its  requests  and  all  pods  where  at  least  one  container  has  a  resource\nrequest  specified,  but  not  the  limit.  It  also  includes  pods  where  one  container’s\nrequests match their limits, but another container has no requests or limits specified.\nBurstable pods get the amount of resources they request, but are allowed to use addi-\ntional resources (up to the limit) if needed.\nUNDERSTANDING HOW THE RELATIONSHIP BETWEEN REQUESTS AND LIMITS DEFINES THE QOS CLASS\nAll three QoS classes and their relationships with requests and limits are shown in fig-\nure 14.4.\nThinking about what QoS class a pod has can make your head spin, because it involves\nmultiple  containers,  multiple  resources,  and  all  the  possible  relationships  between\nrequests and limits. It’s easier if you start by thinking about QoS at the container level\n(although  QoS  classes  are  a  property  of  pods,  not  containers)  and  then  derive  the\npod’s QoS class from the QoS classes of containers. \nFIGURING OUT A CONTAINER’S QOS CLASS\nTable  14.1  shows  the  QoS  class  based  on  how  resource  requests  and  limits  are\ndefined  on  a  single  container.  For  single-container  pods,  the  QoS  class  applies  to\nthe pod as well.\n \nBestEffort\nQoS\nRequestsLimits\nBurstable\nQoS\nRequests\nLimits\nGuaranteed\nQoS\nRequestsLimits\nRequests and\nlimits are not set\nRequests are\nbelow limits\nRequests\nequal limits\nFigure 14.4   Resource requests, limits and QoS classes\n \n\n419Understanding pod QoS classes\nNOTEIf  only  requests  are  set,  but  not  limits,  refer  to  the  table  rows  where\nrequests are less than the limits. If only limits are set, requests default to the\nlimits, so refer to the rows where requests equal limits.\nFIGURING OUT THE QOS CLASS OF A POD WITH MULTIPLE CONTAINERS\nFor multi-container pods, if all the containers have the same QoS class, that’s also the\npod’s QoS class. If at least one container has a different class, the pod’s QoS class is\nBurstable, regardless of what the container classes are. Table 14.2 shows how a two-\ncontainer  pod’s  QoS  class  relates  to  the  classes  of  its  two  containers.  You  can  easily\nextend this to pods with more than two containers.\nNOTEA pod’s QoS class is shown when running kubectl describe pod and\nin the pod’s YAML/JSON manifest in the \nstatus.qosClass field.\nWe’ve explained how QoS classes are determined, but we still need to look at how they\ndetermine which container gets killed in an overcommitted system.\nTable 14.1   The QoS class of a single-container pod based on resource requests and limits\nCPU requests vs. limitsMemory requests vs. limitsContainer QoS class\nNone setNone setBestEffort\nNone setRequests < LimitsBurstable\nNone setRequests = LimitsBurstable\nRequests < LimitsNone setBurstable\nRequests < LimitsRequests < LimitsBurstable\nRequests < LimitsRequests = LimitsBurstable\nRequests = LimitsRequests = LimitsGuaranteed\nTable 14.2   A Pod’s QoS class derived from the classes of its containers\nContainer 1 QoS classContainer 2 QoS classPod’s QoS class\nBestEffort               BestEffort                BestEffort\nBestEffort               Burstable                 Burstable\nBestEffort               Guaranteed                Burstable\nBurstable                Burstable                 Burstable\nBurstable                Guaranteed                Burstable\nGuaranteed               Guaranteed                Guaranteed\n \n\n420CHAPTER 14Managing pods’ computational resources\n14.3.2   Understanding which process gets killed when memory is low\nWhen  the  system  is  overcommitted,  the  QoS  classes  determine  which  container  gets\nkilled first so the freed resources can be given to higher priority pods. First in line to\nget  killed  are  pods  in  the  \nBestEffort  class,  followed  by  Burstable  pods,  and  finally\nGuaranteed pods, which only get killed if system processes need memory.\nUNDERSTANDING HOW QOS CLASSES LINE UP\nLet’s look at the example shown in figure 14.5. Imagine having two single-container\npods,  where  the  first  one  has  the  \nBestEffort  QoS  class,  and  the  second  one’s  is\nBurstable. When the node’s whole memory is already maxed out and one of the pro-\ncesses on the node tries to allocate more memory, the system will need to kill one of\nthe  processes  (perhaps  even  the  process  trying  to  allocate  additional  memory)  to\nhonor the allocation request. In this case, the process running in the \nBestEffort pod\nwill always be killed before the one in the \nBurstable pod.\nObviously, a \nBestEffort pod’s process will also be killed before any Guaranteed pods’\nprocesses are killed. Likewise, a \nBurstable pod’s process will also be killed before that\nof a \nGuaranteed pod. But what happens if there are only two Burstable pods? Clearly,\nthe selection process needs to prefer one over the other.\nUNDERSTANDING HOW CONTAINERS WITH THE SAME QOS CLASS ARE HANDLED\nEach  running  process  has  an  OutOfMemory  (OOM)  score.  The  system  selects  the\nprocess to kill by comparing OOM scores of all the running processes. When memory\nneeds to be freed, the process with the highest score gets killed.\n OOM scores are calculated from two things: the percentage of the available mem-\nory the process is consuming and a fixed OOM score adjustment, which is based on the\npod’s QoS class and the container’s requested memory. When two single-container pods\nexist, both in the \nBurstable class, the system will kill the one using more of its requested\nBestEffort\nQoS pod\nPod A\nFirst in line\nto be killed\nActual usage\nRequestsLimits\nBurstable\nQoS pod\nPod B\nSecond in line\nto be killed\n90% usedRequestsLimits\nBurstable\nQoS pod\nPod C\nThird in line\nto be killed\n70% usedRequestsLimits\nGuaranteed\nQoS pod\nPod D\nLast to\nbe killed\n99% usedRequestsLimits\nFigure 14.5   Which pods get killed first\n \n\n421Setting default requests and limits for pods per namespace\nmemory than the other, percentage-wise. That’s why in figure 14.5, pod B, using 90%\nof  its  requested  memory,  gets  killed  before  pod  C,  which  is  only  using  70%,  even\nthough it’s using more megabytes of memory than pod B. \n This shows you need to be mindful of not only the relationship between requests\nand limits, but also of requests and the expected actual memory consumption. \n14.4   Setting default requests and limits for pods per \nnamespace\nWe’ve looked at how resource requests and limits can be set for each individual con-\ntainer. If you don’t set them, the container is at the mercy of all other containers that\ndo specify resource requests and limits. It’s a good idea to set requests and limits on\nevery container.\n14.4.1   Introducing the LimitRange resource\nInstead of having to do this for every container, you can also do it by creating a Limit-\nRange resource. It allows you to specify (for each namespace) not only the minimum\nand maximum limit you can set on a container for each resource, but also the default\nresource  requests  for  containers  that  don’t  specify  requests  explicitly,  as  depicted  in\nfigure 14.6.\nAPI server\nValidation\nPod A\nmanifest\n- Requests\n- Limits\nPod A\nmanifest\n- Requests\n- Limits\nPod B\nmanifest\n- No\nrequests\nor limits\nPod B\nmanifest\n- No\nrequests\nor limits\nDefaulting\nRejected because\nrequests and limits are\noutside min/max values\nDefaults\napplied\nNamespace XYZ\nLimitRange\nPod B\nmanifest\n- Default\nrequests\n- Default\nlimits\nPod B\n- Default requests\n- Default limits\n- Min/max CPU\n- Min/max memory\n- Default requests\n- Default limits\nFigure 14.6   A LimitRange is used for validation and defaulting pods.\n \n\n422CHAPTER 14Managing pods’ computational resources\nLimitRange  resources  are  used  by  the  LimitRanger  Admission  Control  plugin  (we\nexplained what those plugins are in chapter 11). When a pod manifest is posted to the\nAPI server, the LimitRanger plugin validates the pod spec. If validation fails, the mani-\nfest is rejected immediately. Because of this, a great use-case for LimitRange objects is\nto prevent users from creating pods that are bigger than any node in the cluster. With-\nout  such  a  LimitRange,  the  API  server  will  gladly  accept  the  pod,  but  then  never\nschedule it. \n The limits specified in a LimitRange resource apply to each individual pod/con-\ntainer  or  other  kind  of  object  created  in  the  same  namespace  as  the  LimitRange\nobject. They don’t limit the total amount of resources available across all the pods in\nthe namespace. This is specified through ResourceQuota objects, which are explained\nin section 14.5. \n14.4.2   Creating a LimitRange object\nLet’s look at a full example of a LimitRange and see what the individual properties do.\nThe following listing shows the full definition of a LimitRange resource.\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: example\nspec:\n  limits:\n  - type: Pod           \n    min:                         \n      cpu: 50m                   \n      memory: 5Mi                \n    max:                          \n      cpu: 1                      \n      memory: 1Gi                 \n  - type: Container             \n    defaultRequest:             \n      cpu: 100m                 \n      memory: 10Mi              \n    default:                      \n      cpu: 200m                   \n      memory: 100Mi               \n    min:                         \n      cpu: 50m                   \n      memory: 5Mi                \n    max:                         \n      cpu: 1                     \n      memory: 1Gi                \n    maxLimitRequestRatio:         \n      cpu: 4                      \n      memory: 10                  \nListing 14.10   A LimitRange resource: limits.yaml\nSpecifies the \nlimits for a pod \nas a whole\nMinimum CPU and memory all the \npod’s containers can request in total\nMaximum CPU and memory all the pod’s \ncontainers can request (and limit)\nThe\ncontainer\nlimits are\nspecified\nbelow this\nline.\nDefault requests for CPU and memory \nthat will be applied to containers that \ndon’t specify them explicitly\nDefault limits for containers \nthat don’t specify them\nMinimum and maximum \nrequests/limits that a \ncontainer can have\nMaximum ratio between \nthe limit and request \nfor each resource\n \n\n423Setting default requests and limits for pods per namespace\n  - type: PersistentVolumeClaim      \n    min:                             \n      storage: 1Gi                   \n    max:                             \n      storage: 10Gi                  \nAs you can see from the previous example, the minimum and maximum limits for a\nwhole  pod  can  be  configured.  They  apply  to  the  sum  of  all  the  pod’s  containers’\nrequests and limits. \n Lower down, at the container level, you can set not only the minimum and maxi-\nmum,  but  also  default  resource  requests  (\ndefaultRequest)  and  default  limits\n(\ndefault) that will be applied to each container that doesn’t specify them explicitly. \n Beside the min, max, and default values, you can even set the maximum ratio of\nlimits  vs.  requests.  The  previous  listing  sets  the  CPU  \nmaxLimitRequestRatio  to  4,\nwhich means a container’s CPU limits will not be allowed to be more than four times\ngreater  than  its  CPU  requests.  A  container  requesting  200  millicores  will  not  be\naccepted if its CPU limit is set to 801 millicores or higher. For memory, the maximum\nratio is set to 10.\n In chapter 6 we looked at PersistentVolumeClaims (PVC), which allow you to claim\na certain amount of persistent storage similarly to how a pod’s containers claim CPU\nand memory. In the same way you’re limiting the minimum and maximum amount of\nCPU  a  container  can  request,  you  should  also  limit  the  amount  of  storage  a  single\nPVC can request. A LimitRange object allows you to do that as well, as you can see at\nthe bottom of the example.\n  The  example  shows  a  single  LimitRange  object  containing  limits  for  everything,\nbut  you  could  also  split  them  into  multiple  objects  if  you  prefer  to  have  them  orga-\nnized per type (one for pod limits, another for container limits, and yet another for\nPVCs,  for  example).  Limits  from  multiple  LimitRange  objects  are  all  consolidated\nwhen validating a pod or PVC.\n  Because  the  validation  (and  defaults)  configured  in  a  LimitRange  object  is  per-\nformed by the API server when it receives a new pod or PVC manifest, if you modify\nthe limits afterwards, existing pods and PVCs will not be revalidated—the new limits\nwill only apply to pods and PVCs created afterward. \n14.4.3   Enforcing the limits\nWith your limits in place, you can now try creating a pod that requests more CPU than\nallowed by the LimitRange. You’ll find the YAML for the pod in the code archive. The\nnext listing only shows the part relevant to the discussion.\n    resources:\n      requests:\n        cpu: 2\nListing 14.11   A pod with CPU requests greater than the limit: limits-pod-too-big.yaml\nA LimitRange can also set \nthe minimum and maximum \namount of storage a PVC \ncan request.\n \n\n424CHAPTER 14Managing pods’ computational resources\nThe pod’s single container is requesting two CPUs, which is more than the maximum\nyou set in the LimitRange earlier. Creating the pod yields the following result:\n$ kubectl create -f limits-pod-too-big.yaml \nError from server (Forbidden): error when creating \"limits-pod-too-big.yaml\": \npods \"too-big\" is forbidden: [\n  maximum cpu usage per Pod is 1, but request is 2., \n  maximum cpu usage per Container is 1, but request is 2.]\nI’ve  modified  the  output  slightly  to  make  it  more  legible.  The  nice  thing  about  the\nerror message from the server is that it lists all the reasons why the pod was rejected,\nnot only the first one it encountered. As you can see, the pod was rejected for two rea-\nsons:  you  requested  two  CPUs  for  the  container,  but  the  maximum  CPU  limit  for  a\ncontainer is one. Likewise, the pod as a whole requested two CPUs, but the maximum\nis  one  CPU  (if  this  was  a  multi-container  pod,  even  if  each  individual  container\nrequested  less  than  the  maximum  amount  of  CPU,  together  they’d  still  need  to\nrequest less than two CPUs to pass the maximum CPU for pods). \n14.4.4   Applying default resource requests and limits\nNow let’s also see how default resource requests and limits are set on containers that\ndon’t specify them. Deploy the \nkubia-manual pod from chapter 3 again:\n$ kubectl create -f ../Chapter03/kubia-manual.yaml\npod \"kubia-manual\" created\nBefore  you  set  up  your  LimitRange  object,  all  your  pods  were  created  without  any\nresource requests or limits, but now the defaults are applied automatically when creat-\ning the pod. You can confirm this by describing the \nkubia-manual  pod,  as  shown  in\nthe following listing.\n$ kubectl describe po kubia-manual\nName:           kubia-manual\n...\nContainers:\n  kubia:\n    Limits:\n      cpu:      200m\n      memory:   100Mi\n    Requests:\n      cpu:      100m\n      memory:   10Mi\nThe container’s requests and limits match the ones you specified in the LimitRange\nobject. If you used a different LimitRange specification in another namespace, pods\ncreated  in  that  namespace  would  obviously  have  different  requests  and  limits.  This\nallows admins to configure default, min, and max resources for pods per namespace.\nListing 14.12   Inspecting limits that were applied to a pod automatically\n \n\n425Limiting the total resources available in a namespace\nIf namespaces are used to separate different teams or to separate development, QA,\nstaging, and production pods running in the same Kubernetes cluster, using a differ-\nent LimitRange in each namespace ensures large pods can only be created in certain\nnamespaces, whereas others are constrained to smaller pods.\n But remember, the limits configured in a LimitRange only apply to each individual\npod/container. It’s still possible to create many pods and eat up all the resources avail-\nable in the cluster. LimitRanges don’t provide any protection from that. A Resource-\nQuota object, on the other hand, does. You’ll learn about them next.\n14.5   Limiting the total resources available in a namespace\nAs  you’ve  seen,  LimitRanges  only  apply  to  individual  pods,  but  cluster  admins  also\nneed  a  way  to  limit  the  total  amount  of  resources  available  in  a  namespace.  This  is\nachieved by creating a ResourceQuota object. \n14.5.1   Introducing the ResourceQuota object\nIn chapter 10 we said that several Admission Control plugins running inside the API\nserver  verify  whether  the  pod  may  be  created  or  not.  In  the  previous  section,  I  said\nthat the LimitRanger plugin enforces the policies configured in LimitRange resources.\nSimilarly,  the  ResourceQuota  Admission  Control  plugin  checks  whether  the  pod\nbeing  created  would  cause  the  configured  ResourceQuota  to  be  exceeded.  If  that’s\nthe case, the pod’s creation is rejected. Because resource quotas are enforced at pod\ncreation  time,  a  ResourceQuota  object  only  affects  pods  created  after  the  Resource-\nQuota object is created—creating it has no effect on existing pods.\n A ResourceQuota limits the amount of computational resources the pods and the\namount of storage PersistentVolumeClaims in a namespace can consume. It can also\nlimit  the  number  of  pods,  claims,  and  other  API  objects  users  are  allowed  to  create\ninside the namespace. Because you’ve mostly dealt with CPU and memory so far, let’s\nstart by looking at how to specify quotas for them.\nCREATING A RESOURCEQUOTA FOR CPU AND MEMORY\nThe overall CPU and memory all the pods in a namespace are allowed to consume is\ndefined by creating a ResourceQuota object as shown in the following listing.\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: cpu-and-mem\nspec:\n  hard:\n    requests.cpu: 400m\n    requests.memory: 200Mi\n    limits.cpu: 600m\n    limits.memory: 500Mi\nListing 14.13   A ResourceQuota resource for CPU and memory: quota-cpu-memory.yaml\n \n\n426CHAPTER 14Managing pods’ computational resources\nInstead  of  defining  a  single  total  for  each  resource,  you  define  separate  totals  for\nrequests and limits for both CPU and memory. You’ll notice the structure is a bit dif-\nferent, compared to that of a LimitRange. Here, both the requests and the limits for\nall resources are defined in a single place. \n This ResourceQuota sets the maximum amount of CPU pods in the namespace\ncan  request  to  400  millicores.  The  maximum  total  CPU  limits  in  the  namespace  are\nset  to  600  millicores.  For  memory,  the  maximum  total  requests  are  set  to  200  MiB,\nwhereas the limits are set to 500 MiB.\n  A  ResourceQuota  object  applies  to  the  namespace  it’s  created  in,  like  a  Limit-\nRange, but it applies to all the pods’ resource requests and limits in total and not to\neach individual pod or container separately, as shown in figure 14.7.\nINSPECTING THE QUOTA AND QUOTA USAGE\nAfter you post the ResourceQuota object to the API server, you can use the kubectl\ndescribe\n command to see how much of the quota is already used up, as shown in\nthe following listing.\n$ kubectl describe quota\nName:           cpu-and-mem\nNamespace:      default\nResource        Used   Hard\n--------        ----   ----\nlimits.cpu      200m   600m\nlimits.memory   100Mi  500Mi\nrequests.cpu    100m   400m\nrequests.memory 10Mi   200Mi\nI only have the kubia-manual pod running, so the Used column matches its resource\nrequests and limits. When I run additional pods, their requests and limits are added to\nthe used amounts.\nListing 14.14   Inspecting the ResourceQuota with kubectl describe quota\nLimitRangeResourceQuota\nNamespace: FOO\nPod APod BPod C\nLimitRangeResourceQuota\nNamespace: BAR\nPod DPod EPod F\nFigure 14.7   LimitRanges apply to individual pods; ResourceQuotas apply to all pods in the \nnamespace.\n \n\n427Limiting the total resources available in a namespace\nCREATING A LIMITRANGE ALONG WITH A RESOURCEQUOTA\nOne caveat when creating a ResourceQuota is that you will also want to create a Limit-\nRange object alongside it. In your case, you have a LimitRange configured from the\nprevious section, but if you didn’t have one, you couldn’t run the \nkubia-manual pod,\nbecause it doesn’t specify any resource requests or limits. Here’s what would happen\nin that case:\n$ kubectl create -f ../Chapter03/kubia-manual.yaml\nError from server (Forbidden): error when creating \"../Chapter03/kubia-\nmanual.yaml\": pods \"kubia-manual\" is forbidden: failed quota: cpu-and-\nmem: must specify limits.cpu,limits.memory,requests.cpu,requests.memory\nWhen  a  quota  for  a  specific  resource  (CPU or memory) is configured (request or\nlimit), pods need to have the request or limit (respectively) set for that same resource;\notherwise the API server will not accept the pod. That’s why having a LimitRange with\ndefaults for those resources can make life a bit easier for people creating pods.\n14.5.2   Specifying a quota for persistent storage\nA  ResourceQuota  object  can  also  limit  the  amount  of  persistent  storage  that  can  be\nclaimed in the namespace, as shown in the following listing.\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: storage\nspec:\n  hard:\n    requests.storage: 500Gi                               \n    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi     \n    standard.storageclass.storage.k8s.io/requests.storage: 1Ti\nIn  this  example,  the  amount  of  storage  all  PersistentVolumeClaims  in  a  namespace\ncan  request  is  limited  to  500  GiB  (by  the  \nrequests.storage  entry  in  the  Resource-\nQuota object). But as you’ll remember from chapter 6, PersistentVolumeClaims can\nrequest a dynamically provisioned PersistentVolume of a specific StorageClass. That’s\nwhy Kubernetes also makes it possible to define storage quotas for each StorageClass\nindividually. The previous example limits the total amount of claimable SSD storage\n(designated  by  the  \nssd  StorageClass)  to  300  GiB.  The  less-performant  HDD  storage\n(StorageClass standard) is limited to 1 TiB.\n14.5.3   Limiting the number of objects that can be created\nA ResourceQuota can also be configured to limit the number of Pods, Replication-\nControllers,  Services,  and  other  objects  inside  a  single  namespace.  This  allows  the\ncluster admin to limit the number of objects users can create based on their payment\nListing 14.15   A ResourceQuota for storage: quota-storage.yaml\nThe amount of \nstorage claimable \noverall\nThe amount \nof claimable \nstorage in \nStorageClass ssd\n \n\n428CHAPTER 14Managing pods’ computational resources\nplan, for example, and can also limit the number of public IPs or node ports Ser-\nvices can use. \n The following listing shows what a ResourceQuota object that limits the number of\nobjects may look like.\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: objects\nspec:\n  hard:\n    pods: 10                        \n    replicationcontrollers: 5       \n    secrets: 10                     \n    configmaps: 10                  \n    persistentvolumeclaims: 4       \n    services: 5                      \n    services.loadbalancers: 1        \n    services.nodeports: 2            \n    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2   \nThe ResourceQuota in this listing allows users to create at most 10 Pods in the name-\nspace,  regardless  if  they’re  created  manually  or  by  a  ReplicationController,  Replica-\nSet, DaemonSet, Job, and so on. It also limits the number of ReplicationControllers to\nfive. A maximum of five Services can be created, of which only one can be a \nLoadBal-\nancer\n-type Service, and only two can be NodePort Services. Similar to how the maxi-\nmum amount of requested storage can be specified per StorageClass, the number of\nPersistentVolumeClaims can also be limited per StorageClass.\n Object count quotas can currently be set for the following objects: \nPods\nReplicationControllers \nSecrets\nConfigMaps\nPersistentVolumeClaims\nServices  (in  general),  and  for  two  specific  types  of  Services,  such  as  Load-\nBalancer\n  Services  (services.loadbalancers)  and  NodePort  Services  (ser-\nvices.nodeports\n) \nFinally, you can even set an object count quota for ResourceQuota objects themselves.\nThe number of other objects, such as ReplicaSets, Jobs, Deployments, Ingresses, and\nso on, cannot be limited yet (but this may have changed since the book was published,\nso please check the documentation for up-to-date information).\nListing 14.16   A ResourceQuota for max number of resources: quota-object-count.yaml\nOnly 10 Pods, 5 ReplicationControllers, \n10 Secrets, 10 ConfigMaps, and \n4 PersistentVolumeClaims can be \ncreated in the namespace.\nFive Services overall can be created, \nof which at most one can be a \nLoadBalancer Service and at most \ntwo can be NodePort Services.\nOnly two PVCs can claim storage\nwith the ssd StorageClass.\n \n\n429Limiting the total resources available in a namespace\n14.5.4   Specifying quotas for specific pod states and/or QoS classes\nThe  quotas  you’ve  created  so  far  have  applied  to  all  pods,  regardless  of  their  current\nstate and QoS class. But quotas can also be limited to a set of quota scopes. Four scopes are\ncurrently available: \nBestEffort, NotBestEffort, Terminating, and NotTerminating. \n The \nBestEffort and NotBestEffort scopes determine whether the quota applies\nto  pods  with  the  \nBestEffort  QoS  class  or  with  one  of  the  other  two  classes  (that  is,\nBurstable and Guaranteed). \n  The  other  two  scopes  (\nTerminating  and  NotTerminating)  don’t  apply  to  pods\nthat are (or aren’t) in the process of shutting down, as the name might lead you to\nbelieve.  We  haven’t  talked  about  this,  but  you  can  specify  how  long  each  pod  is\nallowed to run before it’s terminated and marked as \nFailed. This is done by setting\nthe \nactiveDeadlineSeconds field in the pod spec. This property defines the number\nof seconds a pod is allowed to be active on the node relative to its start time before it’s\nmarked as \nFailed and then terminated. The Terminating quota scope applies to pods\nthat  have  the  \nactiveDeadlineSeconds  set,  whereas  the  NotTerminating  applies  to\nthose that don’t. \n  When  creating  a  ResourceQuota,  you  can  specify  the  scopes  that  it  applies  to.  A\npod must match all the specified scopes for the quota to apply to it. Additionally, what\na quota can limit depends on the quota’s scope. \nBestEffort scope can only limit the\nnumber  of  pods,  whereas  the  other  three  scopes  can  limit  the  number  of  pods,\nCPU/memory requests, and CPU/memory limits. \n If, for example, you want the quota to apply only to \nBestEffort, NotTerminating\npods, you can create the ResourceQuota object shown in the following listing.\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: besteffort-notterminating-pods\nspec:\n  scopes:                 \n  - BestEffort            \n  - NotTerminating        \n  hard: \n    pods: 4          \nThis  quota  ensures  that  at  most  four  pods  exist  with  the  BestEffort  QoS  class,\nwhich don’t have an active deadline. If the quota was targeting \nNotBestEffort pods\ninstead,  you  could  also  specify  \nrequests.cpu, requests.memory, limits.cpu,  and\nlimits.memory.\nNOTEBefore you move on to the next section of this chapter, please delete\nall  the  ResourceQuota  and  LimitRange  resources  you  created.  You  won’t\nListing 14.17   ResourceQuota for BestEffort/NotTerminating pods: \nquota-scoped.yaml\nThis quota only applies to pods \nthat have the BestEffort QoS and \ndon’t have an active deadline set.\nOnly four such \npods can exist.\n \n\n430CHAPTER 14Managing pods’ computational resources\nneed them anymore and they may interfere with examples in the following\nchapters.\n14.6   Monitoring pod resource usage\nProperly setting resource requests and limits is crucial for getting the most out of your\nKubernetes  cluster.  If  requests  are  set  too  high,  your  cluster  nodes  will  be  underuti-\nlized  and  you’ll  be  throwing  money  away.  If  you  set  them  too  low,  your  apps  will  be\nCPU-starved  or  even  killed  by  the  OOM  Killer.  How  do  you  find  the  sweet  spot  for\nrequests and limits?\n You find it by monitoring the actual resource usage of your containers under the\nexpected load levels. Once the application is exposed to the public, you should keep\nmonitoring it and adjust the resource requests and limits if required.\n14.6.1   Collecting and retrieving actual resource usages\nHow  does  one  monitor  apps  running  in  Kubernetes?  Luckily,  the  Kubelet  itself\nalready  contains  an  agent  called  cAdvisor,  which  performs  the  basic  collection  of\nresource consumption data for both individual containers running on the node and\nthe node as a whole. Gathering those statistics centrally for the whole cluster requires\nyou to run an additional component called Heapster. \n  Heapster  runs  as  a  pod  on  one  of  the  nodes  and  is  exposed  through  a  regular\nKubernetes  Service,  making  it  accessible  at  a  stable  IP  address.  It  collects  the  data\nfrom  all  cAdvisors  in  the  cluster  and  exposes  it  in  a  single  location.  Figure  14.8\nshows the flow of the metrics data from the pods, through cAdvisor and finally into\nHeapster.\nKubelet\ncAdvisor\nNode 1\nPod\nPod\nKubelet\ncAdvisor\nNode 2\nPod\nKubelet\ncAdvisor\nNode X\nPod\nHeapster\nEach cAdvisor collects metrics from\ncontainers running on its node.\nHeapster runs on one of the nodes as a\npod and collects metrics from all nodes.\nFigure 14.8   The flow of metrics data into Heapster\n \n\n431Monitoring pod resource usage\nThe arrows in the figure show how the metrics data flows. They don’t show which com-\nponent  connects  to  which  to  get  the  data.  The  pods  (or  the  containers  running\ntherein)  don’t  know  anything  about  cAdvisor,  and  cAdvisor  doesn’t  know  anything\nabout Heapster. It’s Heapster that connects to all the cAdvisors, and it’s the cAdvisors\nthat collect the container and node usage data without having to talk to the processes\nrunning inside the pods’ containers.\nENABLING HEAPSTER\nIf  you’re  running  a  cluster  in  Google  Kubernetes  Engine,  Heapster  is  enabled  by\ndefault. If you’re using Minikube, it’s available as an add-on and can be enabled with\nthe following command:\n$ minikube addons enable heapster\nheapster was successfully enabled\nTo  run  Heapster  manually  in  other  types  of  Kubernetes  clusters,  you  can  refer  to\ninstructions located at https://github.com/kubernetes/heapster. \n After enabling Heapster, you’ll need to wait a few minutes for it to collect metrics\nbefore you can see resource usage statistics for your cluster, so be patient. \nDISPLAYING CPU AND MEMORY USAGE FOR CLUSTER NODES\nRunning  Heapster  in  your  cluster  makes  it  possible  to  obtain  resource  usages  for\nnodes  and  individual  pods  through  the  \nkubectl top  command.  To  see  how  much\nCPU and memory is being used on your nodes, you can run the command shown in\nthe following listing.\n$ kubectl top node\nNAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%\nminikube   170m         8%        556Mi           27%\nThis shows the actual, current CPU and memory usage of all the pods running on the\nnode, unlike the \nkubectl describe node command, which shows the amount of CPU\nand memory requests and limits instead of actual runtime usage data. \nDISPLAYING CPU AND MEMORY USAGE FOR INDIVIDUAL PODS\nTo see how much each individual pod is using, you can use the kubectl top pod com-\nmand, as shown in the following listing.\n$ kubectl top pod --all-namespaces\nNAMESPACE      NAME                             CPU(cores)   MEMORY(bytes)\nkube-system    influxdb-grafana-2r2w9           1m           32Mi\nkube-system    heapster-40j6d                   0m           18Mi\nListing 14.18   Actual CPU and memory usage of nodes\nListing 14.19   Actual CPU and memory usages of pods\n \n\n432CHAPTER 14Managing pods’ computational resources\ndefault        kubia-3773182134-63bmb           0m           9Mi\nkube-system    kube-dns-v20-z0hq6               1m           11Mi\nkube-system    kubernetes-dashboard-r53mc       0m           14Mi\nkube-system    kube-addon-manager-minikube      7m           33Mi\nThe outputs of both these commands are fairly simple, so you probably don’t need me\nto explain them, but I do need to warn you about one thing. Sometimes the \ntop pod\ncommand will refuse to show any metrics and instead print out an error like this:\n$ kubectl top pod\nW0312 22:12:58.021885   15126 top_pod.go:186] Metrics not available for pod \ndefault/kubia-3773182134-63bmb, age: 1h24m19.021873823s\nerror: Metrics not available for pod default/kubia-3773182134-63bmb, age: \n1h24m19.021873823s\nIf this happens, don’t start looking for the cause of the error yet. Relax, wait a while,\nand rerun the command—it may take a few minutes, but the metrics should appear\neventually. The \nkubectl top command gets the metrics from Heapster, which aggre-\ngates the data over a few minutes and doesn’t expose it immediately. \nTIPTo see resource usages across individual containers instead of pods, you\ncan use the \n--containers option. \n14.6.2   Storing and analyzing historical resource consumption statistics\nThe top  command  only  shows  current  resource  usages—it  doesn’t  show  you  how\nmuch CPU or memory your pods consumed throughout the last hour, yesterday, or a\nweek ago, for example. In fact, both cAdvisor and Heapster only hold resource usage\ndata for a short window of time. If you want to analyze your pods’ resource consump-\ntion over longer time periods, you’ll need to run additional tools.\n When using Google Kubernetes Engine, you can monitor your cluster with Google\nCloud  Monitoring,  but  when  you’re  running  your  own  local  Kubernetes  cluster\n(either  through  Minikube  or  other  means),  people  usually  use  InfluxDB  for  storing\nstatistics data and Grafana for visualizing and analyzing them. \nINTRODUCING INFLUXDB AND GRAFANA\nInfluxDB is an open source time-series database ideal for storing application metrics\nand other monitoring data. Grafana, also open source, is an analytics and visualization\nsuite  with  a  nice-looking  web  console  that  allows  you  to  visualize  the  data  stored  in\nInfluxDB and discover how your application’s resource usage behaves over time (an\nexample showing three Grafana charts is shown in figure 14.9).\n \n \n\n433Monitoring pod resource usage\nRUNNING INFLUXDB AND GRAFANA IN YOUR CLUSTER\nBoth  InfluxDB  and  Grafana  can  run  as  pods.  Deploying  them  is  straightforward.  All\nthe  necessary  manifests  are  available  in  the  Heapster  Git  repository  at  http://github\n.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.\n  When  using  Minikube,  you  don’t  even  need  to  deploy  them  manually,  because\nthey’re deployed along with Heapster when you enable the Heapster add-on.\nANALYZING RESOURCE USAGE WITH GRAFANA\nTo  discover  how  much  of  each  resource  your  pod  requires  over  time,  open  the\nGrafana web console and explore the predefined dashboards. Generally, you can find\nout the URL of Grafana’s web console with \nkubectl cluster-info:\n$ kubectl cluster-info\n...\nmonitoring-grafana is running at \nhttps://192.168.99.100:8443/api/v1/proxy/namespaces/kube-\nsystem/services/monitoring-grafana\nFigure 14.9   Grafana dashboard showing CPU usage across the cluster\n \n\n434CHAPTER 14Managing pods’ computational resources\nWhen using Minikube, Grafana’s web console is exposed through a NodePort Service,\nso you can open it in your browser with the following command:\n$ minikube service monitoring-grafana -n kube-system\nOpening kubernetes service kube-system/monitoring-grafana in default \nbrowser...\nA new browser window or tab will open and show the Grafana Home screen. On the\nright-hand side, you’ll see a list of dashboards containing two entries:\nCluster\nPods\nTo see the resource usage statistics of the nodes, open the Cluster dashboard. There\nyou’ll  see  several  charts  showing  the  overall  cluster  usage,  usage  by  node,  and  the\nindividual usage for CPU, memory, network, and filesystem. The charts will not only\nshow  the  actual  usage,  but  also  the  requests  and  limits  for  those  resources  (where\nthey apply).\n  If  you  then  switch  over  to  the  Pods  dashboard,  you  can  examine  the  resource\nusages for each individual pod, again with both requests and limits shown alongside\nthe actual usage. \n Initially, the charts show the statistics for the last 30 minutes, but you can zoom out\nand see the data for much longer time periods: days, months, or even years.\nUSING THE INFORMATION SHOWN IN THE CHARTS\nBy looking at the charts, you can quickly see if the resource requests or limits you’ve\nset for your pods need to be raised or whether they can be lowered to allow more pods\nto fit on your nodes. Let’s look at an example. Figure 14.10 shows the CPU and mem-\nory charts for a pod.\n At the far right of the top chart, you can see the pod is using more CPU than was\nrequested in the pod’s manifest. Although this isn’t problematic when this is the only\npod running on the node, you should keep in mind that a pod is only guaranteed as\nmuch of a resource as it requests through resource requests. Your pod may be running\nfine  now,  but  when  other  pods  are  deployed  to  the  same  node  and  start  using  the\nCPU, your pod’s CPU time may be throttled. Because of this, to ensure the pod can\nuse as much CPU as it needs to at any time, you should raise the CPU resource request\nfor the pod’s container.\n The bottom chart shows the pod’s memory usage and request. Here the situation is\nthe  exact  opposite.  The  amount  of  memory  the  pod  is  using  is  well  below  what  was\nrequested in the pod’s spec. The requested memory is reserved for the pod and won’t\nbe  available  to  other  pods.  The  unused  memory  is  therefore  wasted.  You  should\ndecrease the pod’s memory request to make the memory available to other pods run-\nning on the node. \n \n\n435Summary\n14.7   Summary\nThis chapter has shown you that you need to consider your pod’s resource usage and\nconfigure both the resource requests and the limits for your pod to keep everything\nrunning smoothly. The key takeaways from this chapter are\nSpecifying resource requests helps Kubernetes schedule pods across the cluster.\nSpecifying resource limits keeps pods from starving other pods of resources.\nUnused CPU time is allocated based on containers’ CPU requests.\nContainers never get killed if they try to use too much CPU, but they are killed\nif they try to use too much memory.\nIn an overcommitted system, containers also get killed to free memory for more\nimportant pods, based on the pods’ QoS classes and actual memory usage.\nActual CPU usage is higher\nthan what was requested.\nThe application’s CPU time\nwill be throttled when other\napps demand more CPU.\nYou should increase the\nCPU request.\nActual memory usage is well\nbelow requested memory.\nYou’ve reserved too much\nmemory for this app. You’re\nwasting memory, because it\nwon’t ever be used by this\napp and also can’t be used\nby other apps. You should\ndecrease the memory\nrequest.\nCPU request\nCPU usage\nMemory request\nMemory usage\nFigure 14.10   CPU and memory usage chart for a pod\n \n\n436CHAPTER 14Managing pods’ computational resources\nYou can use LimitRange objects to define the minimum, maximum, and default\nresource requests and limits for individual pods.\nYou can use ResourceQuota objects to limit the amount of resources available\nto all the pods in a namespace.\nTo know how high to set a pod’s resource requests and limits, you need to mon-\nitor how the pod uses resources over a long-enough time period.\nIn the next chapter, you’ll see how these metrics can be used by Kubernetes to auto-\nmatically scale your pods.\n \n\n437\nAutomatic scaling\nof pods and cluster nodes\nApplications  running  in  pods  can  be  scaled  out  manually  by  increasing  the\nreplicas  field  in  the  ReplicationController,  ReplicaSet,  Deployment,  or  other\nscalable resource. Pods can also be scaled vertically by increasing their container’s\nresource  requests  and  limits  (though  this  can  currently  only  be  done  at  pod  cre-\nation  time,  not  while  the  pod  is  running).  Although  manual  scaling  is  okay  for\ntimes  when  you  can  anticipate  load  spikes  in  advance  or  when  the  load  changes\ngradually  over  longer  periods  of  time,  requiring  manual  intervention  to  handle\nsudden, unpredictable traffic increases isn’t ideal. \nThis chapter covers\nConfiguring automatic horizontal scaling of pods \nbased on CPU utilization\nConfiguring automatic horizontal scaling of pods \nbased on custom metrics\nUnderstanding why vertical scaling of pods isn’t \npossible yet\nUnderstanding automatic horizontal scaling of \ncluster nodes\n \n\n438CHAPTER 15Automatic scaling of pods and cluster nodes\n  Luckily,  Kubernetes  can  monitor  your  pods  and  scale  them  up  automatically  as\nsoon as it detects an increase in the CPU usage or some other metric. If running on a\ncloud  infrastructure,  it  can  even  spin  up  additional  nodes  if  the  existing  ones  can’t\naccept any more pods. This chapter will explain how to get Kubernetes to do both pod\nand node autoscaling.\n  The  autoscaling  feature  in  Kubernetes  was  completely  rewritten  between  the  1.6\nand  the  1.7  version,  so  be  aware  you  may  find  outdated  information  on  this  subject\nonline.\n15.1   Horizontal pod autoscaling\nHorizontal pod autoscaling is the automatic scaling of the number of pod replicas man-\naged by a controller. It’s performed by the Horizontal controller, which is enabled and\nconfigured  by  creating  a  HorizontalPodAutoscaler  (HPA)  resource.  The  controller\nperiodically checks pod metrics, calculates the number of replicas required to meet\nthe  target  metric  value  configured  in  the  HorizontalPodAutoscaler  resource,  and\nadjusts the \nreplicas field on the target resource (Deployment, ReplicaSet, Replication-\nController, or StatefulSet). \n15.1.1   Understanding the autoscaling process\nThe autoscaling process can be split into three steps:\nObtain metrics of all the pods managed by the scaled resource object.\nCalculate the number of pods required to bring the metrics to (or close to) the\nspecified target value.\nUpdate the replicas field of the scaled resource.\nLet’s examine all three steps next.\nOBTAINING POD METRICS\nThe  Autoscaler  doesn’t  perform  the  gathering  of  the  pod  metrics  itself.  It  gets  the\nmetrics from a different source. As we saw in the previous chapter, pod and node met-\nrics are collected by an agent called cAdvisor, which runs in the Kubelet on each node,\nand then aggregated by the cluster-wide component called Heapster. The horizontal\npod  autoscaler  controller  gets  the  metrics  of  all  the  pods  by  querying  Heapster\nthrough REST calls. The flow of metrics data is shown in figure 15.1 (although all the\nconnections are initiated in the opposite direction).\nThis implies that Heapster must be running in the cluster for autoscaling to work. If\nyou’re  using  Minikube  and  were  following  along  in  the  previous  chapter,  Heapster\nPod(s)cAdvisor(s)\nHorizontal Pod Autoscaler(s)\nHeapster\nFigure 15.1   Flow of metrics from the pod(s) to the HorizontalPodAutoscaler(s)\n \n\n439Horizontal pod autoscaling\nshould  already  be  enabled  in  your  cluster.  If  not,  make  sure  to  enable  the  Heapster\nadd-on before trying out any autoscaling examples.\n Although you don’t need to query Heapster directly, if you’re interested in doing\nso,  you’ll  find  both  the  Heapster  Pod  and  the  Service  it’s  exposed  through  in  the\nkube-system namespace. \nCALCULATING THE REQUIRED NUMBER OF PODS\nOnce the Autoscaler has metrics for all the pods belonging to the resource the Auto-\nscaler  is  scaling  (the  Deployment,  ReplicaSet,  ReplicationController,  or  StatefulSet\nresource),  it  can  use  those  metrics  to  figure  out  the  required  number  of  replicas.  It\nneeds  to  find  the  number  that  will  bring  the  average  value  of  the  metric  across  all\nthose replicas as close to the configured target value as possible. The input to this cal-\nculation is a set of pod metrics (possibly multiple metrics per pod) and the output is a\nsingle integer (the number of pod replicas). \n When the Autoscaler is configured to consider only a single metric, calculating the\nrequired  replica  count  is  simple.  All  it  takes  is  summing  up  the  metrics  values  of  all\nthe  pods,  dividing  that  by  the  target  value  set  on  the  HorizontalPodAutoscaler\nresource, and then rounding it up to the next-larger integer. The actual calculation is\na bit more involved than this, because it also makes sure the Autoscaler doesn’t thrash\naround when the metric value is unstable and changes rapidly. \n When autoscaling is based on multiple pod metrics (for example, both CPU usage\nand  Queries-Per-Second  [QPS]),  the  calculation  isn’t  that  much  more  complicated.\nThe  Autoscaler  calculates  the  replica  count  for  each  metric  individually  and  then\ntakes the highest value (for example, if four pods are required to achieve the target\nCPU usage, and three pods are required to achieve the target QPS, the Autoscaler will\nscale to four pods). Figure 15.2 shows this example.\nA look at changes related to how the Autoscaler obtains metrics\nPrior to Kubernetes version 1.6, the HorizontalPodAutoscaler obtained the metrics\nfrom Heapster directly. In version 1.8, the Autoscaler can get the metrics through an\naggregated  version  of  the  resource  metrics  API  by  starting  the  Controller  Manager\nwith the \n--horizontal-pod-autoscaler-use-rest-clients=true flag. From ver-\nsion 1.9, this behavior will be enabled by default.\nThe core API server will not expose the metrics itself. From version 1.7, Kubernetes\nallows  registering  multiple  API  servers  and  making  them  appear  as  a  single  API\nserver. This allows it to expose metrics through one of those underlying API servers.\nWe’ll explain API server aggregation in the last chapter. \nSelecting what metrics collector to use in their clusters will be up to cluster adminis-\ntrators.  A  simple  translation  layer  is  usually  required  to  expose  the  metrics  in  the\nappropriate API paths and in the appropriate format.\n \n\n440CHAPTER 15Automatic scaling of pods and cluster nodes\nUPDATING THE DESIRED REPLICA COUNT ON THE SCALED RESOURCE\nThe final step of an autoscaling operation is updating the desired replica count field\non the scaled resource object (a ReplicaSet, for example) and then letting the Replica-\nSet controller take care of spinning up additional pods or deleting excess ones.\n  The  Autoscaler  controller  modifies  the  \nreplicas  field  of  the  scaled  resource\nthrough the Scale sub-resource. It enables the Autoscaler to do its work without know-\ning any details of the resource it’s scaling, except for what’s exposed through the Scale\nsub-resource (see figure 15.3).\nThis  allows  the  Autoscaler  to  operate  on  any  scalable  resource,  as  long  as  the  API\nserver exposes the Scale sub-resource for it. Currently, it’s exposed for\nDeployments\nReplicaSets\nReplicationControllers\nStatefulSets\nThese are currently the only objects you can attach an Autoscaler to.\nPod 1\nCPU\nutilization\nQPS\nPod 2Pod 3\nTarget\nCPU utilization\nTarget QPS\nReplicas: 4\nReplicas: 3\nReplicas: 4\n301215\n20\n(15 + 30 + 12) / 20 = 57 / 20\n(60 + 90 + 50) / 50 = 200 / 50\nMax(4, 3)\n50%\n60%90%50%\nFigure 15.2   Calculating the number of replicas from two metrics\nAutoscaler adjusts replicas (++ or --)\nHorizontal Pod Autoscaler\nDeployment, ReplicaSet,\nStatefulSet, or\nReplicationController\nScale\nsub-resource\nFigure 15.3   The Horizontal Pod Autoscaler modifies only on the Scale sub-resource.\n \n\n441Horizontal pod autoscaling\nUNDERSTANDING THE WHOLE AUTOSCALING PROCESS\nYou  now  understand  the  three  steps  involved  in  autoscaling,  so  let’s  visualize  all  the\ncomponents involved in the autoscaling process. They’re shown in figure 15.4.\nThe  arrows  leading  from  the  pods  to  the  cAdvisors,  which  continue  on  to  Heapster\nand finally to the Horizontal Pod Autoscaler, indicate the direction of the flow of met-\nrics  data.  It’s  important  to  be  aware  that  each  component  gets  the  metrics  from  the\nother components periodically (that is, cAdvisor gets the metrics from the pods in a\ncontinuous loop; the same is also true for Heapster and for the HPA controller). The\nend effect is that it takes quite a while for the metrics data to be propagated and a res-\ncaling action to be performed. It isn’t immediate. Keep this in mind when you observe\nthe Autoscaler in action next.\n15.1.2   Scaling based on CPU utilization\nPerhaps the most important metric you’ll want to base autoscaling on is the amount of\nCPU consumed by the processes running inside your pods. Imagine having a few pods\nproviding a service. When their CPU usage reaches 100% it’s obvious they can’t cope\nwith the demand anymore and need to be scaled either up (vertical scaling—increas-\ning the amount of CPU the pods can use) or out (horizontal scaling—increasing the\nnumber  of  pods).  Because  we’re  talking  about  the  horizontal  pod  autoscaler  here,\nAutoscaler adjusts\nreplicas (++ or --)\nHeapster collects\nmetrics from all nodes\ncAdvisor collects metrics\nfrom all containers on a node\nDeploymentReplicaSet\nAutoscaler collects\nmetrics from Heapster\nKubelet\ncAdvisor\nNode 1\nPod\nPod\nKubelet\ncAdvisor\nNode 2\nPod\nNode X\nHeapster\nHorizontal Pod\nAutoscaler\nFigure 15.4   How the autoscaler obtains metrics and rescales the target deployment \n \n\n442CHAPTER 15Automatic scaling of pods and cluster nodes\nwe’re  only  focusing  on  scaling  out  (increasing  the  number  of  pods).  By  doing  that,\nthe average CPU usage should come down. \n Because CPU usage is usually unstable, it makes sense to scale out even before the\nCPU  is  completely  swamped—perhaps  when  the  average  CPU  load  across  the  pods\nreaches or exceeds 80%. But 80% of what, exactly?\nTIPAlways set the target CPU usage well below 100% (and definitely never\nabove 90%) to leave enough room for handling sudden load spikes.\nAs you may remember from the previous chapter, the process running inside a con-\ntainer  is  guaranteed  the  amount  of  CPU  requested  through  the  resource  requests\nspecified for the container. But at times when no other processes need CPU, the pro-\ncess may use all the available CPU on the node. When someone says a pod is consum-\ning 80% of the CPU, it’s not clear if they mean 80% of the node’s CPU, 80% of the\npod’s  guaranteed  CPU  (the  resource  request),  or  80%  of  the  hard  limit  configured\nfor the pod through resource limits. \n As far as the Autoscaler is concerned, only the pod’s guaranteed CPU amount (the\nCPU requests) is important when determining the CPU utilization of a pod. The Auto-\nscaler  compares  the  pod’s  actual  CPU  consumption  and  its  CPU  requests,  which\nmeans the pods you’re autoscaling need to have CPU requests set (either directly or\nindirectly through a LimitRange object) for the Autoscaler to determine the CPU uti-\nlization percentage.\nCREATING A HORIZONTALPODAUTOSCALER BASED ON CPU USAGE\nLet’s see how to create a HorizontalPodAutoscaler now and configure it to scale pods\nbased on their CPU utilization. You’ll create a Deployment similar to the one in chap-\nter 9, but as we’ve discussed, you’ll need to make sure the pods created by the Deploy-\nment  all  have  the  CPU  resource  requests  specified  in  order  to  make  autoscaling\npossible.  You’ll  have  to  add  a  CPU  resource  request  to  the  Deployment’s  pod  tem-\nplate, as shown in the following listing.\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: kubia\nspec:\n  replicas: 3                \n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - image: luksa/kubia:v1     \n        name: nodejs\nListing 15.1   Deployment with CPU requests set: deployment.yaml\nManually setting the \n(initial) desired number \nof replicas to three\nRunning the \nkubia:v1 image\n \n\n443Horizontal pod autoscaling\n        resources:              \n          requests:             \n            cpu: 100m           \nThis is a regular Deployment object—it doesn’t use autoscaling yet. It will run three\ninstances  of  the  \nkubia  NodeJS  app,  with  each  instance  requesting  100  millicores\nof CPU. \n  After  creating  the  Deployment,  to  enable  horizontal  autoscaling  of  its  pods,  you\nneed  to  create  a  HorizontalPodAutoscaler  (HPA)  object  and  point  it  to  the  Deploy-\nment. You could prepare and post the YAML manifest for the HPA, but an easier way\nexists—using the \nkubectl autoscale command:\n$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5\ndeployment \"kubia\" autoscaled\nThis creates the HPA object for you and sets the Deployment called kubia as the scal-\ning target. You’re setting the target CPU utilization of the pods to 30% and specifying\nthe minimum and maximum number of replicas. The Autoscaler will constantly keep\nadjusting the number of replicas to keep their CPU utilization around 30%, but it will\nnever scale down to less than one or scale up to more than five replicas. \nTIPAlways make sure to autoscale Deployments  instead  of  the  underlying\nReplicaSets. This way, you ensure the desired replica count is preserved across\napplication updates (remember that a Deployment creates a new ReplicaSet\nfor each version). The same rule applies to manual scaling, as well.\nLet’s look at the definition of the HorizontalPodAutoscaler resource to gain a better\nunderstanding of it. It’s shown in the following listing.\n$ kubectl get hpa.v2beta1.autoscaling kubia -o yaml\napiVersion: autoscaling/v2beta1            \nkind: HorizontalPodAutoscaler              \nmetadata:\n  name: kubia               \n  ...\nspec:\n  maxReplicas: 5                   \n  metrics:                              \n  - resource:                           \n      name: cpu                         \n      targetAverageUtilization: 30      \n    type: Resource                      \n  minReplicas: 1                   \n  scaleTargetRef:                          \n    apiVersion: extensions/v1beta1         \n    kind: Deployment                       \n    name: kubia                            \nListing 15.2   A HorizontalPodAutoscaler YAML definition\nRequesting 100 millicores \nof CPU per pod\nHPA resources are in the \nautoscaling API group.\nEach HPA has a name (it doesn’t \nneed to match the name of the \nDeployment as in this case).\nThe\nminimum\nand\nmaximum\nnumber of\nreplicas\nyou\nspecified\nYou’d like the Autoscaler to \nadjust the number of pods \nso they each utilize 30% of \nrequested CPU.\nThe target resource \nwhich this Autoscaler \nwill act upon\n \n\n444CHAPTER 15Automatic scaling of pods and cluster nodes\nstatus:\n  currentMetrics: []        \n  currentReplicas: 3        \n  desiredReplicas: 0        \nNOTEMultiple versions of HPA resources exist: the new autoscaling/v2beta1\nand the old autoscaling/v1. You’re requesting the new version here.\nSEEING THE FIRST AUTOMATIC RESCALE EVENT\nIt takes a while for cAdvisor to get the CPU metrics and for Heapster to collect them\nbefore the Autoscaler can take action. During that time, if you display the HPA resource\nwith \nkubectl get, the TARGETS column will show <unknown>:\n$ kubectl get hpa\nNAME      REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS\nkubia     Deployment/kubia   <unknown> / 30%   1         5         0       \nBecause  you’re  running  three  pods  that  are  currently  receiving  no  requests,  which\nmeans their CPU usage should be close to zero, you should expect the Autoscaler to\nscale them down to a single pod, because even with a single pod, the CPU utilization\nwill still be below the 30% target. \n And sure enough, the autoscaler does exactly that. It soon scales the Deployment\ndown to a single replica:\n$ kubectl get deployment\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nkubia     1         1         1            1           23m\nRemember, the autoscaler only adjusts the desired replica count on the Deployment.\nThe Deployment controller then takes care of updating the desired replica count on\nthe ReplicaSet object, which then causes the ReplicaSet controller to delete two excess\npods, leaving one pod running.\n  You  can  use  \nkubectl describe  to  see  more  information  on  the  HorizontalPod-\nAutoscaler and the operation of the underlying controller, as the following listing shows.\n$ kubectl describe hpa\nName:                             kubia\nNamespace:                        default\nLabels:                           <none>\nAnnotations:                      <none>\nCreationTimestamp:                Sat, 03 Jun 2017 12:59:57 +0200\nReference:                        Deployment/kubia\nMetrics:                          ( current / target )\n  resource cpu on pods  \n  (as a percentage of request):   0% (0) / 30%\nMin replicas:                     1\nMax replicas:                     5\nListing 15.3   Inspecting a HorizontalPodAutoscaler with kubectl describe\nThe current status \nof the Autoscaler\n \n\n445Horizontal pod autoscaling\nEvents:\nFrom                        Reason              Message\n----                        ------              ---\nhorizontal-pod-autoscaler   SuccessfulRescale   New size: 1; reason: All \n                                                metrics below target\nNOTEThe output has been modified to make it more readable.\nTurn your focus to the table of events at the bottom of the listing. You see the horizon-\ntal  pod  autoscaler  has  successfully  rescaled  to  one  replica,  because  all  metrics  were\nbelow target. \nTRIGGERING A SCALE-UP\nYou’ve already witnessed your first automatic rescale event (a scale-down). Now, you’ll\nstart sending requests to your pod, thereby increasing its CPU usage, and you should\nsee the autoscaler detect this and start up additional pods.\n You’ll need to expose the pods through a Service, so you can hit all of them through\na single URL. You may remember that the easiest way to do that is with \nkubectl expose:\n$ kubectl expose deployment kubia --port=80 --target-port=8080\nservice \"kubia\" exposed\nBefore you start hitting your pod(s) with requests, you may want to run the follow-\ning command in a separate terminal to keep an eye on what’s happening with the\nHorizontalPodAutoscaler and the Deployment, as shown in the following listing.\n$ watch -n 1 kubectl get hpa,deployment\nEvery 1.0s: kubectl get hpa,deployment                                                                                                \n                                                                  \nNAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  AGE\nhpa/kubia   Deployment/kubia   0% / 30%   1         5         1         45m\nNAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeploy/kubia   1         1         1            1           56m\nTIPList multiple resource types with kubectl get by delimiting them with\na comma. \nIf you’re using OSX, you’ll have to replace the \nwatch command with a loop, manually\nrun \nkubectl get periodically, or use kubectl’s --watch option. But although a plain\nkubectl get  can  show  multiple  types  of  resources  at  once,  that’s  not  the  case  when\nusing the aforementioned \n--watch option, so you’ll need to use two terminals if you\nwant to watch both the HPA and the Deployment objects. \n Keep an eye on the state of those two objects while you run a load-generating pod.\nYou’ll run the following command in another terminal:\n$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox \n➥\n -- sh -c \"while true; do wget -O - -q http://kubia.default; done\"\nListing 15.4   Watching multiple resources in parallel\n \n\n446CHAPTER 15Automatic scaling of pods and cluster nodes\nThis  will  run  a  pod  which  repeatedly  hits  the  kubia  Service.  You’ve  seen  the  -it\noption a few times when running the kubectl exec command. As you can see, it can\nalso be used with \nkubectl run.  It  allows  you  to  attach  the  console  to  the  process,\nwhich will not only show you the process’ output directly, but will also terminate the\nprocess as soon as you press CTRL+C. The \n--rm option causes the pod to be deleted\nafterward, and the \n--restart=Never option causes kubectl run to create an unman-\naged  pod  directly  instead  of  through  a  Deployment  object,  which  you  don’t  need.\nThis combination of options is useful for running commands inside the cluster with-\nout having to piggyback on an existing pod. It not only behaves the same as if you\nwere running the command locally, it even cleans up everything when the command\nterminates. \nSEEING THE AUTOSCALER SCALE UP THE DEPLOYMENT\nAs the load-generator pod runs, you’ll see it initially hitting the single pod. As before,\nit takes time for the metrics to be updated, but when they are, you’ll see the autoscaler\nincrease the number of replicas. In my case, the pod’s CPU utilization initially jumped\nto 108%, which caused the autoscaler to increase the number of pods to four. The\nutilization  on  the  individual  pods  then  decreased  to  74%  and  then  stabilized  at\naround 26%. \nNOTEIf the CPU load in your case doesn’t exceed 30%, try running addi-\ntional load-generators.\nAgain,  you  can  inspect  autoscaler  events  with  \nkubectl describe  to  see  what  the\nautoscaler has done (only the most important information is shown in the following\nlisting).\nFrom    Reason              Message\n----    ------              -------\nh-p-a   SuccessfulRescale   New size: 1; reason: All metrics below target\nh-p-a   SuccessfulRescale   New size: 4; reason: cpu resource utilization \n                            (percentage of request) above target\nDoes  it  strike  you  as  odd  that  the  initial  average  CPU  utilization  in  my  case,  when  I\nonly  had  one  pod,  was  108%,  which  is  more  than  100%?  Remember,  a  container’s\nCPU utilization is the container’s actual CPU usage divided by its requested CPU. The\nrequested CPU defines the minimum, not maximum amount of CPU available to the\ncontainer, so a container may consume more than the requested CPU, bringing the\npercentage over 100. \n Before we go on, let’s do a little math and see how the autoscaler concluded that\nfour  replicas  are  needed.  Initially,  there  was  one  replica  handling  requests  and  its\nCPU  usage  spiked  to  108%.  Dividing  108  by  30  (the  target  CPU  utilization  percent-\nage) gives 3.6, which the autoscaler then rounded up to 4. If you divide 108 by 4, you\nListing 15.5   Events of a HorizontalPodAutoscaler\n \n\n447Horizontal pod autoscaling\nget  27%.  If  the  autoscaler  scales  up  to  four  pods,  their  average  CPU  utilization  is\nexpected to be somewhere in the neighborhood of 27%, which is close to the target\nvalue of 30% and almost exactly what the observed CPU utilization was.\nUNDERSTANDING THE MAXIMUM RATE OF SCALING\nIn my case, the CPU usage shot up to 108%, but in general, the initial CPU usage\ncould spike even higher. Even if the initial average CPU utilization was higher (say\n150%), requiring five replicas to achieve the 30% target, the autoscaler would still\nonly scale up to four pods in the first step, because it has a limit on how many repli-\ncas can be added in a single scale-up operation. The autoscaler will at most double\nthe  number  of  replicas  in  a  single  operation,  if  more  than  two  current  replicas\nexist. If only one or two exist, it will scale up to a maximum of four replicas in a sin-\ngle step. \n  Additionally,  it  has  a  limit  on  how  soon  a  subsequent  autoscale  operation  can\noccur  after  the  previous  one.  Currently,  a  scale-up  will  occur  only  if  no  rescaling\nevent occurred in the last three minutes. A scale-down event is performed even less\nfrequently—every  five  minutes.  Keep  this  in  mind  so  you  don’t  wonder  why  the\nautoscaler  refuses  to  perform  a  rescale  operation  even  if  the  metrics  clearly  show\nthat it should.\nMODIFYING THE TARGET METRIC VALUE ON AN EXISTING HPA OBJECT\nTo wrap up this section, let’s do one last exercise. Maybe your initial CPU utilization\ntarget of 30% was a bit too low, so increase it to 60%. You do this by editing the HPA\nresource  with  the  \nkubectl edit  command.  When  the  text  editor  opens,  change  the\ntargetAverageUtilization field to 60, as shown in the following listing.\n...\nspec:\n  maxReplicas: 5\n  metrics:\n  - resource:\n      name: cpu\n      targetAverageUtilization: 60    \n    type: Resource\n...\nAs  with  most  other  resources,  after  you  modify  the  resource,  your  changes  will  be\ndetected  by  the  autoscaler  controller  and  acted  upon.  You  could  also  delete  the\nresource  and  recreate  it  with  different  target  values,  because  by  deleting  the  HPA\nresource,  you  only  disable  autoscaling  of  the target resource (a Deployment in this\ncase) and leave it at the scale it is at that time. The automatic scaling will resume after\nyou create a new HPA resource for the Deployment.\nListing 15.6   Increasing the target CPU utilization by editing the HPA resource\nChange this \nfrom 30 to 60.\n \n\n448CHAPTER 15Automatic scaling of pods and cluster nodes\n15.1.3   Scaling based on memory consumption\nYou’ve seen how easily the horizontal Autoscaler can be configured to keep CPU uti-\nlization at the target level. But what about autoscaling based on the pods’ memory\nusage? \n  Memory-based  autoscaling  is  much  more  problematic  than  CPU-based  autoscal-\ning. The main reason is because after scaling up, the old pods would somehow need to\nbe forced to release memory. This needs to be done by the app itself—it can’t be done\nby the system. All the system could do is kill and restart the app, hoping it would use\nless  memory  than  before.  But  if  the  app  then  uses  the  same  amount  as  before,  the\nAutoscaler  would  scale  it  up  again.  And  again,  and  again,  until  it  reaches  the  maxi-\nmum number of pods configured on the HPA resource. Obviously, this isn’t what any-\none wants. Memory-based autoscaling was introduced in Kubernetes version 1.8, and\nis configured exactly like CPU-based autoscaling. Exploring it is left up to the reader.\n15.1.4   Scaling based on other and custom metrics\nYou’ve seen how easy it is to scale pods based on their CPU usage. Initially, this was the\nonly autoscaling option that was usable in practice. To have the autoscaler use custom,\napp-defined metrics to drive its autoscaling decisions was fairly complicated. The ini-\ntial  design  of  the  autoscaler  didn’t  make  it  easy  to  move  beyond  simple  CPU-based\nscaling.  This  prompted  the  Kubernetes  Autoscaling  Special  Interest  Group  (SIG)  to\nredesign the autoscaler completely. \n If you’re interested in learning how complicated it was to use the initial autoscaler\nwith custom metrics, I invite you to read my blog post entitled “Kubernetes autoscal-\ning  based  on  custom  metrics  without  using  a  host  port,”  which  you’ll  find  online  at\nhttp://medium.com/@marko.luksa.  You’ll  learn  about  all  the  other  problems  I\nencountered  when  trying  to  set  up  autoscaling  based  on  custom  metrics.  Luckily,\nnewer  versions  of  Kubernetes  don’t  have  those  problems.  I’ll  cover  the  subject  in  a\nnew blog post. \n  Instead  of  going  through  a  complete  example  here,  let’s  quickly  go  over  how  to\nconfigure the autoscaler to use different metrics sources. We’ll start by examining how\nwe  defined  what  metric  to  use  in  our  previous  example.  The  following  listing  shows\nhow your previous HPA object was configured to use the CPU usage metric.\n...\nspec:\n  maxReplicas: 5\n  metrics:\n  - type: Resource      \n    resource:\n      name: cpu                      \n      targetAverageUtilization: 30    \n...\nListing 15.7   HorizontalPodAutoscaler definition for CPU-based autoscaling\nDefines the type \nof metric\nThe resource, whose \nutilization will be monitored\nThe target utilization \nof this resource\n \n\n449Horizontal pod autoscaling\nAs you can see, the metrics field allows you to define more than one metric to use.\nIn the listing, you’re using a single metric. Each entry defines the \ntype of metric—\nin this case, a \nResource metric. You have three types of metrics you can use in an\nHPA object:\nResource\nPods\nObject\nUNDERSTANDING THE RESOURCE METRIC TYPE\nThe Resource type makes the autoscaler base its autoscaling decisions on a resource\nmetric, like the ones specified in a container’s resource requests. We’ve already seen\nhow to do that, so let’s focus on the other two types.\nUNDERSTANDING THE PODS METRIC TYPE\nThe Pods type is used to refer to any other (including custom) metric related to the\npod directly. An example of such a metric could be the already mentioned Queries-\nPer-Second (QPS) or the number of messages in a message broker’s queue (when the\nmessage broker is running as a pod). To configure the autoscaler to use the pod’s QPS\nmetric, the HPA object would need to include the entry shown in the following listing\nunder its \nmetrics field.\n...\nspec:\n  metrics:\n  - type: Pods              \n    resource:\n      metricName: qps             \n      targetAverageValue: 100    \n...\nThe  example  in  the  listing  configures  the  autoscaler  to  keep  the  average  QPS  of  all\nthe  pods  managed  by  the  ReplicaSet  (or  other)  controller  targeted  by  this  HPA\nresource at \n100. \nUNDERSTANDING THE OBJECT METRIC TYPE\nThe Object  metric  type  is  used  when  you  want  to  make  the  autoscaler  scale  pods\nbased on a metric that doesn’t pertain directly to those pods. For example, you may\nwant to scale pods according to a metric of another cluster object, such as an Ingress\nobject.  The  metric  could  be  QPS  as  in  listing  15.8,  the  average  request  latency,  or\nsomething else completely. \n Unlike in the previous case, where the autoscaler needed to obtain the metric for\nall  targeted  pods  and  then  use  the  average  of  those  values,  when  you  use  an  \nObject\nmetric type, the autoscaler obtains a single metric from the single object. In the HPA\nListing 15.8   Referring to a custom pod metric in the HPA\nDefines a pod metric\nThe name of \nthe metric\nThe target average value \nacross all targeted pods\n \n\n450CHAPTER 15Automatic scaling of pods and cluster nodes\ndefinition,  you  need  to  specify  the  target  object  and  the  target  value.  The  following\nlisting shows an example.\n...\nspec:\n  metrics:\n  - type: Object                   \n    resource:\n      metricName: latencyMillis           \n      target: \n        apiVersion: extensions/v1beta1     \n        kind: Ingress                      \n        name: frontend                     \n      targetValue: 20                   \n  scaleTargetRef:                          \n    apiVersion: extensions/v1beta1         \n    kind: Deployment                       \n    name: kubia                            \n...\nIn  this  example,  the  HPA  is  configured  to  use  the  latencyMillis  metric  of  the\nfrontend  Ingress  object.  The  target  value  for  the  metric  is  20.  The  horizontal  pod\nautoscaler will monitor the Ingress’ metric and if it rises too far above the target value,\nthe autoscaler will scale the \nkubia Deployment resource. \n15.1.5   Determining which metrics are appropriate for autoscaling\nYou  need  to  understand  that  not  all  metrics  are  appropriate  for  use  as  the  basis  of\nautoscaling. As mentioned previously, the pods’ containers’ memory consumption isn’t\na  good  metric  for  autoscaling.  The  autoscaler  won’t  function  properly  if  increasing\nthe number of replicas doesn’t result in a linear decrease of the average value of the\nobserved metric (or at least close to linear). \n For example, if you have only a single pod instance and the value of the metric is X\nand  the  autoscaler  scales  up  to  two  replicas,  the  metric  needs  to  fall  to  somewhere\nclose  to  X/2.  An  example  of  such  a  custom  metric  is  Queries  per  Second  (QPS),\nwhich in the case of web applications reports the number of requests the application\nis receiving per second. Increasing the number of replicas will always result in a pro-\nportionate  decrease  of  QPS,  because  a  greater  number  of  pods  will  be  handling  the\nsame total number of requests. \n Before you decide to base the autoscaler on your app’s own custom metric, be sure\nto  think  about  how  its  value  will  behave  when  the  number  of  pods  increases  or\ndecreases.\n15.1.6   Scaling down to zero replicas\nThe horizontal pod autoscaler currently doesn’t allow setting the minReplicas field\nto 0, so the autoscaler will never scale down to zero, even if the pods aren’t doing\nListing 15.9   Referring to a metric of a different object in the HPA\nUse metric of a \nspecific object\nThe name of \nthe metric\nThe specific object whose metric \nthe autoscaler should obtain\nThe\nAutoscaler\nshould\nscale so\nthe value\nof the\nmetric\nstays close\nto this.\nThe scalable resource the \nautoscaler will scale\n \n\n451Vertical pod autoscaling\nanything.  Allowing  the  number  of  pods  to  be  scaled  down  to  zero  can  dramatically\nincrease the utilization of your hardware. When you run services that get requests only\nonce every few hours or even days, it doesn’t make sense to have them running all the\ntime, eating up resources that could be used by other pods. But you still want to have\nthose services available immediately when a client request comes in. \n This is known as idling and un-idling. It allows pods that provide a certain service\nto be scaled down to zero. When a new request comes in, the request is blocked until\nthe pod is brought up and then the request is finally forwarded to the pod. \n Kubernetes currently doesn’t provide this feature yet, but it will eventually. Check\nthe documentation to see if idling has been implemented yet. \n15.2   Vertical pod autoscaling\nHorizontal  scaling  is  great,  but  not  every  application  can  be  scaled  horizontally.  For\nsuch  applications,  the  only  option  is  to  scale  them  vertically—give  them  more  CPU\nand/or  memory.  Because  a  node  usually  has  more  resources  than  a  single  pod\nrequests, it should almost always be possible to scale a pod vertically, right? \n  Because  a  pod’s  resource  requests  are  configured  through  fields  in  the  pod\nmanifest,  vertically  scaling  a  pod  would  be  performed  by  changing  those  fields.  I\nsay “would” because it’s currently not possible to change either resource requests\nor limits of existing pods. Before I started writing the book (well over a year ago), I\nwas  sure  that  by  the  time  I  wrote  this  chapter,  Kubernetes  would  already  support\nproper vertical pod autoscaling, so I included it in my proposal for the table of con-\ntents.  Sadly,  what  seems  like  a  lifetime  later,  vertical  pod  autoscaling  is  still  not\navailable yet. \n15.2.1   Automatically configuring resource requests\nAn experimental feature sets the CPU and memory requests on newly created pods, if\ntheir containers don’t have them set explicitly. The feature is provided by an Admission\nControl  plugin  called  InitialResources.  When  a  new  pod  without  resource  requests  is\ncreated, the plugin looks at historical resource usage data of the pod’s containers (per\nthe underlying container image and tag) and sets the requests accordingly. \n You can deploy pods without specifying resource requests and rely on Kubernetes\nto eventually figure out what each container’s resource needs are. Effectively, Kuber-\nnetes  is  vertically  scaling  the  pod.  For  example,  if  a  container  keeps  running  out  of\nmemory, the next time a pod with that container image is created, its resource request\nfor memory will be set higher automatically.\n15.2.2   Modifying resource requests while a pod is running\nEventually,  the  same  mechanism  will  be  used  to  modify  an  existing  pod’s  resource\nrequests, which means it will vertically scale the pod while it’s running. As I’m writing\nthis,  a  new  vertical  pod  autoscaling  proposal  is  being  finalized.  Please  refer  to  the\n \n\n452CHAPTER 15Automatic scaling of pods and cluster nodes\nKubernetes  documentation  to  find  out  whether  vertical  pod  autoscaling  is  already\nimplemented or not.\n15.3   Horizontal scaling of cluster nodes\nThe  Horizontal  Pod  Autoscaler  creates  additional  pod  instances  when  the  need  for\nthem  arises.  But  what  about  when  all  your  nodes  are  at  capacity  and  can’t  run  any\nmore pods? Obviously, this problem isn’t limited only to when new pod instances are\ncreated by the Autoscaler. Even when creating pods manually, you may encounter the\nproblem  where  none  of  the  nodes  can  accept  the  new  pods,  because  the  node’s\nresources are used up by existing pods. \n In that case, you’d need to delete several of those existing pods, scale them down\nvertically,  or  add  additional  nodes  to  your  cluster.  If  your  Kubernetes  cluster  is  run-\nning on premises, you’d need to physically add a new machine and make it part of the\nKubernetes  cluster.  But  if  your  cluster  is  running  on  a  cloud  infrastructure,  adding\nadditional  nodes  is  usually  a  matter  of  a  few  clicks  or  an  API  call  to  the  cloud  infra-\nstructure. This can be done automatically, right?\n  Kubernetes  includes  the  feature  to  automatically  request  additional  nodes  from\nthe cloud provider as soon as it detects additional nodes are needed. This is per-\nformed by the Cluster Autoscaler.\n15.3.1   Introducing the Cluster Autoscaler\nThe  Cluster  Autoscaler  takes  care  of  automatically  provisioning  additional  nodes\nwhen it notices a pod that can’t be scheduled to existing nodes because of a lack of\nresources on those nodes. It also de-provisions nodes when they’re underutilized for\nlonger periods of time. \nREQUESTING ADDITIONAL NODES FROM THE CLOUD INFRASTRUCTURE\nA  new  node  will  be  provisioned  if,  after  a  new  pod  is  created,  the  Scheduler  can’t\nschedule  it  to  any  of  the  existing  nodes.  The  Cluster  Autoscaler  looks  out  for  such\npods  and  asks  the  cloud  provider  to  start  up  an  additional  node.  But  before  doing\nthat,  it  checks  whether  the  new  node  can  even  accommodate  the  pod.  After  all,  if\nthat’s not the case, it makes no sense to start up such a node.\n  Cloud  providers  usually  group nodes into groups (or pools) of same-sized nodes\n(or  nodes  having  the  same  features).  The  Cluster  Autoscaler  thus  can’t  simply  say\n“Give me an additional node.” It needs to also specify the node type.\n The Cluster Autoscaler does this by examining the available node groups to see if\nat least one node type would be able to fit the unscheduled pod. If exactly one such\nnode group exists, the Autoscaler can increase the size of the node group to have the\ncloud provider add another node to the group. If more than one option is available,\nthe  Autoscaler  must  pick  the  best  one.  The  exact  meaning  of  “best”  will  obviously\nneed to be configurable. In the worst case, it selects a random one. A simple overview\nof how the cluster Autoscaler reacts to an unschedulable pod is shown in figure 15.5.\n \n\n453Horizontal scaling of cluster nodes\nWhen the new node starts up, the Kubelet on that node contacts the API server and\nregisters the node by creating a Node resource. From then on, the node is part of the\nKubernetes cluster and pods can be scheduled to it.\n Simple, right? What about scaling down?\nRELINQUISHING NODES\nThe  Cluster  Autoscaler  also  needs  to  scale  down  the  number  of  nodes  when  they\naren’t being utilized enough. The Autoscaler does this by monitoring the requested\nCPU and memory on all the nodes. If the CPU and memory requests of all the pods\nrunning on a given node are below 50%, the node is considered unnecessary. \n That’s not the only determining factor in deciding whether to bring a node down.\nThe Autoscaler also checks to see if any system pods are running (only) on that node\n(apart from those that are run on every node, because they’re deployed by a Daemon-\nSet, for example). If a system pod is running on a node, the node won’t be relinquished.\nThe same is also true if an unmanaged pod or a pod with local storage is running on the\nnode, because that would cause disruption to the service the pod is providing. In other\nwords,  a  node  will  only  be  returned  to  the  cloud  provider  if  the  Cluster  Autoscaler\nknows the pods running on the node will be rescheduled to other nodes.\n When a node is selected to be shut down, the node is first marked as unschedula-\nble  and  then  all  the  pods  running  on  the  node  are  evicted.  Because  all  those  pods\nbelong to ReplicaSets or other controllers, their replacements are created and sched-\nuled  to  the  remaining  nodes  (that’s  why  the  node  that’s  being  shut  down  is  first\nmarked as unschedulable).\nNode group X\nNode X1\n1. Autoscaler notices a\nPod can’t be scheduled\nto existing nodes\n3. Autoscaler scales up the\nnode group selected in\nprevious step\n2. Autoscaler determines which node\ntype (if any) would be able to fit the\npod. If multiple types could fit the\npod, it selects one of them.\nCluster\nAutoscaler\nPods\nNode X2\nPods\nNode group Y\nNode Y1\nPods\nUnschedulable\npod\nFigure 15.5   The Cluster Autoscaler scales up when it finds a pod that can’t be scheduled to \nexisting nodes.\n \n\n454CHAPTER 15Automatic scaling of pods and cluster nodes\n15.3.2   Enabling the Cluster Autoscaler\nCluster autoscaling is currently available on\nGoogle Kubernetes Engine (GKE)\nGoogle Compute Engine (GCE)\nAmazon Web Services (AWS)\nMicrosoft Azure\nHow you start the Autoscaler depends on where your Kubernetes cluster is running.\nFor your \nkubia cluster running on GKE, you can enable the Cluster Autoscaler like\nthis:\n$ gcloud container clusters update kubia --enable-autoscaling \\\n  --min-nodes=3 --max-nodes=5\nIf your cluster is running on GCE, you need to set three environment variables before\nrunning \nkube-up.sh: \nKUBE_ENABLE_CLUSTER_AUTOSCALER=true\nKUBE_AUTOSCALER_MIN_NODES=3\nKUBE_AUTOSCALER_MAX_NODES=5\nRefer to the Cluster Autoscaler GitHub repo at https://github.com/kubernetes/auto-\nscaler/tree/master/cluster-autoscaler  for  information  on  how  to  enable  it  on  other\nplatforms. \nNOTEThe Cluster Autoscaler publishes its status to the cluster-autoscaler-\nstatus\n ConfigMap in the kube-system namespace.\n15.3.3   Limiting service disruption during cluster scale-down\nWhen a node fails unexpectedly, nothing you can do will prevent its pods from becom-\ning unavailable. But when a node is shut down voluntarily, either by the Cluster Auto-\nscaler or by a human operator, you can make sure the operation doesn’t disrupt the\nservice provided by the pods running on that node through an additional feature.\nManually cordoning and draining nodes\nA node can also be marked as unschedulable and drained manually. Without going\ninto specifics, this is done with the following \nkubectl commands:\nkubectl cordon <node> marks the node as unschedulable (but doesn’t do\nanything with pods running on that node).\nkubectl drain <node> marks the node as unschedulable and then evicts all\nthe pods from the node.\nIn both cases, no new pods are scheduled to the node until you uncordon it again\nwith \nkubectl uncordon <node>.\n \n\n455Horizontal scaling of cluster nodes\n  Certain  services  require  that  a  minimum  number  of  pods  always  keeps  running;\nthis is especially true for quorum-based clustered applications. For this reason, Kuber-\nnetes  provides  a  way  of  specifying  the  minimum  number  of  pods  that  need  to  keep\nrunning while performing these types of operations. This is done by creating a Pod-\nDisruptionBudget resource.\n Even though the name of the resource sounds complex, it’s one of the simplest\nKubernetes  resources  available.  It  contains  only  a  pod  label  selector  and  a  number\nspecifying  the  minimum  number  of  pods  that  must  always  be  available  or,  starting\nfrom Kubernetes version 1.7, the maximum number of pods that can be unavailable.\nWe’ll  look  at  what  a  PodDisruptionBudget  (PDB)  resource  manifest  looks  like,  but\ninstead  of  creating  it  from  a  YAML  file,  you’ll  create  it  with  \nkubectl create pod-\ndisruptionbudget\n and then obtain and examine the YAML later.\n If you want to ensure three instances of your \nkubia pod are always running (they\nhave the label \napp=kubia), create the PodDisruptionBudget resource like this:\n$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3\npoddisruptionbudget \"kubia-pdb\" created\nSimple, right? Now, retrieve the PDB’s YAML. It’s shown in the next listing.\n$ kubectl get pdb kubia-pdb -o yaml\napiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  name: kubia-pdb\nspec:\n  minAvailable: 3         \n  selector:                \n    matchLabels:           \n      app: kubia           \nstatus:\n  ...\nYou  can  also  use  a  percentage  instead  of  an  absolute  number  in  the  minAvailable\nfield. For example, you could state that 60% of all pods with the app=kubia label need\nto be running at all times.\nNOTEStarting with Kubernetes 1.7, the PodDisruptionBudget resource also\nsupports  the  \nmaxUnavailable  field,  which  you  can  use  instead  of  min-\nAvailable\n if you want to block evictions when more than that many pods are\nunavailable. \nWe  don’t  have  much  more  to  say  about  this  resource.  As  long  as  it  exists,  both  the\nCluster Autoscaler and the \nkubectl drain command will adhere to it and will never\nevict  a  pod  with  the  \napp=kubia  label  if  that  would  bring the number of such pods\nbelow three. \nListing 15.10   A PodDisruptionBudget definition\nHow many pods should \nalways be available\nThe label selector that \ndetermines which pods \nthis budget applies to\n \n\n456CHAPTER 15Automatic scaling of pods and cluster nodes\n For example, if there were four pods altogether and minAvailable was set to three\nas in the example, the pod eviction process would evict pods one by one, waiting for\nthe  evicted  pod  to  be  replaced  with  a  new  one  by  the  ReplicaSet  controller,  before\nevicting another pod. \n15.4   Summary\nThis  chapter  has  shown  you  how  Kubernetes  can  scale  not  only  your  pods,  but  also\nyour nodes. You’ve learned that\nConfiguring  the  automatic  horizontal  scaling of pods is as easy as creating a\nHorizontalPodAutoscaler  object  and  pointing  it  to  a  Deployment,  ReplicaSet,\nor ReplicationController and specifying the target CPU utilization for the pods.\nBesides having the Horizontal Pod Autoscaler perform scaling operations based\non the pods’ CPU utilization, you can also configure it to scale based on your\nown  application-provided  custom  metrics  or  metrics  related  to  other  objects\ndeployed in the cluster.\nVertical pod autoscaling isn’t possible yet.\nEven cluster nodes can be scaled automatically if your Kubernetes cluster runs\non a supported cloud provider.\nYou can run one-off processes in a pod and have the pod stopped and deleted\nautomatically as soon you press CTRL+C by using \nkubectl run with the -it and\n--rm options.\nIn the next chapter, you’ll explore advanced scheduling features, such as how to keep\ncertain pods away from certain nodes and how to schedule pods either close together\nor apart.\n \n\n457\nAdvanced scheduling\nKubernetes allows you to affect where pods are scheduled. Initially, this was only\ndone by specifying a node selector in the pod specification, but additional mech-\nanisms were later added that expanded this functionality. They’re covered in this\nchapter.\n16.1   Using taints and tolerations to repel pods from \ncertain nodes\nThe  first  two  features  related  to  advanced  scheduling  that  we’ll  explore  here  are\nthe  node  taints  and  pods’  tolerations  of  those  taints.  They’re  used  for  restricting\nThis chapter covers\nUsing node taints and pod tolerations to keep \npods away from certain nodes\nDefining node affinity rules as an alternative to \nnode selectors\nCo-locating pods using pod affinity \nKeeping pods away from each other using pod \nanti-affinity\n \n\n458CHAPTER 16Advanced scheduling\nwhich pods can use a certain node. A pod can only be scheduled to a node if it toler-\nates the node’s taints.\n  This  is  somewhat  different  from  using  node  selectors  and  node  affinity,  which\nyou’ll learn about later in this chapter. Node selectors and node affinity rules make\nit possible to select which nodes a pod can or can’t be scheduled to by specifically\nadding  that  information  to  the  pod,  whereas  taints  allow  rejecting  deployment  of\npods  to  certain  nodes  by  only  adding  taints  to  the  node  without  having  to  modify\nexisting pods. Pods that you want deployed on a tainted node need to opt in to use\nthe  node,  whereas  with  node  selectors,  pods  explicitly  specify  which  node(s)  they\nwant to be deployed to.\n16.1.1   Introducing taints and tolerations\nThe best path to learn about node taints is to see an existing taint. Appendix B shows\nhow to set up a multi-node cluster with the \nkubeadm tool. By default, the master node\nin such a cluster is tainted, so only Control Plane pods can be deployed on it. \nDISPLAYING A NODE’S TAINTS\nYou can see the node’s taints using kubectl describe node, as shown in the follow-\ning listing.\n$ kubectl describe node master.k8s\nName:         master.k8s\nRole:\nLabels:       beta.kubernetes.io/arch=amd64\n              beta.kubernetes.io/os=linux\n              kubernetes.io/hostname=master.k8s\n              node-role.kubernetes.io/master=\nAnnotations:  node.alpha.kubernetes.io/ttl=0\n              volumes.kubernetes.io/controller-managed-attach-detach=true\nTaints:       node-role.kubernetes.io/master:NoSchedule      \n...\nThe master node has a single taint. Taints have a key, value, and an effect, and are repre-\nsented  as  \n<key>=<value>:<effect>.  The  master  node’s  taint  shown  in  the  previous\nlisting has the key \nnode-role.kubernetes.io/master, a null value (not shown in the\ntaint), and the effect of \nNoSchedule. \n This taint prevents pods from being scheduled to the master node, unless those pods\ntolerate this taint. The pods that tolerate it are usually system pods (see figure 16.1).\n \n \n \n \nListing 16.1   Describing the master node in a cluster created with kubeadm\nThe master node \nhas one taint.\n \n\n459Using taints and tolerations to repel pods from certain nodes\nDISPLAYING A POD’S TOLERATIONS\nIn a cluster installed with kubeadm, the kube-proxy cluster component runs as a pod\non  every  node,  including  the  master  node,  because  master  components  that  run  as\npods may also need to access Kubernetes Services. To make sure the kube-proxy pod\nalso runs on the master node, it includes the appropriate toleration. In total, the pod\nhas three tolerations, which are shown in the following listing.\n$ kubectl describe po kube-proxy-80wqm -n kube-system\n...\nTolerations:    node-role.kubernetes.io/master=:NoSchedule\n                node.alpha.kubernetes.io/notReady=:Exists:NoExecute\n                node.alpha.kubernetes.io/unreachable=:Exists:NoExecute\n...\nAs you can see, the first toleration matches the master node’s taint, allowing this kube-\nproxy pod to be scheduled to the master node. \nNOTEDisregard the equal sign, which is shown in the pod’s tolerations, but\nnot in the node’s taints. Kubectl apparently displays taints and tolerations dif-\nferently when the taint’s/toleration’s value is \nnull.\nUNDERSTANDING TAINT EFFECTS\nThe two other tolerations on the kube-proxy pod define how long the pod is allowed\nto run on nodes that aren’t ready or are unreachable (the time in seconds isn’t shown,\nListing 16.2   A pod’s tolerations\nSystem pod may be\nscheduled to master\nnode because its\ntoleration matches\nthe node’s taint.\nSystem pod\nMaster node\nTaint:\nnode-role.kubernetes.io\n/master:NoSchedule\nToleration:\nnode-role.kubernetes.io\n/master:NoSchedule\nRegular pod\nRegular node\nNo taints\nNo tolerations\nPods with no tolerations\nmay only be scheduled\nto nodes without taints.\nFigure 16.1   A pod is only scheduled to a node if it tolerates the node’s taints.\n \n\n460CHAPTER 16Advanced scheduling\nbut  can  be  seen  in  the  pod’s  YAML).  Those  two  tolerations  refer  to  the  NoExecute\ninstead of the NoSchedule effect. \n Each taint has an effect associated with it. Three possible effects exist:\nNoSchedule, which means pods won’t be scheduled to the node if they don’t tol-\nerate the taint.\nPreferNoSchedule is a soft version of NoSchedule, meaning the scheduler will\ntry to avoid scheduling the pod to the node, but will schedule it to the node if it\ncan’t schedule it somewhere else. \nNoExecute, unlike NoSchedule and PreferNoSchedule that only affect schedul-\ning, also affects pods already running on the node. If you add a \nNoExecute taint\nto  a  node,  pods  that  are  already  running  on  that  node  and  don’t  tolerate  the\nNoExecute taint will be evicted from the node. \n16.1.2   Adding custom taints to a node\nImagine having a single Kubernetes cluster where you run both production and non-\nproduction workloads. It’s of the utmost importance that non-production pods never\nrun on the production nodes. This can be achieved by adding a taint to your produc-\ntion nodes. To add a taint, you use the \nkubectl taint command:\n$ kubectl taint node node1.k8s node-type=production:NoSchedule\nnode \"node1.k8s\" tainted\nThis adds a taint with key node-type, value production and the NoSchedule effect. If\nyou now deploy multiple replicas of a regular pod, you’ll see none of them are sched-\nuled to the node you tainted, as shown in the following listing.\n$ kubectl run test --image busybox --replicas 5 -- sleep 99999\ndeployment \"test\" created\n$ kubectl get po -o wide\nNAME                READY  STATUS    RESTARTS   AGE   IP          NODE\ntest-196686-46ngl   1/1    Running   0          12s   10.47.0.1   node2.k8s\ntest-196686-73p89   1/1    Running   0          12s   10.47.0.7   node2.k8s\ntest-196686-77280   1/1    Running   0          12s   10.47.0.6   node2.k8s\ntest-196686-h9m8f   1/1    Running   0          12s   10.47.0.5   node2.k8s\ntest-196686-p85ll   1/1    Running   0          12s   10.47.0.4   node2.k8s\nNow, no one can inadvertently deploy pods onto the production nodes. \n16.1.3   Adding tolerations to pods\nTo deploy production pods to the production nodes, they need to tolerate the taint\nyou added to the nodes. The manifests of your production pods need to include the\nYAML snippet shown in the following listing.\n \nListing 16.3   Deploying pods without a toleration\n \n\n461Using taints and tolerations to repel pods from certain nodes\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: prod\nspec:\n  replicas: 5\n  template:\n    spec:\n      ...\n      tolerations:\n      - key: node-type         \n        Operator: Equal        \n        value: production      \n        effect: NoSchedule     \nIf  you  deploy  this  Deployment,  you’ll  see  its  pods  get  deployed  to  the  production\nnode, as shown in the next listing.\n$ kubectl get po -o wide\nNAME                READY  STATUS    RESTARTS   AGE   IP          NODE\nprod-350605-1ph5h   0/1    Running   0          16s   10.44.0.3   node1.k8s\nprod-350605-ctqcr   1/1    Running   0          16s   10.47.0.4   node2.k8s\nprod-350605-f7pcc   0/1    Running   0          17s   10.44.0.6   node1.k8s\nprod-350605-k7c8g   1/1    Running   0          17s   10.47.0.9   node2.k8s\nprod-350605-rp1nv   0/1    Running   0          17s   10.44.0.4   node1.k8s\nAs you can see in the listing, production pods were also deployed to node2, which isn’t\na production node. To prevent that from happening, you’d also need to taint the non-\nproduction nodes with a taint such as \nnode-type=non-production:NoSchedule. Then\nyou’d also need to add the matching toleration to all your non-production pods.\n16.1.4   Understanding what taints and tolerations can be used for\nNodes can have more than one taint and pods can have more than one toleration. As\nyou’ve seen, taints can only have a key and an effect and don’t require a value. Tolera-\ntions  can  tolerate  a  specific  value  by  specifying  the  \nEqual  operator  (that’s  also  the\ndefault operator if you don’t specify one), or they can tolerate any value for a specific\ntaint key if you use the \nExists operator.\nUSING TAINTS AND TOLERATIONS DURING SCHEDULING\nTaints  can  be  used  to  prevent  scheduling  of  new  pods  (NoSchedule  effect)  and  to\ndefine  unpreferred  nodes  (\nPreferNoSchedule  effect)  and  even  evict  existing  pods\nfrom a node (\nNoExecute).\n You can set up taints and tolerations any way you see fit. For example, you could\npartition  your  cluster  into  multiple  partitions,  allowing  your  development  teams  to\nschedule pods only to their respective nodes. You can also use taints and tolerations\nListing 16.4   A production Deployment with a toleration: production-deployment.yaml\nListing 16.5   Pods with the toleration are deployed on production node1\nThis toleration allows the \npod to be scheduled to \nproduction nodes.\n \n\n462CHAPTER 16Advanced scheduling\nwhen several of your nodes provide special hardware and only part of your pods need\nto use it.\nCONFIGURING HOW LONG AFTER A NODE FAILURE A POD IS RESCHEDULED\nYou  can  also  use  a  toleration  to  specify  how  long  Kubernetes  should  wait  before\nrescheduling  a  pod  to  another  node  if  the  node  the  pod  is  running  on  becomes\nunready or unreachable. If you look at the tolerations of one of your pods, you’ll see\ntwo tolerations, which are shown in the following listing.\n$ kubectl get po prod-350605-1ph5h -o yaml\n...\n  tolerations:\n  - effect: NoExecute                            \n    key: node.alpha.kubernetes.io/notReady       \n    operator: Exists                             \n    tolerationSeconds: 300                       \n  - effect: NoExecute                              \n    key: node.alpha.kubernetes.io/unreachable      \n    operator: Exists                               \n    tolerationSeconds: 300                         \nThese two tolerations say that this pod tolerates a node being notReady or unreach-\nable\n for 300 seconds. The Kubernetes Control Plane, when it detects that a node is no\nlonger  ready  or  no  longer  reachable,  will  wait  for  300  seconds  before  it  deletes  the\npod and reschedules it to another node.\n These two tolerations are automatically added to pods that don’t define them. If\nthat  five-minute  delay  is  too  long  for  your  pods,  you  can  make  the  delay  shorter  by\nadding those two tolerations to the pod’s spec.\nNOTEThis is currently an alpha feature, so it may change in future versions\nof Kubernetes. Taint-based evictions also aren’t enabled by default. You enable\nthem  by  running  the  Controller  Manager  with  the  \n--feature-gates=Taint-\nBasedEvictions=true\n option.\n16.2   Using node affinity to attract pods to certain nodes\nAs you’ve learned, taints are used to keep pods away from certain nodes. Now you’ll\nlearn about a newer mechanism called node affinity, which allows you to tell Kuberne-\ntes to schedule pods only to specific subsets of nodes.\nCOMPARING NODE AFFINITY TO NODE SELECTORS\nThe  initial  node  affinity  mechanism  in  early  versions  of  Kubernetes  was  the  node-\nSelector\n field in the pod specification. The node had to include all the labels speci-\nfied in that field to be eligible to become the target for the pod. \n  Node  selectors  get  the  job  done  and  are  simple,  but  they  don’t  offer  everything\nthat  you  may  need.  Because  of  that,  a  more  powerful  mechanism  was  introduced.\nListing 16.6   Pod with default tolerations\nThe pod tolerates the node being \nnotReady for 300 seconds, before \nit needs to be rescheduled.\nThe same applies to the \nnode being unreachable.\n \n\n463Using node affinity to attract pods to certain nodes\nNode  selectors  will  eventually  be  deprecated,  so  it’s  important  you  understand  the\nnew node affinity rules.\n  Similar  to  node  selectors,  each  pod  can  define its own node affinity rules. These\nallow you to specify either hard requirements or preferences. By specifying a prefer-\nence, you tell Kubernetes which nodes you prefer for a specific pod, and Kubernetes\nwill try to schedule the pod to one of those nodes. If that’s not possible, it will choose\none of the other nodes. \nEXAMINING THE DEFAULT NODE LABELS\nNode  affinity  selects  nodes  based  on  their  labels,  the  same  way  node  selectors  do.\nBefore you see how to use node affinity, let’s examine the labels of one of the nodes in\na  Google  Kubernetes  Engine  cluster  (GKE)  to  see  what  the  default  node  labels  are.\nThey’re shown in the following listing.\n$ kubectl describe node gke-kubia-default-pool-db274c5a-mjnf\nName:     gke-kubia-default-pool-db274c5a-mjnf\nRole:\nLabels:   beta.kubernetes.io/arch=amd64\n          beta.kubernetes.io/fluentd-ds-ready=true\n          beta.kubernetes.io/instance-type=f1-micro\n          beta.kubernetes.io/os=linux\n          cloud.google.com/gke-nodepool=default-pool\n          failure-domain.beta.kubernetes.io/region=europe-west1         \n          failure-domain.beta.kubernetes.io/zone=europe-west1-d         \n          kubernetes.io/hostname=gke-kubia-default-pool-db274c5a-mjnf   \nThe node has many labels, but the last three are the most important when it comes to\nnode  affinity  and  pod  affinity,  which  you’ll  learn  about  later.  The  meaning  of  those\nthree labels is as follows:\nfailure-domain.beta.kubernetes.io/region specifies the geographical region\nthe node is located in.\nfailure-domain.beta.kubernetes.io/zone  specifies  the  availability  zone  the\nnode is in.\nkubernetes.io/hostname is obviously the node’s hostname.\nThese  and  other  labels  can  be  used  in  pod  affinity  rules.  In  chapter  3,  you  already\nlearned how you can add a custom label to nodes and use it in a pod’s node selector.\nYou used the custom label to deploy pods only to nodes with that label by adding a node\nselector to the pods. Now, you’ll see how to do the same using node affinity rules.\n16.2.1   Specifying hard node affinity rules\nIn the example in chapter 3, you used the node selector to deploy a pod that requires\na GPU only to nodes that have a GPU. The pod spec included the \nnodeSelector field\nshown in the following listing.\nListing 16.7   Default labels of a node in GKE\nThese three\nlabels are the\nmost important\nones related to\nnode affinity.\n \n\n464CHAPTER 16Advanced scheduling\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-gpu\nspec:\n  nodeSelector:          \n    gpu: \"true\"          \n  ...\nThe nodeSelector field specifies that the pod should only be deployed on nodes that\ninclude the \ngpu=true label. If you replace the node selector with a node affinity rule,\nthe pod definition will look like the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-gpu\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: gpu\n            operator: In\n            values:\n            - \"true\"\nThe first thing you’ll notice is that this is much more complicated than a simple node\nselector. But that’s because it’s much more expressive. Let’s examine the rule in detail. \nMAKING SENSE OF THE LONG NODEAFFINITY ATTRIBUTE NAME\nAs you can see, the pod’s spec section contains an affinity field that contains a node-\nAffinity\n field, which contains a field with an extremely long name, so let’s focus on\nthat first.\n Let’s break it down into two parts and examine what they mean:\nrequiredDuringScheduling... means the rules defined under this field spec-\nify the labels the node must have for the pod to be scheduled to the node.\n...IgnoredDuringExecution  means  the  rules  defined  under  the  field  don’t\naffect pods already executing on the node. \nAt this point, let me make things easier for you by letting you know that affinity cur-\nrently only affects pod scheduling and never causes a pod to be evicted from a node.\nThat’s why all the rules right now always end with \nIgnoredDuringExecution. Eventu-\nally, Kubernetes will also support \nRequiredDuringExecution, which means that if you\nListing 16.8   A pod using a node selector: kubia-gpu-nodeselector.yaml\nListing 16.9   A pod using a nodeAffinity rule: kubia-gpu-nodeaffinity.yaml\nThis pod is only scheduled \nto nodes that have the \ngpu=true label.\n \n\n465Using node affinity to attract pods to certain nodes\nremove a label from a node, pods that require the node to have that label will be\nevicted from such a node. As I’ve said, that’s not yet supported in Kubernetes, so let’s\nnot concern ourselves with the second part of that long field any longer.\nUNDERSTANDING NODESELECTORTERMS\nBy keeping what was explained in the previous section in mind, it’s easy to understand\nthat  the  \nnodeSelectorTerms  field  and  the  matchExpressions  field  define  which\nexpressions  the  node’s  labels  must  match  for  the  pod  to  be  scheduled  to  the  node.\nThe single expression in the example is simple to understand. The node must have a\ngpu label whose value is set to true. \n This pod will therefore only be scheduled to nodes that have the \ngpu=true label, as\nshown in figure 16.2.\nNow comes the more interesting part. Node also affinity allows you to prioritize nodes\nduring scheduling. We’ll look at that next.\n16.2.2   Prioritizing nodes when scheduling a pod\nThe biggest benefit of the newly introduced node affinity feature is the ability to spec-\nify which nodes the Scheduler should prefer when scheduling a specific pod. This is\ndone through the \npreferredDuringSchedulingIgnoredDuringExecution field.\n  Imagine  having  multiple  datacenters  across  different  countries.  Each  datacenter\nrepresents a separate availability zone. In each zone, you have certain machines meant\nonly for your own use and others that your partner companies can use. You now want\nto  deploy  a  few  pods  and  you’d  prefer  them  to  be  scheduled  to  \nzone1  and  to  the\nNode with a GPU\nPod\nNode affinity\nRequired label:\ngpu=true\nPod\nNo node affinity\ngpu: true\nNode with a GPUNode without a GPUNode without a GPU\ngpu: true\nThis pod may be scheduled only\nto nodes with gpu=true label\nThis pod may be\nscheduled to any node\nFigure 16.2   A pod’s node affinity specifies which labels a node must have for the pod to be \nscheduled to it.\n \n\n466CHAPTER 16Advanced scheduling\nmachines  reserved  for  your  company’s  deployments.  If  those  machines  don’t  have\nenough room for the pods or if other important reasons exist that prevent them from\nbeing scheduled there, you’re okay with them being scheduled to the machines your\npartners use and to the other zones. Node affinity allows you to do that.\nLABELING NODES\nFirst, the nodes need to be labeled appropriately. Each node needs to have a label that\ndesignates the availability zone the node belongs to and a label marking it as either a\ndedicated or a shared node.\n  Appendix  B  explains  how  to  set  up  a  three-node  cluster  (one  master  and  two\nworker nodes) in VMs running locally. In the following examples, I’ll use the two worker\nnodes  in  that  cluster,  but  you  can  also  use  Google  Kubernetes  Engine  or  any  other\nmulti-node cluster. \nNOTEMinikube isn’t the best choice for running these examples, because it\nruns only one node.\nFirst, label the nodes, as shown in the next listing.\n$ kubectl label node node1.k8s availability-zone=zone1\nnode \"node1.k8s\" labeled\n$ kubectl label node node1.k8s share-type=dedicated\nnode \"node1.k8s\" labeled\n$ kubectl label node node2.k8s availability-zone=zone2\nnode \"node2.k8s\" labeled\n$ kubectl label node node2.k8s share-type=shared\nnode \"node2.k8s\" labeled\n$ kubectl get node -L availability-zone -L share-type\nNAME         STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE\nmaster.k8s   Ready     4d        v1.6.4    <none>              <none>\nnode1.k8s    Ready     4d        v1.6.4    zone1               dedicated\nnode2.k8s    Ready     4d        v1.6.4    zone2               shared\nSPECIFYING PREFERENTIAL NODE AFFINITY RULES\nWith the node labels set up, you can now create a Deployment that prefers dedicated\nnodes in zone1. The following listing shows the Deployment manifest.\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: pref\nspec:\n  template:\n    ...\n    spec:\n      affinity:\n        nodeAffinity:\nListing 16.10   Labeling nodes\nListing 16.11   Deployment with preferred node affinity: preferred-deployment.yaml\n \n\n467Using node affinity to attract pods to certain nodes\n          preferredDuringSchedulingIgnoredDuringExecution:    \n          - weight: 80                               \n            preference:                              \n              matchExpressions:                      \n              - key: availability-zone               \n                operator: In                         \n                values:                              \n                - zone1                              \n          - weight: 20                     \n            preference:                    \n              matchExpressions:            \n              - key: share-type            \n                operator: In               \n                values:                    \n                - dedicated                \n      ...\nLet’s examine the listing closely. You’re defining a node affinity preference, instead of\na  hard  requirement.  You  want  the  pods  scheduled  to  nodes  that  include  the  labels\navailability-zone=zone1  and  share-type=dedicated.  You’re  saying  that  the  first\npreference  rule  is  important  by  setting  its  \nweight  to  80,  whereas  the  second  one  is\nmuch less important (\nweight is set to 20).\nUNDERSTANDING HOW NODE PREFERENCES WORK\nIf your cluster had many nodes, when scheduling the pods of the Deployment in the\nprevious  listing,  the  nodes  would  be  split  into  four  groups,  as  shown  in  figure  16.3.\nNodes whose \navailability-zone and share-type labels match the pod’s node affin-\nity are ranked the highest. Then, because of how the weights in the pod’s node affinity\nrules are configured, next come the \nshared nodes in zone1, then come the dedicated\nnodes in the other zones, and at the lowest priority are all the other nodes.\nYou’re\nspecifying\npreferences,\nnot hard\nrequirements.\nYou prefer the pod to be \nscheduled to zone1. This \nis your most important \npreference.\nYou also prefer that your \npods be scheduled to \ndedicated nodes, but this is \nfour times less important \nthan your zone preference.\nNode\nTop priority\nAvailability zone 1\nPod\nPriority: 2Priority: 3Priority: 4\nNode affinity\nPreferred labels:\navail-zone:zone1 (weight 80)\nshare:dedicated (weight 20)\navail-zone:zone1\nshare:dedicated\nNode\navail-zone:zone1\nshare:shared\nNode\nAvailability zone 2\navail-zone:zone2\nshare:dedicated\nNode\navail-zone:zone2\nshare:shared\nThis pod may be scheduled to\nany node, but certain nodes are\npreferred based on their labels.\nFigure 16.3   Prioritizing nodes based on a pod’s node affinity preferences\n \n\n468CHAPTER 16Advanced scheduling\nDEPLOYING THE PODS IN THE TWO-NODE CLUSTER\nIf  you  create  this  Deployment  in  your  two-node  cluster,  you  should  see  most  (if  not\nall) of your pods deployed to \nnode1. Examine the following listing to see if that’s true.\n$ kubectl get po -o wide\nNAME                READY   STATUS    RESTARTS  AGE   IP          NODE\npref-607515-1rnwv   1/1     Running   0         4m    10.47.0.1   node2.k8s\npref-607515-27wp0   1/1     Running   0         4m    10.44.0.8   node1.k8s\npref-607515-5xd0z   1/1     Running   0         4m    10.44.0.5   node1.k8s\npref-607515-jx9wt   1/1     Running   0         4m    10.44.0.4   node1.k8s\npref-607515-mlgqm   1/1     Running   0         4m    10.44.0.6   node1.k8s\nOut  of  the  five  pods  that  were  created,  four  of  them  landed  on  node1  and  only  one\nlanded on \nnode2. Why did one of them land on node2 instead of node1? The reason is\nthat besides the node affinity prioritization function, the Scheduler also uses other pri-\noritization functions to decide where to schedule a pod. One of those is the \nSelector-\nSpreadPriority\n function, which makes sure pods belonging to the same ReplicaSet or\nService are spread around different nodes so a node failure won’t bring the whole ser-\nvice down. That’s most likely what caused one of the pods to be scheduled to \nnode2.\n You can try scaling the Deployment up to 20 or more and you’ll see the majority of\npods will be scheduled to \nnode1. In my test, only two out of the 20 were scheduled to\nnode2. If you hadn’t defined any node affinity preferences, the pods would have been\nspread around the two nodes evenly.\n16.3   Co-locating pods with pod affinity and anti-affinity\nYou’ve seen how node affinity rules are used to influence which node a pod is scheduled\nto. But these rules only affect the affinity between a pod and a node, whereas sometimes\nyou’d like to have the ability to specify the affinity between pods themselves. \n  For  example,  imagine  having  a  frontend  and  a  backend  pod.  Having  those  pods\ndeployed  near  to  each  other  reduces  latency  and  improves  the  performance  of  the\napp. You could use node affinity rules to ensure both are deployed to the same node,\nrack, or datacenter, but then you’d have to specify exactly which node, rack, or data-\ncenter to schedule them to, which is not the best solution. It’s better to let Kubernetes\ndeploy your pods anywhere it sees fit, while keeping the frontend and backend pods\nclose together. This can be achieved using pod affinity. Let’s learn more about it with\nan example.\n16.3.1   Using inter-pod affinity to deploy pods on the same node\nYou’ll deploy a backend pod and five frontend pod replicas with pod affinity config-\nured so that they’re all deployed on the same node as the backend pod.\n First, deploy the backend pod:\n$ kubectl run backend -l app=backend --image busybox -- sleep 999999\ndeployment \"backend\" created\nListing 16.12   Seeing where pods were scheduled\n \n\n469Co-locating pods with pod affinity and anti-affinity\nThis  Deployment  is  not  special  in  any  way. The only thing you need to note is the\napp=backend label you added to the pod using the -l option. This label is what you’ll\nuse in the frontend pod’s \npodAffinity configuration. \nSPECIFYING POD AFFINITY IN A POD DEFINITION\nThe frontend pod’s definition is shown in the following listing.\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 5\n  template:\n    ...\n    spec:\n      affinity:\n        podAffinity:                                 \n          requiredDuringSchedulingIgnoredDuringExecution:   \n          - topologyKey: kubernetes.io/hostname           \n            labelSelector:                                \n              matchLabels:                                \n                app: backend                              \n      ...\nThe listing shows that this Deployment will create pods that have a hard requirement\nto be deployed on the same node (specified by the \ntopologyKey  field)  as  pods  that\nhave the \napp=backend label (see figure 16.4).\nListing 16.13   Pod using podAffinity: frontend-podaffinity-host.yaml\nDefining \npodAffinity rules\nDefining a hard \nrequirement, not \na preference\nThe pods of this Deployment \nmust be deployed on the \nsame node as the pods that \nmatch the selector.\nAll frontend pods will\nbe scheduled only to\nthe node the backend\npod was scheduled to.\nSome nodeOther nodes\nFrontend pods\nBackend\npod\nPod affinity\nLabel selector:app=backend\nTopology key:hostname\napp: backend\nFigure 16.4   Pod affinity allows scheduling pods to the node where other pods \nwith a specific label are.\n \n\n470CHAPTER 16Advanced scheduling\nNOTEInstead of the simpler matchLabels field, you could also use the more\nexpressive \nmatchExpressions field.\nDEPLOYING A POD WITH POD AFFINITY\nBefore you create this Deployment, let’s see which node the backend pod was sched-\nuled to earlier:\n$ kubectl get po -o wide\nNAME                   READY  STATUS   RESTARTS  AGE  IP         NODE\nbackend-257820-qhqj6   1/1    Running  0         8m   10.47.0.1  node2.k8s\nWhen you create the frontend pods, they should be deployed to node2 as well. You’re\ngoing to create the Deployment and see where the pods are deployed. This is shown\nin the next listing.\n$ kubectl create -f frontend-podaffinity-host.yaml\ndeployment \"frontend\" created\n$ kubectl get po -o wide\nNAME                   READY  STATUS    RESTARTS  AGE  IP         NODE\nbackend-257820-qhqj6   1/1    Running   0         8m   10.47.0.1  node2.k8s\nfrontend-121895-2c1ts  1/1    Running   0         13s  10.47.0.6  node2.k8s\nfrontend-121895-776m7  1/1    Running   0         13s  10.47.0.4  node2.k8s\nfrontend-121895-7ffsm  1/1    Running   0         13s  10.47.0.8  node2.k8s\nfrontend-121895-fpgm6  1/1    Running   0         13s  10.47.0.7  node2.k8s\nfrontend-121895-vb9ll  1/1    Running   0         13s  10.47.0.5  node2.k8s\nAll the frontend pods were indeed scheduled to the same node as the backend pod.\nWhen scheduling the frontend pod, the Scheduler first found all the pods that match\nthe \nlabelSelector  defined  in  the  frontend  pod’s  podAffinity  configuration  and\nthen scheduled the frontend pod to the same node.\nUNDERSTANDING HOW THE SCHEDULER USES POD AFFINITY RULES\nWhat’s interesting is that if you now delete the backend pod, the Scheduler will sched-\nule  the  pod  to  \nnode2  even  though  it  doesn’t  define  any  pod  affinity  rules  itself  (the\nrules are only on the frontend pods). This makes sense, because otherwise if the back-\nend pod were to be deleted by accident and rescheduled to a different node, the fron-\ntend pods’ affinity rules would be broken. \n You can confirm the Scheduler takes other pods’ pod affinity rules into account, if\nyou increase the Scheduler’s logging level and then check its log. The following listing\nshows the relevant log lines.\n... Attempting to schedule pod: default/backend-257820-qhqj6\n... ...\n... backend-qhqj6 -> node2.k8s: Taint Toleration Priority, Score: (10)\nListing 16.14   Deploying frontend pods and seeing which node they’re scheduled to\nListing 16.15   Scheduler log showing why the backend pod is scheduled to node2\n \n\n471Co-locating pods with pod affinity and anti-affinity\n... backend-qhqj6 -> node1.k8s: Taint Toleration Priority, Score: (10)\n... backend-qhqj6 -> node2.k8s: InterPodAffinityPriority, Score: (10)\n... backend-qhqj6 -> node1.k8s: InterPodAffinityPriority, Score: (0)\n... backend-qhqj6 -> node2.k8s: SelectorSpreadPriority, Score: (10)\n... backend-qhqj6 -> node1.k8s: SelectorSpreadPriority, Score: (10)\n... backend-qhqj6 -> node2.k8s: NodeAffinityPriority, Score: (0)\n... backend-qhqj6 -> node1.k8s: NodeAffinityPriority, Score: (0)\n... Host node2.k8s => Score 100030\n... Host node1.k8s => Score 100022\n... Attempting to bind backend-257820-qhqj6 to node2.k8s\nIf you focus on the two lines in bold, you’ll see that during the scheduling of the back-\nend pod, \nnode2 received a higher score than node1 because of inter-pod affinity. \n16.3.2   Deploying pods in the same rack, availability zone, or \ngeographic region\nIn  the  previous  example,  you  used  podAffinity  to  deploy  frontend  pods  onto  the\nsame  node  as  the  backend  pods.  You  probably  don’t  want  all  your  frontend  pods  to\nrun  on  the  same  machine,  but  you’d  still  like  to  keep  them  close  to  the  backend\npod—for example, run them in the same availability zone. \nCO-LOCATING PODS IN THE SAME AVAILABILITY ZONE\nThe cluster I’m using runs in three VMs on my local machine, so all the nodes are in\nthe same availability zone, so to speak. But if the nodes were in different zones, all I’d\nneed to do to run the frontend pods in the same zone as the backend pod would be to\nchange the \ntopologyKey property to failure-domain.beta.kubernetes.io/zone. \nCO-LOCATING PODS IN THE SAME GEOGRAPHICAL REGION\nTo allow the pods to be deployed in the same region instead of the same zone (cloud\nproviders usually have datacenters located in different geographical regions and split\ninto  multiple  availability  zones  in  each  region),  the  \ntopologyKey  would  be  set  to\nfailure-domain.beta.kubernetes.io/region.\nUNDERSTANDING HOW TOPOLOGYKEY WORKS\nThe way topologyKey works is simple. The three keys we’ve mentioned so far aren’t\nspecial. If you want, you can easily use your own \ntopologyKey, such as rack, to have\nthe  pods  scheduled  to  the  same  server  rack.  The  only  prerequisite  is  to  add  a  \nrack\nlabel to your nodes. This scenario is shown in figure 16.5.\n For example, if you had 20 nodes, with 10 in each rack, you’d label the first ten as\nrack=rack1 and the others as rack=rack2. Then, when defining a pod’s podAffinity,\nyou’d set the \ntoplogyKey to rack. \n When the Scheduler is deciding where to deploy a pod, it checks the pod’s \npod-\nAffinity\n config, finds the pods that match the label selector, and looks up the nodes\nthey’re  running  on.  Specifically,  it  looks  up  the  nodes’  label  whose  key  matches  the\ntopologyKey field specified in podAffinity. Then it selects all the nodes whose label\n \n\n472CHAPTER 16Advanced scheduling\nmatches  the  values  of  the  pods  it  found  earlier.  In  figure  16.5,  the  label  selector\nmatched  the  backend  pod,  which  runs  on  Node  12.  The  value  of  the  \nrack  label  on\nthat node equals \nrack2, so when scheduling a frontend pod, the Scheduler will only\nselect among the nodes that have the \nrack=rack2 label.\nNOTEBy  default,  the  label  selector  only  matches  pods  in  the  same  name-\nspace  as  the  pod  that’s  being  scheduled.  But  you  can  also  select  pods  from\nother  namespaces  by  adding  a  \nnamespaces  field  at  the  same  level  as  label-\nSelector\n.\n16.3.3   Expressing pod affinity preferences instead of hard requirements\nEarlier, when we talked about node affinity, you saw that nodeAffinity can be used to\nexpress a hard requirement, which means a pod is only scheduled to nodes that match\nthe node affinity rules. It can also be used to specify node preferences, to instruct the\nScheduler to schedule the pod to certain nodes, while allowing it to schedule it any-\nwhere else if those nodes can’t fit the pod for any reason.\n The same also applies to \npodAffinity. You can tell the Scheduler you’d prefer to\nhave your frontend pods scheduled onto the same node as your backend pod, but if\nthat’s not possible, you’re okay with them being scheduled elsewhere. An example of\na  Deployment  using  the  \npreferredDuringSchedulingIgnoredDuringExecution  pod\naffinity rule is shown in the next listing.\nFrontend pods will be\nscheduled to nodes in\nthe same rack as the\nbackend pod.\nNode 1\nRack 1\nrack: rack1\nNode 2\nrack: rack1\nNode 3\n...\nrack: rack1\nNode 10\nrack: rack1\nNode 11\nRack 2\nrack: rack2\nNode 12\nrack: rack2\n...\nNode 20\nrack: rack2\nBackend\npod\napp: backend\nFrontend pods\nPod affinity (required)\nLabel selector:app=backend\nTopology key:rack\nFigure 16.5   The topologyKey in podAffinity determines the scope of where the pod \nshould be scheduled to.\n \n\n473Co-locating pods with pod affinity and anti-affinity\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 5\n  template:\n    ...\n    spec:\n      affinity:\n        podAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:  \n          - weight: 80                                        \n            podAffinityTerm:                                  \n              topologyKey: kubernetes.io/hostname             \n              labelSelector:                                  \n                matchLabels:                                  \n                  app: backend                                \n      containers: ...\nAs in nodeAffinity preference rules, you need to define a weight for each rule. You\nalso need to specify the \ntopologyKey and labelSelector, as in the hard-requirement\npodAffinity rules. Figure 16.6 shows this scenario.\nDeploying this pod, as with your \nnodeAffinity example, deploys four pods on the same\nnode as the backend pod, and one pod on the other node (see the following listing).\nListing 16.16   Pod affinity preference\nPreferred \ninstead of \nRequired\nA weight and a \npodAffinity term is \nspecified as in the \nprevious example\nThe Scheduler will prefer\nNode 2 for frontend pods,\nbut may schedule pods\nto Node 1 as well.\nNode 1Node 2\nBackend\npod\napp: backend\nFrontend pod\nPod affinity (preferred)\nLabel selector:app=backend\nTopology key:hostname\nhostname: node2hostname: node1\nFigure 16.6   Pod affinity can be used to make the Scheduler prefer nodes where \npods with a certain label are running. \n \n\n474CHAPTER 16Advanced scheduling\n$ kubectl get po -o wide\nNAME                   READY  STATUS   RESTARTS  AGE  IP          NODE\nbackend-257820-ssrgj   1/1    Running  0         1h   10.47.0.9   node2.k8s\nfrontend-941083-3mff9  1/1    Running  0         8m   10.44.0.4   node1.k8s\nfrontend-941083-7fp7d  1/1    Running  0         8m   10.47.0.6   node2.k8s\nfrontend-941083-cq23b  1/1    Running  0         8m   10.47.0.1   node2.k8s\nfrontend-941083-m70sw  1/1    Running  0         8m   10.47.0.5   node2.k8s\nfrontend-941083-wsjv8  1/1    Running  0         8m   10.47.0.4   node2.k8s\n16.3.4   Scheduling pods away from each other with pod anti-affinity\nYou’ve seen how to tell the Scheduler to co-locate pods, but sometimes you may want\nthe  exact  opposite.  You  may  want  to  keep  pods  away  from  each  other.  This  is  called\npod  anti-affinity.  It’s  specified  the  same  way  as  pod  affinity,  except  that  you  use  the\npodAntiAffinity  property  instead  of  podAffinity,  which  results  in  the  Scheduler\nnever choosing nodes where pods matching the \npodAntiAffinity’s label selector are\nrunning, as shown in figure 16.7.\nAn example of why you’d want to use pod anti-affinity is when two sets of pods inter-\nfere with each other’s performance if they run on the same node. In that case, you\nwant to tell the Scheduler to never schedule  those  pods  on  the  same  node.  Another\nexample would be to force the Scheduler to spread pods of the same group across dif-\nferent availability zones or regions, so that a failure of a whole zone (or region) never\nbrings the service down completely. \nListing 16.17   Pods deployed with podAffinity preferences\nThese pods will NOT be scheduled\nto the same node(s) where pods\nwith app=foo label are running.\nSome nodeOther nodes\nPods\nPod: foo\nPod(required)anti-affinity\nLabel selector:app=foo\nTopology key:hostname\napp: foo\nFigure 16.7   Using pod anti-affinity to keep pods away from nodes that run pods \nwith a certain label.\n \n\n475Co-locating pods with pod affinity and anti-affinity\nUSING ANTI-AFFINITY TO SPREAD APART PODS OF THE SAME DEPLOYMENT\nLet’s see how to force your frontend pods to be scheduled to different nodes. The fol-\nlowing listing shows how the pods’ anti-affinity is configured.\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 5\n  template:\n    metadata:\n      labels:                  \n        app: frontend          \n    spec:\n      affinity:\n        podAntiAffinity:                                      \n          requiredDuringSchedulingIgnoredDuringExecution:     \n          - topologyKey: kubernetes.io/hostname            \n            labelSelector:                                 \n              matchLabels:                                 \n                app: frontend                              \n      containers: ...\nThis time, you’re defining podAntiAffinity instead of podAffinity, and you’re mak-\ning  the  \nlabelSelector  match  the  same  pods  that  the  Deployment  creates.  Let’s  see\nwhat happens when you create this Deployment. The pods created by it are shown in\nthe following listing.\n$ kubectl get po -l app=frontend -o wide\nNAME                    READY  STATUS   RESTARTS  AGE  IP         NODE\nfrontend-286632-0lffz   0/1    Pending  0         1m   <none>\nfrontend-286632-2rkcz   1/1    Running  0         1m   10.47.0.1  node2.k8s\nfrontend-286632-4nwhp   0/1    Pending  0         1m   <none>\nfrontend-286632-h4686   0/1    Pending  0         1m   <none>\nfrontend-286632-st222   1/1    Running  0         1m   10.44.0.4  node1.k8s\nAs you can see, only two pods were scheduled—one to node1, the other to node2. The\nthree remaining pods are all \nPending, because the Scheduler isn’t allowed to schedule\nthem to the same nodes.\nUSING PREFERENTIAL POD ANTI-AFFINITY\nIn this case, you probably should have specified a soft requirement instead (using the\npreferredDuringSchedulingIgnoredDuringExecution  property).  After  all,  it’s  not\nsuch a big problem if two frontend pods run on the same node. But in scenarios where\nthat’s a problem, using \nrequiredDuringScheduling is appropriate. \nListing 16.18   Pods with anti-affinity: frontend-podantiaffinity-host.yaml\nListing 16.19   Pods created by the Deployment\nThe frontend pods have \nthe app=frontend label.\nDefining hard-\nrequirements for \npod anti-affinity\nA frontend pod must not \nbe scheduled to the same \nmachine as a pod with \napp=frontend label.\n \n\n476CHAPTER 16Advanced scheduling\n As with pod affinity, the topologyKey property determines the scope of where the\npod  shouldn’t  be  deployed  to.  You  can  use  it  to  ensure  pods  aren’t  deployed  to  the\nsame  rack,  availability  zone,  region,  or  any  custom  scope  you  create  using  custom\nnode labels.\n16.4   Summary\nIn this chapter, we looked at how to ensure pods aren’t scheduled to certain nodes or\nare only scheduled to specific nodes, either because of the node’s labels or because of\nthe pods running on them.\n You learned that\nIf you add a taint to a node, pods won’t be scheduled to that node unless they\ntolerate that taint.\nThree types of taints exist: NoSchedule completely prevents scheduling, Prefer-\nNoSchedule\n isn’t as strict, and NoExecute even evicts existing pods from a node.\nThe NoExecute taint is also used to specify how long the Control Plane should\nwait before rescheduling the pod when the node it runs on becomes unreach-\nable or unready.\nNode affinity allows you to specify which nodes a pod should be scheduled to. It\ncan be used to specify a hard requirement or to only express a node preference.\nPod affinity is used to make the Scheduler deploy pods to the same node where\nanother pod is running (based on the pod’s labels). \nPod  affinity’s  topologyKey  specifies  how  close  the  pod  should  be  deployed  to\nthe other pod (onto the same node or onto a node in the same rack, availability\nzone, or availability region).\nPod anti-affinity can be used to keep certain pods away from each other. \nBoth  pod  affinity  and  anti-affinity,  like  node  affinity,  can  either  specify  hard\nrequirements or preferences.\nIn the next chapter, you’ll learn about best practices for developing apps and how to\nmake them run smoothly in a Kubernetes environment.\n \n\n477\nBest practices\nfor developing apps\nWe’ve now covered most of what you need to know to run your apps in Kubernetes.\nWe’ve explored what each individual resource does and how it’s used. Now we’ll see\nhow  to  combine  them  in  a  typical  application  running  on  Kubernetes.  We’ll  also\nlook at how to make an application run smoothly. After all, that’s the whole point\nof using Kubernetes, isn’t it? \n Hopefully, this chapter will help to clear up any misunderstandings and explain\nthings that weren’t explained clearly yet. Along the way, we’ll also introduce a few\nadditional concepts that haven’t been mentioned up to this point.\nThis chapter covers\nUnderstanding which Kubernetes resources \nappear in a typical application\nAdding post-start and pre-stop pod lifecycle hooks\nProperly terminating an app without breaking \nclient requests\nMaking apps easy to manage in Kubernetes\nUsing init containers in a pod\nDeveloping locally with Minikube\n \n\n478CHAPTER 17Best practices for developing apps\n17.1   Bringing everything together\nLet’s start by looking at what an actual application consists of. This will also give you a\nchance to see if you remember everything you’ve  learned  so  far  and  look  at  the  big\npicture. Figure 17.1 shows the Kubernetes components used in a typical application.\nA typical application manifest contains one or more Deployment and/or StatefulSet\nobjects. Those include a pod template containing one or more containers, with a live-\nness  probe  for  each  of  them  and  a  readiness  probe  for  the  service(s)  the  container\nprovides  (if  any).  Pods  that  provide  services  to  others  are  exposed  through  one  or\nmore Services. When they need to be reachable from outside the cluster, the Services\nare  either  configured  to  be  \nLoadBalancer  or  NodePort-type  Services,  or  exposed\nthrough an Ingress resource. \n The pod templates (and the pods created from them) usually reference two types\nof Secrets—those for pulling container images from private image registries and those\nused  directly  by  the  process  running  inside  the  pods.  The  Secrets  themselves  are\nusually not part of the application manifest, because they aren’t configured by the\napplication  developers  but  by  the  operations  team.  Secrets  are  usually  assigned  to\nServiceAccounts, which are assigned to individual pods. \nDefined in the app manifest by the developer\nPod template\nDeployment\nlabels\nPod(s)\nLabel selector\nlabels\nCreated automatically at runtime\nCreated by a cluster admin beforehand\nContainer(s)\nVolume(s)\nReplicaSet(s)\nEndpoints\n• Health probes\n• Environment variables\n• Volume mounts\n• Resource reqs/limits\nHorizontal\nPodAutoscaler\nStatefulSet\nDaemonSet\nJob\nCronJob\nPersistent\nVolume\nConfigMap\nService\nPersistent\nVolume\nClaim\nSecret(s)\nService\naccount\nStorage\nClass\nLimitRange\nResourceQuota\nIngress\nimagePullSecret\nFigure 17.1   Resources in a typical application\n \n\n479Understanding the pod’s lifecycle\n The application also contains one or more ConfigMaps, which are either used to\ninitialize environment variables or mounted as a \nconfigMap  volume  in  the  pod.  Cer-\ntain pods use additional volumes, such as an \nemptyDir or a gitRepo volume, whereas\npods requiring persistent storage use \npersistentVolumeClaim volumes. The Persistent-\nVolumeClaims are also part of the application manifest, whereas StorageClasses refer-\nenced by them are created by system administrators upfront. \n In certain cases, an application also requires the use of Jobs or CronJobs. Daemon-\nSets aren’t normally part of application deployments, but are usually created by sysad-\nmins  to  run  system  services  on  all  or  a  subset  of  nodes.  HorizontalPodAutoscalers\nare either included in the manifest by the developers or added to the system later by\nthe ops team. The cluster administrator also creates LimitRange and ResourceQuota\nobjects  to  keep  compute  resource  usage  of  individual  pods  and  all  the  pods  (as  a\nwhole) under control.\n  After  the  application  is  deployed,  additional  objects  are  created  automatically  by\nthe  various  Kubernetes  controllers.  These  include  service  Endpoints  objects  created\nby  the  Endpoints  controller,  ReplicaSets  created  by  the  Deployment  controller,  and\nthe actual pods created by the ReplicaSet (or Job, CronJob, StatefulSet, or DaemonSet)\ncontrollers.\n Resources are often labeled with one or more labels to keep them organized. This\ndoesn’t apply only to pods but to all other resources as well. In addition to labels, most\nresources also contain annotations that describe each resource, list the contact infor-\nmation  of  the  person  or  team  responsible  for  it,  or  provide  additional  metadata  for\nmanagement and other tools. \n At the center of all this is the Pod, which arguably is the most important Kuberne-\ntes resource. After all, each of your applications runs inside it. To make sure you know\nhow to develop apps that make the most out of their environment, let’s take one last\nclose look at pods—this time from the application’s perspective. \n17.2   Understanding the pod’s lifecycle\nWe’ve  said  that  pods  can  be  compared  to  VMs  dedicated  to  running  only  a  single\napplication. Although an application running inside a pod is not unlike an application\nrunning in a VM, significant differences do exist. One example is that apps running in\na  pod  can  be  killed  any  time,  because  Kubernetes  needs  to  relocate  the  pod  to\nanother  node  for  a  reason  or  because  of  a  scale-down  request.  We’ll  explore  this\naspect next.\n17.2.1   Applications must expect to be killed and relocated\nOutside  Kubernetes,  apps  running  in  VMs  are  seldom  moved  from  one  machine  to\nanother.  When  an  operator  moves  the  app,  they  can  also  reconfigure  the  app  and\nmanually  check  that  the  app  is  running  fine  in  the  new  location.  With  Kubernetes,\napps  are  relocated  much  more  frequently  and  automatically—no  human  operator\n \n\n480CHAPTER 17Best practices for developing apps\nreconfigures them and makes sure they still run properly after the move. This means\napplication  developers  need  to  make  sure  their  apps  allow  being  moved  relatively\noften. \nEXPECTING THE LOCAL IP AND HOSTNAME TO CHANGE\nWhen a pod is killed and run elsewhere (technically, it’s a new pod instance replac-\ning the old one; the pod isn’t relocated), it not only has a new IP address but also a\nnew  name  and  hostname.  Most  stateless  apps  can  usually  handle  this  without  any\nadverse effects, but stateful apps usually can’t. We’ve learned that stateful apps can\nbe run through a StatefulSet, which ensures that when the app starts up on a new\nnode after being rescheduled, it will still see the same host name and persistent state\nas before. The pod’s IP will change nevertheless. Apps need to be prepared for that\nto happen. The application developer therefore should never base membership in a\nclustered app on the member’s IP address, and if basing it on the hostname, should\nalways use a StatefulSet.\nEXPECTING THE DATA WRITTEN TO DISK TO DISAPPEAR\nAnother thing to keep in mind is that if the app writes data to disk, that data may not be\navailable after the app is started inside a new pod, unless you mount persistent storage at\nthe  location  the  app  is  writing  to.  It  should  be  clear  this  happens  when  the  pod  is\nrescheduled, but files written to disk will disappear even in scenarios that don’t involve\nany  rescheduling.  Even  during  the  lifetime  of  a  single  pod,  the  files  written  to  disk  by\nthe app running in the pod may disappear. Let me explain this with an example.\n Imagine an app that has a long and computationally intensive initial startup proce-\ndure.  To  help  the  app  come  up  faster  on  subsequent  startups,  the  developers  make\nthe app cache the results of the initial startup on disk (an example of this would be\nthe scanning of all Java classes for annotations at startup and then writing the results\nto an index file). Because apps in Kubernetes run in containers by default, these files\nare written to the container’s filesystem. If the container is then restarted, they’re all\nlost, because the new container starts off with a completely new writable layer (see fig-\nure 17.2).\n Don’t forget that individual containers may be restarted for several reasons, such\nas  because  the  process  crashes,  because  the  liveness  probe  returned  a  failure,  or\nbecause  the  node  started  running  out  of  memory  and  the  process  was  killed  by  the\nOOMKiller.  When  this  happens,  the  pod  is  still  the  same,  but  the  container  itself  is\ncompletely new. The Kubelet doesn’t run the same container again; it always creates a\nnew container. \nUSING VOLUMES TO PRESERVE DATA ACROSS CONTAINER RESTARTS\nWhen  its  container  is  restarted,  the  app  in  the  example  will  need  to  perform  the\nintensive startup procedure again. This may or may not be desired. To make sure data\nlike this isn’t lost, you need to use at least a pod-scoped volume. Because volumes live\nand die together with the pod, the new container will be able to reuse the data written\nto the volume by the previous container (figure 17.3).\n \n\n481Understanding the pod’s lifecycle\nContainer\nProcess\nWrites to\nFilesystem\nWritable layer\nRead-only layer\nRead-only layer\nImage layers\nContainer crashes\nor is killed\nPod\nNew container\nNew process\nFilesystem\nNew writable layer\nRead-only layer\nRead-only layer\nImage layers\nNew container started\n(part of the same pod)\nNew container\nstarts with new\nwriteable layer:\nall files are lost\nFigure 17.2   Files written to the container’s filesystem are lost when the container is restarted.\nContainer\nProcess\nWrites to\nCan read\nthe same files\nFilesystem\nvolumeMount\nContainer crashes\nor is killed\nPod\nNew container\nNew process\nFilesystem\nvolumeMount\nNew container started\n(part of the same pod)\nNew process can\nuse data preserved\nin the volume\nVolume\nFigure 17.3   Using a volume to persist data across container restarts\n \n\n482CHAPTER 17Best practices for developing apps\nUsing  a  volume  to  preserve  files  across  container  restarts  is  a  great  idea  sometimes,\nbut not always. What if the data gets corrupted and causes the newly created process\nto  crash  again?  This  will  result  in  a  continuous crash loop (the pod will show the\nCrashLoopBackOff status). If you hadn’t used a volume, the new container would start\nfrom  scratch  and  most  likely  not  crash.  Using  volumes  to  preserve  files  across  con-\ntainer  restarts  like  this  is  a  double-edged  sword.  You  need  to  think  carefully  about\nwhether to use them or not.\n17.2.2   Rescheduling of dead or partially dead pods\nIf  a  pod’s  container  keeps  crashing,  the  Kubelet  will  keep  restarting  it  indefinitely.\nThe time between restarts will be increased exponentially until it reaches five minutes.\nDuring those five minute intervals, the pod is essentially dead, because its container’s\nprocess isn’t running. To be fair, if it’s a multi-container pod, certain containers may\nbe running normally, so the pod is only partially dead. But if a pod contains only a sin-\ngle container, the pod is effectively dead and completely useless, because no process is\nrunning in it anymore.\n  You  may  find  it  surprising  to  learn  that  such  pods  aren’t  automatically  removed\nand rescheduled, even if they’re part of a ReplicaSet or similar controller. If you cre-\nate a ReplicaSet with a desired replica count of three, and then one of the containers\nin one of those pods starts crashing, Kubernetes will not delete and replace the pod.\nThe end result is a ReplicaSet with only two properly running replicas instead of the\ndesired three (figure 17.4).\nYou’d probably expect the pod to be deleted and replaced with another pod instance\nthat might run successfully on another node. After all, the container may be crashing\nbecause of a node-related problem that doesn’t manifest itself on other nodes. Sadly,\nthat isn’t the case. The ReplicaSet controller doesn’t care if the pods are dead—all it\nReplicaSet\nDesired replicas: 3\nActual replicas: 3\nOnly two pods are actually\nperforming their jobs\nThird pod’s status is Running,\nbut its container keeps crashing,\nwith significant delays between\nrestarts (CrashLoopBackOff)\nWe want\nthree pods\nPod\nRunning\ncontainer\nPod\nRunning\ncontainer\nPod\nDead\ncontainer\nFigure 17.4   A ReplicaSet controller doesn’t reschedule dead pods.\n \n\n483Understanding the pod’s lifecycle\ncares  about  is  that  the  number  of  pods  matches  the  desired  replica  count,  which  in\nthis case, it does.\n  If  you’d  like  to  see  for  yourself,  I’ve  included  a  YAML  manifest  for  a  ReplicaSet\nwhose  pods  will  keep  crashing  (see  file  replicaset-crashingpods.yaml  in  the  code\narchive). If you create the ReplicaSet and inspect the pods that are created, the follow-\ning listing is what you’ll see.\n$ kubectl get po\nNAME                  READY     STATUS             RESTARTS   AGE\ncrashing-pods-f1tcd   0/1       CrashLoopBackOff   5          6m     \ncrashing-pods-k7l6k   0/1       CrashLoopBackOff   5          6m\ncrashing-pods-z7l3v   0/1       CrashLoopBackOff   5          6m\n$ kubectl describe rs crashing-pods\nName:           crashing-pods\nReplicas:       3 current / 3 desired                       \nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed      \n$ kubectl describe po crashing-pods-f1tcd\nName:           crashing-pods-f1tcd\nNamespace:      default\nNode:           minikube/192.168.99.102\nStart Time:     Thu, 02 Mar 2017 14:02:23 +0100\nLabels:         app=crashing-pods\nStatus:         Running                      \nIn a way, it’s understandable that Kubernetes behaves this way. The container will be\nrestarted every five minutes in the hope that the underlying cause of the crash will be\nresolved.  The  rationale  is  that  rescheduling  the  pod  to  another  node  most  likely\nwouldn’t fix the problem anyway, because the app is running inside a container and\nall the nodes should be mostly equivalent. That’s not always the case, but it is most of\nthe time. \n17.2.3   Starting pods in a specific order\nOne other difference between apps running in pods and those managed manually is\nthat  the  ops  person  deploying  those  apps  knows  about  the  dependencies  between\nthem. This allows them to start the apps in order. \nUNDERSTANDING HOW PODS ARE STARTED\nWhen you use Kubernetes to run your multi-pod applications, you don’t have a built-\nin way to tell Kubernetes to run certain pods first and the rest only when the first pods\nare already up and ready to serve. Sure, you could post the manifest for the first app\nand then wait for the pod(s) to be ready before you post the second manifest, but your\nListing 17.1   ReplicaSet and pods that keep crashing\nThe pod’s status shows the Kubelet is\ndelaying the restart because the\ncontainer keeps crashing.\nNo action taken \nby the controller, \nbecause current \nreplicas match \ndesired replicas\nThree \nreplicas are \nshown as \nrunning.\nkubectl describe \nalso shows pod’s \nstatus as running\n \n\n484CHAPTER 17Best practices for developing apps\nwhole system is usually defined in a single YAML or JSON containing multiple Pods,\nServices, and other objects. \n  The  Kubernetes  API  server  does  process  the  objects  in  the  YAML/JSON  in  the\norder they’re listed, but this only means they’re written to etcd in that order. You have\nno guarantee that pods will also be started in that order. \n But you can prevent a pod’s main container from starting until a precondition is\nmet. This is done by including an init containers in the pod. \nINTRODUCING INIT CONTAINERS\nIn addition to regular containers, pods can also include init containers. As the name\nsuggests, they can be used to initialize the pod—this often means writing data to the\npod’s volumes, which are then mounted into the pod’s main container(s).\n A pod may have any number of init containers. They’re executed sequentially and\nonly  after  the  last  one  completes  are  the  pod’s  main  containers  started.  This  means\ninit containers can also be used to delay the start of the pod’s main container(s)—for\nexample, until a certain precondition is met. An init container could wait for a service\nrequired by the pod’s main container to be up and ready. When it is, the init container\nterminates  and  allows  the  main  container(s) to be started. This way, the main con-\ntainer wouldn’t use the service before it’s ready.\n Let’s look at an example of a pod using an init container to delay the start of the\nmain  container.  Remember  the  \nfortune  pod  you  created  in  chapter  7?  It’s  a  web\nserver that returns a fortune quote as a response to client requests. Now, let’s imagine\nyou have a \nfortune-client pod that requires the fortune Service to be up and run-\nning  before  its  main  container  starts.  You  can  add  an  init  container,  which  checks\nwhether the Service is responding to requests. Until that’s the case, the init container\nkeeps retrying. Once it gets a response, the init container terminates and lets the main\ncontainer start.\nADDING AN INIT CONTAINER TO A POD\nInit containers can be defined in the pod spec like main containers but through the\nspec.initContainers field. You’ll find the complete YAML for the fortune-client pod\nin  the  book’s  code  archive.  The  following  listing  shows  the  part  where  the  init  con-\ntainer is defined.\nspec:\n  initContainers:      \n  - name: init\n    image: busybox\n    command:\n    - sh\n    - -c\n    - 'while true; do echo \"Waiting for fortune service to come up...\";  \n    \n➥ wget http://fortune -q -T 1 -O /dev/null >/dev/null 2>/dev/null   \n    \n➥ && break; sleep 1; done; echo \"Service is up! Starting main       \n    \n➥ container.\"'\nListing 17.2   An init container defined in a pod: fortune-client.yaml\nYou’re defining \nan init container, \nnot a regular \ncontainer.\nThe init container runs a\nloop that runs until the\nfortune Service is up.\n \n\n485Understanding the pod’s lifecycle\nWhen you deploy this pod, only its init container is started. This is shown in the pod’s\nstatus when you list pods with \nkubectl get:\n$ kubectl get po\nNAME             READY     STATUS     RESTARTS   AGE\nfortune-client   0/1       Init:0/1   0          1m\nThe STATUS column shows that zero of one init containers have finished. You can see\nthe log of the init container with \nkubectl logs:\n$ kubectl logs fortune-client -c init\nWaiting for fortune service to come up...\nWhen running the kubectl logs command, you need to specify the name of the init\ncontainer with the \n-c switch (in the example, the name of the pod’s init container is\ninit, as you can see in listing 17.2).\n  The  main  container  won’t  run  until  you  deploy  the  \nfortune  Service  and  the\nfortune-server pod. You’ll find them in the fortune-server.yaml file. \nBEST PRACTICES FOR HANDLING INTER-POD DEPENDENCIES\nYou’ve seen how an init container can be used to delay starting the pod’s main con-\ntainer(s) until a precondition is met (making sure the Service the pod depends on is\nready, for example), but it’s much better to write apps that don’t require every service\nthey  rely  on  to  be  ready  before  the  app  starts  up.  After  all,  the  service  may  also  go\noffline later, while the app is already running.\n  The  application  needs  to  handle  internally  the  possibility  that  its  dependencies\naren’t ready. And don’t forget readiness probes. If an app can’t do its job because one\nof  its  dependencies  is  missing,  it  should  signal  that  through  its  readiness  probe,  so\nKubernetes  knows  it,  too,  isn’t  ready.  You’ll  want  to  do  this  not  only  because  it  pre-\nvents the app from being added as a service endpoint, but also because the app’s read-\niness  is  also  used  by  the  Deployment  controller  when  performing  a  rolling  update,\nthereby preventing a rollout of a bad version. \n17.2.4   Adding lifecycle hooks\nWe’ve  talked  about  how  init  containers  can  be used to hook into the startup of the\npod, but pods also allow you to define two lifecycle hooks:\nPost-start hooks\nPre-stop hooks\nThese lifecycle hooks are specified per container, unlike init containers, which apply\nto the whole pod. As their names suggest, they’re executed when the container starts\nand before it stops. \n Lifecycle hooks are similar to liveness and readiness probes in that they can either\nExecute a command inside the container\nPerform an HTTP GET request against a URL\n \n\n486CHAPTER 17Best practices for developing apps\nLet’s look at the two hooks individually to see what effect they have on the container\nlifecycle.\nUSING A POST-START CONTAINER LIFECYCLE HOOK\nA post-start hook is executed immediately after the container’s main process is started.\nYou use it to perform additional operations when the application starts. Sure, if you’re\nthe author of the application running in the container, you can always perform those\noperations inside the application code itself. But when you’re running an application\ndeveloped  by  someone  else,  you  mostly  don’t  want  to  (or  can’t)  modify  its  source\ncode. Post-start hooks allow you to run additional commands without having to touch\nthe app. These may signal to an external listener that the app is starting, or they may\ninitialize the application so it can start doing its job.\n The hook is run in parallel with the main process. The name might be somewhat\nmisleading, because it doesn’t wait for the main process to start up fully (if the process\nhas an initialization procedure, the Kubelet obviously can’t wait for the procedure to\ncomplete, because it has no way of knowing when that is). \n But even though the hook runs asynchronously, it does affect the container in two\nways. Until the hook completes, the container will stay in the \nWaiting state with the\nreason \nContainerCreating. Because of this, the pod’s status will be Pending instead of\nRunning. If the hook fails to run or returns a non-zero exit code, the main container\nwill be killed. \n A pod manifest containing a post-start hook looks like the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-poststart-hook\nspec:\n  containers:\n  - image: luksa/kubia\n    name: kubia\n    lifecycle:          \n      postStart:        \n        exec:                                                               \n          command:                                                          \n          - sh                                                              \n          - -c                                                              \n          - \"echo 'hook will fail with exit code 15'; sleep 5; exit 15\"     \nIn  the  example,  the  echo, sleep,  and  exit  commands  are  executed  along  with  the\ncontainer’s main process as soon as the container is created. Rather than run a com-\nmand like this, you’d typically run a shell script or a binary executable file stored in\nthe container image. \n Sadly, if the process started by the hook logs to the standard output, you can’t see\nthe output anywhere. This makes debugging lifecycle hooks painful. If the hook fails,\nListing 17.3   A pod with a post-start lifecycle hook: post-start-hook.yaml\nThe hook is executed as \nthe container starts.\nIt executes the\npostStart.sh\nscript in the /bin\ndirectory inside\nthe container.\n \n\n487Understanding the pod’s lifecycle\nyou’ll only see a FailedPostStartHook warning among the pod’s events (you can see\nthem using \nkubectl describe pod). A while later, you’ll see more information on why\nthe hook failed, as shown in the following listing.\nFailedSync   Error syncing pod, skipping: failed to \"StartContainer\" for \n             \"kubia\" with PostStart handler: command 'sh -c echo 'hook \n             will fail with exit code 15'; sleep 5 ; exit 15' exited \n             with 15: : \"PostStart Hook Failed\" \nThe number 15 in the last line is the exit code of the command. When using an HTTP\nGET hook handler, the reason may look like the following listing (you can try this by\ndeploying the post-start-hook-httpget.yaml file from the book’s code archive).\nFailedSync   Error syncing pod, skipping: failed to \"StartContainer\" for \n             \"kubia\" with PostStart handler: Get \n             http://10.32.0.2:9090/postStart: dial tcp 10.32.0.2:9090: \n             getsockopt: connection refused: \"PostStart Hook Failed\" \nNOTEThe  post-start  hook  is  intentionally  misconfigured  to  use  port  9090\ninstead of the correct port 8080, to show what happens when the hook fails.\nThe standard and error outputs of command-based post-start hooks aren’t logged any-\nwhere, so you may want to have the process the hook invokes log to a file in the con-\ntainer’s  filesystem,  which  will  allow  you  to  examine  the  contents  of  the  file  with\nsomething like this:\n$ kubectl exec my-pod cat logfile.txt \nIf the container gets restarted for whatever reason (including because the hook failed),\nthe file may be gone before you can examine it. You can work around that by mount-\ning an \nemptyDir volume into the container and having the hook write to it.\nUSING A PRE-STOP CONTAINER LIFECYCLE HOOK\nA  pre-stop  hook  is  executed  immediately  before  a  container  is  terminated.  When  a\ncontainer  needs  to  be  terminated,  the  Kubelet  will  run  the  pre-stop  hook,  if  config-\nured,  and  only  then  send  a  \nSIGTERM  to  the  process  (and  later  kill  the  process  if  it\ndoesn’t terminate gracefully). \n A pre-stop hook can be used to initiate a graceful shutdown of the container, if it\ndoesn’t shut down gracefully upon receipt of a \nSIGTERM signal. They can also be used\nto perform arbitrary operations before shutdown without having to implement those\noperations  in  the  application  itself  (this  is  useful  when  you’re  running  a  third-party\napp, whose source code you don’t have access to and/or can’t modify). \n Configuring a pre-stop hook in a pod manifest isn’t very different from adding a\npost-start hook. The previous example showed a post-start hook that executes a com-\nListing 17.4   Pod’s events showing the exit code of the failed command-based hook\nListing 17.5   Pod’s events showing the reason why an HTTP GET hook failed\n \n\n488CHAPTER 17Best practices for developing apps\nmand, so we’ll look at a pre-stop hook that performs an HTTP GET request now. The\nfollowing listing shows how to define a pre-stop HTTP GET hook in a pod.\n    lifecycle:\n      preStop:            \n        httpGet:          \n          port: 8080          \n          path: shutdown      \nThe pre-stop hook defined in this listing performs an HTTP GET request to http://\nPOD_IP:8080/shutdown  as  soon  as  the  Kubelet  starts  terminating  the  container.\nApart from the \nport and path shown in the listing, you can also set the fields scheme\n(HTTP  or  HTTPS)  and  host,  as  well  as  httpHeaders  that  should  be  sent  in  the\nrequest.  The  \nhost  field  defaults  to  the  pod  IP.  Be  sure  not  to  set  it  to  localhost,\nbecause localhost would refer to the node, not the pod.\n  In  contrast  to  the  post-start  hook,  the  container  will  be  terminated  regardless  of\nthe result of the hook—an error HTTP response code or a non-zero exit code when\nusing a command-based hook will not prevent the container from being terminated.\nIf  the  pre-stop  hook  fails,  you’ll  see  a  \nFailedPreStopHook  warning  event  among  the\npod’s events, but because the pod is deleted soon afterward (after all, the pod’s dele-\ntion is what triggered the pre-stop hook in the first place), you may not even notice\nthat the pre-stop hook failed to run properly. \nTIPIf the successful completion of the pre-stop hook is critical to the proper\noperation  of  your  system,  verify  whether  it’s being executed at all. I’ve wit-\nnessed  situations  where  the  pre-stop  hook  didn’t  run  and  the  developer\nwasn’t even aware of that.\nUSING A PRE-STOP HOOK BECAUSE YOUR APP DOESN’T RECEIVE THE SIGTERM SIGNAL\nMany developers make the mistake of defining a pre-stop hook solely to send a SIGTERM\nsignal to their apps in the pre-stop hook. They do this because they don’t see their appli-\ncation  receive  the  \nSIGTERM  signal  sent  by  the  Kubelet.  The  reason  why  the  signal  isn’t\nreceived by the application isn’t because Kubernetes isn’t sending it, but because the sig-\nnal  isn’t  being  passed  to  the  app  process  inside  the  container  itself.  If  your  container\nimage is configured to run a shell, which in turn runs the app process, the signal may be\neaten up by the shell itself, instead of being passed down to the child process.\n In such cases, instead of adding a pre-stop hook to send the signal directly to your\napp, the proper fix is to make sure the shell passes the signal to the app. This can be\nachieved by handling the signal in the shell script running as the main container pro-\ncess and then passing it on to the app. Or you could not configure the container image\nto run a shell at all and instead run the application binary directly. You do this by using\nthe  exec  form  of  \nENTRYPOINT  or  CMD  in  the  Dockerfile:  ENTRYPOINT [\"/mybinary\"]\ninstead of ENTRYPOINT /mybinary.\nListing 17.6   A pre-stop hook YAML snippet: pre-stop-hook-httpget.yaml\nThis is a pre-stop hook that \nperforms an HTTP GET request.\nThe request is sent to \nhttp://POD_IP:8080/shutdown.\n \n\n489Understanding the pod’s lifecycle\n A container using the first form runs the mybinary executable as its main process,\nwhereas the second form runs a shell as the main process with the \nmybinary process\nexecuted as a child of the shell process.\nUNDERSTANDING THAT LIFECYCLE HOOKS TARGET CONTAINERS, NOT PODS\nAs a final thought on post-start and pre-stop hooks, let me emphasize that these lifecy-\ncle  hooks  relate  to  containers,  not  pods.  You  shouldn’t  use  a  pre-stop  hook  for  run-\nning  actions  that  need  to  be  performed  when  the  pod  is  terminating.  The  reason  is\nthat the pre-stop hook gets called when the container is being terminated (most likely\nbecause of a failed liveness probe). This may happen multiple times in the pod’s life-\ntime, not only when the pod is in the process of being shut down. \n17.2.5   Understanding pod shutdown\nWe’ve touched on the subject of pod termination, so let’s explore this subject in more\ndetail and go over exactly what happens during pod shutdown. This is important for\nunderstanding how to cleanly shut down an application running in a pod.\n Let’s start at the beginning. A pod’s shut-down is triggered by the deletion of the\nPod  object  through  the  API  server.  Upon  receiving  an  HTTP  DELETE  request,  the\nAPI server doesn’t delete the object yet, but only sets a \ndeletionTimestamp field in it.\nPods that have the \ndeletionTimestamp field set are terminating. \n  Once  the  Kubelet  notices  the  pod  needs  to  be  terminated,  it  starts  terminating\neach of the pod’s containers. It gives each container time to shut down gracefully, but\nthe  time  is  limited.  That  time  is  called  the  termination  grace  period  and  is  configu-\nrable per pod. The timer starts as soon as the termination process starts. Then the fol-\nlowing sequence of events is performed:\n1Run the pre-stop hook, if one is configured, and wait for it to finish.\n2Send the SIGTERM signal to the main process of the container.\n3Wait  until  the  container  shuts  down  cleanly  or  until  the  termination  grace\nperiod runs out.\n4Forcibly kill the process with SIGKILL, if it hasn’t terminated gracefully yet.\nThe sequence of events is illustrated in figure 17.5.\nPre-stop hook process\nTermination grace period\nMain container process\nContainer shutdown\ninitiated\nContainer killed\nif still running\nTime\nSIGTERM\nSIGKILL\nFigure 17.5   The container termination sequence\n \n\n490CHAPTER 17Best practices for developing apps\nSPECIFYING THE TERMINATION GRACE PERIOD\nThe termination grace period can be configured in the pod spec by setting the spec.\nterminationGracePeriodSeconds\n field. It defaults to 30, which means the pod’s con-\ntainers will be given 30 seconds to terminate gracefully before they’re killed forcibly. \nTIPYou should set the grace period to long enough so your process can fin-\nish cleaning up in that time. \nThe grace period specified in the pod spec can also be overridden when deleting the\npod like this:\n$ kubectl delete po mypod --grace-period=5\nThis will make the Kubelet wait five seconds for the pod to shut down cleanly. When\nall the pod’s containers stop, the Kubelet notifies the API server and the Pod resource\nis  finally  deleted.  You  can  force  the  API  server  to  delete  the  resource  immediately,\nwithout waiting for confirmation, by setting the grace period to zero and adding the\n--force option like this:\n$ kubectl delete po mypod --grace-period=0 --force\nBe careful when using this option, especially with pods of a StatefulSet. The Stateful-\nSet controller takes great care to never run two instances of the same pod at the same\ntime  (two  pods  with  the  same  ordinal  index  and  name  and  attached  to  the  same\nPersistentVolume).  By  force-deleting  a  pod,  you’ll  cause  the  controller  to  create  a\nreplacement  pod  without  waiting  for  the  containers  of  the  deleted  pod  to  shut\ndown. In other words, two instances of the same pod might be running at the same\ntime, which may cause your stateful cluster to malfunction. Only delete stateful pods\nforcibly when you’re absolutely sure the pod isn’t running anymore or can’t talk to\nthe other members of the cluster (you can be sure of this when you confirm that the\nnode that hosted the pod has failed or has been disconnected from the network and\ncan’t reconnect). \n Now that you understand how containers are shut down, let’s look at it from the\napplication’s  perspective  and  go  over  how  applications  should  handle  the  shutdown\nprocedure.\nIMPLEMENTING THE PROPER SHUTDOWN HANDLER IN YOUR APPLICATION\nApplications  should  react  to  a  SIGTERM  signal  by  starting  their  shut-down  procedure\nand terminating when it finishes. Instead of handling the \nSIGTERM signal, the applica-\ntion can be notified to shut down through a pre-stop hook. In both cases, the app\nthen only has a fixed amount of time to terminate cleanly. \n But what if you can’t predict how long the app will take to shut down cleanly? For\nexample, imagine your app is a distributed data store. On scale-down, one of the pod\ninstances  will  be  deleted  and  therefore  shut  down.  In  the  shut-down  procedure,  the\n \n\n491Understanding the pod’s lifecycle\npod  needs  to  migrate  all  its  data  to  the  remaining  pods  to  make  sure  it’s  not  lost.\nShould the pod start migrating the data upon receiving a termination signal (through\neither the \nSIGTERM signal or through a pre-stop hook)? \n Absolutely not! This is not recommended for at least the following two reasons:\nA  container  terminating  doesn’t  necessarily  mean  the  whole  pod  is  being\nterminated.\nYou have no guarantee the shut-down procedure will finish before the process\nis killed.\nThis second scenario doesn’t happen only when the grace period runs out before the\napplication has finished shutting down gracefully, but also when the node running\nthe  pod  fails  in  the  middle  of  the  container  shut-down  sequence.  Even  if  the  node\nthen  starts  up  again,  the  Kubelet  will  not  restart  the  shut-down  procedure  (it  won’t\neven start up the container again). There are absolutely no guarantees that the pod\nwill be allowed to complete its whole shut-down procedure.\nREPLACING CRITICAL SHUT-DOWN PROCEDURES WITH DEDICATED SHUT-DOWN PROCEDURE PODS\nHow  do  you  ensure  that  a  critical  shut-down  procedure  that  absolutely  must  run  to\ncompletion  does  run  to  completion  (for  example,  to  ensure  that  a  pod’s  data  is\nmigrated to other pods)?\n One solution is for the app (upon receipt of a termination signal) to create a new\nJob resource that would run a new pod, whose sole job is to migrate the deleted pod’s\ndata to the remaining pods. But if you’ve been paying attention, you’ll know that you\nhave  no  guarantee  the  app  will  indeed  manage  to  create  the  Job  object  every  single\ntime. What if the node fails exactly when the app tries to do that? \n The proper way to handle this problem is by having a dedicated, constantly run-\nning pod that keeps checking for the existence of orphaned data. When this pod finds\nthe orphaned data, it can migrate it to the remaining pods. Rather than a constantly\nrunning pod, you can also use a CronJob resource and run the pod periodically. \n You may think StatefulSets could help here, but they don’t. As you’ll remember,\nscaling down a StatefulSet leaves PersistentVolumeClaims orphaned, leaving the data\nstored on the PersistentVolume stranded. Yes, upon a subsequent scale-up, the Persistent-\nVolume  will  be  reattached  to  the  new  pod  instance,  but  what  if  that  scale-up  never\nhappens  (or  happens  after  a  long  time)?  For  this  reason,  you  may  want  to  run  a\ndata-migrating pod also when using StatefulSets (this scenario is shown in figure 17.6).\nTo  prevent  the  migration  from  occurring  during  an  application  upgrade,  the  data-\nmigrating  pod  could  be  configured  to  wait  a  while  to  give  the  stateful  pod  time  to\ncome up again before performing the migration.\n \n \n \n\n492CHAPTER 17Best practices for developing apps\n17.3   Ensuring all client requests are handled properly\nYou now have a good sense of how to make pods shut down cleanly. Now, we’ll look at\nthe pod’s lifecycle from the perspective of the pod’s clients (clients consuming the ser-\nvice the pod is providing). This is important to understand if you don’t want clients to\nrun into problems when you scale pods up or down.\n It goes without saying that you want all client requests to be handled properly. You\nobviously don’t want to see broken connections when pods are starting up or shutting\ndown. By itself, Kubernetes doesn’t prevent this from happening. Your app needs to\nfollow a few rules to prevent broken connections. First, let’s focus on making sure all\nconnections are handled properly when the pod starts up.\n17.3.1   Preventing broken client connections when a pod is starting up\nEnsuring each connection is handled properly at pod startup is simple if you under-\nstand how Services and service Endpoints work. When a pod is started, it’s added as an\nendpoint to all the Services, whose label selector matches the pod’s labels. As you may\nremember from chapter 5, the pod also needs to signal to Kubernetes that it’s ready.\nUntil it is, it won’t become a service endpoint and therefore won’t receive any requests\nfrom clients. \n If you don’t specify a readiness probe in your pod spec, the pod is always considered\nready. It will start receiving requests almost immediately—as soon as the first kube-proxy\nupdates the \niptables rules on its node and the first client pod tries to connect to the\nservice. If your app isn’t ready to accept connections by then, clients will see “connec-\ntion refused” types of errors.\n  All  you  need  to  do  is  make  sure  that  your  readiness  probe  returns  success  only\nwhen your app is ready to properly handle incoming requests. A good first step is to\nadd an HTTP GET readiness probe and point it to the base URL of your app. In many\nPod\nA-0\nPod\nA-1\nStatefulSet A\nReplicas: 2\nScale\ndown\nPVC\nA-0\nPV\nPVC\nA-1\nPV\nPod\nA-0\nStatefulSet A\nReplicas: 1\nTransfers data to\nremaining pod(s)\nConnects to\norphaned PVC\nData-migrating\nPod\nJob\nPVC\nA-0\nPV\nPVC\nA-1\nPV\nFigure 17.6   Using a dedicated pod to migrate data \n \n\n493Ensuring all client requests are handled properly\ncases that gets you far enough and saves you from having to implement a special read-\niness endpoint in your app. \n17.3.2   Preventing broken connections during pod shut-down\nNow let’s see what happens at the other end of a pod’s life—when the pod is deleted and\nits  containers  are  terminated.  We’ve  already  talked  about  how  the  pod’s  containers\nshould start shutting down cleanly as soon they receive the \nSIGTERM signal (or when its\npre-stop hook is executed). But does that ensure all client requests are handled properly? \n How should the app behave when it receives a termination signal? Should it con-\ntinue  to  accept  requests?  What  about  requests  that  have  already  been  received  but\nhaven’t  completed  yet?  What  about  persistent  HTTP  connections,  which  may  be  in\nbetween  requests,  but  are  open  (when  no  active  request  exists  on  the  connection)?\nBefore we can answer those questions, we need to take a detailed look at the chain of\nevents that unfolds across the cluster when a Pod is deleted. \nUNDERSTANDING THE SEQUENCE OF EVENTS OCCURRING AT POD DELETION\nIn chapter 11 we took an in-depth look at what components make up a Kubernetes clus-\nter. You need to always keep in mind that those components run as separate processes on\nmultiple machines. They aren’t all part of a single big monolithic process. It takes time\nfor all the components to be on the same page regarding the state of the cluster. Let’s\nexplore this fact by looking at what happens across the cluster when a Pod is deleted.\n When a request for a pod deletion is received by the API server, it first modifies the\nstate in etcd and then notifies its watchers of the deletion. Among those watchers are\nthe Kubelet and the Endpoints controller. The two sequences of events, which happen\nin parallel (marked with either A or B), are shown in figure 17.7.\nA2. Stop\ncontainers\nAPI serverkube-proxy\nKubelet\nWorker node\nEndpoints\ncontroller\nkube-proxy\nPod\n(containers)\nClient\nDelete\npod\nB1. Pod deletion\nnotification\nB2. Remove pod\nas endpoint\nA1. Pod deletion\nnotification\nB3. Endpoint\nmodification\nnotification\nB4. Remove pod\nfromiptables\nB4. Remove pod\nfromiptables\niptables\niptables\nWorker node\nFigure 17.7   Sequence of events that occurs when a Pod is deleted\n \n\n494CHAPTER 17Best practices for developing apps\nIn the A sequence of events, you’ll see that as soon as the Kubelet receives the notifica-\ntion that the pod should be terminated, it initiates the shutdown sequence as explained\nin section 17.2.5 (run the pre-stop hook, send \nSIGTERM, wait for a period of time, and\nthen  forcibly  kill  the  container  if  it  hasn’t  yet  terminated  on  its  own).  If  the  app\nresponds to the \nSIGTERM by immediately ceasing to receive client requests, any client\ntrying to connect to it will receive a Connection Refused error. The time it takes for\nthis to happen from the time the pod is deleted is relatively short because of the direct\npath from the API server to the Kubelet.\n Now, let’s look at what happens in the other sequence of events—the one leading\nup  to  the  pod  being  removed  from  the  \niptables  rules  (sequence  B  in  the  figure).\nWhen the Endpoints controller (which runs in the Controller Manager in the Kuber-\nnetes  Control  Plane)  receives  the  notification  of  the  Pod  being  deleted,  it  removes\nthe pod as an endpoint in all services that the pod is a part of. It does this by modify-\ning the Endpoints API object by sending a REST request to the API server. The API\nserver then notifies all clients watching the Endpoints object. Among those watchers\nare  all  the  kube-proxies  running  on  the  worker  nodes.  Each  of  these  proxies  then\nupdates  the  \niptables  rules  on  its  node,  which  is  what  prevents  new  connections\nfrom  being  forwarded  to  the  terminating  pod.  An  important  detail  here  is  that\nremoving the \niptables rules has no effect on existing connections—clients who are\nalready  connected  to  the  pod  will  still  send  additional  requests  to  the  pod  through\nthose existing connections.\n Both of these sequences of events happen in parallel. Most likely, the time it takes\nto shut down the app’s process in the pod is slightly shorter than the time required for\nthe \niptables  rules  to  be  updated.  The  chain  of  events  that  leads  to  iptables  rules\nbeing  updated  is  considerably  longer  (see  figure  17.8),  because  the  event  must  first\nreach the Endpoints controller, which then sends a new request to the API server, and\nA2. Send\nSIGTERM\nAPI server\nAPI server\nKubelet\nEndpoints\ncontroller\nContainer(s)\nA1. Watch\nnotification\n(pod modified)\nB1. Watch\nnotification\n(pod modified)\nB2. Remove pod’s IP\nfrom endpoints\nkube-proxy\nB4. Update\niptables\nrules\niptables\nkube-proxy\niptables\nTime\nB3. Watch notification\n(endpoints changed)\nFigure 17.8   Timeline of events when pod is deleted\n \n\n495Ensuring all client requests are handled properly\nthen the API server must notify the kube-proxy before the proxy finally modifies the\niptables  rules.  A  high  probability  exists  that  the  SIGTERM  signal  will  be  sent  well\nbefore the \niptables rules are updated on all nodes.\n The end result is that the pod may still receive client requests after it was sent the\ntermination signal. If the app closes the server socket and stops accepting connections\nimmediately,  this  will  cause  clients  to  receive  “Connection  Refused”  types  of  errors\n(similar to what happens at pod startup if your app isn’t capable of accepting connec-\ntions immediately and you don’t define a readiness probe for it). \nSOLVING THE PROBLEM\nGoogling solutions to this problem makes it seem as though adding a readiness probe\nto your pod will solve the problem. Supposedly, all you need to do is make the readi-\nness  probe  start  failing  as  soon  as  the  pod  receives  the  \nSIGTERM.  This  is  supposed  to\ncause  the  pod  to  be  removed  as  the  endpoint  of  the  service.  But  the  removal  would\nhappen only after the readiness probe fails for a few consecutive times (this is configu-\nrable  in  the  readiness  probe  spec).  And,  obviously,  the  removal  then  still  needs  to\nreach the kube-proxy before the pod is removed from \niptables rules. \n In reality, the readiness probe has absolutely no bearing on the whole process at\nall. The Endpoints controller removes the pod from the service Endpoints as soon as\nit receives notice of the pod being deleted (when the \ndeletionTimestamp field in the\npod’s spec is no longer \nnull). From that point on, the result of the readiness probe\nis irrelevant.\n What’s the proper solution to the problem? How can you make sure all requests\nare handled fully?\n It’s clear the pod needs to keep accepting connections even after it receives the ter-\nmination  signal  up  until  all  the  kube-proxies  have  finished  updating  the  \niptables\nrules.  Well,  it’s  not  only  the  kube-proxies.  There  may  also  be  Ingress  controllers  or\nload balancers forwarding connections to the pod directly, without going through the\nService  (\niptables).  This  also  includes  clients  using  client-side  load-balancing.  To\nensure none of the clients experience broken connections, you’d have to wait until all\nof them somehow notify you they’ll no longer forward connections to the pod. \n That’s impossible, because all those components are distributed across many dif-\nferent computers. Even if you knew the location of every one of them and could wait\nuntil all of them say it’s okay to shut down the pod, what do you do if one of them\ndoesn’t  respond?  How  long  do  you  wait  for  the  response?  Remember,  during  that\ntime, you’re holding up the shut-down process. \n The only reasonable thing you can do is wait for a long-enough time to ensure all\nthe proxies have done their job. But how long is long enough? A few seconds should\nbe enough in most situations, but there’s no guarantee it will suffice every time. When\nthe  API  server  or  the  Endpoints  controller  is  overloaded,  it  may  take  longer  for  the\nnotification to reach the kube-proxy. It’s important to understand that you can’t solve\nthe problem perfectly, but even adding a 5- or 10-second delay should improve the\nuser  experience  considerably.  You  can  use  a  longer  delay,  but  don’t  go  overboard,\n \n\n496CHAPTER 17Best practices for developing apps\nbecause  the  delay  will  prevent  the  container  from  shutting  down  promptly  and  will\ncause the pod to be shown in lists long after it has been deleted, which is always frus-\ntrating to the user deleting the pod.\nWRAPPING UP THIS SECTION\nTo recap—properly shutting down an application includes these steps:\nWait for a few seconds, then stop accepting new connections. \nClose all keep-alive connections not in the middle of a request.\nWait for all active requests to finish.\nThen shut down completely.\nTo understand what’s happening with the connections and requests during this pro-\ncess, examine figure 17.9 carefully.\nNot as simple as exiting the process immediately upon receiving the termination sig-\nnal, right? Is it worth going through all this? That’s for you to decide. But the least you\ncan do is add a pre-stop hook that waits a few seconds, like the one in the following\nlisting, perhaps.\n    lifecycle:                    \n      preStop:                    \n        exec:                     \n          command:                \n          - sh\n          - -c\n          - \"sleep 5\"\nListing 17.7   A pre-stop hook for preventing broken connections\nDelay (few seconds)\nKey:\nConnection\nRequest\niptablesrules\nupdated on all nodes\n(no new connections\nafter this point)\nStop\naccepting new\nconnections\nClose inactive\nkeep-alive\nconnections\nand wait for\nactive requests\nto finish\nWhen last\nactive request\ncompletes,\nshut down\ncompletely\nTime\nSIGTERM\nFigure 17.9   Properly handling existing and new connections after receiving a termination signal\n \n\n497Making your apps easy to run and manage in Kubernetes\nThis  way,  you  don’t  need  to  modify  the  code  of  your  app  at  all.  If  your  app  already\nensures all in-flight requests are processed completely, this pre-stop delay may be all\nyou need.\n17.4   Making your apps easy to run and manage in Kubernetes\nI hope you now have a better sense of how to make your apps handle clients nicely.\nNow we’ll look at other aspects of how an app should be built to make it easier to man-\nage in Kubernetes.\n17.4.1   Making manageable container images\nWhen  you  package  your  app  into  an  image,  you  can  choose  to  include  the  app’s\nbinary executable and any additional libraries it needs, or you can package up a whole\nOS filesystem along with the app. Way too many people do this, even though it’s usu-\nally unnecessary.\n Do you need every single file from an OS distribution in your image? Probably not.\nMost of the files will never be used and will make your image larger than it needs to\nbe. Sure, the layering of images makes sure each individual layer is downloaded only\nonce, but even having to wait longer than necessary the first time a pod is scheduled\nto a node is undesirable.\n Deploying new pods and scaling them should be fast. This demands having small\nimages without unnecessary cruft. If you’re building apps using the Go language, your\nimages don’t need to include anything else apart from the app’s single binary execut-\nable  file.  This  makes  Go-based  container  images  extremely  small  and  perfect  for\nKubernetes.\nTIPUse the FROM scratch directive in the Dockerfile for these images.\nBut in practice, you’ll soon see these minimal images are extremely difficult to debug.\nThe  first  time  you  need  to  run  a  tool  such  as  \nping, dig, curl,  or  something  similar\ninside  the  container,  you’ll  realize  how  important  it  is  for  container  images  to  also\ninclude at least a limited set of these tools. I can’t tell you what to include and what\nnot  to  include  in  your  images,  because  it  depends  on  how  you  do  things,  so  you’ll\nneed to find the sweet spot yourself.\n17.4.2   Properly tagging your images and using imagePullPolicy wisely\nYou’ll also soon learn that referring to the latest image tag in your pod manifests will\ncause problems, because you can’t tell which version of the image each individual pod\nreplica is running. Even if initially all your pod replicas run the same image version, if\nyou push a new version of the image under the \nlatest tag, and then pods are resched-\nuled  (or  you  scale  up  your  Deployment),  the  new  pods  will  run  the  new  version,\nwhereas  the  old  ones  will  still  be  running  the  old  one.  Also,  using  the  \nlatest  tag\nmakes it impossible to roll back to a previous version (unless you push the old version\nof the image again).\n \n\n498CHAPTER 17Best practices for developing apps\n It’s almost mandatory to use tags containing a proper version designator instead\nof \nlatest, except maybe in development. Keep in mind that if you use mutable tags\n(you push changes to the same tag), you’ll need to set the \nimagePullPolicy field in\nthe pod spec to \nAlways. But if you use that in production pods, be aware of the big\ncaveat associated with it. If the image pull policy is set to \nAlways, the container run-\ntime  will  contact  the  image  registry  every  time  a  new  pod  is  deployed.  This  slows\ndown pod startup a bit, because the node needs to check if the image has been mod-\nified. Worse yet, this policy prevents the pod from starting up when the registry can-\nnot be contacted.\n17.4.3   Using multi-dimensional instead of single-dimensional labels\nDon’t  forget  to  label  all  your  resources,  not  only  Pods.  Make  sure  you  add  multiple\nlabels to each resource, so they can be selected across each individual dimension. You\n(or the ops team) will be grateful you did it when the number of resources increases.\n Labels may include things like\nThe name of the application (or perhaps microservice) the resource belongs to\nApplication tier (front-end, back-end, and so on)\nEnvironment (development, QA, staging, production, and so on)\nVersion\nType of release (stable, canary, green or blue for green/blue deployments, and\nso on)\nTenant (if you’re running separate pods for each tenant instead of using name-\nspaces)\nShard for sharded systems\nThis will allow you to manage resources in groups instead of individually and make it\neasy to see where each resource belongs.\n17.4.4   Describing each resource through annotations\nTo  add  additional  information  to  your  resources  use  annotations.  At  the  least,\nresources  should  contain  an  annotation  describing  the  resource  and  an  annotation\nwith contact information of the person responsible for it. \n  In  a  microservices  architecture,  pods  could  contain  an  annotation  that  lists  the\nnames of the other services the pod is using. This makes it possible to show dependen-\ncies  between  pods.  Other  annotations  could  include  build  and  version  information\nand metadata used by tooling or graphical user interfaces (icon names, and so on).\n Both labels and annotations make managing running applications much easier, but\nnothing is worse than when an application starts crashing and you don’t know why.\n17.4.5   Providing information on why the process terminated\nNothing  is  more  frustrating  than  having  to  figure  out  why  a  container  terminated\n(or is even terminating continuously), especially if it happens at the worst possible\n \n\n499Making your apps easy to run and manage in Kubernetes\nmoment.  Be  nice  to  the  ops  people  and  make  their  lives  easier  by  including  all  the\nnecessary debug information in your log files. \n  But  to  make  triage  even  easier,  you  can  use  one  other  Kubernetes  feature  that\nmakes it possible to show the reason why a container terminated in the pod’s status.\nYou do this by having the process write a termination message to a specific file in the\ncontainer’s filesystem. The contents of this file are read by the Kubelet when the con-\ntainer terminates and are shown in the output of \nkubectl describe pod. If an applica-\ntion uses this mechanism, an operator can quickly see why the app terminated without\neven having to look at the container logs. \n The default file the process needs to write the message to is /dev/termination-log,\nbut it can be changed by setting the \nterminationMessagePath field in the container\ndefinition in the pod spec. \n You can see this in action by running a pod whose container dies immediately, as\nshown in the following listing.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-termination-message\nspec:\n  containers:\n  - image: busybox\n    name: main\n    terminationMessagePath: /var/termination-reason         \n    command:\n    - sh\n    - -c\n    - 'echo \"I''ve had enough\" > /var/termination-reason ; exit 1'   \nWhen running this pod, you’ll soon see the pod’s status shown as CrashLoopBackOff.\nIf you then use \nkubectl describe, you can see why the container died, without having\nto dig down into its logs, as shown in the following listing.\n$ kubectl describe po\nName:           pod-with-termination-message\n...\nContainers:\n...\n    State:      Waiting\n      Reason:   CrashLoopBackOff\n    Last State: Terminated\n      Reason:   Error\n      Message:  I've had enough          \n      Exit Code:        1\n      Started:          Tue, 21 Feb 2017 21:38:31 +0100\n      Finished:         Tue, 21 Feb 2017 21:38:31 +0100\nListing 17.8   Pod writing a termination message: termination-message.yaml\nListing 17.9   Seeing the container’s termination message with kubectl describe\nYou’re overriding the \ndefault path of the \ntermination message file.\nThe container\nwill write the\nmessage to\nthe file just\nbefore exiting.\nYou can see the reason \nwhy the container died \nwithout having to \ninspect its logs.\n \n\n500CHAPTER 17Best practices for developing apps\n    Ready:              False\n    Restart Count:      6\nAs you can see, the “I’ve had enough” message the process wrote to the file /var/ter-\nmination-reason is shown in the container’s \nLast State section. Note that this mecha-\nnism isn’t limited only to containers that crash. It can also be used in pods that run a\ncompletable task and terminate successfully (you’ll find an example in the file termi-\nnation-message-success.yaml). \n This mechanism is great for terminated containers, but you’ll probably agree that\na similar mechanism would also be useful for showing app-specific status messages of\nrunning,  not  only  terminated,  containers.  Kubernetes  currently  doesn’t  provide  any\nsuch functionality and I’m not aware of any plans to introduce it.\nNOTEIf the container doesn’t write the message to any file, you can set the\nterminationMessagePolicy  field  to  FallbackToLogsOnError.  In  that  case,\nthe  last  few  lines  of  the  container’s  log  are  used  as  its  termination  message\n(but only when the container terminates unsuccessfully).\n17.4.6   Handling application logs\nWhile we’re on the subject of application logging, let’s reiterate that apps should write\nto the standard output instead of files. This makes it easy to view logs with the \nkubectl\nlogs\n command. \nTIPIf a container crashes and is replaced with a new one, you’ll see the new\ncontainer’s  log.  To  see  the  previous  container’s  logs,  use  the  \n--previous\noption with kubectl logs.\nIf the application logs to a file instead of the standard output, you can display the log\nfile using an alternative approach: \n$ kubectl exec <pod> cat <logfile>\nThis  executes  the  cat  command  inside  the  container  and  streams  the  logs  back  to\nkubectl, which prints them out in your terminal. \nCOPYING LOG AND OTHER FILES TO AND FROM A CONTAINER\nYou  can  also  copy  the  log  file  to  your  local  machine  using  the  kubectl cp  command,\nwhich we haven’t looked at yet. It allows you to copy files from and into a container. For\nexample,  if  a  pod  called  \nfoo-pod  and  its  single  container  contains  a  file  at  /var/log/\nfoo.log\n, you can transfer it to your local machine with the following command:\n$ kubectl cp foo-pod:/var/log/foo.log foo.log\nTo copy a file from your local machine into the pod, specify the pod’s name in the sec-\nond argument:\n$ kubectl cp localfile foo-pod:/etc/remotefile\n \n\n501Making your apps easy to run and manage in Kubernetes\nThis copies the file localfile to /etc/remotefile inside the pod’s container. If the pod has\nmore than one container, you specify the container using the \n-c containerName option.\nUSING CENTRALIZED LOGGING\nIn a production system, you’ll want to use a centralized, cluster-wide logging solution,\nso  all  your  logs  are  collected  and  (permanently)  stored  in  a  central  location.  This\nallows  you  to  examine  historical  logs  and  analyze  trends.  Without  such  a  system,  a\npod’s  logs  are  only  available  while  the  pod  exists. As soon as it’s deleted, its logs are\ndeleted also. \n Kubernetes by itself doesn’t provide any kind of centralized logging. The compo-\nnents  necessary  for  providing  a  centralized  storage  and  analysis  of  all  the  container\nlogs must be provided by additional components, which usually run as regular pods in\nthe cluster. \n Deploying centralized logging solutions is easy. All you need to do is deploy a few\nYAML/JSON  manifests  and  you’re  good  to  go.  On  Google  Kubernetes  Engine,  it’s\neven easier. Check the Enable Stackdriver Logging checkbox when setting up the clus-\nter. Setting up centralized logging on an on-premises Kubernetes cluster is beyond the\nscope of this book, but I’ll give you a quick overview of how it’s usually done.\n You may have already heard of the ELK stack composed of ElasticSearch, Logstash,\nand Kibana. A slightly modified variation is the EFK stack, where Logstash is replaced\nwith FluentD. \n When using the EFK stack for centralized logging, each Kubernetes cluster node\nruns  a  FluentD  agent  (usually  as  a  pod  deployed  through  a  DaemonSet),  which  is\nresponsible for gathering the logs from the containers, tagging them with pod-specific\ninformation,  and  delivering  them  to  ElasticSearch,  which  stores  them  persistently.\nElasticSearch is also deployed as a pod somewhere in the cluster. The logs can then be\nviewed and analyzed in a web browser through Kibana, which is a web tool for visualiz-\ning ElasticSearch data. It also usually runs as a pod and is exposed through a Service.\nThe three components of the EFK stack are shown in the following figure.\nNOTEIn the next chapter, you’ll learn about Helm charts. You can use charts\ncreated by the Kubernetes community to deploy the EFK stack instead of cre-\nating your own YAML manifests. \nNode 1\nContainer logsKibana\nWeb\nbrowser\nFluentD\nNode 2\nContainer logs\nFluentD\nNode 3\nContainer logs\nFluentDElasticSearch\nFigure 17.10   Centralized logging with FluentD, ElasticSearch, and Kibana\n \n\n502CHAPTER 17Best practices for developing apps\nHANDLING MULTI-LINE LOG STATEMENTS\nThe  FluentD  agent  stores  each  line  of  the  log  file  as  an  entry  in  the  ElasticSearch\ndata store. There’s one problem with that. Log statements spanning multiple lines,\nsuch as exception stack traces in Java, appear as separate entries in the centralized\nlogging system. \n  To  solve  this  problem,  you  can  have  the  apps  output  JSON  instead  of  plain  text.\nThis  way,  a  multiline  log  statement  can  be  stored  and  shown  in  Kibana  as  a  single\nentry. But that makes viewing logs with \nkubectl logs much less human-friendly. \n The solution may be to keep outputting human-readable logs to standard output,\nwhile writing JSON logs to a file and having them processed by FluentD. This requires\nconfiguring  the  node-level  FluentD  agent  appropriately  or  adding  a  logging  sidecar\ncontainer to every pod. \n17.5   Best practices for development and testing\nWe’ve  talked  about  what  to  be  mindful  of  when  developing  apps,  but  we  haven’t\ntalked  about  the  development  and  testing  workflows  that  will  help  you  streamline\nthose processes. I don’t want to go into too much detail here, because everyone needs\nto find what works best for them, but here are a few starting points.\n17.5.1   Running apps outside of Kubernetes during development\nWhen you’re developing an app that will run in a production Kubernetes cluster, does\nthat mean you also need to run it in Kubernetes during development? Not really. Hav-\ning to build the app after each minor change, then build the container image, push it\nto a registry, and then re-deploy the pods would make development slow and painful.\nLuckily, you don’t need to go through all that trouble.\n You can always develop and run apps on your local machine, the way you’re used\nto.  After  all,  an  app  running  in  Kubernetes  is  a  regular  (although  isolated)  process\nrunning  on  one  of  the  cluster  nodes.  If  the  app  depends  on  certain  features  the\nKubernetes environment provides, you can easily replicate that environment on your\ndevelopment machine.\n I’m not even talking about running the app in a container. Most of the time, you\ndon’t need that—you can usually run the app directly from your IDE. \nCONNECTING TO BACKEND SERVICES\nIn production, if the app connects to a backend Service and uses the BACKEND_SERVICE\n_HOST\n and BACKEND_SERVICE_PORT environment variables to find the Service’s coordi-\nnates, you can obviously set those environment variables on your local machine manu-\nally  and  point  them  to  the  backend  Service,  regardless  of  if  it’s  running  outside  or\ninside a Kubernetes cluster. If it’s running inside Kubernetes, you can always (at least\ntemporarily) make the Service accessible externally by changing it to a \nNodePort or a\nLoadBalancer-type Service. \n \n\n503Best practices for development and testing\nCONNECTING TO THE API SERVER\nSimilarly,  if  your  app  requires  access  to  the  Kubernetes  API  server  when  running\ninside a Kubernetes cluster, it can easily talk to the API server from outside the cluster\nduring  development.  If  it  uses  the  ServiceAccount’s  token  to  authenticate  itself,  you\ncan always copy the ServiceAccount’s Secret’s files to your local machine with \nkubectl\ncp\n. The API server doesn’t care if the client accessing it is inside or outside the cluster. \n If the app uses an ambassador container like the one described in chapter 8, you\ndon’t  even  need  those  Secret  files.  Run  \nkubectl proxy  on  your  local  machine,  run\nyour app locally, and it should be ready to talk to your local \nkubectl proxy (as long as\nit and the ambassador container bind the proxy to the same port).\n In this case, you’ll need to make sure the user account your local \nkubectl is using\nhas the same privileges as the ServiceAccount the app will run under.\nRUNNING INSIDE A CONTAINER EVEN DURING DEVELOPMENT\nWhen during development you absolutely have to run the app in a container for what-\never reason, there is a way of avoiding having to build the container image every time.\nInstead of baking the binaries into the image, you can always mount your local filesys-\ntem  into  the  container  through  Docker  volumes,  for  example.  This  way,  after  you\nbuild a new version of the app’s binaries, all you need to do is restart the container (or\nnot even that, if hot-redeploy is supported). No need to rebuild the image.\n17.5.2   Using Minikube in development\nAs you can see, nothing forces you to run your app inside Kubernetes during develop-\nment. But you may do that anyway to see how the app behaves in a true Kubernetes\nenvironment.\n You may have used Minikube to run examples in this book. Although a Minikube\ncluster  runs  only  a  single  worker  node,  it’s  nevertheless  a  valuable  method  of  trying\nout your app in Kubernetes (and, of course, developing all the resource manifests that\nmake up your complete application). Minikube doesn’t offer everything that a proper\nmulti-node Kubernetes cluster usually provides, but in most cases, that doesn’t matter.\nMOUNTING LOCAL FILES INTO THE MINIKUBE VM AND THEN INTO YOUR CONTAINERS\nWhen you’re developing with Minikube and you’d like to try out every change to your\napp in your Kubernetes cluster, you can mount your local filesystem into the Minikube\nVM  using  the  \nminikube mount  command  and  then  mount  it  into  your  containers\nthrough a \nhostPath volume. You’ll find additional instructions on how to do that\nin the Minikube documentation at https://github.com/kubernetes/minikube/tree/\nmaster/docs.\nUSING THE DOCKER DAEMON INSIDE THE MINIKUBE VM TO BUILD YOUR IMAGES\nIf  you’re  developing  your  app  with  Minikube  and  planning  to  build  the  container\nimage after every change, you can use the Docker daemon inside the Minikube VM to\ndo the building, instead of having to build the image through your local Docker dae-\nmon, push it to a registry, and then have it pulled by the daemon in the VM. To use\n \n\n504CHAPTER 17Best practices for developing apps\nMinikube’s Docker daemon, all you need to do is point your DOCKER_HOST  environ-\nment variable to it. Luckily, this is much easier than it sounds. All you need to do is\nrun the following command on your local machine:\n$ eval $(minikube docker-env)\nThis  will  set  all  the  required  environment  variables  for  you.  You  then  build  your\nimages  the  same  way  as  if  the  Docker  daemon  was  running  on  your  local  machine.\nAfter  you  build  the  image,  you  don’t  need  to  push  it  anywhere,  because  it’s  already\nstored locally on the Minikube VM, which means new pods can use the image immedi-\nately. If your pods are already running, you either need to delete them or kill their\ncontainers so they’re restarted.\nBUILDING IMAGES LOCALLY AND COPYING THEM OVER TO THE MINIKUBE VM DIRECTLY\nIf you can’t use the daemon inside the VM to build the images, you still have a way to\navoid  having  to  push  the  image  to  a  registry  and  have  the  Kubelet  running  in  the\nMinikube  VM  pull  it.  If  you  build  the  image on your local machine, you can copy it\nover to the Minikube VM with the following command:\n$ docker save <image> | (eval $(minikube docker-env) && docker load)\nAs  before,  the  image  is  immediately  ready  to be used in a pod. But make sure the\nimagePullPolicy in your pod spec isn’t set to Always, because that would cause the\nimage to be pulled from the external registry again and you’d lose the changes you’ve\ncopied over.\nCOMBINING MINIKUBE WITH A PROPER KUBERNETES CLUSTER\nYou have virtually no limit when developing apps with Minikube. You can even com-\nbine a Minikube cluster with a proper Kubernetes cluster. I sometimes run my devel-\nopment  workloads  in  my  local  Minikube  cluster  and  have  them  talk  to  my  other\nworkloads that are deployed in a remote multi-node Kubernetes cluster thousands of\nmiles away. \n Once I’m finished with development, I can move my local workloads to the remote\ncluster with no modifications and with absolutely no problems thanks to how Kuber-\nnetes abstracts away the underlying infrastructure from the app.\n17.5.3   Versioning and auto-deploying resource manifests\nBecause Kubernetes uses a declarative model, you never have to figure out the current\nstate of your deployed resources and issue imperative commands to bring that state to\nwhat you desire. All you need to do is tell Kubernetes your desired state and it will take\nall the necessary actions to reconcile the cluster state with the desired state.\n You can store your collection of resource manifests in a Version Control System,\nenabling  you  to  perform  code  reviews,  keep  an  audit  trail,  and  roll  back  changes\nwhenever necessary. After each commit, you can run the \nkubectl apply command to\nhave your changes reflected in your deployed resources. \n \n\n505Best practices for development and testing\n If you run an agent that periodically (or when it detects a new commit) checks out\nyour manifests from the Version Control System (VCS), and then runs the \napply com-\nmand, you can manage your running apps simply by committing changes to the VCS\nwithout having to manually talk to the Kubernetes API server. Luckily, the people at\nBox (which coincidently was used to host this book’s manuscript and other materials)\ndeveloped and released a tool called \nkube-applier, which does exactly what I described.\nYou’ll find the tool’s source code at https://github.com/box/kube-applier.\n You can use multiple branches to deploy the manifests to a development, QA, stag-\ning, and production cluster (or in different namespaces in the same cluster).\n17.5.4   Introducing Ksonnet as an alternative to writing YAML/JSON \nmanifests\nWe’ve  seen  a  number  of  YAML  manifests  throughout  the  book.  I  don’t  see  writing\nYAML as too big of a problem, especially once you learn how to use \nkubectl explain\nto see the available options, but some people do. \n Just as I was finalizing the manuscript for this book, a new tool called Ksonnet was\nannounced. It’s a library built on top of Jsonnet, which is a data templating language\nfor building JSON data structures. Instead of writing the complete JSON by hand, it\nlets  you  define  parameterized  JSON  fragments,  give  them  a  name,  and  then  build  a\nfull JSON manifest by referencing those fragments by name, instead of repeating the\nsame JSON code in multiple locations—much like you use functions or methods in a\nprogramming language. \n Ksonnet defines the fragments you’d find in Kubernetes resource manifests, allow-\ning  you  to  quickly  build  a  complete  Kubernetes  resource  JSON  manifest  with  much\nless code. The following listing shows an example.\nlocal k = import \"../ksonnet-lib/ksonnet.beta.1/k.libsonnet\";\nlocal container = k.core.v1.container;\nlocal deployment = k.apps.v1beta1.deployment;\nlocal kubiaContainer =                              \n  container.default(\"kubia\", \"luksa/kubia:v1\") +    \n  container.helpers.namedPort(\"http\", 8080);        \ndeployment.default(\"kubia\", kubiaContainer) +    \ndeployment.mixin.spec.replicas(3)                \nThe  kubia.ksonnet  file  shown  in  the  listing  is  converted  to  a  full  JSON  Deployment\nmanifest when you run the following command:\n$ jsonnet kubia.ksonnet\nListing 17.10   The kubia Deployment written with Ksonnet: kubia.ksonnet\nThis defines a container called kubia, \nwhich uses the luksa/kubia:v1 image \nand includes a port called http.\nThis will be expanded into a full \nDeployment resource. The kubiaContainer \ndefined here will be included in the \nDeployment’s pod template.\n \n\n506CHAPTER 17Best practices for developing apps\nThe power of Ksonnet and Jsonnet becomes apparent when you realize you can define\nyour  own  higher-level  fragments  and  make  all  your  manifests  consistent  and  duplica-\ntion-free. You’ll find more information on using and installing Ksonnet and Jsonnet at\nhttps://github.com/ksonnet/ksonnet-lib.\n17.5.5   Employing Continuous Integration and Continuous Delivery \n(CI/CD)\nWe’ve touched on automating the deployment of Kubernetes resources two sections\nback, but you may want to set up a complete CI/CD pipeline for building your appli-\ncation binaries, container images, and resource manifests and then deploying them in\none or more Kubernetes clusters.\n You’ll find many online resources talking about this subject. Here, I’d like to point\nyou  specifically  to  the  Fabric8  project  (http://fabric8.io),  which  is  an  integrated\ndevelopment  platform  for  Kubernetes.  It  includes  Jenkins,  the  well-known,  open-\nsource automation system, and various other tools to deliver a full CI/CD pipeline\nfor  DevOps-style  development,  deployment,  and  management  of  microservices  on\nKubernetes.\n If you’d like to build your own solution, I also suggest looking at one of the Google\nCloud  Platform’s  online  labs  that  talks  about  this  subject.  It’s  available  at  https://\ngithub.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.\n17.6   Summary\nHopefully, the information in this chapter has given you an even deeper insight into\nhow  Kubernetes  works  and  will  help  you  build apps that feel right at home when\ndeployed to a Kubernetes cluster. The aim of this chapter was to\nShow  you  how  all  the  resources  covered  in  this  book  come  together  to  repre-\nsent a typical application running in Kubernetes.\nMake  you  think  about  the  difference  between  apps  that  are  rarely  moved\nbetween machines and apps running as pods, which are relocated much more\nfrequently.\nHelp you understand that your multi-component apps (or microservices, if you\nwill) shouldn’t rely on a specific start-up order.\nIntroduce init containers, which can be used to initialize a pod or delay the start\nof the pod’s main containers until a precondition is met.\nTeach you about container lifecycle hooks and when to use them.\nGain  a  deeper  insight  into  the  consequences  of  the  distributed  nature  of\nKubernetes components and its eventual consistency model.\nLearn  how  to  make  your  apps  shut  down  properly  without  breaking  client\nconnections.\n \n\n507Summary\nGive you a few small tips on how to make your apps easier to manage by keep-\ning  image  sizes  small,  adding  annotations  and  multi-dimensional  labels  to  all\nyour resources, and making it easier to see why an application terminated.\nTeach  you  how  to  develop  Kubernetes  apps  and  run  them  locally  or  in  Mini-\nkube before deploying them on a proper multi-node cluster.\nIn the next and final chapter, we’ll learn  how  you  can  extend  Kubernetes  with  your\nown custom API objects and controllers and how others have done it to create com-\nplete Platform-as-a-Service solutions on top of Kubernetes.\n \n\n508\nExtending Kubernetes\nYou’re  almost  done.  To  wrap  up,  we’ll  look  at  how  you  can  define  your  own  API\nobjects and create controllers for those objects. We’ll also look at how others have\nextended Kubernetes and built Platform-as-a-Service solutions on top of it.\n18.1   Defining custom API objects\nThroughout the book, you’ve learned about the API objects that Kubernetes pro-\nvides  and  how  they’re  used  to  build  application  systems.  Currently,  Kubernetes\nusers mostly use only these objects even though they represent relatively low-level,\ngeneric concepts. \nThis chapter covers\nAdding custom objects to Kubernetes\nCreating a controller for the custom object\nAdding custom API servers\nSelf-provisioning of services with the Kubernetes \nService Catalog\nRed Hat’s OpenShift Container Platform\nDeis Workflow and Helm\n \n\n509Defining custom API objects\n As the Kubernetes ecosystem evolves, you’ll see more and more high-level objects,\nwhich will be much more specialized than the resources Kubernetes supports today.\nInstead of dealing with Deployments, Services, ConfigMaps, and the like, you’ll create\nand manage objects that represent whole applications or software services. A custom\ncontroller  will  observe  those  high-level  objects  and  create  low-level  objects  based  on\nthem. For example, to run a messaging broker inside a Kubernetes cluster, all you’ll\nneed to do is create an instance of a Queue  resource  and  all  the  necessary  Secrets,\nDeployments, and Services will be created by a custom Queue controller. Kubernetes\nalready provides ways of adding custom resources like this. \n18.1.1   Introducing CustomResourceDefinitions\nTo define a new resource type, all you need to do is post a CustomResourceDefinition\nobject (CRD) to the Kubernetes API server. The CustomResourceDefinition object is\nthe description of the custom resource type. Once the CRD is posted, users can then\ncreate  instances  of  the  custom  resource  by  posting  JSON  or  YAML  manifests  to  the\nAPI server, the same as with any other Kubernetes resource.\nNOTEPrior to Kubernetes 1.7, custom resources were defined through Third-\nPartyResource objects, which were similar to CustomResourceDefinitions, but\nwere removed in version 1.8.\nCreating a CRD so that users can create objects of the new type isn’t a useful feature if\nthose  objects  don’t  make  something  tangible  happen  in  the  cluster.  Each  CRD  will\nusually  also  have  an  associated  controller  (an  active  component  doing  something\nbased  on  the  custom  objects),  the  same  way  that  all  the  core  Kubernetes  resources\nhave an associated controller, as was explained in chapter 11. For this reason, to prop-\nerly  show  what  CustomResourceDefinitions  allow  you  to  do  other  than  adding\ninstances of a custom object, a controller must be deployed as well. You’ll do that in\nthe next example.\nINTRODUCING THE EXAMPLE CUSTOMRESOURCEDEFINITION\nLet’s imagine you want to allow users of your Kubernetes cluster to run static websites\nas easily as possible, without having to deal with Pods, Services, and other Kubernetes\nresources. What you want to achieve is for users to create objects of type Website that\ncontain  nothing  more  than  the  website’s  name  and  the  source  from  which  the  web-\nsite’s files (HTML, CSS, PNG, and others) should be obtained. You’ll use a Git reposi-\ntory  as  the  source  of  those  files.  When  a  user  creates  an  instance  of  the  Website\nresource, you want Kubernetes to spin up a new web server pod and expose it through\na Service, as shown in figure 18.1.\n To create the Website resource, you want users to post manifests along the lines of\nthe one shown in the following listing.\n \n \n \n \n\n510CHAPTER 18Extending Kubernetes\nkind: Website        \nmetadata:\n  name: kubia             \nspec:\n  gitRepo: https://github.com/luksa/kubia-website-example.git   \nLike  all  other  resources,  your  resource  contains  a  kind  and  a  metadata.name  field,\nand like most resources, it also contains a \nspec section. It contains a single field called\ngitRepo  (you  can  choose  the  name)—it  specifies  the  Git  repository  containing  the\nwebsite’s files. You’ll also need to include an \napiVersion field, but you don’t know yet\nwhat its value must be for custom resources.\n  If  you  try  posting  this  resource  to  Kubernetes,  you’ll  receive  an  error  because\nKubernetes doesn’t know what a Website object is yet:\n$ kubectl create -f imaginary-kubia-website.yaml\nerror: unable to recognize \"imaginary-kubia-website.yaml\": no matches for \n➥ /, Kind=Website\nBefore you can create instances of your custom object, you need to make Kubernetes\nrecognize them.\nCREATING A CUSTOMRESOURCEDEFINITION OBJECT\nTo make Kubernetes accept your custom Website resource instances, you need to post\nthe CustomResourceDefinition shown in the following listing to the API server.\napiVersion: apiextensions.k8s.io/v1beta1       \nkind: CustomResourceDefinition                 \nmetadata:\n  name: websites.extensions.example.com      \nspec:\n  scope: Namespaced                          \nListing 18.1   An imaginary custom resource: imaginary-kubia-website.yaml\nListing 18.2   A CustomResourceDefinition manifest: website-crd.yaml\nWebsite\nkind: Website\nmetadata:\nname: kubia\nspec:\ngitRepo:\ngithub.com/.../kubia.git\nPod:\nkubia-website\nService:\nkubia-website\nFigure 18.1   Each Website object should result in the creation of a Service and an HTTP \nserver Pod.\nA custom \nobject kind\nThe name of the website \n(used for naming the \nresulting Service and Pod)\nThe Git \nrepository \nholding the \nwebsite’s files\nCustomResourceDefinitions belong \nto this API group and version.\nThe full\nname of\nyour\ncustom\nobject\nYou want Website resources \nto be namespaced.\n \n\n511Defining custom API objects\n  group: extensions.example.com                \n  version: v1                                  \n  names:                                    \n    kind: Website                           \n    singular: website                       \n    plural: websites                        \nAfter you post the descriptor to Kubernetes, it will allow you to create any number of\ninstances of the custom Website resource. \n You can create the CRD from the website-crd.yaml file available in the code archive:\n$ kubectl create -f website-crd-definition.yaml\ncustomresourcedefinition \"websites.extensions.example.com\" created\nI’m sure you’re wondering about the long name of the CRD. Why not call it Website?\nThe reason is to prevent name clashes. By adding a suffix to the name of the CRD\n(which will usually include the name of the organization that created the CRD), you\nkeep CRD names unique. Luckily, the long name doesn’t mean you’ll need to create\nyour Website resources with \nkind: websites.extensions.example.com, but as kind:\nWebsite\n, as specified in the names.kind property of the CRD. The extensions.exam-\nple.com\n part is the API group of your resource. \n  You’ve  seen  how  creating  Deployment  objects  requires  you  to  set  \napiVersion  to\napps/v1beta1 instead of v1. The part before the slash is the API group (Deployments\nbelong to the \napps API group), and the part after it is the version name (v1beta1 in\nthe case of Deployments). When creating instances of the custom Website resource,\nthe \napiVersion property will need to be set to extensions.example.com/v1.\nCREATING AN INSTANCE OF A CUSTOM RESOURCE\nConsidering  what  you  learned,  you’ll  now  create  a  proper  YAML  for  your  Website\nresource instance. The YAML manifest is shown in the following listing.\napiVersion: extensions.example.com/v1       \nkind: Website                               \nmetadata:\n  name: kubia                                \nspec:\n  gitRepo: https://github.com/luksa/kubia-website-example.git\nThe kind of your resource is Website, and the apiVersion  is  composed  of  the  API\ngroup and the version number you defined in the CustomResourceDefinition.\n Create your Website object now:\n$ kubectl create -f kubia-website.yaml\nwebsite \"kubia\" created\nListing 18.3   A custom Website resource: kubia-website.yaml\nDefine an API group and version \nof the Website resource.\nYou need to specify the various \nforms of the custom object’s name.\nYour custom API\ngroup and version\nThis manifest \ndescribes a Website \nresource instance.\nThe name of the \nWebsite instance\n \n\n512CHAPTER 18Extending Kubernetes\nThe  response  tells  you  that  the  API  server  has  accepted  and  stored  your  custom\nWebsite object. Let’s see if you can now retrieve it. \nRETRIEVING INSTANCES OF A CUSTOM RESOURCE\nList all the websites in your cluster:\n$ kubectl get websites\nNAME      KIND\nkubia     Website.v1.extensions.example.com\nAs  with  existing  Kubernetes  resources,  you  can  create  and  then  list  instances  of  cus-\ntom resources. You can also use \nkubectl describe to see the details of your custom\nobject, or retrieve the whole YAML with \nkubectl get, as in the following listing.\n$ kubectl get website kubia -o yaml\napiVersion: extensions.example.com/v1\nkind: Website\nmetadata:\n  creationTimestamp: 2017-02-26T15:53:21Z\n  name: kubia\n  namespace: default\n  resourceVersion: \"57047\"\n  selfLink: /apis/extensions.example.com/v1/.../default/websites/kubia\n  uid: b2eb6d99-fc3b-11e6-bd71-0800270a1c50\nspec:\n  gitRepo: https://github.com/luksa/kubia-website-example.git\nNote that the resource includes everything that was in the original YAML definition,\nand that Kubernetes has initialized additional metadata fields the way it does with all\nother resources. \nDELETING AN INSTANCE OF A CUSTOM OBJECT\nObviously, in addition to creating and retrieving custom object instances, you can also\ndelete them:\n$ kubectl delete website kubia\nwebsite \"kubia\" deleted\nNOTEYou’re  deleting  an  instance  of  a  Website,  not  the  Website  CRD\nresource. You could also delete the CRD object itself, but let’s hold off on that\nfor  a  while,  because  you’ll  be  creating  additional  Website  instances  in  the\nnext section. \nLet’s go over everything you’ve done. By creating a CustomResourceDefinition object,\nyou  can  now  store,  retrieve,  and  delete  custom  objects  through  the  Kubernetes  API\nserver. These objects don’t do anything yet. You’ll need to create a controller to make\nthem do something. \nListing 18.4   Full Website resource definition retrieved from the API server\n \n\n513Defining custom API objects\n In general, the point of creating custom objects like this isn’t always to make some-\nthing happen when the object is created. Certain custom objects are used to store data\ninstead  of  using  a  more  generic  mechanism  such  as  a  ConfigMap.  Applications  run-\nning  inside  pods  can  query  the  API  server  for  those  objects  and  read  whatever  is\nstored in them. \n But in this case, we said you wanted the existence of a Website object to result in\nthe spinning up of a web server serving the contents of the Git repository referenced\nin the object. We’ll look at how to do that next.\n18.1.2   Automating custom resources with custom controllers\nTo make your Website objects run a web server pod exposed through a Service, you’ll\nneed to build and deploy a Website controller, which will watch the API server for the\ncreation  of  Website  objects  and  then  create  the  Service  and  the  web  server  Pod  for\neach of them. \n  To  make  sure  the  Pod  is  managed  and  survives  node  failures,  the  controller  will\ncreate a Deployment resource instead of an unmanaged Pod directly. The controller’s\noperation is summarized in figure 18.2.\nI’ve  written  a  simple  initial  version  of  the  controller,  which  works  well  enough  to\nshow  CRDs  and  the  controller  in  action,  but  it’s  far  from  being  production-ready,\nbecause  it’s  overly  simplified.  The  container  image  is  available  at  docker.io/luksa/\nwebsite-controller:latest,  and  the  source  code  is  at  https://github.com/luksa/k8s-\nwebsite-controller. Instead of going through its source code, I’ll explain what the con-\ntroller does.\nAPI server\nWebsites\nWebsite:\nkubia\nDeployments\nDeployment:\nkubia-website\nServices\nService:\nkubia-website\nWebsite\ncontroller\nWatches\nCreates\nFigure 18.2   The Website controller \nwatches for Website objects and \ncreates a Deployment and a Service.\n \n\n514CHAPTER 18Extending Kubernetes\nUNDERSTANDING WHAT THE WEBSITE CONTROLLER DOES\nImmediately upon startup, the controller starts to watch Website objects by requesting\nthe following URL:\nhttp://localhost:8001/apis/extensions.example.com/v1/websites?watch=true\nYou  may  recognize  the  hostname  and  port—the  controller  isn’t  connecting  to  the\nAPI  server  directly,  but  is  instead  connecting  to  the  \nkubectl proxy  process,  which\nruns in a sidecar container in the same pod and acts as the ambassador to the API\nserver (we examined the ambassador pattern in chapter 8). The proxy forwards the\nrequest  to  the  API  server,  taking  care  of  both  TLS  encryption  and  authentication\n(see figure 18.3).\nThrough the connection opened by this HTTP GET request, the API server will send\nwatch events for every change to any Website object.\n The API server sends the \nADDED watch event every time a new Website object is cre-\nated. When the controller receives such an event, it extracts the Website’s name and\nthe URL of the Git repository from the Website object it received in the watch event\nand creates a Deployment and a Service object by posting their JSON manifests to the\nAPI server. \n  The  Deployment  resource  contains  a  template  for  a  pod  with  two  containers\n(shown in figure 18.4): one running an nginx server and another one running a git-\nsync  process,  which  keeps  a  local  directory  synced  with  the  contents  of  a  Git  repo.\nThe local directory is shared with the nginx container through an \nemptyDir volume\n(you  did  something  similar  to  that  in  chapter  6,  but  instead  of  keeping  the  local\ndirectory  synced  with  a  Git  repo,  you  used  a  \ngitRepo  volume  to  download  the  Git\nrepo’s  contents  at  pod  startup;  the  volume’s  contents  weren’t  kept  in  sync  with  the\nGit  repo  afterward).  The  Service  is  a  \nNodePort  Service,  which  exposes  your  web\nserver  pod  through  a  random  port  on  each  node  (the  same  port  is  used  on  all\nnodes). When a pod is created by the Deployment object, clients can access the web-\nsite through the node port.\nPod: website-controller\nContainer: main\nWebsite controller\nGET http://localhost:8001/apis/extensions.\nexample.com/v1/websites?watch=true\nGET https://kubernetes:443/apis/extensions.\nexample.com/v1/websites?watch=true\nAuthorization: Bearer <token>\nContainer: proxy\nkubectl proxy\nAPI server\nFigure 18.3   The Website controller talks to the API server through a proxy (in the ambassador container).\n \n\n515Defining custom API objects\nThe API server also sends a DELETED watch event when a Website resource instance is\ndeleted. Upon receiving the event, the controller deletes the Deployment and the Ser-\nvice  resources  it  created  earlier.  As  soon  as  a  user  deletes  the  Website  instance,  the\ncontroller will shut down and remove the web server serving that website.\nNOTEMy  oversimplified  controller  isn’t  implemented  properly.  The  way  it\nwatches  the  API  objects  doesn’t  guarantee  it  won’t  miss  individual  watch\nevents. The proper way to watch objects through the API server is to not only\nwatch  them,  but  also  periodically  re-list  all  objects  in  case  any  watch  events\nwere missed. \nRUNNING THE CONTROLLER AS A POD\nDuring development, I ran the controller on my local development laptop and used a\nlocally running \nkubectl proxy process (not running as a pod) as the ambassador to\nthe Kubernetes API server. This allowed me to develop quickly, because I didn’t need\nto  build  a  container  image  after  every  change  to  the  source  code  and  then  run  it\ninside Kubernetes. \n When I’m ready to deploy the controller into production, the best way is to run the\ncontroller inside Kubernetes itself, the way you do with all the other core controllers.\nTo run the controller in Kubernetes, you can deploy it through a Deployment resource.\nThe following listing shows an example of such a Deployment.\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: website-controller\nspec:\n  replicas: 1                      \n  template:\nListing 18.5   A Website controller Deployment: website-controller.yaml\nPod\nWebserver\ncontainer\nWeb client\ngit-sync\ncontainer\nServes website to\nweb client through\na random port\nClones Git repo\ninto volume and\nkeeps it synced\nemptyDir\nvolume\nFigure 18.4   The pod serving \nthe website specified in the \nWebsite object\nYou’ll run a single \nreplica of the \ncontroller.\n \n\n516CHAPTER 18Extending Kubernetes\n    metadata:\n      name: website-controller\n      labels:\n        app: website-controller\n    spec:\n      serviceAccountName: website-controller    \n      containers:                                    \n      - name: main                                   \n        image: luksa/website-controller              \n      - name: proxy                                  \n        image: luksa/kubectl-proxy:1.6.2             \nAs you can see, the Deployment deploys a single replica of a two-container pod. One\ncontainer  runs  your  controller,  whereas  the  other  one  is  the  ambassador  container\nused for simpler communication with the API server. The pod runs under its own spe-\ncial ServiceAccount, so you’ll need to create it before you deploy the controller:\n$ kubectl create serviceaccount website-controller\nserviceaccount \"website-controller\" created\nIf  Role  Based  Access  Control  (RBAC)  is  enabled  in  your  cluster,  Kubernetes  will  not\nallow the controller to watch Website resources or create Deployments or Services. To\nallow it to do that, you’ll need to bind the \nwebsite-controller ServiceAccount to the\ncluster-admin ClusterRole, by creating a ClusterRoleBinding like this:\n$ kubectl create clusterrolebinding website-controller \n➥ --clusterrole=cluster-admin \n➥ --serviceaccount=default:website-controller\nclusterrolebinding \"website-controller\" created\nOnce  you  have  the  ServiceAccount  and  ClusterRoleBinding  in  place,  you  can  deploy\nthe controller’s Deployment. \nSEEING THE CONTROLLER IN ACTION\nWith the controller now running, create the kubia Website resource again:\n$ kubectl create -f kubia-website.yaml\nwebsite \"kubia\" created\nNow, let’s check the controller’s logs (shown in the following listing) to see if it has\nreceived the watch event.\n$ kubectl logs website-controller-2429717411-q43zs -c main\n2017/02/26 16:54:41 website-controller started.\n2017/02/26 16:54:47 Received watch event: ADDED: kubia: https://github.c...\n2017/02/26 16:54:47 Creating services with name kubia-website in namespa... \n2017/02/26 16:54:47 Response status: 201 Created\n2017/02/26 16:54:47 Creating deployments with name kubia-website in name... \n2017/02/26 16:54:47 Response status: 201 Created\nListing 18.6   Displaying logs of the Website controller\nIt will run \nunder a special \nServiceAccount.\nTwo containers: the \nmain container and \nthe proxy sidecar\n \n\n517Defining custom API objects\nThe logs show that the controller received the ADDED event and that it created a Service\nand a Deployment for the \nkubia-website Website. The API server responded with a\n201 Created  response,  which  means  the  two  resources  should  now  exist.  Let’s  verify\nthat the Deployment, Service and the resulting  Pod  were  created.  The  following  list-\ning lists all Deployments, Services and Pods.\n$ kubectl get deploy,svc,po\nNAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE  AGE\ndeploy/kubia-website        1         1         1            1          4s\ndeploy/website-controller   1         1         1            1          5m\nNAME                CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\nsvc/kubernetes      10.96.0.1      <none>        443/TCP        38d\nsvc/kubia-website   10.101.48.23   <nodes>       80:32589/TCP   4s\nNAME                                     READY     STATUS    RESTARTS   AGE\npo/kubia-website-1029415133-rs715        2/2       Running   0          4s\npo/website-controller-1571685839-qzmg6   2/2       Running   1          5m\nThere they are. The kubia-website Service, through which you can access your web-\nsite, is available on port \n32589 on all cluster nodes. You can access it with your browser.\nAwesome, right? \n Users of your Kubernetes cluster can now deploy static websites in seconds, with-\nout knowing anything about Pods, Services, or any other Kubernetes resources, except\nyour custom Website resource. \n Obviously, you still have room for improvement. The controller could, for exam-\nple, watch for Service objects and as soon as the node port is assigned, write the URL\nthe  website  is  accessible  at  into  the  \nstatus  section  of  the  Website  resource  instance\nitself.  Or  it  could  also  create  an  Ingress  object  for  each  website.  I’ll  leave  the  imple-\nmentation of these additional features to you as an exercise.\n18.1.3   Validating custom objects\nYou may have noticed that you didn’t specify any kind of validation schema in the Web-\nsite CustomResourceDefinition. Users can include any field they want in the YAML of\ntheir Website object. The API server doesn’t validate the contents of the YAML (except\nthe  usual  fields  like  \napiVersion, kind,  and  metadata),  so  users  can  create  invalid\nWebsite objects (without a \ngitRepo field, for example). \n Is it possible to add validation to the controller and prevent invalid objects from\nbeing accepted by the API server? It isn’t, because the API server first stores the object,\nthen returns a success response to the client (\nkubectl), and only then notifies all the\nwatchers (the controller is one of them). All the controller can really do is validate\nthe  object  when  it  receives  it  in  a  watch  event,  and  if  the  object  is  invalid,  write  the\nerror message to the Website object (by updating the object through a new request to\nthe API server). The user wouldn’t be notified of the error automatically. They’d have\nListing 18.7   The Deployment, Service, and Pod created for the kubia-website\n \n\n518CHAPTER 18Extending Kubernetes\nto notice the error message by querying the API server for the Website object. Unless\nthe user does this, they have no way of knowing whether the object is valid or not.\n  This  obviously  isn’t  ideal.  You’d  want  the  API  server  to  validate  the  object  and\nreject  invalid  objects  immediately.  Validation  of  custom  objects  was  introduced  in\nKubernetes version 1.8 as an alpha feature. To have the API server validate your cus-\ntom  objects,  you  need  to  enable  the  \nCustomResourceValidation  feature  gate  in  the\nAPI server and specify a JSON schema in the CRD.\n18.1.4   Providing a custom API server for your custom objects\nA better way of adding support for custom objects in Kubernetes is to implement your\nown API server and have the clients talk directly to it. \nINTRODUCING API SERVER AGGREGATION\nIn  Kubernetes  version  1.7,  you  can  integrate  your  custom  API  server  with  the  main\nKubernetes API server, through API server aggregation. Initially, the Kubernetes API\nserver  was  a  single  monolithic  component.  From  Kubernetes  version  1.7,  multiple\naggregated API servers will be exposed at a single location. Clients can connect to the\naggregated  API  and  have  their  requests  transparently  forwarded  to  the  appropriate\nAPI server. This way, the client wouldn’t even be aware that multiple API servers han-\ndle  different  objects  behind  the  scenes.  Even  the  core  Kubernetes  API  server  may\neventually end up being split into multiple smaller API servers and exposed as a single\nserver through the aggregator, as shown in figure 18.5.\nIn  your  case,  you  could  create  an  API  server  responsible  for  handling  your  Website\nobjects. It could validate those objects the way the core Kubernetes API server validates\nthem. You’d no longer need to create a CRD to represent those objects, because you’d\nimplement the Website object type into the custom API server directly. \n Generally, each API server is responsible for storing their own resources. As shown\nin figure 18.5, it can either run its own instance of etcd (or a whole etcd cluster), or it\nMain\nAPI server\nCustom\nAPI server Y\nCustom\nAPI server X\nkubectl\nUses its own etcd instance\nfor storing its resources\nUses CustomResourceDefinitions\nin main API server as storage\nmechanism\netcd\netcd\nFigure 18.5   API server aggregation\n \n\n519Extending Kubernetes with the Kubernetes Service Catalog\ncan store its resources in the core API server’s etcd store by creating CRD instances in\nthe core API server. In that case, it needs to create a CRD object first, before creating\ninstances of the CRD, the way you did in the example.\nREGISTERING A CUSTOM API SERVER\nTo  add  a  custom  API  server  to  your  cluster,  you’d  deploy  it  as  a  pod  and  expose  it\nthrough a Service. Then, to integrate it into the main API server, you’d deploy a YAML\nmanifest describing an APIService resource like the one in the following listing.\napiVersion: apiregistration.k8s.io/v1beta1   \nkind: APIService                             \nmetadata:\n  name: v1alpha1.extensions.example.com\nspec:\n  group: extensions.example.com           \n  version: v1alpha1                      \n  priority: 150\n  service:                    \n    name: website-api         \n    namespace: default        \nAfter creating the APIService resource from the previous listing, client requests sent\nto the main API server that contain any resource from the \nextensions.example.com\nAPI group and version v1alpha1 would be forwarded to the custom API server pod(s)\nexposed through the \nwebsite-api Service. \nCREATING CUSTOM CLIENTS\nWhile you can create custom resources from YAML files using the regular kubectl cli-\nent, to make deployment of custom objects even easier, in addition to providing a cus-\ntom  API  server,  you  can  also  build  a  custom  CLI  tool.  This  will  allow  you  to  add\ndedicated  commands  for  manipulating  those  objects,  similar  to  how  \nkubectl  allows\ncreating  Secrets,  Deployments,  and  other  resources  through  resource-specific  com-\nmands like \nkubectl create secret or kubectl create deployment.\n As I’ve already mentioned, custom API servers, API server aggregation, and other\nfeatures related to extending Kubernetes are currently being worked on intensively, so\nthey  may  change  after  the  book  is  published.  To  get  up-to-date  information  on  the\nsubject, refer to the Kubernetes GitHub repos at http://github.com/kubernetes.\n18.2   Extending Kubernetes with the Kubernetes Service \nCatalog\nOne of the first additional API servers that will be added to Kubernetes through API\nserver aggregation is the Service Catalog API server. The Service Catalog is a hot topic\nin the Kubernetes community, so you may want to know about it. \n  Currently,  for  a  pod  to  consume  a  service  (here  I  use  the  term  generally,  not  in\nrelation  to  Service  resources;  for  example,  a  database  service  includes  everything\nListing 18.8   An APIService YAML definition \nThis is an APIService \nresource.\nThe API group this API \nserver is responsible for\nThe supported API version\nThe Service the custom API \nserver is exposed through\n \n\n520CHAPTER 18Extending Kubernetes\nrequired to allow users to use a database in their app), someone needs to deploy the\npods providing the service, a Service resource, and possibly a Secret so the client pod\ncan  use  it  to  authenticate  with  the  service.  That  someone  is  usually  the  same  user\ndeploying the client pod or, if a team is dedicated to deploying these types of general\nservices, the user needs to file a ticket and wait for the team to provision the service.\nThis means the user needs to either create the manifests for all the components of the\nservice,  know  where  to  find  an  existing  set  of  manifests,  know  how  to  configure  it\nproperly, and deploy it manually, or wait for the other team to do it. \n But Kubernetes is supposed to be an easy-to-use, self-service system. Ideally, users\nwhose apps require a certain service (for example, a web application requiring a back-\nend database), should be able to say to Kubernetes. “Hey, I need a PostgreSQL data-\nbase. Please provision one and tell me where and how I can connect to it.” This will\nsoon be possible through the Kubernetes Service Catalog. \n18.2.1   Introducing the Service Catalog\nAs  the  name  suggests,  the  Service  Catalog  is  a  catalog  of  services.  Users  can  browse\nthrough  the  catalog  and  provision  instances  of  the  services  listed  in  the  catalog  by\nthemselves without having to deal with Pods, Services, ConfigMaps, and other resources\nrequired  for  the  service  to  run.  You’ll  recognize that this is similar to what you did\nwith the Website custom resource.\n Instead of adding custom resources to the API server for each type of service, the\nService Catalog introduces the following four generic API resources:\nA ClusterServiceBroker, which describes an (external) system that can provision\nservices\nA ClusterServiceClass, which describes a type of service that can be provisioned\nA ServiceInstance, which is one instance of a service that has been provisioned\nA  ServiceBinding,  which  represents  a  binding  between  a  set  of  clients  (pods)\nand a ServiceInstance\nThe  relationships  between  those  four  resources  are  shown  in  the  figure  18.6  and\nexplained in the following paragraphs.\nIn a nutshell, a cluster admin creates a ClusterServiceBroker resource for each service\nbroker whose services they’d like to make available in the cluster. Kubernetes then asks\nthe  broker  for  a  list  of  services  that  it  can  provide  and  creates  a  ClusterServiceClass\nresource for each of them. When a user requires a service to be provisioned, they create\nan ServiceInstance resource and then a ServiceBinding to bind that ServiceInstance to\nClient pods\nServiceBinding\nServiceInstance\nClusterServiceClass(es)\nClusterServiceBroker\nFigure 18.6   The relationships between Service Catalog API resources. \n \n\n521Extending Kubernetes with the Kubernetes Service Catalog\ntheir pods. Those pods are then injected with a Secret that holds all the necessary cre-\ndentials and other data required to connect to the provisioned ServiceInstance.\n The Service Catalog system architecture is shown in figure 18.7.\nThe components shown in the figure are explained in the following sections.\n18.2.2   Introducing the Service Catalog API server and Controller \nManager\nSimilar  to  core  Kubernetes,  the  Service  Catalog  is  a  distributed  system  composed  of\nthree components:\nService Catalog API Server\netcd as the storage\nController Manager, where all the controllers run\nThe four Service Catalog–related resources we introduced earlier are created by post-\ning  YAML/JSON  manifests  to  the  API  server.  It  then  stores  them  into  its  own  etcd\ninstance or uses CustomResourceDefinitions in the main API server as an alternative\nstorage mechanism (in that case, no additional etcd instance is required). \n  The  controllers  running  in  the  Controller  Manager  are  the  ones  doing  some-\nthing with those resources. They obviously talk to the Service Catalog API server, the\nway other core Kubernetes controllers talk to the core API server. Those controllers\ndon’t  provision  the  requested  services  themselves.  They  leave  that  up  to  external\nservice brokers, which are registered by creating ServiceBroker resources in the Ser-\nvice Catalog API.\nKubernetes clusterExternal system(s)\nKubernetes Service Catalog\nClient pods\nProvisioned\nservices\nBroker A\nBroker B\netcd\nService\nCatalog\nAPI server\nController\nManager\nkubectl\nProvisioned\nservices\nClient pods use the\nprovisioned services\nFigure 18.7   The architecture of the Service Catalog\n \n\n522CHAPTER 18Extending Kubernetes\n18.2.3   Introducing Service Brokers and the OpenServiceBroker API\nA cluster administrator can register one or more external ServiceBrokers in the Ser-\nvice Catalog. Every broker must implement the OpenServiceBroker API.\nINTRODUCING THE OPENSERVICEBROKER API\nThe Service Catalog talks to the broker through that API. The API is relatively simple.\nIt’s a REST API providing the following operations:\nRetrieving the list of services with GET /v2/catalog\nProvisioning a service instance (PUT /v2/service_instances/:id)\nUpdating a service instance (PATCH /v2/service_instances/:id)\nBinding  a  service  instance  (PUT /v2/service_instances/:id/service_bind-\nings/:binding_id\n)\nUnbinding  an  instance  (DELETE /v2/service_instances/:id/service_bind-\nings/:binding_id\n)\nDeprovisioning a service instance (DELETE /v2/service_instances/:id)\nYou’ll  find  the  OpenServiceBroker  API  spec  at  https://github.com/openservicebro-\nkerapi/servicebroker.\nREGISTERING BROKERS IN THE SERVICE CATALOG\nThe cluster administrator registers a broker by posting a ServiceBroker resource man-\nifest to the Service Catalog API, like the one shown in the following listing.\napiVersion: servicecatalog.k8s.io/v1alpha1    \nkind: ClusterServiceBroker                                  \nmetadata:\n  name: database-broker                          \nspec:\n  url: http://database-osbapi.myorganization.org  \nThe  listing  describes  an  imaginary  broker  that  can  provision  databases  of  different\ntypes. After the administrator creates the ClusterServiceBroker resource, a controller\nin  the  Service  Catalog  Controller  Manager  connects  to  the  URL  specified  in  the\nresource to retrieve the list of services this broker can provision.\n  After  the  Service  Catalog  retrieves  the  list  of  services,  it  creates  a  ClusterService-\nClass resource for each of them. Each ClusterServiceClass resource describes a sin-\ngle  type  of  service  that  can  be  provisioned  (an  example  of  a  ClusterServiceClass  is\n“PostgreSQL database”). Each ClusterServiceClass has one or more service plans asso-\nciated with it. These allow the user to choose the level of service they need (for exam-\nple, a database ClusterServiceClass could provide a “Free” plan, where the size of the\nListing 18.9   A ClusterServiceBroker manifest: database-broker.yaml\nThe resource kind and \nthe API group and version\nThe name of this broker\nWhere the Service Catalog\ncan contact the broker\n(its OpenServiceBroker [OSB] API URL)\n \n\n523Extending Kubernetes with the Kubernetes Service Catalog\ndatabase  is  limited  and  the  underlying  storage  is  a  spinning  disk,  and  a  “Premium”\nplan, with unlimited size and SSD storage). \nLISTING THE AVAILABLE SERVICES IN A CLUSTER\nUsers  of  the  Kubernetes  cluster  can  retrieve  a  list  of  all  services  that  can  be  provi-\nsioned in the cluster with \nkubectl get serviceclasses, as shown in the following\nlisting.\n$ kubectl get clusterserviceclasses\nNAME                KIND\npostgres-database   ClusterServiceClass.v1alpha1.servicecatalog.k8s.io\nmysql-database      ServiceClass.v1alpha1.servicecatalog.k8s.io\nmongodb-database    ServiceClass.v1alpha1.servicecatalog.k8s.io\nThe listing shows ClusterServiceClasses for services that your imaginary database bro-\nker could provide. You can compare ClusterServiceClasses to StorageClasses, which we\ndiscussed in chapter 6. StorageClasses allow you to select the type of storage you’d like\nto use in your pods, while ClusterServiceClasses allow you to select the type of service.\n You can see details of one of the ClusterServiceClasses by retrieving its YAML. An\nexample is shown in the following listing.\n$ kubectl get serviceclass postgres-database -o yaml\napiVersion: servicecatalog.k8s.io/v1alpha1\nbindable: true\nbrokerName: database-broker                     \ndescription: A PostgreSQL database\nkind: ClusterServiceClass\nmetadata:\n  name: postgres-database\n  ...\nplanUpdatable: false\nplans:\n- description: A free (but slow) PostgreSQL instance        \n  name: free                                                \n  osbFree: true                                             \n  ...\n- description: A paid (very fast) PostgreSQL instance      \n  name: premium                                            \n  osbFree: false                                           \n  ...\nThe ClusterServiceClass in the listing contains two plans—a free plan, and a premium\nplan.  You  can  see  that  this  ClusterServiceClass  is  provided  by  the  database-broker\nbroker.\nListing 18.10   List of ClusterServiceClasses in a cluster\nListing 18.11   A ClusterServiceClass definition\nThis ClusterServiceClass \nis provided by the \ndatabase-broker.\nA free plan for \nthis service\nA paid plan\n \n\n524CHAPTER 18Extending Kubernetes\n18.2.4   Provisioning and using a service\nLet’s imagine the pods you’re deploying need to use a database. You’ve inspected the\nlist  of  available  ClusterServiceClasses  and  have  chosen  to  use  the  \nfree plan of the\npostgres-database ClusterServiceClass. \nPROVISIONING A SERVICEINSTANCE\nTo  have  the  database  provisioned  for  you,  all  you  need  to  do  is  create  a  Service-\nInstance resource, as shown in the following listing.\napiVersion: servicecatalog.k8s.io/v1alpha1\nkind: ServiceInstance\nmetadata:\n  name: my-postgres-db                     \nspec:\n  clusterServiceClassName: postgres-database        \n  clusterServicePlanName: free                             \n  parameters:\n    init-db-args: --data-checksums         \nYou  created  a  ServiceInstance  called  my-postgres-db  (that  will  be  the  name  of  the\nresource  you’re  deploying)  and  specified  the  ClusterServiceClass  and  the  chosen\nplan. You’re also specifying a parameter, which is specific for each broker and Cluster-\nServiceClass. Let’s imagine you looked up the possible parameters in the broker’s doc-\numentation.\n As soon as you create this resource, the Service Catalog will contact the broker the\nClusterServiceClass  belongs  to  and  ask  it  to  provision  the  service.  It  will  pass  on  the\nchosen ClusterServiceClass and plan names, as well as all the parameters you specified.\n It’s then completely up to the broker to know what to do with this information. In\nyour case, your database broker will probably spin up a new instance of a PostgreSQL\ndatabase  somewhere—not  necessarily  in  the  same  Kubernetes  cluster  or  even  in\nKubernetes at all. It could run a Virtual Machine and run the database in there. The\nService Catalog doesn’t care, and neither does the user requesting the service. \n You can check if the service has been provisioned successfully by inspecting the\nstatus  section  of  the  my-postgres-db  ServiceInstance  you  created,  as  shown  in  the\nfollowing listing.\n$ kubectl get instance my-postgres-db -o yaml\napiVersion: servicecatalog.k8s.io/v1alpha1\nkind: ServiceInstance\n...\nstatus:\n  asyncOpInProgress: false\n  conditions:\nListing 18.12   A ServiceInstance manifest: database-instance.yaml\nListing 18.13   Inspecting the status of a ServiceInstance\nYou’re giving this \nInstance a name.\nThe ServiceClass \nand Plan you want\nAdditional parameters \npassed to the broker\n \n\n525Extending Kubernetes with the Kubernetes Service Catalog\n  - lastTransitionTime: 2017-05-17T13:57:22Z\n    message: The instance was provisioned successfully    \n    reason: ProvisionedSuccessfully                       \n    status: \"True\"\n    type: Ready                   \nA database instance is now running somewhere, but how do you use it in your pods?\nTo do that, you need to bind it.\nBINDING A SERVICEINSTANCE\nTo  use  a  provisioned  ServiceInstance  in  your  pods,  you  create  a  ServiceBinding\nresource, as shown in the following listing.\napiVersion: servicecatalog.k8s.io/v1alpha1\nkind: ServiceBinding\nmetadata:\n  name: my-postgres-db-binding\nspec:\n  instanceRef:                          \n    name: my-postgres-db                \n  secretName: postgres-secret           \nThe listing shows that you’re defining a ServiceBinding resource called my-postgres-\ndb-binding\n,  in  which  you’re  referencing  the  my-postgres-db  service  instance  you\ncreated earlier. You’re also specifying a name of a Secret. You want the Service Catalog\nto  put  all  the  necessary  credentials  for  accessing  the  service  instance  into  a  Secret\ncalled \npostgres-secret. But where are you binding the ServiceInstance to your pods?\nNowhere, actually.\n Currently, the Service Catalog doesn’t yet make it possible to inject pods with the\nServiceInstance’s  credentials.  This  will  be  possible  when  a  new  Kubernetes  feature\ncalled  PodPresets  is  available.  Until  then,  you  can  choose  a  name  for  the  Secret\nwhere you want the credentials to be stored in and mount that Secret into your pods\nmanually.\n When you submit the ServiceBinding resource from the previous listing to the Ser-\nvice  Catalog  API  server,  the  controller  will  contact  the  Database  broker  once  again\nand  create  a  binding  for  the  ServiceInstance  you  provisioned  earlier.  The  broker\nresponds with a list of credentials and other data necessary for connecting to the data-\nbase.  The  Service  Catalog  creates  a  new  Secret  with  the  name  you  specified  in  the\nServiceBinding resource and stores all that data in the Secret. \nUSING THE NEWLY CREATED SECRET IN CLIENT PODS\nThe Secret created by the Service Catalog system can be mounted into pods, so they\ncan read its contents and use them to connect to the provisioned service instance (a\nPostgreSQL database in the example). The Secret could look like the one in the fol-\nlowing listing.\nListing 18.14   A ServiceBinding: my-postgres-db-binding.yaml\nThe database was \nprovisioned successfully.\nIt’s ready to be used.\nYou’re referencing the \ninstance you created \nearlier.\nYou’d like the credentials \nfor accessing the service \nstored in this Secret.\n \n\n526CHAPTER 18Extending Kubernetes\n$ kubectl get secret postgres-secret -o yaml\napiVersion: v1\ndata:\n  host: <base64-encoded hostname of the database>     \n  username: <base64-encoded username>                 \n  password: <base64-encoded password>                 \nkind: Secret\nmetadata:\n  name: postgres-secret\n  namespace: default\n  ...\ntype: Opaque\nBecause you can choose the name of the Secret yourself, you can deploy pods before\nprovisioning  or  binding  the  service.  As  you  learned  in  chapter  7,  the  pods  won’t  be\nstarted until such a Secret exists. \n If necessary, multiple bindings can be created for different pods. The service bro-\nker  can  choose  to  use  the  same  set  of  credentials  in  every  binding,  but  it’s  better  to\ncreate a new set of credentials for every binding instance. This way, pods can be pre-\nvented from using the service by deleting the ServiceBinding resource.\n18.2.5   Unbinding and deprovisioning\nOnce you no longer need a ServiceBinding, you can delete it the way you delete other\nresources:\n$ kubectl delete servicebinding my-postgres-db-binding\nservicebinding \"my-postgres-db-binding\" deleted\nWhen you do this, the Service Catalog controller will delete the Secret and call the bro-\nker to perform an unbinding operation. The service instance (in your case a PostgreSQL\ndatabase) is still running. You can therefore create a new ServiceBinding if you want.\n But if you don’t need the database instance anymore, you should delete the Service-\nInstance resource also:\n$ kubectl delete serviceinstance my-postgres-db\nserviceinstance \"my-postgres-db \" deleted\nDeleting the ServiceInstance resource causes the Service Catalog to perform a depro-\nvisioning operation on the service broker. Again, exactly what that means is up to the\nservice  broker,  but  in  your  case,  the  broker  should  shut  down  the  PostgreSQL  data-\nbase instance that it created when we provisioned the service instance.\n18.2.6   Understanding what the Service Catalog brings\nAs  you’ve  learned,  the  Service  Catalog  enables  service  providers  make  it  possible  to\nexpose those services in any Kubernetes cluster by registering the broker in that cluster.\nListing 18.15   A Secret holding the credentials for connecting to the service instance\nThis is what the pod \nshould use to connect to \nthe database service.\n \n\n527Platforms built on top of Kubernetes\nFor example, I’ve been involved with the  Service  Catalog  since  early  on  and  have\nimplemented  a  broker,  which  makes  it  trivial  to  provision  messaging  systems  and\nexpose them to pods in a Kubernetes cluster. Another team has implemented a broker\nthat makes it easy to provision Amazon Web Services. \n  In  general,  service  brokers  allow  easy  provisioning  and  exposing  of  services  in\nKubernetes and will make Kubernetes an even more awesome platform for deploying\nyour applications. \n18.3   Platforms built on top of Kubernetes\nI’m sure you’ll agree that Kubernetes is a great system by itself. Given that it’s easily\nextensible  across  all  its  components,  it’s  no  wonder  companies  that  had  previously\ndeveloped  their  own  custom  platforms  are  now  re-implementing  them  on  top  of\nKubernetes.  Kubernetes  is,  in  fact,  becoming  a  widely  accepted  foundation  for  the\nnew generation of Platform-as-a-Service offerings.\n Among the best-known PaaS systems built on Kubernetes are Deis Workflow and\nRed Hat’s OpenShift. We’ll do a quick overview of both systems to give you a sense of\nwhat they offer on top of all the awesome stuff Kubernetes already offers.\n18.3.1   Red Hat OpenShift Container Platform\nRed  Hat  OpenShift  is  a  Platform-as-a-Service  and  as  such,  it  has  a  strong  focus  on\ndeveloper  experience.  Among  its  goals  are  enabling  rapid  development  of  applica-\ntions, as well as easy deployment, scaling, and long-term maintenance of those apps.\nOpenShift  has  been  around  much  longer  than  Kubernetes.  Versions  1  and  2  were\nbuilt from the ground up and had nothing to do with Kubernetes, but when Kuberne-\ntes was announced, Red Hat decided to rebuild OpenShift version 3 from scratch—\nthis time on top of Kubernetes. When a company  such  as  Red  Hat  decides  to  throw\naway an old version of their software and build a new one on top of an existing tech-\nnology like Kubernetes, it should be clear to everyone how great Kubernetes is.\n Kubernetes automates rollouts and application scaling, whereas OpenShift also auto-\nmates the actual building of application images and their automatic deployment with-\nout requiring you to integrate a Continuous Integration solution into your cluster. \n OpenShift also provides user and group management, which allows you to run a\nproperly  secured  multi-tenant  Kubernetes  cluster,  where  individual  users  are  only\nallowed  to  access  their  own  Kubernetes  namespaces  and  the  apps  running  in  those\nnamespaces are also fully network-isolated from each other by default. \nINTRODUCING ADDITIONAL RESOURCES AVAILABLE IN OPENSHIFT\nOpenShift  provides  some  additional  API  objects  in  addition  to  all  those  available  in\nKubernetes. We’ll explain them in the next few paragraphs to give you a good over-\nview of what OpenShift does and what it provides.\n The additional resources include\nUsers & Groups\nProjects\n \n\n528CHAPTER 18Extending Kubernetes\nTemplates\nBuildConfigs\nDeploymentConfigs\nImageStreams\nRoutes\nAnd others\nUNDERSTANDING USERS, GROUPS, AND PROJECTS\nWe’ve  said  that  OpenShift  provides  a  proper  multi-tenant  environment  to  its  users.\nUnlike Kubernetes, which doesn’t have an API object for representing an individual\nuser of the cluster (but does have ServiceAccounts that represent services running in\nit), OpenShift provides powerful user management features, which make it possible to\nspecify what each user can do and what they cannot. These features pre-date the Role-\nBased Access Control, which is now the standard in vanilla Kubernetes.\n Each user has access to certain Projects, which are nothing more than Kubernetes\nNamespaces with additional annotations. Users can only act on resources that reside\nin  the  projects  the  user  has  access  to.  Access  to  the  project  is  granted  by  a  cluster\nadministrator. \nINTRODUCING APPLICATION TEMPLATES\nKubernetes  makes  it  possible  to  deploy  a  set  of  resources  through  a  single  JSON  or\nYAML  manifest.  OpenShift  takes  this  a  step  further  by  allowing  that  manifest  to  be\nparameterizable. A parameterizable list in OpenShift is called a Template; it’s a list of\nobjects whose definitions can include placeholders that get replaced with parameter\nvalues when you process and then instantiate a template (see figure 18.8).\nThe template itself is a JSON or YAML file containing a list of parameters that are ref-\nerenced in resources defined in that same JSON/YAML. The template can be stored\nin the API server like any other object. Before a template can be instantiated, it needs\nTemplate\nParameters\nAPP_NAME=\"kubia\"\nVOL_CAPACITY=\"5 Gi\"\n...\nPod\nname: $(APP_NAME)\nService\nname: $(APP_NAME)\nTemplate\nPod\nname: kubia\nService\nname: kubia\nPod\nname: kubia\nService\nname: kubia\nProcessCreate\nFigure 18.8   OpenShift templates\n \n\n529Platforms built on top of Kubernetes\nto  be  processed.  To  process  a  template,  you  supply  the  values  for  the  template’s\nparameters and then OpenShift replaces the references to the parameters with those\nvalues. The result is a processed template, which is exactly like a Kubernetes resource\nlist that can then be created with a single POST request.\n  OpenShift  provides  a  long  list  of  pre-fabricated  templates  that  allow  users  to\nquickly run complex applications by specifying a few arguments (or none at all, if the\ntemplate  provides  good  defaults  for  those  arguments).  For  example,  a  template  can\nenable the creation of all the Kubernetes resources necessary to run a Java EE appli-\ncation  inside  an  Application  Server,  which  connects  to  a  back-end  database,  also\ndeployed as part of that same template. All those components can be deployed with a\nsingle command.\nBUILDING IMAGES FROM SOURCE USING BUILDCONFIGS\nOne of the best features of OpenShift is the ability to have OpenShift build and imme-\ndiately deploy an application in the OpenShift cluster by pointing it to a Git repository\nholding the application’s source code. You don’t need to build the container image at\nall—OpenShift does that for you. This is done by creating a resource called Build-\nConfig,  which  can  be  configured  to  trigger  builds  of  container  images  immediately\nafter a change is committed to the source Git repository. \n Although OpenShift doesn’t monitor the Git repository itself, a hook in the repos-\nitory can notify OpenShift of the new commit. OpenShift will then pull the changes\nfrom the Git repository and start the build process. A build mechanism called Source\nTo  Image  can  detect  what  type  of  application  is  in  the  Git  repository  and  run  the\nproper build procedure for it. For example, if it detects a pom.xml file, which is used\nin  Java  Maven-formatted  projects,  it  runs  a  Maven  build.  The  resulting  artifacts  are\npackaged  into  an  appropriate  container  image,  and  are  then  pushed  to  an  internal\ncontainer registry (provided by OpenShift). From there, they can be pulled and run\nin the cluster immediately. \n By creating a BuildConfig object, developers can thus point to a Git repo and not\nworry  about  building  container  images.  Developers have almost no need to know\nanything  about  containers.  Once  the  ops  team  deploys  an  OpenShift  cluster  and\ngives  developers  access  to  it,  those  developers  can  develop  their  code,  commit,  and\npush it to a Git repo, the same way they used to before we started packaging apps into\ncontainers.  Then  OpenShift  takes  care  of  building,  deploying,  and  managing  apps\nfrom that code.\nAUTOMATICALLY DEPLOYING NEWLY BUILT IMAGES WITH DEPLOYMENTCONFIGS\nOnce a new container image is built, it can also automatically be deployed in the clus-\nter.  This  is  enabled  by  creating  a  DeploymentConfig  object  and  pointing  it  to  an\nImageStream. As the name suggests, an ImageStream is a stream of images. When an\nimage is built, it’s added to the ImageStream. This enables the DeploymentConfig to\nnotice the newly built image and allows it to take action and initiate a rollout of the\nnew image (see figure 18.9).\n \n\n530CHAPTER 18Extending Kubernetes\nA DeploymentConfig is almost identical to the Deployment object in Kubernetes, but\nit pre-dates it. Like a Deployment object, it has a configurable strategy for transition-\ning between Deployments. It contains a pod template used to create the actual pods,\nbut  it  also  allows  you  to  configure  pre-  and  post-deployment  hooks.  In  contrast  to  a\nKubernetes Deployment, it creates ReplicationControllers instead of ReplicaSets and\nprovides a few additional features.\nEXPOSING SERVICES EXTERNALLY USING ROUTES\nEarly on, Kubernetes didn’t provide Ingress objects. To expose Services to the outside\nworld,  you  needed  to  use  \nNodePort  or  LoadBalancer-type  Services.  But  at  that  time,\nOpenShift already provided a better option through a Route resource. A Route is sim-\nilar to an Ingress, but it provides additional configuration related to TLS termination\nand traffic splitting. \n Similar to an Ingress controller, a Route needs a Router, which is a controller that\nprovides the load balancer or proxy. In contrast to Kubernetes, the Router is available\nout of the box in OpenShift. \nTRYING OUT OPENSHIFT\nIf you’re interested in trying out OpenShift, you can start by using Minishift, which is\nthe  OpenShift  equivalent  of  Minikube,  or  you  can  try  OpenShift  Online  Starter  at\nhttps://manage.openshift.com, which is a free multi-tenant, hosted solution provided\nto get you started with OpenShift. \n18.3.2   Deis Workflow and Helm\nA company called Deis, which has recently been acquired by Microsoft, also provides a\nPaaS  called  Workflow,  which  is  also  built  on  top  of  Kubernetes.  Besides  Workflow,\nPods\nBuilder pod\nReplication\nController\nBuildConfig\nGit repo\nDeploymentConfig\nImageStream\nBuild trigger\nClones Git repo, builds new\nimage from source, and adds\nit to the ImageStream\nWatches for new images in ImageStream\nand rolls out new version (similarly to a\nDeployment)\nFigure 18.9   BuildConfigs and DeploymentConfigs in OpenShift\n \n\n531Platforms built on top of Kubernetes\nthey’ve also developed a tool called Helm, which is gaining traction in the Kubernetes\ncommunity as a standard way of deploying existing apps in Kubernetes. We’ll take a\nbrief look at both.\nINTRODUCING DEIS WORKFLOW\nYou can deploy Deis Workflow to any existing Kubernetes cluster (unlike OpenShift,\nwhich is a complete cluster with a modified API server and other Kubernetes compo-\nnents). When you run Workflow, it creates a set of Services and ReplicationControllers,\nwhich then provide developers with a simple, developer-friendly environment. \n Deploying new versions of your app is triggered by pushing your changes with \ngit\npush\n deis master  and  letting  Workflow  take  care  of  the  rest.  Similar  to  OpenShift,\nWorkflow  also  provides  a  source  to  image  mechanism,  application  rollouts  and  roll-\nbacks,  edge  routing,  and  also  log  aggregation,  metrics,  and  alerting,  which  aren’t\navailable in core Kubernetes. \n To run Workflow in your Kubernetes cluster, you first need to install the Deis Work-\nflow and Helm CLI tools and then install Workflow into your cluster. We won’t go into\nhow  to  do  that  here,  but  if  you’d  like  to  learn  more,  visit  the  website  at  https://deis\n.com/workflow. What we’ll explore here is the Helm tool, which can be used without\nWorkflow and has gained popularity in the community.\nDEPLOYING RESOURCES THROUGH HELM\nHelm is a package manager for Kubernetes (similar to OS package managers like yum\nor apt in Linux or homebrew in MacOS). \n Helm is comprised of two things:\nA helm CLI tool (the client).\nTiller, a server component running as a Pod inside the Kubernetes cluster.\nThose  two  components  are  used  to  deploy  and  manage  application  packages  in  a\nKubernetes cluster. Helm application packages are called Charts. They’re combined\nwith a Config, which contains configuration information and is merged into a Chart\nto create a Release, which is a running instance of an application (a combined Chart\nand Config). You deploy and manage Releases using the \nhelm CLI tool, which talks to\nthe  Tiller  server,  which  is  the  component  that  creates  all  the  necessary  Kubernetes\nresources defined in the Chart, as shown in figure 18.10.\n You can create charts yourself and keep them  on  your  local  disk,  or  you  can  use\nany existing chart, which is available in the growing list of helm charts maintained by\nthe community at https://github.com/kubernetes/charts. The list includes charts for\napplications such as PostgreSQL, MySQL, MariaDB, Magento, Memcached, MongoDB,\nOpenVPN, PHPBB, RabbitMQ, Redis, WordPress, and others.\n Similar to how you don’t build and install apps developed by other people to your\nLinux  system  manually,  you  probably  don’t  want  to  build  and  manage  your  own\nKubernetes manifests for such applications, right? That’s why you’ll want to use Helm\nand the charts available in the GitHub repository I mentioned. \n \n\n532CHAPTER 18Extending Kubernetes\nWhen you want to run a PostgreSQL or a MySQL database in your Kubernetes cluster,\ndon’t start writing manifests for them. Instead, check if someone else has already gone\nthrough the trouble and prepared a Helm chart for it. \n Once someone prepares a Helm chart for a specific application and adds it to the\nHelm chart GitHub repo, installing the whole application takes a single one-line com-\nmand. For example, to run MySQL in your Kubernetes cluster, all you need to do is\nclone the charts Git repo to your local machine and run the following command (pro-\nvided you have Helm’s CLI tool and Tiller running in your cluster):\n$ helm install --name my-database stable/mysql\nThis  will  create  all  the  necessary  Deployments,  Services,  Secrets,  and  PersistentVolu-\nmeClaims needed to run MySQL in your cluster. You don’t need to concern yourself\nwith what components you need and how to configure them to run MySQL properly.\nI’m sure you’ll agree this is awesome.\nTIPOne of the most interesting charts available in the repo is an OpenVPN\nchart,  which  runs  an  OpenVPN  server  inside  your  Kubernetes  cluster  and\nallows  you  to  enter  the  pod  network  through  VPN  and  access  Services  as  if\nyour local machine was a pod in the cluster. This is useful when you’re devel-\noping apps and running them locally.\nThese were several examples of how Kubernetes can be extended and how companies\nlike Red Hat and Deis (now Microsoft) have extended it. Now go and start riding the\nKubernetes wave yourself!\nKubernetes cluster\nChart\nand\nConfig\nHelm\nCharts\n(files on\nlocal disk)\nTiller\n(pod)\nDeployments,\nServices, and\nother objects\nhelm\nCLI tool\nManages\ncharts\nCombines Chart and\nConfig into a Release\nCreates Kubernetes objects\ndefined in the Release\nFigure 18.10   Overview of Helm\n \n\n533Summary\n18.4   Summary\nThis final chapter has shown you how you can go beyond the existing functionalities\nKubernetes provides and how companies like Dies and Red Hat have done it. You’ve\nlearned how\nCustom  resources  can  be  registered  in  the  API  server  by  creating  a  Custom-\nResourceDefinition object.\nInstances of custom objects can be stored, retrieved, updated, and deleted with-\nout having to change the API server code.\nA custom controller can be implemented to bring those objects to life.\nKubernetes can be extended with custom API servers through API aggregation.\nKubernetes Service Catalog makes it possible to self-provision external services\nand expose them to pods running in the Kubernetes cluster.\nPlatforms-as-a-Service built on top of Kubernetes make it easy to build contain-\nerized applications inside the same Kubernetes cluster that then runs them. \nA package manager called Helm makes deploying existing apps without requir-\ning you to build resource manifests for them.\nThank you for taking the time to read through this long book. I hope you’ve learned\nas much from reading it as I have from writing it.\n \n\n534\nappendix A\nUsing kubectl\nwith multiple clusters\nA.1Switching between Minikube and Google Kubernetes \nEngine\nThe examples in this book can either be run in a cluster created with Minikube, or\none created with Google Kubernetes Engine (GKE). If you plan on using both, you\nneed  to  know  how  to  switch  between  them.  A  detailed  explanation  of  how  to  use\nkubectl with multiple clusters is described in the next section. Here we look at how\nto switch between Minikube and GKE.\nSWITCHING TO MINIKUBE\nLuckily,  every  time  you  start  up  a  Minikube  cluster  with  minikube start,  it  also\nreconfigures \nkubectl to use it:\n$ minikube start\nStarting local Kubernetes cluster...\n...\nSetting up kubeconfig...                            \nKubectl is now configured to use the cluster.       \nAfter switching from Minikube to GKE, you can switch back by stopping Minikube\nand starting it up again. \nkubectl will then be re-configured to use the Minikube clus-\nter again.\nSWITCHING TO GKE\nTo switch to using the GKE cluster, you can use the following command:\n$ gcloud container clusters get-credentials my-gke-cluster\nThis will configure kubectl to use the GKE cluster called my-gke-cluster.\nMinikube sets up kubectl every \ntime you start the cluster.\n \n\n535Using kubectl with multiple clusters or namespaces\nGOING FURTHER\nThese  two  methods  should  be  enough  to  get  you  started  quickly,  but  to  understand\nthe complete picture of using \nkubectl with multiple clusters, study the next section. \nA.2Using kubectl with multiple clusters or namespaces\nIf you need to switch between different Kubernetes clusters, or if you want to work in a\ndifferent  namespace  than  the  default  and  don’t  want  to  specify  the  \n--namespace\noption every time you run kubectl, here’s how to do it.\nA.2.1Configuring the location of the kubeconfig file\nThe config used by kubectl is usually stored in the ~/.kube/config file. If it’s stored\nsomewhere else, the \nKUBECONFIG environment variable needs to point to its location. \nNOTEYou  can  use  multiple  config  files  and  have  kubectl  use  them  all  at\nonce by specifying all of them in the \nKUBECONFIG environment variable (sepa-\nrate them with a colon).\nA.2.2Understanding the contents of the kubeconfig file\nAn example config file is shown in the following listing.\napiVersion: v1\nclusters:\n- cluster:                                                 \n    certificate-authority: /home/luksa/.minikube/ca.crt    \n    server: https://192.168.99.100:8443                    \n  name: minikube                                           \ncontexts:\n- context:                          \n    cluster: minikube               \n    user: minikube                  \n    namespace: default              \n  name: minikube                    \ncurrent-context: minikube             \nkind: Config\npreferences: {}\nusers:\n- name: minikube                                             \n  user:                                                      \n    client-certificate: /home/luksa/.minikube/apiserver.crt  \n    client-key: /home/luksa/.minikube/apiserver.key          \nThe kubeconfig file consists of four sections:\n■\nA list of clusters\n■\nA list of users\n■\nA list of contexts\n■\nThe name of the current context\nListing A.1   Example kubeconfig file\nContains \ninformation about a \nKubernetes cluster\nDefines a \nkubectl \ncontext\nThe current context \nkubectl uses\nContains \na user’s \ncredentials\n \n\n536APPENDIX AUsing kubectl with multiple clusters\nEach cluster, user, and context has a name. The name is used to refer to the context,\nuser, or cluster. \nCLUSTERS\nA  cluster  entry  represents  a  Kubernetes  cluster  and  contains  the  URL  of  the  API\nserver,  the  certificate  authority  (CA)  file,  and  possibly  a  few  other  configuration\noptions  related  to  communication  with  the  API  server.  The  CA  certificate  can  be\nstored in a separate file and referenced in the kubeconfig file, or it can be included in\nit directly in the \ncertificate-authority-data field.\nUSERS\nEach user defines the credentials to use when talking to an API server. This can be a\nusername and password pair, an authentication token, or a client key and certificate.\nThe certificate and key can be included in the kubeconfig file (through the \nclient-\ncertificate-data\n  and  client-key-data  properties)  or  stored  in  separate  files  and\nreferenced in the config file, as shown in listing A.1.\nCONTEXTS\nA context ties together a cluster, a user, and the default namespace kubectl should use\nwhen performing commands. Multiple contexts can point to the same user or cluster. \nTHE CURRENT CONTEXT\nWhile there can be multiple contexts defined in the kubeconfig file, at any given time\nonly one of them is the current context. Later we’ll see how the current context can\nbe changed.\nA.2.3Listing, adding, and modifying kube config entries\nYou can edit the file manually to add, modify, and remove clusters, users, or contexts,\nbut you can also do it through one of the \nkubectl config commands.\nADDING OR MODIFYING A CLUSTER\nTo add another cluster, use the kubectl config set-cluster command:\n$ kubectl config set-cluster my-other-cluster \n➥ --server=https://k8s.example.com:6443 \n➥ --certificate-authority=path/to/the/cafile\nThis will add a cluster called my-other-cluster with the API server located at https://\nk8s.example.com:6443. To see additional options you can pass to the command, run\nkubectl config set-cluster to have it print out usage examples.\n If a cluster by that name already exists, the \nset-cluster command will overwrite\nits configuration options. \nADDING OR MODIFYING USER CREDENTIALS\nAdding and modifying users is similar to adding or modifying a cluster. To add a user\nthat authenticates with the API server using a username and password, run the follow-\ning command:\n$ kubectl config set-credentials foo --username=foo --password=pass\n \n\n537Using kubectl with multiple clusters or namespaces\nTo use token-based authentication, run the following instead:\n$ kubectl config set-credentials foo --token=mysecrettokenXFDJIQ1234\nBoth these examples store user credentials under the name foo. If you use the same\ncredentials for authenticating  against  different  clusters,  you can define a single user\nand use it with both clusters. \nTYING CLUSTERS AND USER CREDENTIALS TOGETHER\nA context defines which user to use with which cluster, but can also define the name-\nspace that \nkubectl should use, when you don’t specify the namespace explicitly with\nthe \n--namespace or -n option.\n The following command is used to create a new context that ties together the clus-\nter and the user you created:\n$ kubectl config set-context some-context --cluster=my-other-cluster \n➥ --user=foo --namespace=bar\nThis  creates  a  context  called  some-context that uses the my-other-cluster  cluster\nand the \nfoo user credentials. The default namespace in this context is set to bar. \n  You  can  also  use  the  same  command  to  change  the  namespace  of  your  current\ncontext, for example. You can get the name of the current context like so:\n$ kubectl config current-context\nminikube\nYou then change the namespace like this:\n$ kubectl config set-context minikube --namespace=another-namespace\nRunning this simple command once is much more user-friendly compared to having\nto include the \n--namespace option every time you run kubectl.\nTIPTo  easily  switch  between  namespaces, define an alias like this: alias\nkcd='kubectl\n config set-context $(kubectl config current-context)\n--namespace\n '.  You  can  then  switch  between  namespaces  with  kcd some-\nnamespace\n.\nA.2.4Using kubectl with different clusters, users, and contexts\nWhen  you  run  kubectl  commands,  the  cluster,  user,  and  namespace  defined  in  the\nkubeconfig’s current context are used, but you can override them using the following\ncommand-line options:\n■\n--user to use a different user from the kubeconfig file.\n■\n--username and --password to use a different username and/or password (they\ndon’t need to be specified in the config file). If using other types of authentica-\ntion, you can use \n--client-key and --client-certificate or --token.\n■\n--cluster to use a different cluster (must be defined in the config file).\n \n\n538APPENDIX AUsing kubectl with multiple clusters\n■\n--server to specify the URL of a different server (which isn’t in the config file).\n■\n--namespace to use a different namespace.\nA.2.5Switching between contexts \nInstead of modifying the current context as in one of the previous examples, you can\nalso use the \nset-context command to create an additional context and then switch\nbetween contexts. This is handy when working with multiple clusters (use \nset-clus-\nter\n to create cluster entries for them). \n Once you have multiple contexts set up, switching between them is trivial:\n$ kubectl config use-context my-other-context\nThis switches the current context to my-other-context. \nA.2.6Listing contexts and clusters\nTo list all the contexts defined in your kubeconfig file, run the following command:\n$ kubectl config get-contexts\nCURRENT   NAME          CLUSTER       AUTHINFO            NAMESPACE\n*         minikube      minikube      minikube            default\n          rpi-cluster   rpi-cluster   admin/rpi-cluster\n          rpi-foo       rpi-cluster   admin/rpi-cluster   foo\nAs you can see, I’m using three different contexts. The rpi-cluster and the rpi-foo\ncontexts use the same cluster and credentials, but default to different namespaces.\n Listing clusters is similar:\n$ kubectl config get-clusters\nNAME\nrpi-cluster\nminikube\nCredentials can’t be listed for security reasons.\nA.2.7Deleting contexts and clusters\nTo clean up the list of contexts or clusters, you can either delete the entries from the\nkubeconfig file manually or use the following two commands:\n$ kubectl config delete-context my-unused-context\nand\n$ kubectl config delete-cluster my-old-cluster\n \n\n539\nappendix B\nSetting up a multi-node\ncluster with kubeadm\nThis appendix shows how to install a Kubernetes cluster with multiple nodes. You’ll\nrun the nodes inside virtual machines through VirtualBox, but you can also use a\ndifferent virtualization tool or bare-metal machines. To set up both the master and\nthe worker nodes, you’ll use the \nkubeadm tool.\nB.1Setting up the OS and required packages\nFirst,  you  need  to  download  and  install  VirtualBox,  if  you  don’t  have  it  installed\nalready.  You  can  download  it  from  https://www.virtualbox.org/wiki/Downloads.\nOnce  you  have  it  running,  download  the  CentOS  7  minimal  ISO  image  from\nwww.centos.org/download.  You  can  also  use  a  different  Linux  distribution,  but\nmake sure it’s supported by checking the http://kubernetes.io website.\nB.1.1Creating the virtual machine\nNext,  you’ll  create  the  VM  for  your  Kubernetes  master.  Start  by  clicking  the  New\nicon  in  the  upper-left  corner.  Then  enter  “k8s-master”  as  the  name,  and  select\nLinux as the Type and Red Hat (64-bit) as the version, as shown in figure B.1.\n After clicking the Next button, you can set the VM’s memory size and set up the\nhard disk. If you have enough memory, select at least 2GB (keep in mind you’ll run\nthree such VMs). When creating the hard disk, leave the default options selected.\nHere’s what they were in my case:\n■\nHard disk file type: VDI (VirtualBox Disk Image)\n■\nStorage on physical hard disk: Dynamically allocated\n■\nFile location and size: k8s-master, size 8GB\n \n\n540APPENDIX BSetting up a multi-node cluster with kubeadm\nB.1.2Configuring the network adapter for the VM\nOnce  you’re  done  creating  the  VM,  you  need  to  configure  its  network  adapter,\nbecause the default won’t allow you to run multiple nodes properly. You’ll configure\nthe adapter so it uses the Bridged Adapter mode. This will connect your VMs to the\nsame network your host computer is in. Each VM will get its own IP address, the same\nway as if it were a physical machine connected to the same switch your host computer\nis  connected  to.  Other  options  are  much  more  complicated,  because  they  usually\nrequire two network adapters to be set up.\n To configure the network adapter, make sure the VM is selected in the main Virtual-\nBox window and then click the Settings icon (next to the New icon you clicked before). \n A window like the one shown in figure B.2 will appear. On the left-hand side, select\nNetwork and then, in the main panel on the right, select Attached to: Bridged Adapter,\nas  shown  in  the  figure.  In  the  Name  drop-down  menu,  select  your  host  machine’s\nadapter, which you use to connect your machine to the network.\nFigure B.1   Creating a Virtual Machine in VirtualBox\n \n\n541Setting up the OS and required packages\nB.1.3Installing the operating system\nYou’re now ready to run the VM and install the operating system. Ensure the VM is still\nselected in the list and click on the Start icon at the top of the VirtualBox main window.\nSELECTING THE START-UP DISK\nBefore the VM starts up, VirtualBox will ask you what start-up disk to use. Click the icon\nnext to the drop-down list (shown in figure B.3) and then find and select the CentOS\nISO image you downloaded earlier. Then click Start to boot up the VM.\nFigure B.2   Configuring the network adapter for the VM\nFigure B.3   Selecting \nthe installation ISO image\n \n\n542APPENDIX BSetting up a multi-node cluster with kubeadm\nINITIATING THE INSTALL\nWhen the VM starts up, a textual menu screen will appear. Use the cursor up key to\nselect the Install CentOS Linux 7 option and press the Enter button. \nSETTING INSTALLATION OPTIONS\nAfter  a  few  moments,  a  graphical  Welcome  to  CentOS  Linux  7  screen  will  appear,\nallowing you to select the language you wish to use. I suggest you keep the language\nset to English. Click the Continue button to get to the main setup screen as shown in\nfigure B.4.\nTIPWhen  you  click  into  the  VM’s  window,  your  keyboard  and  mouse  will  be\ncaptured by the VM. To release them, press the key shown at the bottom-right\ncorner of the VirtualBox window the VM is running in. This is usually the Right\nControl key on Windows and Linux or the left Command key on MacOS.\nFirst,  click  Installation  Destination  and  then  immediately  click  the  Done  button  on\nthe screen that appears (you don’t need to click anywhere else). \n Then click on Network & Host Name. On the next screen, first enable the network\nadapter by clicking the ON/OFF switch in the top right corner. Then enter the host\nFigure B.4   The main setup screen\n \n\n543Setting up the OS and required packages\nname into the field at the bottom left, as shown in figure B.5. You’re currently setting\nup the master, so set the host name to master.k8s. Click the Apply button next to the\ntext field to confirm the new host name.\nTo return to the main setup screen, click the Done button in the top-left corner.\n  You  also  need  to  set  the  correct  time  zone.  Click  Date  &  Time  and  then,  on  the\nscreen that opens, select the Region and City or click your location on the map. Return\nto the main screen by clicking the Done button in the top-left corner.\nRUNNING THE INSTALL\nTo start the installation, click the Begin Installation button in the bottom-right corner.\nA screen like the one in figure B.6 will appear. While the OS is being installed, set the\nFigure B.5   Setting the hostname and configuring the network adapter\nFigure B.6   Setting the root password while the OS is being installed and rebooting afterward\n \n\n544APPENDIX BSetting up a multi-node cluster with kubeadm\nroot password and create a user account, if you want. When the installation completes,\nclick the Reboot button at the bottom right.\nB.1.4Installing Docker and Kubernetes\nLog into the machine as root. First, you need to disable two security features: SELinux\nand the firewall.\nDISABLING SELINUX\nTo disable SELinux, run the following command:\n# setenforce 0\nBut  this  only  disables  it  temporarily  (until  the  next  reboot).  To  disable  it  perma-\nnently,  edit  the  /etc/selinux/config  file  and  change  the  \nSELINUX=enforcing  line  to\nSELINUX=permissive.\nDISABLING THE FIREWALL\nYou’ll  also  disable  the  firewall,  so  you  don’t  run  into  any  firewall-related  problems.\nRun the following command:\n# systemctl disable firewalld && systemctl stop firewalld\nRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1...\nRemoved symlink /etc/systemd/system/basic.target.wants/firewalld.service.\nADDING THE KUBERNETES YUM REPO\nTo make the Kubernetes RPM packages available to the yum package manager, you’ll\nadd a kubernetes.repo file to the /etc/yum.repos.d/ directory as shown in the follow-\ning listing.\n# cat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\n        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nEOF\nNOTEMake sure no whitespace exists after EOF if you’re copying and pasting. \nINSTALLING DOCKER, KUBELET, KUBEADM, KUBECTL, AND KUBERNETES-CNI\nNow you’re ready to install all the packages you need:\n# yum install -y docker kubelet kubeadm kubectl kubernetes-cni\nListing B.1   Adding the Kubernetes RPM repo\n \n\n545Setting up the OS and required packages\nAs you can see, you’re installing quite a few packages. Here’s what they are:\n■\ndocker—The container runtime\n■\nkubelet—The Kubernetes node agent, which will run everything for you\n■\nkubeadm—A tool for deploying multi-node Kubernetes clusters\n■\nkubectl—The command line tool for interacting with Kubernetes\n■\nkubernetes-cni—The Kubernetes Container Networking Interface\nOnce they’re all installed, you need to manually enable the \ndocker and the kubelet\nservices:\n# systemctl enable docker && systemctl start docker\n# systemctl enable kubelet && systemctl start kubelet\nENABLING THE NET.BRIDGE.BRIDGE-NF-CALL-IPTABLES KERNEL OPTION\nI’ve noticed that something disables the bridge-nf-call-iptables kernel parameter,\nwhich is required for Kubernetes services to operate properly. To rectify the problem,\nyou need to run the following two commands:\n# sysctl -w net.bridge.bridge-nf-call-iptables=1\n# echo \"net.bridge.bridge-nf-call-iptables=1\" > /etc/sysctl.d/k8s.conf\nDISABLING SWAP\nThe  Kubelet  won’t  run  if  swap  is  enabled,  so  you’ll  disable  it  with  the  following\ncommand:\n# swapoff -a &&  sed -i '/ swap / s/^/#/' /etc/fstab\nB.1.5Cloning the VM\nEverything you’ve done up to this point must be done on every machine you plan to\nuse in your cluster. If you’re doing this on bare metal, you need to repeat the process\ndescribed  in  the  previous  section  at  least  two  more  times—for  each  worker  node.  If\nyou’re building the cluster using virtual machines, now’s the time to clone the VM, so\nyou end up with three different VMs.\nSHUTTING DOWN THE VM\nTo clone the machine in VirtualBox, first shut down the VM by running the shutdown\ncommand:\n# shutdown now\nCLONING THE VM\nNow, right-click on the VM in the VirtualBox UI and select Clone. Enter the name for\nthe new machine as shown in figure B.7 (for example, k8s-node1 for the first clone or\nk8s-node2 for the second one). Make sure you check the Reinitialize the MAC address\nof  all  network  cards  option,  so  each  VM  uses  different  MAC  addresses  (because\nthey’re going to be located in the same network).\n \n\n546APPENDIX BSetting up a multi-node cluster with kubeadm\nClick  the  Next  button  and  then  make  sure  the  Full  clone  option  is  selected  before\nclicking Next again. Then, on the next screen, click Clone (leave the Current machine\nstate option selected).\n Repeat the process for the VM for the second node and then start all three VMs by\nselecting all three and clicking the Start icon. \nCHANGING THE HOSTNAME ON THE CLONED VMS\nBecause you created two clones from your master VM, all three VMs have the same host-\nname configured. Therefore, you need to change the hostnames of the two clones. To\ndo that, log into each of the two nodes (as root) and run the following command:\n# hostnamectl --static set-hostname node1.k8s\nNOTEBe sure to set the hostname to node2.k8s on the second node.\nCONFIGURING NAME RESOLUTION FOR ALL THREE HOSTS\nYou need to ensure that all three nodes are resolvable either by adding records to a\nDNS server or by editing the /etc/hosts file on all of them. For example, you need to\nadd the following three lines to the hosts file (replace the IPs with those of your VMs),\nas shown in the following listing.\n192.168.64.138 master.k8s\n192.168.64.139 node1.k8s\n192.168.64.140 node2.k8s\nListing B.2   Entries to add to /etc/hosts on each cluster node\nFigure B.7   Cloning the master VM\n \n\n547Configuring the master with kubeadm\nYou  can  get  each  node’s  IP  by  logging  into  the  node  as  root,  running  ip addr  and\nfinding the IP address associated with the \nenp0s3 network adapter, as shown in the fol-\nlowing listing.\n# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state \nUP qlen 1000\n    link/ether 08:00:27:db:c3:a4 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.64.138/24 brd 192.168.64.255 scope global dynamic enp0s3\n       valid_lft 59414sec preferred_lft 59414sec\n    inet6 fe80::77a9:5ad6:2597:2e1b/64 scope link\n       valid_lft forever preferred_lft forever\nThe command’s output in the previous listing shows that the machine’s IP address is\n192.168.64.138.  You’ll  need  to  run  this  command  on each of your nodes to get all\ntheir IPs.\nB.2Configuring the master with kubeadm\nYou’re now ready to finally set up the Kubernetes Control Plane on your master node. \nRUNNING KUBEADM INIT TO INITIALIZE THE MASTER\nThanks to the awesome kubeadm tool, all you need to do to initialize the master is run\na single command, as shown in the following listing.\n# kubeadm init\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production \nclusters.\n[init] Using Kubernetes version: v.1.8.4\n...\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  http://kubernetes.io/docs/admin/addons/\nYou can now join any number of machines by running the following on each node \nas root:\nkubeadm join --token eb3877.3585d0423978c549 192.168.64.138:6443 \n--discovery-token-ca-cert-hash \nsha256:037d2c5505294af196048a17f184a79411c7b1eac48aaa0ad137075be3d7a847\nNOTEWrite down the command shown in the last line of kubeadm init’s out-\nput. You’ll need it later.\nListing B.3   Looking up each node’s IP address\nListing B.4   Initializing the master node with kubeadm init\n \n\n548APPENDIX BSetting up a multi-node cluster with kubeadm\nKubeadm has deployed all the necessary Control Plane components, including etcd,\nthe  API  server,  Scheduler,  and  Controller  Manager.  It  has  also  deployed  the  kube-\nproxy, making Kubernetes services available from the master node. \nB.2.1Understanding how kubeadm runs the components\nAll these components are running as containers. You can use the docker ps command\nto confirm this. But \nkubeadm doesn’t use Docker directly to run them. It deploys their\nYAML descriptors to the /etc/kubernetes/manifests directory. This directory is moni-\ntored by the Kubelet, which then runs these components through Docker. The com-\nponents run as Pods. You can see them with the \nkubectl get command. But first, you\nneed to configure \nkubectl.\nRUNNING KUBECTL ON THE MASTER\nYou installed kubectl along with docker, kubeadm, and other packages in one of the\ninitial steps. But you can’t use \nkubectl to talk to your cluster without first configuring\nit through a kubeconfig file.\n Luckily, the necessary configuration is stored in the /etc/kubernetes/admin.conf\nfile. All you need to do is make \nkubectl use it by setting the KUBECONFIG environment\nvariable, as explained in appendix A:\n# export KUBECONFIG=/etc/kubernetes/admin.conf\nLISTING THE PODS\nTo test kubectl, you can list the pods of the Control Plane (they’re in the kube-system\nnamespace), as shown in the following listing.\n# kubectl get po -n kube-system\nNAME                                 READY     STATUS    RESTARTS   AGE\netcd-master.k8s                      1/1       Running   0          21m\nkube-apiserver-master.k8s            1/1       Running   0          22m\nkube-controller-manager-master.k8s   1/1       Running   0          21m\nkube-dns-3913472980-cn6kz            0/3       Pending   0          22m\nkube-proxy-qb709                     1/1       Running   0          22m\nkube-scheduler-master.k8s            1/1       Running   0          21m\nLISTING NODES\nYou’re  finished  with  setting  up  the  master,  but  you  still  need  to  set  up  the  nodes.\nAlthough  you  already  installed  the  Kubelet  on  both  of  your  two  worker  nodes  (you\neither  installed  each  node  separately  or  cloned  the  initial  VM  after  you  installed  all\nthe  required  packages),  they  aren’t  part  of  your  Kubernetes  cluster  yet.  You  can  see\nthat by listing nodes with \nkubectl:\n# kubectl get node\nNAME         STATUS     ROLES     AGE       VERSION\nmaster.k8s   NotReady   master    2m        v1.8.4\nListing B.5   System pods in the kube-system namespace\n \n\n549Configuring worker nodes with kubeadm\nSee, only the master is listed as a node. And even the master is shown as being Not-\nReady. You’ll see why later. Now, you’ll set up your two nodes.\nB.3Configuring worker nodes with kubeadm\nWhen  using  kubeadm,  configuring  worker  nodes  is  even  easier  than  configuring  the\nmaster.  In  fact,  when  you  ran  the  \nkubeadm init  command  to  set  up  your  master,  it\nalready told you how to configure your worker nodes (repeated in the next listing).\nYou can now join any number of machines by running the following on each node \nas root:\nkubeadm join --token eb3877.3585d0423978c549 192.168.64.138:6443 \n--discovery-token-ca-cert-hash \nsha256:037d2c5505294af196048a17f184a79411c7b1eac48aaa0ad137075be3d7a847\nAll you need to do is run the kubeadm join command with the specified token and the\nmaster’s IP address/port on both of your nodes. It then takes less than a minute for\nthe nodes to register themselves with the master. You can confirm they’re registered\nby running the \nkubectl get node command on the master again:\n# kubectl get nodes\nNAME         STATUS     ROLES     AGE       VERSION\nmaster.k8s   NotReady   master    3m        v1.8.4\nnode1.k8s    NotReady   <none>    3s        v1.8.4\nnode2.k8s    NotReady   <none>    5s        v1.8.4\nOkay, you’ve made progress. Your Kubernetes cluster now consists of three nodes, but\nnone of them are ready. Let’s investigate.\n Let’s use the \nkubectl describe command in the following listing to see more\ninformation.  Somewhere  at  the  top,  you’ll  see  a  list  of  \nConditions,  showing  the\ncurrent conditions on the node. One of them will show the following \nReason and\nMessage.\n# kubectl describe node node1.k8s\n...\nKubeletNotReady    runtime network not ready: NetworkReady=false \n                   reason:NetworkPluginNotReady message:docker: \n                   network plugin is not ready: cni config uninitialized\nAccording to this, the Kubelet isn’t fully ready, because the container network (CNI)\nplugin  isn’t  ready,  which  is  expected,  because  you  haven’t  deployed  the  CNI  plugin\nyet. You’ll deploy one now.\nListing B.6   Last part of the output of the kubeadm init command\nListing B.7   Kubectl describe shows why the node isn’t ready\n \n\n550APPENDIX BSetting up a multi-node cluster with kubeadm\nB.3.1Setting up the container network\nYou’ll install the Weave Net container networking plugin, but several alternatives are\nalso available. They’re listed among the available Kubernetes add-ons at http://kuber-\nnetes.io/docs/admin/addons/.\n Deploying the Weave Net plugin (like most other add-ons) is as simple as this:\n$ kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl \nversion | base64 | tr -d '\\n')\nThis will deploy a DaemonSet and a few security-related resources (refer to chapter 12\nfor  an  explanation  of  the  ClusterRole  and  ClusterRoleBinding,  which  are  deployed\nalongside the DaemonSet).\n  Once  the  DaemonSet  controller  creates  the  pods  and  they’re  started  on  all  your\nnodes, the nodes should become ready:\n# k get node\nNAME         STATUS    ROLES     AGE       VERSION\nmaster.k8s   Ready     master    9m        v1.8.4\nnode1.k8s    Ready     <none>    5m        v1.8.4\nnode2.k8s    Ready     <none>    5m        v1.8.4\nAnd that’s it. You now have a fully functioning three-node Kubernetes cluster with an\noverlay network provided by Weave Net. All the required components, except for the\nKubelet itself, are running as pods, managed by the Kubelet, as shown in the follow-\ning listing.\n# kubectl get po --all-namespaces\nNAMESPACE     NAME                                 READY     STATUS    AGE\nkube-system   etcd-master.k8s                      1/1       Running   1h\nkube-system   kube-apiserver-master.k8s            1/1       Running   1h\nkube-system   kube-controller-manager-master.k8s   1/1       Running   1h\nkube-system   kube-dns-3913472980-cn6kz            3/3       Running   1h\nkube-system   kube-proxy-hcqnx                     1/1       Running   24m\nkube-system   kube-proxy-jvdlr                     1/1       Running   24m\nkube-system   kube-proxy-qb709                     1/1       Running   1h\nkube-system   kube-scheduler-master.k8s            1/1       Running   1h\nkube-system   weave-net-58zbk                      2/2       Running   7m\nkube-system   weave-net-91kjd                      2/2       Running   7m\nkube-system   weave-net-vt279                      2/2       Running   7m\nB.4Using the cluster from your local machine\nUp to this point, you’ve used kubectl on the master node to talk to the cluster. You’ll\nprobably want to configure the \nkubectl instance on your local machine, too. \n To do that, you need to copy the /etc/kubernetes/admin.conf file from the mas-\nter to your local machine with the following command:\n$ scp root@192.168.64.138:/etc/kubernetes/admin.conf ~/.kube/config2\nListing B.8   System pods in the kube-system namespace after deploying Weave Net\n \n\n551Using the cluster from your local machine\nReplace the IP with that of your master. Then you point the KUBECONFIG environment\nvariable to the ~/.kube/config2 file like this:\n$ export KUBECONFIG=~/.kube/config2\nKubectl will now use this config file. To switch back to using the previous one, unset\nthe environment variable. \n You’re now all set to use the cluster from your local machine.\n \n\n552\nappendix C\nUsing other container\nruntimes\nC.1Replacing Docker with rkt\nWe’ve mentioned rkt (pronounced rock-it) a few times in this book. Like Docker, it\nruns  applications  in  isolated  containers,  using  the  same  Linux  technologies  as\nthose used by Docker. Let’s look at how rkt differs from Docker and how to try it in\nMinikube.\n  The  first  great  thing  about  rkt  is  that  it  directly  supports  the  notion  of  a  Pod\n(running  multiple  related  containers),  unlike  Docker,  which  only  runs  individual\ncontainers. Rkt is based on open standards and was built with security in mind from\nthe start (for example, images are signed, so you can be sure they haven’t been tam-\npered  with).  Unlike  Docker,  which  initially  had  a  client-server  based  architecture\nthat  didn’t  play  well  with  init  systems  such  as  systemd,  rkt  is  a  CLI  tool  that  runs\nyour container directly, instead of telling a daemon to run it. A nice thing about rkt\nis that it can run existing Docker-formatted container images, so you don’t need to\nrepackage your applications to get started with rkt.\nC.1.1Configuring Kubernetes to use rkt\nAs  you  may  remember  from  chapter  11,  the  Kubelet  is  the  only  Kubernetes\ncomponent  that  talks  to  the  Container  Runtime.  To  get  Kubernetes  to  use  rkt\ninstead of Docker, you need to configure the Kubelet to use it by running it with\nthe \n--container-runtime=rkt  command-line  option.  But  be  aware  that  support\nfor rkt isn’t as mature as support for Docker. \n Please refer to the Kubernetes documentation for more information on how to\nuse rkt and what is or isn’t supported. Here, we’ll go over a quick example to get\nyou started.\n \n\n553Replacing Docker with rkt\nC.1.2Trying out rkt with Minikube\nLuckily, to get started with rkt on Kubernetes, all you need is the same Minikube exe-\ncutable you’re already using. To use rkt as the container runtime in Minikube, all you\nneed to do is start Minikube with the following two options:\n$ minikube start --container-runtime=rkt --network-plugin=cni \nNOTEYou may need to run minikube delete to delete the existing Minikube\nVM first. \nThe \n--container-runtime=rkt option obviously configures the Kubelet to use rkt as\nthe  Container  Runtime,  whereas  the  \n--network-plugin=cni  makes  it  use  the  Con-\ntainer Network Interface as the network plugin. Without this option, pods won’t run,\nso it’s imperative you use it.\nRUNNING A POD\nOnce  the  Minikube  VM  is  up,  you  can  interact  with  Kubernetes  exactly  like  before.\nYou can deploy the kubia app with the \nkubectl run command, for example:\n$ kubectl run kubia --image=luksa/kubia --port 8080\ndeployment \"kubia\" created\nWhen the pod starts up, you can see it’s running through rkt by inspecting its contain-\ners with \nkubectl describe, as shown in the following listing.\n$ kubectl describe pods\nName:           kubia-3604679414-l1nn3\n...\nStatus:         Running\nIP:             10.1.0.2\nControllers:    ReplicaSet/kubia-3604679414\nContainers:\n  kubia:\n    Container ID:       rkt://87a138ce-...-96e375852997:kubia    \n    Image:              luksa/kubia\n    Image ID:           rkt://sha512-5bbc5c7df6148d30d74e0...    \n...\nYou  can  also  try  hitting  the  pod’s  HTTP  port  to  see  if  it’s  responding  properly  to\nHTTP requests. You can do this by creating a \nNodePort Service or by using kubectl\nport-forward\n, for example. \nINSPECTING THE RUNNING CONTAINERS IN THE MINIKUBE VM\nTo get more familiar with rkt, you can try logging into the Minikube VM with the fol-\nlowing command:\n$ minikube ssh\nListing C.1   Pod running with rkt\nThe container \nand image IDs \nmention rkt \ninstead of \nDocker.\n \n\n554APPENDIX CUsing other container runtimes\nThen, you can use rkt list to see the running pods and containers, as shown in the\nfollowing listing.\n$ rkt list\nUUID      APP                 IMAGE NAME                       STATE   ...\n4900e0a5  k8s-dashboard       gcr.io/google_containers/kun...  running ...\n564a6234  nginx-ingr-ctrlr    gcr.io/google_containers/ngi...  running ...\n5dcafffd  dflt-http-backend   gcr.io/google_containers/def...  running ...\n707a306c  kube-addon-manager  gcr.io/google-containers/kub...  running ...\n87a138ce  kubia               registry-1.docker.io/luksa/k...  running ...\nd97f5c29  kubedns             gcr.io/google_containers/k8s...  running ...\n          dnsmasq             gcr.io/google_containers/k8...            \n          sidecar             gcr.io/google_containers/k8...\nYou can see the kubia container, as well as other system containers running (the ones\ndeployed  in  pods  in  the  \nkube-system  namespace).  Notice  how  the  bottom  two  con-\ntainers don’t have anything listed in the \nUUID or STATE columns? That’s because they\nbelong to the same pod as the \nkubedns container listed above them. \n  Rkt  prints  containers  belonging  to  the  same  pod  grouped  together.  Each  pod\n(instead of each container) has its own UUID and state. If you tried doing this when\nyou were using Docker as the Container Runtime, you’ll appreciate how much easier\nit  is  to  see  all  the  pods  and  their  containers  with  rkt.  You’ll  notice  no  infrastructure\ncontainer  exists  for  each  pod  (we  explained  them  in  chapter  11).  That’s  because  of\nrkt’s native support for pods.\nLISTING CONTAINER IMAGES\nIf you’ve played around with Docker CLI commands, you’ll get familiar quickly with\nrkt’s commands. Run \nrkt without any arguments and you’ll see all the commands you\ncan run. For example, to list container images, you run the command in the follow-\ning listing.\n$ rkt image list\nID           NAME                          SIZE    IMPORT TIME  LAST USED\nsha512-a9c3  ...addon-manager:v6.4-beta.1  245MiB  24 min ago   24 min ago\nsha512-a078  .../rkt/stage1-coreos:1.24.0  224MiB  24 min ago   24 min ago\nsha512-5bbc  ...ker.io/luksa/kubia:latest  1.3GiB  23 min ago   23 min ago\nsha512-3931  ...es-dashboard-amd64:v1.6.1  257MiB  22 min ago   22 min ago\nsha512-2826  ...ainers/defaultbackend:1.0  15MiB   22 min ago   22 min ago\nsha512-8b59  ...s-controller:0.9.0-beta.4  233MiB  22 min ago   22 min ago\nsha512-7b59  ...dns-kube-dns-amd64:1.14.2  100MiB  21 min ago   21 min ago\nsha512-39c6  ...nsmasq-nanny-amd64:1.14.2  86MiB   21 min ago   21 min ago\nsha512-89fe  ...-dns-sidecar-amd64:1.14.2  85MiB   21 min ago   21 min ago\nThese are all Docker-formatted container images. You can also try building images in\nthe  OCI  image  format  (OCI  stands  for  Open  Container  Initiative)  with  the  acbuild\nListing C.2   Listing running containers with rkt list\nListing C.3   Listing images with rkt image list\n \n\n555Using other container runtimes through the CRI\ntool  (available  at  https://github.com/containers/build)  and  running  them  with  rkt.\nDoing that is outside the scope of this book, so I’ll let you try it on your own.\n  The  information  explained  in  this  appendix so far should be enough to get you\nstarted  using  rkt  with  Kubernetes.  Refer  to  the  rkt  documentation  at  https://coreos\n.com/rkt  and  Kubernetes  documentation  at  https://kubernetes.io/docs  for  addi-\ntional information.\nC.2Using other container runtimes through the CRI\nKubernetes’ support for other container runtimes doesn’t stop with Docker and rkt.\nBoth  of  those  runtimes  were  initially  integrated  directly  into  Kubernetes,  but  in\nKubernetes version 1.5, the Container Runtime Interface (CRI) was introduced. CRI\nis a plugin API enabling easy integration of other container runtimes with Kuberne-\ntes.  People  are  now  free  to  plug  other  container  runtimes  into  Kubernetes  without\nhaving to dig deep into Kubernetes code. All they need to do is implement a few inter-\nface methods. \n From Kubernetes version 1.6 onward, CRI is the default interface the Kubelet uses.\nBoth Docker and rkt are now used through the CRI (no longer directly).\nC.2.1Introducing the CRI-O Container Runtime\nBeside Docker and rkt, a new CRI implementation called CRI-O allows Kubernetes to\ndirectly  launch  and  manage  OCI-compliant  containers,  without  requiring  you  to\ndeploy any additional Container Runtime. \n You can try CRI-O with Minikube by starting it with \n--container-runtime=crio.\nC.2.2Running apps in virtual machines instead of containers \nKubernetes  is  a  container  orchestration  system,  right?  Throughout  the  book,  we\nexplored many features that show that it’s much more than an orchestration system,\nbut the bottom line is that when you run an app with Kubernetes, the app always runs\ninside a container, right? You may find it surprising that’s no longer the case.\n New CRI implementations are being developed that allow Kubernetes to run apps\nin virtual machines instead of in containers. One such implementation, called Frakti,\nallows you to run regular Docker-based container images directly through a hypervi-\nsor,  which  means  each  container  runs  its  own  kernel.  This  allows  much  better  isola-\ntion between containers compared to when they use the same kernel. \n  And  there’s  more.  Another  CRI  implementation  is  the  Mirantis  Virtlet,  which\nmakes it possible to run actual VM images (in the QCOW2 image file format, which is\none  of  the  formats  used  by  the  QEMU  virtual  machine  tool)  instead  of  container\nimages.  When  you  use  the  Virtlet  as  the  CRI  plugin,  Kubernetes  spins  up  a  VM  for\neach pod. How awesome is that?\n \n\n556\nappendix D\nCluster Federation\nIn  the  section  about  high  availability  in  chapter  11  we  explored  how  Kubernetes\ncan deal with failures of individual machines and even failures of whole server racks\nor the supporting infrastructure. But what if the whole datacenter goes dark?\n To make sure you’re not susceptible to datacenter-wide outages, apps should be\ndeployed  in  multiple  datacenters  or  cloud  availability  zones.  When  one  of  those\ndatacenters or availability zones becomes unavailable, client requests can be routed\nto the apps running in the remaining healthy datacenters or zones.\n While Kubernetes doesn’t require you to run the Control Plane and the nodes\nin the same datacenter, you’ll almost always want to do that to keep network latency\nbetween  them  low  and  to  reduce  the  possibility  of  them  becoming  disconnected\nfrom each other. Instead of having a single cluster spread across multiple locations,\na  better  alternative  is  to  have  an  individual  Kubernetes  cluster  in  every  location.\nWe’ll explore this approach in this appendix.\nD.1Introducing Kubernetes Cluster Federation\nKubernetes  allows  you  to  combine  multiple  clusters  into  a  cluster  of  clusters\nthrough Cluster Federation. It allows users to deploy and manage apps across mul-\ntiple clusters running in different locations in the world, but also across different\ncloud  providers  combined  with  on-premises  clusters  (hybrid  cloud).  The  motiva-\ntion  for  Cluster  Federation  isn’t  only  to  ensure  high  availability,  but  also  to  com-\nbine multiple heterogeneous clusters into a single super-cluster managed through\na single management interface. \n For example, by combining an on-premises cluster with one running on a cloud\nprovider’s infrastructure, you can run privacy-sensitive components of your applica-\ntion  system  on-premises,  while  the  non-sensitive  parts  can  run  in  the  cloud.\nAnother example is initially running your application only in a small on-premises\ncluster,  but  when  the  application’s  compute  requirements  exceed  the  cluster’s\n \n\n557Understanding the architecture\ncapacity, letting the application spill over to a cloud-based cluster, which is automati-\ncally provisioned on the cloud provider’s infrastructure.\nD.2Understanding the architecture\nLet’s take a quick look at what Kubernetes Cluster Federation is. A cluster of clusters\ncan be compared to a regular cluster where instead of nodes, you have complete clus-\nters.  Just  as  a  Kubernetes  cluster  consists  of  a  Control  Plane  and  multiple  worker\nnodes, a federated cluster consists of a Federated Control Plane and multiple Kuber-\nnetes  clusters.  Similar  to  how  the  Kubernetes  Control  Plane  manages  applications\nacross  a  set  of  worker  nodes,  the  Federated  Control  Plane  does  the  same  thing,  but\nacross a set of clusters instead of nodes. \n The Federated Control Plane consists of three things:\n■\netcd for storing the federated API objects\n■\nFederation API server\n■\nFederation Controller Manager\nThis isn’t much different from the regular Kubernetes Control Plane. etcd stores the\nfederated API objects, the API server is the REST endpoint all other components talk\nto,  and  the  Federation  Controller  Manager  runs  the  various  federation  controllers\nthat perform operations based on the API objects you create through the API server. \n Users talk to the Federation API server to create federated API objects (or feder-\nated  resources).  The  federation  controllers  watch  these  objects  and  then  talk  to  the\nunderlying clusters’ API servers to create regular Kubernetes resources. The architec-\nture of a federated cluster is shown in figure D.1.\nSan Francisco\nControl Plane\netcd\nFederation Controller Manager\nController\nManager\nAPI server\netcd\nFederation\nAPI server\nScheduler\nWorker node\nKubelet\nWorker node\nKubelet\nWorker node\nKubelet\nControl Plane\netcd\nController\nManager\nAPI server\nScheduler\nWorker node\nKubelet\nWorker node\nKubelet\nWorker node\nKubelet\nLondon\nOther\nlocations\nFigure D.1   Cluster Federation with clusters in different geographical locations\n \n\n558APPENDIX ACluster Federation\nD.3Understanding federated API objects\nThe  federated  API  server  allows  you  to  create  federated  variants  of  the  objects  you\nlearned about throughout the book. \nD.3.1Introducing federated versions of Kubernetes resources\nAt the time of writing this, the following federated resources are supported:\n■\nNamespaces\n■\nConfigMaps and Secrets\n■\nServices and Ingresses\n■\nDeployments, ReplicaSets, Jobs, and Daemonsets\n■\nHorizontalPodAutoscalers\nNOTECheck the Kubernetes Cluster Federation documentation for an up-to-\ndate list of supported federated resources.\nIn  addition  to  these  resources,  the  Federated  API  server  also  supports  the  Cluster\nobject, which represents an underlying Kubernetes cluster, the same way a Node object\nrepresents a worker node in a regular Kubernetes cluster. To help you visualize how fed-\nerated objects relate to the objects created in the underlying clusters, see figure D.2.\nSan Francisco\nNamespace: foo\nReplicaSet X-432\nReplicas: 3\nPod X-432-5\nDeployment X\nReplicas:3\nFederated resources\nSecret W\nSecret Y\nNamespace: foo\nDeployment X\nReplicas:5\nSecret Y\nLondon\nNamespace: foo\nReplicaSet X-432\nReplicas: 2\nPod X-432-8\nDeployment X\nReplicas:2\nSecret W\nSecret Y\nCluster:\nLondon\nCluster:\nSan Francisco\nSecret WIngress Z\nFigure D.2   The relationship between federated resources and regular resources in underlying clusters\n \n\n559Understanding federated API objects\nD.3.2Understanding what federated resources do\nFor  part  of  the  federated  objects,  when  you  create  the  object  in  the  Federation  API\nserver, the controllers running in the Federation Controller Manager will create regu-\nlar  cluster-scoped  resources  in  all  underlying  Kubernetes  clusters  and  manage  them\nuntil the federated object is deleted. \n For certain federated resource types, the resources created in the underlying clus-\nters are exact replicas of the federated resource; for others, they’re slightly modified\nversions,  whereas  certain  federated  resources  don’t  cause  anything  to  be  created  in\nthe underlying clusters at all. The replicas are kept in sync with the original federated\nversions. But the synchronization is one-directional only—from the federation server\ndown to the underlying clusters. If you modify the resource in an underlying cluster,\nthe changes will not be synced up to the Federation API server.\n For example, if you create a namespace in the federated API server, a namespace\nwith the same name will be created in all underlying clusters. If you then create a fed-\nerated ConfigMap inside that namespace, a ConfigMap with that exact name and con-\ntents will be created in all underlying clusters, in the same namespace. This also applies\nto Secrets, Services, and DaemonSets.\n  ReplicaSets  and  Deployments  are  different.  They  aren’t  blindly  copied  to  the\nunderlying clusters, because that’s not what the user usually wants. After all, if you cre-\nate a Deployment with a desired replica count of 10, you probably don’t want 10 pod\nreplicas running in each underlying cluster. You want 10 replicas in total. Because of\nthis, when you specify a desired replica count in a Deployment or ReplicaSet, the Fed-\neration  controllers  create  underlying  Deployments/ReplicaSets  so  that  the  sum  of\ntheir desired replica counts equals the desired replica count specified in the federated\nDeployment or ReplicaSet. By default, the replicas are spread evenly across the clus-\nters, but this can be overridden.\nNOTECurrently,  you  need  to  connect  to  each  cluster’s  API  server  individu-\nally to get the list of pods running in that cluster. You can’t list all the clusters’\npods through the Federated API server.\nA federated Ingress resource, on the other hand, doesn’t result in the creation of any\nIngress objects in the underlying clusters. You may remember from chapter 5 that an\nIngress represents a single point of entry for external clients to a Service. Because of\nthis, a federated Ingress resource creates a global, multi-cluster-wide entry point to the\nServices across all underlying clusters. \nNOTEAs  for  regular  Ingresses,  a  federated  Ingress  controller  is  required\nfor this. \nSetting up federated Kubernetes clusters is outside the scope of this book, so you can\nlearn  more  about  the  subject  by  referring  to  the  Cluster  Federation  sections  in  the\nuser  and  administration  guides  in  the  Kubernetes  online  documentation  at  http://\nkubernetes.io/docs/.\n \n\n \n\n561\nindex\nSymbols\n$(ENV_VAR) syntax199\n$(ENV_VARIABLE_NAME) \nsyntax205\n$(VAR) syntax198\n* (asterisk) character117\n- - (double dash)125\nNumerics\n137 exit code88\n143 exit code89\n8080 port48, 67\n8888 port67\nA\n-a option35\nABAC (Attribute-based access \ncontrol)353\naccess controls. See RBAC\naccounts. See service accounts\nactions353–354\nactiveDeadlineSeconds \nproperty116\nad hoc tasks112\nadapters\nnetwork, configuring for \nVMs  540\nnode networks, shutting \ndown  304–305\nADDED watch event514\nadd-ons328–330\ncomponents  310\ndeploying  328–329\nDNS servers  329\nIngress controllers  329\nusing  329–330\naddresses attribute138\naddresses, of API servers239–240\nadmin ClusterRole, granting full \ncontrol of namespaces \nwith372\nalgorithms, default \nscheduling319\naliases\ncreating for external \nservices  134\nfor kubectl  41–42\nall keyword82\n--all option81\n--all-namespaces option144\nallocatable resources408\nallowed capabilities, \nconfiguring394–395\nadding capabilities to all \ncontainers  395\ndropping capabilities from \ncontainers  395\nspecifying which capabilities \ncan be added to \ncontainers  394\nallowedCapabilities394\nAlwaysPullImages317\nAmazon Web Services. See AWS\nambassador containers\ncommunicating with API servers \nthrough  245\npatterns  244\nrunning curl pods with  244–245\nsimplifying API server commu-\nnication with  243–245\nannotations\nadding  76\ndescribing resources through\n498\nmodifying  76\nof objects, looking up  75–76\nupdating  232\nAPI (application program inter-\nface)\naggregation  518–519\ncustom, providing for custom \nobjects  518–519\ndefining custom objects  508–519\nautomating custom resources \nwith custom \ncontrollers  513–517\nCRD  509–513\nproviding custom API \nservers for custom \nobjects  518–519\nvalidating custom \nobjects  517–518\nfederated objects  558–559\nService Catalog  521\nSee also OpenServiceBroker API\nAPI servers233–248, 316–318, \n346–374\naccessing through kubectl \nproxy  234–235\nauthentication of  346–353\ngroups  346–348\nservice accounts  348–353\nusers  347–348\nauthorizing clients  317\ncommunicating from within \npods  238–243\ncommunicating with pods \nthrough  295–297\ncommunicating with, through \nambassador containers  245\n \n\nINDEX562\nAPI servers (continued)\nconnecting to  503\nconnecting to cluster-internal \nservices through  299\nexploring  235–236, 248\nfinding addresses  239–240\nmodifying resources in \nrequests  317\nnotifying clients of resource \nchanges  318–319\npersistently storing \nresources  318\nrunning multiple instances \nof  343\nrunning static pods \nwithout  326–327\nsecuring clusters with \nRBAC  353–373\nbinding roles to service \naccounts  359–360\ndefault ClusterRoleBindings\n371–373\ndefault ClusterRoles  371–373\ngranting authorization \npermissions  373\nincluding service accounts \nfrom other namespaces \nin RoleBinding  361\nRBAC authorization \nplugins  353–354\nRBAC resources  355–357\nusing ClusterRoleBindings\n362–371\nusing ClusterRoles  362–371\nusing RoleBindings  358–359\nusing Roles  358–359\nsimplifying communication\n243–245\nambassador container \npatterns  244\nrunning curl pod  244–245\nusing client libraries to commu-\nnicate with  246–248\nbuilding libraries with \nOpenAPI  248\nbuilding libraries with \nSwagger  248\ninteracting with FABRIC8 \nJava client  247–248\nusing existing client \nlibraries  246–247\nusing custom service account \ntokens to communicate \nwith  352–353\nvalidating resources  318\nverifying identity of  240–241\nSee also\n REST API\napiVersion property106, 358, 510\napp=kubia label98, 123\napp=pc label selector72\napplication logs65–66\napplication program interface. See \nAPI\napplication templates, in Red Hat \nOpenShift Container \nplatform528–529\napplications\naccessing  32\nbest practices for \ndeveloping  477, 502, \n506–507\nauto-deploying resource \nmanifests  504–505\nemploying CI/CD  506\nensuring client requests are \nhandled properly  492–\n497\nKsonnet as alternative to \nwriting JSON/YAML \nmanifests  505–506\npod lifecycles  479–491\nusing Minikube  503–504\nversioning resource \nmanifests  504–505\ncompromised  373\ncontainerized, \nconfiguring  191–192\ncreating  290–291\ncreating versions of  268\ndeploying through \nStatefulSets  291–295\ncreating governing \nservices  292–294\ncreating persistent \nvolumes  291–292\ncreating StatefulSets  294\nexamining PersistentVolume-\nClaims  295\nexamining stateful pods\n294–295\ndescribing resources through \nannotations  498\ndescriptions, effect on running \ncontainers  19–20\nexamining nodes  51–52\ndisplaying pod IP when list-\ning pods  51\ndisplaying pod node when \nlisting pods  51\ninspecting details of pod with \nkubectl describe  52\nexposing through services using \nsingle YAML file  255\nhandling logs  500–502\ncopying files to and from \ncontainers  500–501\ncopying logs to and from \ncontainers  500–501\nhandling multi-line log \nstatements  502\nusing centralized \nlogging  501\nhighly available  341\nhorizontally scaling  48–50\nincreasing replica count  49\nrequests hitting three pods \nwhen hitting services  50\nresults of scale-out  49–50\nvisualizing new states  50\nimplementing shutdown han-\ndlers in  490–491\nin containers, limits as seen \nby  415–416\nkilling  479–482\nexpecting data written to disk \nto disappear  480\nexpecting hostnames to \nchange  480\nexpecting local IP to \nchange  480\nusing volumes to preserve \ndata across container \nrestarts  480–482\nmaking container images  497\nmanaging  497–502\nmonolithic, vs. microservices\n3–6\nmulti-tier, splitting into multiple \npods  59\nNode.js\ncreating  28–29\ndeploying  42–44\nnon-horizontally scalable, using \nleader-election for  341\noverview  478–479\nproviding consistent environ-\nment to  6\nproviding information about pro-\ncess termination  498–500\nrelocating  479–482\nexpecting data written to disk \nto disappear  480\nexpecting hostnames to \nchange  480\nexpecting local IP to \nchange  480\nusing volumes to preserve \ndata across container \nrestarts  480–482\nrunning  19–21, 497–502\nin VMs instead of \ncontainers  555\nkeeping containers \nrunning  20–\n21\n \n\nINDEX563\napplications (continued)\nlocating containers  21\noutside of Kubernetes during \ndevelopment  502–503\nscaling number of copies  21\nthrough services using single \nYAML file  255\nshut-down procedures  496–497\nsplitting into microservices  3–4\ntagging images  497–498\nupdating declaratively using \nDeployment  261–278\nblocking rollouts of bad \nversions  274–278\ncontrolling rate of \nrollout  271–273\ncreating Deployments\n262–264\npausing rollout process\n273–274\nrolling back \ndeployments  268–270\nupdating Deployments\n264–268\nusing imagePullPolicy  497–498\nusing multi-dimensional labels \nvs. single-dimensional \nlabels  498\nusing pre-stop hooks when not \nreceiving SIGTERM \nsignal  488–489\nargs array196\narguments\ndefining in Docker  193–195\nCMD instruction  193\nENTRYPOINT instruction\n193\nmaking INTERVAL configu-\nrable in fortune images\n194–195\nshell forms vs. exec \nforms  193–194\noverriding in Kubernetes\n195–196\nSee also command-line argu-\nments\nasterisks117\nat-most-one semantics290\nAttribute-based access control. See \nABAC\nauthenticating\nAPI servers  346–353\ngroups  347–348\nservice accounts  348, \n351–\n353\nusers  347–348\nclients with authentication \nplugins  317\ncreating Secrets for, with \nDocker registries  223\nwith API servers  241–242\nauthorization plugins, RBAC\n353–354\nauthorizations\nclients with plugins for  317\ngranting permissions  373\nservice accounts tying into  349\nauto-deploying, resource \nmanifests504–505\nautomatic scaling23\nautomating custom resources \nwith custom controllers\n513–517\nrunning controllers as \npods  515–516\nWebsite controllers  514–515\nAutoscaler, using to scale up \nDeployments446–447\nautoscaling\nmetrics appropriate for  450\nprocess of  438–441\ncalculating required number \nof pods  439\nobtaining pod metrics\n438–439\nupdating replica count on \nscaled resources  440\nSee also horizontal autoscaling\navailability zones\nco-locating pods in same  471\ndeploying pods in same\n471–472\navailability-zone label467\nAWS (Amazon Web Services)37, \n174, 454\nawsElasticBlockStore volume162, \n174\nazureDisk volume162, 174\nazureFile volume174\nB\nbackend services, connecting \nto502\nbackend-database129–130\nbase images29\nbash process33\nbatch API groups, REST \nendpoints236–237\nBestEffort class, assigning pods \nto417\nbinary data, using Secrets for217\nbinding\nRoles to service accounts\n359–360\nservice instances  525\nto host ports without using \nhost network namespaces\n377–379\nblocking rollouts274–278\nconfiguring deadlines for \nrollouts  278\ndefining readiness probes to \nprevent rollouts  275\nminReadySeconds  274–275\npreventing rollouts with readi-\nness probes  277–278\nupdating deployments with \nkubectl apply  276–277\nblue-green deployment253\nBorg16\nbrokers. See service brokers\nBuildConfigs, building images \nfrom source using529\nBurstable QoS class, assigning to \npods418\nbusybox image26, 112\nC\n-c option66, 245\nCA (certificate authority)170, \n240\n--cacert option240\ncAdvisor431\ncanary release69, 273\ncapabilities\nadding to all containers  395\ndropping from containers\n385–386, 395\nkernel, adding to \ncontainers  384–385\nspecifying which can be added \nto containers  394\nSee also allowed capabilities\ncapacity, of nodes407–408\nCAP_CHOWN capability385, 395\nCAP_SYS_TIME capability384\ncategorizing worker nodes with \nlabels74\nCentOS ISO image541\ncentral processing units. See CPUs\ncephfs volume163\ncert files, from Secrets221\ncertificate-authority-data field536\ncertificates, TLS147–149\nCI/CD (Continuous Integration \nand Continuous \nDelivery)506\ncinder volume163\nclaiming\nbenefits of  182\nPersistentVolumes  179–181\nclient binaries, downloading39\n \n\nINDEX564\nclient libraries, communicating \nwith API servers through\n246–248\nbuilding libraries with \nOpenAPI  248\nbuilding libraries with \nSwagger  248\nexploring API servers with \nSwagger UI  248\ninteracting with FABRIC8 Java \nclient  247–248\nusing existing client \nlibraries  246–247\nclient-certificate-data \nproperty536\nclient-key-data property536\nclients\nauthenticating  317\nauthorizing  317\ncustom, creating  519\nhandling requests  492–497\nnon-preservation of IP  142\nnotifying of resource \nchanges  318–319\npods, using newly created \nSecrets in  525–526\npreventing broken \nconnections  492–497\nsequence of events at pod \ndeletion  493–495\ntroubleshooting  495–496\nSee also external clients\ncloning VMs545–547\nchanging hostname on  546\nconfiguring name resolution for \nhosts  546–547\nshutting down  545\nCloud infrastructure, requesting \nnodes from452–453\nCluster Autoscaler452–453\nenabling  454\nrelinquishing nodes  453\nrequesting additional nodes \nfrom Cloud infrastructure\n452–453\ncluster events, observing332–333\nCluster Federation556–559\narchitecture of  557\nfederated API objects  558–559\noverview  556–557\ncluster nodes\ndisplaying CPU usage for  431\ndisplaying memory usage \nfor  431\nhorizontal scaling of  452–456\nsecuring  375–403\nconfiguring container secu-\nrity contexts  380–389\nisolating pod networks\n399–402\nrestricting use of security-\nrelated features  389–399\nusing host node namespaces \nin pods  376–380\nclustered data stores303–304\ncluster-internal services, connect-\ning through API servers299\nclusterIP field154, 293\nClusterIP service45\ncluster-level resources, allowing \naccess to362–365\nClusterRoleBindings\ncombing with \nClusterRoles  370–371\ncombining with Role \nresources  370–371\ncombining with \nRoleBindings  370–371\ndefault  371–373\nusing  362–371\nallowing access to cluster-level \nresources  362–365\nallowing access to non-\nresource URLs  365–367\nClusterRoles362–371\nallowing access to cluster-level \nresources  362–365\nallowing access to non-resource \nURLs  365–367\ncombining with ClusterRole-\nBindings  370–371\ncombining with \nRoleBindings  370–371\ncombining with Roles  370–371\ndefault  371–373\nallowing modifying \nresources  372\nallowing read-only access to \nresources  372\ngranting full control of \nnamespaces  372\nusing to grant access to \nresources in specific \nnamespaces  367–370\nclusters\nadding  536\ncombining Minikube with  504\nconfirming communication \nwith kubectl  38\nconnecting to services living \noutside of  131–134\ncreating alias for external \nservices  134\nservice endpoints  131–134\ncreating with three nodes  39\ndeleting  538\nenabling RBAC resources \nin  356\netcd, running  342–343\nhighly available, running\n341–345\nmaking applications highly \navailable  341\nmaking Control Plane com-\nponents highly \navailable  342–345\nhosted, using with GKE  38–41\nin Docker  36–42\nrunning local single-node \nclusters with Minikube\n37–38\nsetting up aliases for \nkubectl  41–42\nsetting up command-line \ncompletion for \nkubectl  41–42\nin kubeconfig files  536\nKubernetes, architecture of\n18–19\nlimiting service disruption during \nscale-down of  454–456\nlisting  538\nlisting job instances in  237–238\nlisting nodes  40\nlisting services available in  523\nlocal single-node, running with \nMinikube  37–38\nmodifying  536\nmulti-node with kubeadm\n539–551\nconfiguring masters  547–549\nconfiguring worker \nnodes  549–550\nsetting up operating \nsystems  539\nsetting up required \npackages  539\noverview of  39\nrunning Grafana in  433\nrunning InfluxDB in  433\nsecuring with RBAC  353–373\nbinding roles to service \naccounts  359–360\ndefault ClusterRoleBindings\n371–373\ndefault ClusterRoles  371–373\ngranting authorization \npermissions  373\nincluding service accounts \nfrom other \nnamespaces  361\nRBAC authorization \nplugins  353–354\nRBAC resources  355–357\n \n\nINDEX565\nclusters (continued)\nstarting with Minikube  37–38\ntesting services from within  124\ntwo-node, deploying pods \nin  468\ntying with user credentials  537\nusing from local machines\n550–551\nusing kubectl with \nmultiple  534–538\nadding kube config \nentries  536–537\nconfiguring location of \nkubeconfig files  535\ncontents of kubeconfig \nfiles  535–536\ndeleting contexts  538\nlisting contexts  538\nlisting kube config \nentries  536–537\nmodifying kube config \nentries  536–537\nswitching between \ncontexts  538\nswitching between Minikube \nand GKE  534–535\nSee also ClusterRoleBindings; \nClusterRoles; RoleBind-\nings; Roles; cluster nodes\nCMD instruction193\nCNI (Container Network \nInterface)335, 338, 549\ncollecting resource usages\n430–432\nCOMMAND column334\ncommand-line arguments\npassing ConfigMap entries \nas  204–205\npassing to containers  192–196\ncommand-line completion, for \nkubectl41–42\ncommands\ndefining in Docker  193–195\nCMD instruction  193\nENTRYPOINT instruction\n193\nmaking INTERVAL configu-\nrable in fortune images\n194–195\nshell forms vs. exec forms\n193–194\noverriding in Kubernetes\n195–196\nremotely executing, in running \ncontainers  124–126\ncommunication\nbetween pods on different \nnodes  337–338\nbetween pods on same \nnode  336–337\nof components  311\nwith API servers  352–353\nwith pods through API \nservers  295–297\ncompletions property114\ncomponents\nadd-ons  310\ncommunicating  311\ninvolved in controllers  330\nisolating with Linux container \ntechnologies  8\nof Control Plane  310\nmaking highly available\n342–345\nusing leader-election in  344\nof Kubernetes  310–312\nrunning  311–312\nrunning with kubeadm  548–549\nComponentStatus311\ncomputational resources404–436\nlimiting resources available to \ncontainers  412–416\nexceeding limits  414–415\nlimits as seen by applications \nin containers  415–416\nsetting hard limits for \nresources containers can \nuse  412–413\nlimiting total resources available \nin namespaces  425–429\nlimiting objects that can be \ncreated  427–428\nResourceQuota \nresources  425–427\nspecifying quotas for per-\nsistent storage  427\nspecifying quotas for specific \npod states  429\nspecifying quotas for specific \nQoS classes  429\nmonitoring pod resource \nusage  430–434\nanalyzing historical resource \nconsumption \nstatistics  432–434\ncollecting resource \nusages  430–432\nretrieving resource \nusages  430–432\nstoring historical resource \nconsumption \nstatistics  432–434\npod QoS classes  417–421\ndefining QoS classes for \npods  417–419\nkilling processes  420–\n421\nrequesting resources for pod \ncontainers  405–412\ncreating pods with resource \nrequests  405–406\ndefining custom resources\n411–412\neffect of CPU requests on \nCPU time sharing  411\neffect of resource requests on \nscheduling  406–410\nrequesting custom \nresources  411–412\nsetting default limits for pods \nper namespace  421–425\napplying default resource \nlimits  424–425\ncreating LimitRange \nobjects  422–423\nenforcing limits  423–424\nLimitRange resources\n421–422\nconditions, using multiple in label \nselectors72\nconfig, updating without restart-\ning application211–213\nediting ConfigMap  212\nsignaling Nginx to reload \nconfig  212\nupdating ConfigMap  213\nupdating files automatically\n212–213\nconfig-file.conf file201\nCONFIG_FOO-BAR variable204\nconfigMap volume\nexamining mounted \ncontents  209\nexposing ConfigMap entries as \nfiles  205–211\nsetting file permissions for files \nin  211\nConfigMaps\ncreating  199–201, 206–207\ncombining options  201\nentries from contents of \nfiles  201\nfrom files in directories  201\nusing kubectl create \nConfigMap command\n200–201\ndecoupling configurations \nwith  198–213\nediting  212\nexposing entries as files  205–211\nexamining mounted config-\nMap volume contents\n209\nmounting directory hiding \nexisting files  210\n \n\nINDEX566\nConfigMaps (continued)\nsetting file permissions for \nfiles  211\nverifying Nginx uses mounted \nconfig file  208\nexposing entries in volume\n209–210\nmounting entries as files\n210–211\nnon-existing, referencing in \npods  203\noverview  198\npassing entries  202–205\nupdating  213\nusing entries in volume  207–208\nversus Secrets  217–218\nconfigurations, decoupling with \nConfigMap\ncreating ConfigMaps  200–201\npassing ConfigMap \nentries  202–205\nupdating application config \nwithout restarting \napplication  211–213\nusing configMap volume to \nexpose ConfigMap entries \nas files  205–211\nconfiguring\nallowed capabilities  394–395\nadding capabilities to all \ncontainers  395\ndropping capabilities from \ncontainers  395\nspecifying which capabilities \ncan be added to \ncontainers  394\ncontainer security \ncontexts  380–389\nadding individual kernel \ncapabilities to \ncontainers  384–385\ndropping capabilities from \ncontainers  385–386\npreventing containers from \nrunning as root  382\npreventing processes from \nwriting to container \nfilesystems  386–387\nrunning containers as specific \nuser  381–382\nrunning pods in privileged \nmode  382–384\nrunning pods without specify-\ning security contexts\n381\nsharing volumes when con-\ntainers run as different \nusers  387–389\ncontainerized applications\n191–192\ndeadlines for rollouts  278\ndefault capabilities  394–395\nadding capabilities to all \ncontainers  395\ndropping capabilities from \ncontainers  395\nspecifying which capabilities \ncan be added to \ncontainers  394\ndisallowed capabilities  394–395\nadding capabilities to all \ncontainers  395\ndropping capabilities from \ncontainers  395\nspecifying which capabilities \ncan be added to \ncontainers  394\nhost in Ingress pointing to \nIngress IP address  145\nIngress, to handle TLS \ntraffic  147–149\nINTERVAL in fortune \nimages  194–195\nJob templates  117\nKubernetes to use rkt  552\nlocation of kubeconfig files  535\nmasters with kubeadm  547–549\nrunning components with \nkubeadm  548–549\nrunning kubeadm init to ini-\ntialize masters  547–548\nname resolution for hosts\n546–547\nnetwork adapters for VMs  540\npod rescheduling after node \nfailures  462\nproperties of liveness \nprobes  88–89\nresource requests \nautomatically  451\nschedule  117\nservice endpoints \nmanually  132–134\ncreating endpoints resource \nfor services without \nselectors  133–134\ncreating services without \nselectors  132–133\nsession affinity on services  126\ntab completion for kubectl  41–42\nworker nodes with kubeadm\n549–550\nconnections\nexternal  141–142\npreventing breakage when pods \nshut down  493–497\nsequence of events at pod \ndeletion  493–495\ntroubleshooting  495–496\npreventing breakage when pods \nstart up  492–493\nsignaling when pods ready to \naccept  149–153\nto API servers  503\nto backend services  502\nto services living outside \ncluster  131–134\ncreating alias for external \nservices  134\nservice endpoints  131–\n134\nto services through FQDN  130\nto services through load \nbalancers  139–141\ncontainer images\nbuilding  29–32\nimage layers  30–31\noverview  30\nwith Dockerfile vs. \nmanually  31–32\ncreating Dockerfile for  29\ncreating Node.js \napplications  28–29\nin Docker container \nplatform  15\npushing image to image \nregistry  35–36\npushing images to Docker \nHub  36\nrunning images on different \nmachines  36\ntagging images under addi-\ntional tags  35\nremoving containers  34–35\nrunning  32–33\naccessing applications  32\nlisting all running \ncontainers  32\nobtaining information about \ncontainers  33\nstopping containers  34–35\nversioning  28\nviewing environment of run-\nning containers  33–34\nisolating filesystems  34\nprocesses running in host \noperating system  34\nrunning shells inside \nexisting  33\nwith out-of-range user IDs  393\nContainer Network Interface. See \nCNI\ncontainer networking interface. \nSee Kubernetes-CNI\ncontainer ports, specifying63–65\n \n\nINDEX567\ncontainer runtimes552–555\nreplacing Docker with rkt\n552–555\nconfiguring Kubernetes to \nuse rkt  552\nusing rkt with Minikube\n553–555\nusing through CRI  555\nCRI-O Container Runtime\n555\nrunning applications in VMs \ninstead of \ncontainers  555\nCONTAINER_CPU_REQUEST\n_MILLICORES variable228\nContainerCreating486\nCONTAINER_MEMORY_LIMIT\n_KIBIBYTES variable228\n--container-runtime=rkt option\n552–553\ncontainers7–16\nadding capabilities to all  395\nadding individual kernel capa-\nbilities to  384–385\ncomparing VMs to  8–10\nconfiguring security \ncontexts  380–389\npreventing containers from \nrunning as root  382\nrunning containers as specific \nuser  381–382\nrunning pods in privileged \nmode  382–384\nrunning pods without \nspecifying security \ncontexts  381\ncopying files to and from\n500–501\ncopying logs to and from\n500–501\ndetermining QoS class of  418\nDocker container platform\n12–15\nbuilding images  13\ncomparing VMs to  14–15\nconcepts  12–13\ndistributing images  13\nimage layers  15\nportability limitations of con-\ntainer images  15\nrunning images  13\ndropping capabilities \nfrom  385–386, 395\nexisting, running shells inside  33\nexploring  33\nimages\ncreating  290–291\nlisting  554–555\nInit containers, adding to \npods  484–485\ninstead of VMs, running appli-\ncations in  555\nisolating filesystems  34\nlimiting resource available \nto  412–416\nlimits as seen by applications \nin  415–416\nLinux, isolating components \nwith  8\nlisting all  32\nlocating  21\nmaking images  497\nmechanisms for isolation  11\nmounting local files into  503\nmultiple vs. one with multiple \nprocesses  56–57\nobtaining information about  33\nof pods\nrequesting resources \nfor  405–412\nresources requests for\n411–412\nrunning with kubelet  332\norganizing across pods  58–60\nsplitting into multiple pods \nfor scaling  59\nsplitting multi-tier applica-\ntions into multiple \npods  59\noverview  8–12\nisolating processes with Linux \nNamespaces  11\nlimiting resources available to \nprocess  11–12\npartial isolation between  57\npassing command-line argu-\nments to  192–196\ndefining arguments in \nDocker  193–195\ndefining command in \nDocker  193–195\noverriding arguments in \nKubernetes  195–196\noverriding command in \nKubernetes  195–196\nrunning fortune pods with \ncustom interval  195–196\npassing ConfigMap entries \nto  202–203\npods with multiple, determin-\ning QoS classes of  419\npost-start, using lifecycle \nhooks  486–487\npre-stop\nusing lifecycle hooks\n487–488\nusing lifecycle hooks when \napplication not receiving \nSIGTERM signal\n488–489\npreventing processes from writ-\ning to filesystems  386–387\nprocesses running in host oper-\nating system  34\nremotely executing commands \nin  124–126\nremoving  34–35\nrkt container platform, as alter-\nnative to Docker  15–16\nrunning  20–21\neffect of application descrip-\ntions on  19–20\ninspecting in Minikube \nVM  553–554\nviewing environment of  33–34\nrunning applications inside \nduring development  503\nrunning shells in  130–131\nseeing all node CPU cores  416\nseeing node memory  415–416\nsetting environment variables \nfor  196–198\nconfiguring INTERVAL in \nfortune images through \nenvironment variables\n197\ndisadvantages of hardcoding \nenvironment \nvariables  198\nreferring to environment \nvariables in variable \nvalues  198\nsetting hard limits for resources \nused by  412–413\ncreating pods with resource \nlimits  412–413\novercommitting limits  413\nsetting up networks  550\nsharing data between with \nvolumes  163–169\nusing emptyDir volume\n163–166\nusing Git repository as \nstarting point for \nvolume  166–169\nsharing IP  57\nsharing port space  57\nsharing volumes when running \nas different users  387–389\nspecifying environment vari-\nables in definitions  197\nspecifying name when retriev-\ning logs of multi-container \npods  66\n \n\nINDEX568\ncontainers (continued)\nspecifying which capabilities \ncan be added to  394\nstopping  34–35\ntargeting  489\nusing Secrets to pass sensitive \ndata to  213–223\nConfigMaps versus Secrets\n217–218\ncreating Secrets  216\ndefault token Secrets  214–215\nimage pull Secrets  222–223\noverview of Secrets  214\nusing Secrets in pods\n218–222\nusing volumes to preserve data \nacross restarts  480–482\nwhen to use multiple in \npod  59–60\nwith same QoS classes, \nhandling  420–421\nSee also container images; side-\ncar containers\n--containers option432\ncontainers, Docker-based13\nContentAgent container162\ncontexts\ncurrent, in kubeconfig files  536\ndeleting  538\nin kubeconfig files  536\nlisting  538\nswitching between  538\nusing kubectl with  537–538\nSee also security contexts\ncontinuous delivery6–7\nbenefits of  7\nrole of developers in  7\nrole of sysadmins in  7\nContinuous Integration and Con-\ntinuous Delivery. See CI/CD\nControl Plane18–19\ncomponents of  310, 344\nmaking components highly \navailable  342–345\nensuring high availability of \ncontrollers  343–344\nensuring high availability of \nScheduler  343–344\nrunning etcd clusters  342–343\nrunning multiple instances of \nAPI servers  343\nusing leader-election in Con-\ntrol Plane components\n344\nController Manager, Control \nPlane component19\ncontrollers145, 321\n–326, 330–333\nchain of events  331–332\nDeployment controller cre-\nates ReplicaSet  331\nkubelet runs pod \ncontainers  332\nReplicaSet controller creates \npods  332\nScheduler assigns node to \nnewly created pods  332\ncomponents involved in  330\ncustom, automating custom \nresources with  513–517\nDaemonSet  324\nDeployment controllers  324\nEndpoints controllers  325\nensuring high availability \nof  343–344\nJob controllers  324\nNamespace controllers  325\nNode controllers  324\nobserving cluster events  332–333\noverview  322, 326\nPersistentVolume \ncontrollers  325–326\nremoving pods from  100\nReplicaSet  324\nreplication managers  323–324\nrunning as pods  515–516\nService controllers  324\nStatefulSet controllers  324\nWebsite  514–515\ncopies, scaling number of21\ncopying\nfiles to and from containers\n500–501\nimages to Minikube VM \ndirectly  504\nlogs to and from \ncontainers  500–501\nCPUs (central processing units)\ncreating Horizontal pod Auto-\nscaler based on  442–443\ncreating ResourceQuota \nresources for  425–426\ndisplaying usage\nfor cluster nodes  431\nfor pods  431–432\nnodes, seen by containers  416\nscaling based on  441–447\nautomatic rescale \nevents  444–445\ncreating Horizontal pod \nAutoscaler based on \nCPU usage  442–443\nmaximum rate of scaling  447\nmodifying target metric val-\nues on existing HPA \nobjects  447\ntriggering scale-ups  445–446\nusing Autoscaler to scale up \nDeployments  446–447\ntime sharing  411\nCrashLoopBackOff\n414, 499\nCRD (CustomResource-\nDefinitions)509–513\ncreating CRD objects  510–511\ncreating instances of custom \nresources  511–512\ndeleting instances of custom \nobjects  512–513\nexamples of  509–510\nretrieving instances of custom \nresources  512\ncreation_method label72\ncredentials. See user credentials\nCRI (Container Runtime Inter-\nface), using container run-\ntimes through555\nCronJob resources, creating\n116–117\nCRUD (Create, Read, Update, \nDelete)316\nCSR (CertificateSigning-\nRequest)148\ncurl command124–126, 130–131, \n167\ncurl pods, running with additional \nambassador containers\n244–245\nCURL_CA_BUNDLE variable241\nCURRENT column49\ncustom intervals, running fortune \npods with195–196\ncustom-namespace.yaml file78\nD\n-d flag32\nDaemonSets324, 328, 550, 559\ncreating  111\ncreating YAML definition  110\nexamples of  109\nrunning one pod on each node \nwith  108–112\nrunning pods on certain nodes \nwith  109–112\nadding required label to \nnodes  111\nremoving required label from \nnodes  111–112\nrunning pods on every node \nwith  109\ndashboard52–53\naccessing when running in man-\naged GKE  52\naccessing when using \nMinikube  53\n \n\nINDEX569\ndata\nbinary, using Secrets for  217\npassing to containers using \nSecrets  213–223\nConfigMaps versus \nSecrets  217–218\ncreating Secrets  216\ndefault token Secrets\n214–215\nimage pull Secrets  222–223\noverview of Secrets  214\nusing Secrets in pods\n218–222\npersisted by previous pod\n173–174\nusing volumes to preserve \nacross container \nrestarts  480–482\nwriting to persistent storage by \nadding documents to Mon-\ngoDB database  173\nwritten to disks, \ndisappearing  480\ndata stores, clustered303–304\ndeadlines, configuring for \nrollouts278\ndeclarative scaling103\ndecoupling\nconfigurations with \nConfigMap  198–213\nConfigMaps  198–199\ncreating ConfigMaps\n200–201\npassing all entries of Config-\nMap as environment \nvariables at once  204\npassing ConfigMap entries as \ncommand-line \narguments  204–205\npassing ConfigMap entries to \ncontainers as environ-\nment variables  202–203\nupdating application config \nwithout restarting \napplication  211, 213\nusing configMap volume to \nexpose ConfigMap \nentries as files  205, 211\npods from underlying storage \ntechnologies  176–184\nbenefits of using claims  182\nPersistentVolumeClaims\n176–177, 179–181\nPersistentVolumes  176–184\ndefault capabilities, \nconfiguring394–\n395, 403\nadding capabilities to all \ncontainers  395\ndropping capabilities from \ncontainer  395\nspecifying which capabilities \ncan be added to \ncontainer  394\ndefault policy397\ndefault token Secret214–215\ndefaultAddCapabilities394\ndefaultMode property232\nDeis Helm package manager\n530–533\nDELETED watch event515\ndeleting\nclusters  538\ncontexts  538\ninstances of custom \nobjects  512–513\nPersistentVolumeClaims  288\nPet pods  297–298\npods  80–82, 252–253, 306\nby deleting whole \nnamespaces  80–81\nby name  80\nforcibly  307\nin namespace while keeping \nnamespace  81–82\nmanually  306–307\nsequence of events  493–495\nusing label selectors  80\npods marked for  307\nReplicationControllers  103–104\nresources in namespace  82\ndeletionTimestamp field489, 495\ndelivery. See continuous delivery\ndependencies\ninter-pod  485\noverview  5\ndeploying\nadd-ons  328–329\napplications through \nStatefulSets  291–295\ncreating governing \nservices  292–294\ncreating persistent \nvolumes  291–292\ncreating StatefulSets  294\nexamining PersistentVolume-\nClaims  295\nexamining stateful pods\n294–295\napplications, simplifying  21–22\nmicroservices  5\nnewly built images automatically \nwith DeploymentConfigs\n529–530\nNode.js applications  42–44\nbehind the scenes  44\nlisting pods  43–44\npods\nin same availability \nzone  471–472\nco-locating pods in same \navailability zone  471\ntopologyKey  471–472\nin same geographical \nregions  471–472\nco-locating pods in \nsame geographical \nregion  471\ntopologyKey  471–472\nin same rack  471–472\nin two-node clusters  468\non same nodes, with \ninter-pod affinity\n468–471\nwith container images \nwith out-of-range user \nIDs  393\nwith pod affinity  470\nwith runAsUser outside of \npolicy ranges  393\nprivileged containers, creating \nPodSecurityPolicy to \nallow  396–397\nresources through Helm\n531–533\nversions  269\nDeployment controllers\ncreating ReplicaSet controller \nwith  331\noverview  324\nDeployment resource\nbenefits of  267–268\ncreating  262–264\nmanifest, creating  262\nrolling back to specific \nrevision  270\nstrategies  264–265\nupdating  264–268\nslowing down rolling \nupdates  265\ntriggering rolling \nupdates  265–267\nusing for updating applications \ndeclaratively  261–278\nblocking rollouts of bad \nversions  274–278\ncontrolling rate of \nrollout  271–273\npausing rollout process\n273–274\nrolling back \ndeployments  268–270\nDeploymentConfigs, deploying \nnewly built images automati-\ncally with529–530\n \n\nINDEX570\ndeployments250–279\ndisplaying rollout history of  270\nperforming automatic rolling \nupdates  254–261\nobsolescence of kubectl roll-\ning-update  260–261\nperforming rolling updates \nwith kubectl  256–260\nrunning initial version of \napplications  254–255\nrolling back  268–270\ncreating application \nversions  268\ndeploying versions  269\nrolling back to specific Deploy-\nment revision  270\nundoing rollouts  269\nrollouts, displaying status of  263\nupdating applications running \nin pods  251–253\ndeleting old pods  252–253\nreplacing old pods  252\nspinning up new pods\n252–253\nupdating with kubectl \napply  276–277\nusing anti-affinity with pods of \nsame  475\nusing Autoscaler to scale \nup  446–447\ndeprovisioning, instances526\ndescribe command409\ndescriptors\nJSON, creating pods from  61–67\nYAML\ncreating for pods  63–65\ncreating pods from  61–67\nof existing pods  61–63\nDESIRED column49\ndevelopers, role in continuous \ndelivery7\nDevOps7\ndirectories\nmounting hides existing \nfiles  210\nusing multiple in same \nvolume  282\ndirectory traversal attack353\ndisabling, firewalls544\ndisallowed capabilities, \nconfiguring394–395\nadding capabilities to all \ncontainers  395\ndropping capabilities from \ncontainer  395\nspecifying which capabilities \ncan be added to \ncontainer  394\ndisk=ssd label110\ndisks, data written to \ndisappearing480\ndisruptions, of services454–456\ndistributing Docker images13\nDNS (Domain Name System)300\ndiscovering pods through\n155–156\ndiscovering services through  129\nimplementing peer discovery \nthrough  301–302\nrecords returned for headless \nservice  155–156\nservers  329\ndnsPolicy property129\nDocker container platform25–54\nclusters in  36–42\nrunning local single-node clus-\nters with Minikube  37–38\nsetting up aliases for \nkubectl  41–42\nsetting up command-line \ncompletion for \nkubectl  41–42\nusing hosted clusters with \nGKE  38–41\ncomparing VMs to  14–15\nconcepts  12–13\ncontainer images  26–36\nbuilding  29–32\ncreating Dockerfile for  29\ncreating Node.js \napplications  28–29\npushing images to image \nregistry  35–36\nremoving containers  34–35\nrunning  32–33\nstopping containers  34–35\nviewing environment of run-\nning containers  33–34\ndefining arguments in  193–195\nCMD instruction  193\nENTRYPOINT \ninstruction  193\nmaking INTERVAL configu-\nrable in fortune \nimages  194–195\nshell forms vs. exec \nforms  193–194\ndefining commands in  193–195\nCMD instruction  193\nENTRYPOINT instruction\n193\nmaking INTERVAL configu-\nrable in fortune \nimages  194–195\nshell forms vs. exec \nforms  193–194\nimages\nbuilding  13\ndistributing  13\nlayers  15\nportability limitations of  15\nrunning  13\ninstalling  26–28\nregistries, creating Secrets for \nauthenticating with  223\nrkt container platform as alter-\nnative to  15–16\nrunning first application on \nKubernetes\naccessing web applications\n45–47\ndeploying Node.js \napplications  42–44\nexamining which nodes \napplication is running \non  51–52\nhorizontally scaling \napplications  48–50\nlogical parts of systems  47–48\nusing Kubernetes \ndashboard  52–53\nrunning Hello World \ncontainer  26–28\nbehind the scenes  27\nrunning other images  27\nversioning container \nimages  28\nDocker Hub\npushing images to  36\nusing private image repositories \non  222\nDocker Hub ID35\ndocker images command35\nDocker platform\nreplacing with rkt  552–555\nconfiguring Kubernetes to \nuse rkt  552\nusing rkt with Minikube\n553–555\nusing daemon inside Minikube \nVM  503–504\ndocker ps command548\ndocker pull command44\nDocker Registry13\ndocker run command26\nDocker tool, installing544–545\ndisabling firewalls  544\ndisabling SELinux  544\nenabling net.bridge.bridge-nf-\ncall-iptables Kernel \noptions  545\nDockerfile\nbuilding images with  31–32\ncreating for container images  29\n \n\nINDEX571\nDOCKER_HOST variable32, 504\nDocker-registry Secrets, using \nin pod definitions223\ndocuments, writing data to \npersistent storage by \nadding to MongoDB \ndatabase173\nDoesNotExist operator107\nDomain Name System. See DNS\ndouble dash character125\ndownloading client binaries39\ndowntime, reducing by \nrunning multiple \ninstances341\nDownward API\nbenefits of using  233\npassing metadata through\n226–233\navailable metadata  226–227\nexposing metadata through \nenvironment \nvariables  227–230\npassing metadata through \nfiles in downwardAPI \nvolume  230–233\ndownwardAPI volume, passing \nmetadata through files \nin163, 230–233\nbenefits of using Downward \nAPI  233\nreferring to container-level \nmetadata in volume \nspecification  233\nupdating annotations  232\nupdating labels  232\ndownwardAPI.items attribute231\nDSL (Domain-Specific-\nLanguage)248\ndynamic provisioning\nof PersistentVolumes\n184–189\ndefining available storage \ntypes through Storage-\nClass resources  185\nrequesting storage class in \nPersistentVolume-\nClaims  185–187\nwithout specifying storage \nclass  187–189\nwithout specifying storage class\ncreating PersistentVolume-\nClaims  188–189\nexamining default storage \nclasses  188\nlisting storage classes  187–188\nPersistentVolumeClaims \nbound to pre-provisioned \nPersistent-Volumes  189\nE\necho command27\nedit ClusterRole, allowing modifi-\ncation of resources with372\nediting\nConfigMap  212\ndefinitions, scaling Replication-\nController by  102–103\nEDITOR variable102\nEFK Stack501\nelastic block store volume, AWS174\nELK Stack501\nemptyDir volume162–166\ncreating pods  164–165\nseeing pods in action  165\nspecifying medium for  166\nusing in pods  163–164\n--enable-swagger-ui=true \noption248\nEndpoints controllers325\nendpoints resources, creating for \nservices without selectors\n133–134\nENTRYPOINT instruction193\nenv command128\nenv label72\nenv=debug label70\nenv=devel label105\nenv=prod label70\nenv=production label105\nenvFrom attribute204\nenvironment variables\nconfiguring INTERVAL in \nfortune images through\n197–198\ndisadvantages of \nhardcoding  198\ndiscovering services \nthrough  128–129\nexposing metadata \nthrough  227–230\nexposing Secrets entries \nthrough  221–222\npassing all ConfigMap entries \nas  204\npassing ConfigMap entries to \ncontainers as  202–203\nreferring to in variable \nvalues  198\nsetting for containers  196, 198\nspecifying in container \ndefinitions  197\nephemeral pods121\netcd cluster518\netcd stores312–316\nensuring consistency of stored \nobjects  314–315\nensuring consistency when \nclustered  315\nensuring validity of stored \nobjects  314–315\nnumber of instances  316\nrunning clusters  342–343\nstoring resources in  313–314\netcd, Control Plane component19\nexec forms, versus shell \nforms193–194\nExec probe86, 90, 150\nExists operator107, 461\nexplain command175\nexposing\nmultiple ports in same \nservice  126–127\nmultiple services through same \nIngress  146–147\nmapping different services to \ndifferent hosts  147\nmapping different services to \ndifferent paths of same \nhost  146\nservices externally through \nIngress resources  142–149\nbenefits of using  142–143\nconfiguring Ingress to handle \nTLS traffic  147–149\ncreating Ingress \nresources  144\nusing Ingress controllers\n143–144\nservices externally using \nRoutes  530\nservices to external clients\n134–142\nexternal connections  141–142\nthrough external load \nbalancers  138–141\nusing NodePort \nservices  135–138\nexternal clients\nallowing NodePort services \naccess to  137–138\nexposing services to  134–142\nexternal connections\n141–142\nthrough external load \nbalancers  138–141\nusing NodePort \nservices  135–138\nexternal services, creating alias \nfor134\nExternalIP138\nEXTERNAL-IP column136\nExternalName services, \ncreating134\nexternal-traffic annotation141\n \n\nINDEX572\nF\nFABRIC8 Java client, interacting \nwith247–248\nFailedPostStartHook487\nFailedPreStopHook488\nFallbackToLogsOnError500\nfeatures. See security-related fea-\ntures\nfederation. See Cluster Federation\nfiles\ncopying to and from \ncontainers  500–501\ncreating ConfigMap entries \nfrom contents of  201\nin configMap volumes, setting \nfile permissions for  211\nin directories, creating Config-\nMap from  201\nin downwardAPI volume, \npassing metadata through\n230–233\nin sync with gitRepo \nvolume  168\nmounted config, verifying \nNginx using  208\nmounting  503\nmounting ConfigMap entries \nas  210–211\nmounting directory hiding \nexisting  210\non worker node filesystems, \naccessing  169–170\nupdating automatically  212–213\nusing configMap volume to \nexpose ConfigMap entries \nas  205, 211\nfilesystems\nof containers\nisolating  34\npreventing processes from \nwriting to  386–387\nworker node, accessing files \non  169–170\nfirewalls\nchanging rules to let external \nclients access NodePort \nservices  137–138\ndisabling  544\nFlannel312\nflat networks, inter-pod58\nflexVolume volume163\nflocker volume163\nfoo namespace358\nfoo.default.svc.cluster.local \ndomain286\nFOO_SECRET variable221\n--force option490\nfortune\nimages, configuring INTERVAL \nvariables in  194–195\npods, running with custom \nintervals  195–196\nfortune command\n163, 165\nfortune images, INTERVAL vari-\nables in197\nfortune-config config map, \nmodifying to enable HTTPS\n218–219\nfortune-https directory216\nfortune-https Secret, mounting in \npods219–220\nfortuneloop container209\nfortuneloop.sh script164\nfortune-pod.yaml file164\nforwarding, local network port to \nport in pod67\nFQDN (fully qualified domain \nname)129–130, 134\nFROM scratch directive497\n--from-file argument201\nfsGroup policies392–394\ndeploying pod with container \nimage with an out-of-range \nuser ID  393\nusing MustRunAs rule  392–393\nfsGroup property388\nG\ngateways58\nGCE (Google Compute \nEngine)185, 454\ncreating persistent disks  171–172\nusing in pod volumes  171–174\ncreating GCE persistent \ndisks  171–172\ncreating pods using gcePer-\nsistentDisk volumes  172\nre-creating pods  173–174\nverifying pod can read data \npersisted by previous \npod  173–174\nwriting data to persistent stor-\nage by adding docu-\nments to MongoDB \ndatabase  173\ngcePersistentDisk volumes\ncreating pods  172\noverview  162, 190\ngcloud command39, 171\ngcloud compute ssh command97\n--generator flag42\ngeographical regions\nco-locating pods in same  471\ndeploying pods in same  471–472\nGit repositories\nas starting point for \nvolumes  166–169\nfiles in sync with gitRepo \nvolume  168\ngitRepo volume  169\nsidecar containers  168\ncloned, running web server pod \nserving files from  167\nprivate, using gitRepo volume \nwith  168\nGit sync container168\ngitRepo field510, 517\ngitRepo volumes\nfiles in sync with  168\noverview  169\nusing with private Git \nrepositories  168\nGKE (Google Container \nEngine)36, 454\naccessing dashboard when run-\nning in  52\nand Minikube, switching \nbetween  534–535\nswitching from Minikube \nto  534\nswitching to Minikube \nfrom  534\nusing hosted clusters with\n38–41\ncreating clusters with three \nnodes  39\ndownloading client \nbinaries  39\ngetting overview of \nclusters  39\nlisting cluster nodes  40\nretrieving additional details \nof objects  41\nsetting up Google Cloud \nprojects  39\nglusterfs volume\n163\nGoogle Cloud, setting up \nprojects39\nGoogle Compute Engine. See GCE\nGoogle Container Registry35\ngoverning services\ncreating  292–294\noverview  285–287\ngpu=true label75, 464–465\nGrafana suite\nanalyzing resource usage \nwith  433–434\noverview  432\nrunning in clusters  433\ngroup ID (gid)381\ngrouping resources, with \nnamespaces76–80\n \n\nINDEX573\ngroups347–348\nassigning different PodSecurity-\nPolicies to  396–399\nin Red Hat OpenShift Con-\ntainer platform  528\nGuaranteed class, assigning pods \nto417–418\nguarantees, StatefulSets289–290\nat-most-one semantics  290\nimplications of stable \nidentity  289–290\nimplications of stable \nstorage  289–290\nH\nhardcoding environment vari-\nables, disadvantages of198\nhardware, improving utilization \nof22\nheadless services\ncreating  154–155\nDNS returned for  155–156\nusing for discovering individual \npods  154–156\ndiscovering all pods  156\ndiscovering pods through \nDNS  155–156\nhealth checking22–23\nhealth HTTP endpoint89\nHeapster aggregator, \nenabling431\nHello World container26–28\nbehind the scenes  27\nrunning other images  27\nversioning container images  28\nHelm. See Deis Helm package \nmanager\nhooks. See lifecycle hooks\nhops. See network hops\nhorizontal autoscaling, of \npods438–451\nautoscaling process  438–441\nmetrics appropriate for \nautoscaling  450\nscaling\ndown to zero replicas\n450–451\non CPU utilization  441–447\non memory \nconsumption  448\non other and custom \nmetrics  448–450\nHorizontal pod Autoscaler, creat-\ning based on CPU \nusage442–443\nhorizontal scaling, of cluster \nnodes452–456\nCluster Autoscaler  452–453\nenabling Cluster \nAutoscaler  454\nlimiting service disruption \nduring cluster scale-\ndown  454–456\nHost header147\nhost networks, namespaces\n377–379\nhost nodes, using namespaces in \npods376\n–380\nhost operating systems, container \nprocesses running in34\nhost ports, binding to377–379\nhostIPC property380\nhostnames\nchanging on cloned VMs  546\nexpecting changes to  480\nhostNetwork property376\nhostPath volumes162, 169–170\nhostPort property377\nhosts\nconfiguring name resolution \nfor  546–547\nmapping different services \nto  147\nmapping different services to \npaths of same  146\nHPA (HorizontalPodAutoscaler)\nmodifying target metric values \non existing objects  447\noverview  443\nhtml-generator container165\nHTTP GET probe85, 150\nHTTP-based liveness probes, \ncreating86–87\nhttpGet liveness probe87\nHTTPS (Hypertext Transfer \nProtocol Secure)\n218–219\nhybrid cloud556\nhypervisors9\nI\nidentities\nnetwork, providing  285–287\ngoverning service, \noverview  285–287\nscaling StatefulSets  287\nof API servers, verifying\n240–241\nproviding stable for pods\n282–284\nstable, implications of\n289–290\nif statement268\nIgnoredDuringExecution464\nimage layers15, 30–31\nimage pull Secrets222–223, \n351\ncreating Secrets for authenticat-\ning with Docker \nregistries  223\nspecifying on every pod  223\nusing Docker-registry Secrets in \npod definitions  223\nusing private image repositories \non Docker Hub  222\nimage registry\npushing images to  35–36\nrunning images on different \nmachines  36\ntagging images under addi-\ntional tags  35\npushing images to Docker \nHub  36\nimagePullPolicy\noverview  256, 317\nusing  497–498\nimagePullSecrets field222\nimages\nbuilding\nfrom source using \nBuildConfigs  529\nlocally  504\nusing Docker daemon inside \nMinikube VM  503–504\ncopying to Minikube VM \ndirectly  504\ndeploying automatically \nwith DeploymentConfigs\n529–530\nDocker\nbuilding  13\ndistributing  13\nportability limitations of  15\nrunning  13\nfortune\nconfiguring INTERVAL vari-\nables in  194–195\nconfiguring INTERVAL vari-\nables through environ-\nment variables  197\nof containers\ncreating  290–291, 497\nlisting  554–555\npushing to Docker Hub  36\npushing to image registry\n35–36\nrunning on different \nmachines  36\ntagging  35, 497–498\nSee also container images\nImageStream529\nIn operator107\n \n\nINDEX574\nInfluxDB database\noverview  432\nrunning in clusters  433\nIngress resource135\naccessing pods through  145\naccessing services through  145\nbenefits of using  142–143\nconfiguring to handle TLS \ntraffic  147–149\ncreating  144\ncreating TLS certificate \nfor  147–149\nexposing multiple services \nthrough  146–147\nmapping different services to \ndifferent hosts  147\nmapping different services to \ndifferent paths of same \nhost  146\nexposing services externally \nthrough  142–149\nobtaining IP address of  145\noverview  145\nIngresses559\nInit containers, adding to \npods484–485\ninitialDelaySeconds property88\ninitializing masters with kubeadm \ninit547–548\ninitiating install of OS542\ninspecting node capacity407–408\ninstalling\nDocker  544–545\ndisabling firewalls  544\ndisabling SELinux  544\nenabling net.bridge.bridge-\nnf-call-iptables Kernel \noptions  545\nkubeadm  544–545\nkubectl  38, 544–545\nKubelet  544–545\nKubernetes  544–545\nadding Kubernetes yum \nrepo  544\ndisabling firewalls  544\ndisabling SELinux  544\nenabling net.bridge.bridge-\nnf-call-iptables Kernel \noption  545\nKubernetes-CNI  544–545\nMinikube  37\nOS  541–544\ninitiating  542\nrunning install  543–544\nselecting start-up disks  541\nsetting installation \noptions  542–543\nInternet protocol. See IP\ninter-pod affinity, to deploy pods \non same nodes468–\n471\ndeploying pods with pod \naffinity  470\nspecifying pod affinity in pod \ndefinitions  469\nusing pod affinity rules with \nScheduler  470–471\nINTERVAL variables, configuring \nin fortune images\noverview  194–195\nthrough environment \nvariables  197\nintervals, running fortune pods \nwith195–196\nIP (Internet protocol)\ncontainers sharing  57\nexternal, accessing services \nthrough  46–47\nIngress address  145\nlocal, expecting changes to  480\nof client, non-preservation of  142\nservice, pinging  131\nIP addresses121\nIPC (Inter-Process Communication)\noverview  11, 56, 376\nusing namespaces  379–380\niptables rules327–328, 339–340, \n345, 492, 494–495\niscsi volume163\nisolating\ncomponents with Linux con-\ntainer technologies  8\ncontainer filesystems  34\ncontainers, mechanisms for  11\nnetworks between Kubernetes \nnamespaces  401–402\npartially between containers of \nsame pod  57\npod networks  399–402\nprocesses with Linux \nNamespaces  11\n-it option33\nitems attribute138\nJ\nJob controllers324\nJob resources112\nconfiguring templates  117\nrunning multiple pod instances \nin  114–116\nrunning Job pods in \nparallel  115\nrunning Job pods \nsequentially  115\nscaling Job  116\nrunning on pods  114\njobs\ndefining  113–114\nlisting instances in \nclusters  237–238\nretrieving instances by \nname  238\nscheduling  116–118\ncreating CronJob  116–117\noverview  117\nJSON format\ncreating pods from descriptors\nsending requests to pods\n66–67\nusing kubectl create to create \npods  65\nviewing application logs\n65–66\nmanifests, writing  505–506\nJSONPath138\nK\nkeep-alive connections140\nkernel capabilities, adding to \ncontainers384–385\nKernels, enabling \nnet.bridge.bridge-nf-call-\niptables option545\nkey files, from Secrets221\nkey=value format232\nkilling applications479–482\nexpecting data written to disk to \ndisappear  480\nexpecting hostnames to \nchange  480\nexpecting local IP to \nchange  480\nusing volumes to preserve data \nacross container \nrestarts  480–482\nKsonnet, as alternative to writing \nJSON manifests505–506\nkube config\nadding entries  536–537\nadding clusters  536\nadding user credentials\n536–537\ntying clusters and user cre-\ndentials together  537\nlisting entries  536–537\nadding clusters  536\nadding user credentials\n536–537\nmodifying clusters  536\nmodifying user \ncredentials  536–537\ntying clusters and user cre-\ndentials together  537\n \n\nINDEX575\nkube config (continued)\nmodifying entries  536–537\nmodifying clusters  536\nmodifying user credentials\n536–537\ntying clusters and user cre-\ndentials together  537\nkubeadm init command549\nkubeadm join command549\nkubeadm tool\nconfiguring masters with\n547–549\nconfiguring worker nodes \nwith  549–550\ninstalling  544–545\nrunning components with\n548–549\nlisting nodes  548–549\nlisting pods  548\nrunning kubectl on \nmasters  548\nrunning init to initialize \nmasters  547–548\nsetting up multi-node clusters \nwith  539–551\nsetting up operating \nsystems  539\nsetting up required \npackages  539\nusing clusters from local \nmachines  550–551\nkubeconfig files\nconfiguring location of  535\ncontents of  535–536\nKUBECONFIG variable535\nkubectl\naliases for  41–42\ncommand-line completion\n41–42\nconfiguring tab completion \nfor  41–42\nconfirming cluster communicat-\ning with  38\ncreating additional users for  398\ninstalling  38\nlogs, retrieving with pod logs  66\nperforming rolling updates \nwith  256–260\nreplacing old pods with new \npods by scaling two \nReplicationControllers\n259–260\nsteps performed before roll-\ning update commences\n258–259\nproxy\naccessing API servers \nthrough  234–235\nexploring Kubernetes API \nthrough  235–236\nusing with different \nclusters  537–538\nusing with different \ncontexts  537–538\nusing with different users\n537–538\nusing with multiple \nclusters  534–538\nadding kube config \nentries  536–537\nconfiguring location of \nkubeconfig files  535\ncontents of kubeconfig \nfiles  535–536\ndeleting clusters  538\ndeleting contexts  538\nlisting clusters  538\nlisting contexts  538\nlisting kube config \nentries  536–537\nmodifying kube config \nentries  536–537\nswitching between \ncontexts  538\nswitching between Minikube \nand GKE  534–535\nusing with multiple \nNamespaces  535–538\nadding kube config \nentries  536–537\nconfiguring location of \nkubeconfig files  535\ncontents of kubeconfig \nfiles  535–536\ndeleting clusters  538\ndeleting contexts  538\nlisting clusters  538\nlisting contexts  538\nlisting kube config \nentries  536–537\nmodifying kube config \nentries  536–537\nswitching between \ncontexts  538\nkubectl annotate command76\nkubectl apply command266, \n276–277, 504\nkubectl autoscale command443\nkubectl cluster-info command38, \n52\nkubectl command-line tool39\nkubectl cp command500\nkubectl create command65, 94, \n114, 178, 255\nkubectl create configmap \ncommand200\n–201\nkubectl create deployment \ncommand519\nkubectl create -f command65\nkubectl create namespace \ncommand78–79\nkubectl create role command359\nkubectl create secret \ncommand519\nkubectl delete command104\nkubectl describe command41, 52, \n76, 95, 131, 408, 444, 512, \n549, 553\nkubectl describe node \ncommand409\nkubectl describe pod \ncommand215, 487, 499\nkubectl edit command151\nkubectl edit method266\nkubectl exec command124, 128, \n130, 152, 446\nkubectl explain command64, 505\nkubectl expose command66, 123\nkubectl get command48, 95\nkubectl get events332\nkubectl get pods command51, 124\nkubectl get serviceclasses \ncommand523\nkubectl get services command46\nkubectl get svc command129\nkubectl logs command87, 485, 502\nkubectl patch method266\nkubectl port-forward \ncommand67\nkubectl proxy command244\n, 503\nkubectl proxy process514–515\nkubectl replace method266\nkubectl rolling-update \ncommand257, 260–261, 264\nkubectl rollout status \ncommand271\nkubectl run command61, 82\nkubectl scale command102–103, \n116\nkubectl set image command\n265–266\nkubectl tool\ninstalling  544–545\nrunning on masters  548\nkube-dns129\nKUBE_EDITOR environment \nvariable102\nKubelet node agent, \ninstalling544–545\nkubelets326–327\noverview  326\nrunning pod containers with  332\nrunning static pods without API \nservers  326–327\n \n\nINDEX576\nkube-proxy19, 312, 327, 330, 345\noverview  339\nusing iptables  339–340\nkube-public namespace77\nKubernetes16–24\narchitecture of  310–330\nadd-ons  328–330\nAPI servers  316–319\nclusters  18–19\ncomponents  310–312\ncomponents of Control \nPlane  310\ncomponents running on \nworker nodes  310\nController  321–326\netcd  312–316\nkubelet  326–327\nScheduler  319–321\nService Proxy  327–328\nbenefits of using  21–24\nautomatic scaling  23\nhealth checking  22–23\nimproving hardware \nutilization  22\nself-healing  22–23\nsimplifying application \ndeployment  21–22\nsimplifying application \ndevelopment  23–24\ndashboard  52–53\naccessing when running in \nmanaged GKE  52\naccessing when using \nMinikube  53\ninstalling  544–545\nadding Kubernetes yum \nrepo  544\ndisabling firewalls  544\ndisabling SELinux  544\nenabling net.bridge.bridge-\nnf-call-iptables Kernel \noption  545\nmaster, checking node status as \nseen by  305\norigins of  16\noverriding arguments in\n195–196\noverriding commands in\n195–196\noverview  16–18\nfocusing on core application \nfeatures  17–18\nimproving resource \nutilization  18\nrunning applications in  19–21\neffect of application descrip-\ntion on running \ncontainers  19–20\nkeeping containers \nrunning  20–21\nlocating containers  21\nscaling number of copies  21\nwhen indicated  2–7\ncontinuous delivery  6–7\nmicroservices vs. monolithic \napplications  3–6\nproviding consistent applica-\ntion environment  6\nKubernetes CNI (Container Net-\nworking Interface)545\nKubernetes Control Plane18\nKUBERNETES_SERVICE_HOST \nvariable239\nKUBERNETES_SERVICE_PORT \nvariable239\nkube-scheduler resource344\nkube-system namespace77, 312, \n314, 344, 454, 548\nkubia42\nkubia-2qneh100\nkubia-container32–33\nkubia-dmdck100\nkubia-gpu.yaml file74\nkubia-manual63\nkubia-rc.yaml file93\nKUBIA_SERVICE_HOST \nvariable129\nKUBIA_SERVICE_PORT \nvariable129\nkubia-svc.yaml file123\nkubia-website service517\nL\nlabel selectors\nchanging for Replication-\nController  100–101\ndeleting pods using  80\neffect of changing  93\nlisting pods using  71–72\nlisting subsets of pods \nthrough  71–72\nReplicaSets  107\nusing multiple conditions in  72\nlabels\nadding to nodes  111\nadding to pods managed by \nReplicationControllers  99\ncategorizing worker nodes \nwith  74\nconstraining pod scheduling \nwith  73–75\ncategorizing worker nodes \nwith labels  74\nscheduling pods to specific \nnodes  74–75\nmulti-dimensional vs. single-\ndimensional  498\nof existing pods, modifying\n70–71\nof managed pods  99–100\norganizing pods with  67, 71\noverview  68–69\nremoving from nodes  111–112\nspecifying when creating \npods  69–70\nupdating  232\nLAN (local area network)58\nlatest tag28\n--leader-elect option343\nleader-election\nfor non-horizontally scalable \napplications  341\nusing in Control Plane \ncomponents  344\nLeastRequestedPriority407\nlibraries\nbuilding with OpenAPI  248\nbuilding with Swagger  248\nSee also client libraries\nlifecycles, of pods479–491\nadding lifecycle hooks  485–489\nhooks\nadding  485–489\ntargeting containers with  489\nusing post-start container\n486–487\nusing pre-stop container\n487–488\nusing pre-stop when applica-\ntion not receiving \nSIGTERM signal\n488–489\nkilling applications  479–482\npod shutdowns  489–491\nrelocating applications  479–482\nrescheduling dead pods\n482–483\nrescheduling partially dead \npods  482–483\nstarting pods in specific \norder  483–485\nlimiting resources\navailable in namespaces  425–429\nlimiting objects that can be \ncreated  427–428\nResourceQuota \nresources  425–427\nspecifying quotas for per-\nsistent storage  427\nspecifying quotas for specific \npod states  429\nspecifying quotas for specific \nQoS classes  429\n \n\nINDEX577\nlimiting resources (continued)\navailable to containers  412–416\nexceeding limits  414–415\nlimits as seen by applications \nin containers  415–416\nsetting hard limits for \nresources containers can \nuse  412–413\nLimitRange objects, creating\n422–423\nlimits\nas seen by applications in \ncontainers  415–416\ncontainers seeing all node \nCPU cores  416\ncontainers seeing node \nmemory  415–416\nenforcing  423–424\nexceeding  414–415\nfor pods per namespace, setting \ndefaults  421–425\novercommitting  413\nsetting for resources used by \ncontainers  412–413\nSee also resource limits\nLinux Control Groups \n(cgroups)11\nLinux Namespaces11\nLinux OS\ncontainer technologies, isolat-\ning components with  8\nNamespaces, isolating pro-\ncesses with  11\nlisting\nall running containers  32\ncluster nodes  40\nclusters  538\ncontainer images  554–555\ncontexts  538\njob instances in clusters  237–238\nkube config entries  536–537\nadding clusters  536\nadding user credentials\n536–537\nmodifying clusters  536\nmodifying user credentials\n536–537\ntying clusters and user cre-\ndentials together  537\nnodes  548–549\nPersistentVolumeClaims  180\nPersistentVolumes  180–181\npods  43–44, 51, 71–72, \n548\nservices  46\nservices available in clusters  523\nstorage classes  187–188\nsubsets through label \nselectors  71–72\nliveness probes85–86\nconfiguring properties of\n88–89\ncreating  89–90\nHTTP-based  86–87\nimplementing retry loops \nin  90\nkeeping probes light  90\nwhat liveness probe should \ncheck  89–90\nin action  87–88\nload balancers\nconnecting to services \nthrough  139–141\nexternal, exposing services \nthrough  138–141\nLoadBalancer service\ncreating  139\noverview  135, 530\nlocal area network. See LAN\nlocal machines, using clusters \nfrom550–551\nLogRotator container162\nlogs\ncentralized, using  501\ncopying to and from \ncontainers  500–501\nhandling multi-line \nstatements  502\nkubectl, retrieving with pod \nlogs  66\nof applications  500–502\nof multi-container pods, specify-\ning container name when \nretrieving  66\npod, retrieving with kubectl \nlogs  66\nSee also application logs\nlogVol162\nls command151\nM\nmanifests\nDeployment resource, \ncreating  262\nJSON, writing  505–506\nresources  504–505\nYAML, writing  505–506\nmapping\ndifferent services to different \nhosts  147\ndifferent services to different \npaths of same host  146\nmaster node18\nmasters\nconfiguring with \nkubeadm  547–549\nrunning kubeadm init to \ninitialize  547–548\nrunning kubectl on  548\nmatchExpressions property107, \n465, 470\nmatchLabels field105, 107, \n470\nmaxSurge property271–272\nmaxUnavailable property\n271–274, 277, 455\nmedium, specifying for emptyDir \nvolume166\nmemory\ncreating ResourceQuota \nresources for  425–426\ndisplaying usage\nfor cluster nodes  431\nfor pods  431–432\nkilling processes when low\n420–421\nhandling containers with \nsame QoS class  420–421\nsequence of QoS classes  420\nnodes, as seen by containers\n415–416\nscaling based on consumption \nof  448\nSecret volumes stored in \nmemory  221\nmetadata\ncontainer-level, in volume \nspecifications  233\nexposing through environment \nvariables  227–230\npassing through Downward \nAPI  226–233\navailable metadata  226–227\npassing through files in \ndownwardAPI volume\n230–\n233\nmetadata section201\nmetadata.name field510\nmetadata.resourceVersion \nfield313\nmetric types\nObject, scaling based on\n449–450\npods\nobtaining  438–439\nscaling based on  449\nresource, scaling based on  449\nmetrics\nappropriate for autoscaling\n450\ncustom, scaling based on\n448–450\nmodifying target values on exist-\ning HPA objects  447\n \n\nINDEX578\nmicroservices3–6\ndeploying  5\ndivergence of environment \nrequirements  5–6\nscaling  4\nsplitting applications into  3–4\nminAvailable field455\nminikube delete command553\nminikube mount command503\nminikube node408\nMinikube tool\naccessing dashboard when \nusing  53\nand GKE, switching \nbetween  534–535\ncombining with Kubernetes \nclusters  504\ninspecting running containers \nin  553–554\ninstalling  37\nrunning local single-node \nKubernetes clusters with\n37–38\nstarting clusters with  37–38\nswitching from GKE to  534\nswitching to GKE from  534\nusing in development  503–504\nbuilding images locally  504\ncopying images to Minikube \nVM directly  504\nmounting local files into \ncontainers  503\nmounting local files into \nMinikube VM  503\nusing Docker daemon inside \nMinikube VM to build \nimages  503–504\nusing rkt with  553–555\nlisting container images\n554–555\nrunning pods  553\nMinikube VM (virtual machine)\ndirectly copying images to  504\nmounting local files into  503\nusing Docker daemon inside, to \nbuild images  503–504\nMinishift530\nminReadySeconds attribute265, \n274–275\nMongoDB database, adding docu-\nments to173\nmonitoring pod resource \nusage430–434\nanalyzing historical resource \nconsumption statistics\n432–434\ncollecting resource usages\n430–432\nretrieving resource usages\n430–\n432\nstoring historical resource \nconsumption statistics\n432–434\nmonolithic applications, vs. \nmicroservices3–6\nMostRequestedPriority407\nMount (mnt) Namespace11\nmountable Secrets350–351\nmounted config files, verifying \nNginx using208\nmounting\nConfigMap entries as files\n210–211\ndirectories hides existing \nfiles  210\nfortune-https Secret in \npods  219–220\nlocal files into containers  503\nlocal files into Minikube \nVM  503\nmulti-tier applications, splitting \ninto multiple pods59\nMustRunAs rules, using392–393\nMustRunAsNonRoot rules, using \nin runAsUser fields394\nmy-nginx-config.conf entry209\nmy-postgres-db service525\nMYSQL_ROOT_PASSWORD \nvariable192\nN\n-n flag79\n-n option537\nnames\nconfiguring resolution for \nhosts  546–547\ndeleting pods by  80\nof containers  66\nretrieving job instances by  238\nnames.kind property511\nNamespace controllers325\n--namespace option535, 537\nNamespaceLifecycle317\nnamespaces\naccessing resources in, using \nClusterRoles  367–370\ncreating\nfrom YAML files  78\nwith kubectl create \nnamespace  78–79\ndeleting pods in while \nkeeping  81–82\ndiscovering pods of  77–78\nenabling network isolation \nin  399\ngranting full control of, with \nadmin ClusterRole  372\ngrouping resources with  76–80\nhost network, binding to host \nports without using\n377–379\nhost nodes, using in pods\n376–380\nincluding service accounts in \nRoleBindings  361\nIPC, using  379–380\nisolating networks between\n401–402\nisolation provided by  79–80\nlimiting resources available \nin  425–429\nlimiting objects that can be \ncreated  427–428\nResourceQuota resources\n425–427\nspecifying quotas for per-\nsistent storage  427\nspecifying quotas for specific \npod states  429\nspecifying quotas for specific \nQoS classes  429\nLinux, isolating processes \nwith  11\nmanaging objects in  79\nnode network, using in \npods  376–377\nnode PID  379–380\nof pods  242\npods in, allowing some to con-\nnect to server pods  400\nsetting default limits for pods \nper  421\n–425\napplying default resource \nlimits  424–425\ncreating LimitRange \nobjects  422–423\nenforcing limits  423–424\nLimitRange resources  421–422\nsetting default requests for pods \nper  421–425\napplying default resource \nrequests  424–425\ncreating LimitRange \nobjects  422–423\nenforcing limits  423–424\nLimitRange resources\n421–422\nusing kubectl with multiple\n535–538\nadding kube config entries\n536–537\nconfiguring location of \nkubeconfig files  535\n \n\nINDEX579\nnamespaces (continued)\ncontents of kubeconfig \nfiles  535–536\ndeleting clusters  538\ndeleting contexts  538\nlisting clusters  538\nlisting contexts  538\nlisting kube config \nentries  536–537\nmodifying kube config \nentries  536–537\nswitching between \ncontexts  538\nwhy needed  77\nNAS (network-attached \nstorage)171\nNAT (Network Address \nTranslation)58, 335\nnet.bridge.bridge-nf-call-iptables \nKernel option545\nNetwork (net) namespace11\nnetwork hops, preventing141\nnetworks\nbetween pods  335–338\narchitecture of  335–336\nCNI (Container Network \nInterface)  338\nenabling communication \nbetween pods on differ-\nent nodes  337–338\nenabling communication \nbetween pods on same \nnode  336–337\noverview  336–338\nconfiguring adapters for \nVMs  540\nenabling isolation in \nnamespaces  399\nisolating between Kubernetes \nnamespaces  401–402\nlocal ports, forwarding port in \npod  67\nnode namespaces, using in \npods  376–377\nnode, shutting down \nadapters  304–305\nof containers, setting up  550\nproviding stable identities\n285–287\ngoverning service, \noverview  285–287\nscaling StatefulSets  287\nsecuring  375–403\nconfiguring container \nsecurity contexts\n380–389\nisolating pod networks\n399–402\nrestricting use of security-\nrelated features in \npods  389–\n399\nusing host node namespaces \nin pods  376–380\nsimulating node disconnection \nfrom  304–306\nchecking node status as seen \nby Kubernetes \nmaster  305\npods with unknown \nstatus  305–306\nSee also flat networks; host net-\nworks\nNFS (Network File System)175\nnfs volume162\nnginx container334, 514\nNginx software\nsignaling to reload config  212\nusing cert files from Secrets\n221\nusing key files from Secrets  221\nverifying use of mounted config \nfiles  208\nnode affinity\nspecifying hard rules  463–465\nnodeAffinity attribute \nnames  464–465\nnodeSelectorTerms  465\nspecifying preferential \nrules  466–467\nusing to attract pods to \nnodes  462–468\nexamining default node \nlabels  462–463\nprioritizing nodes when \nscheduling  465–468\nvs. node selectors  462–463\nNode controllers324\nnode failures304–307\ndeleting pods manually  306–307\nsimulating node disconnection \nfrom network  304–306\nchecking node status as seen \nby Kubernetes \nmaster  305\npods with unknown \nstatus  305–306\nshutting down node network \nadapters  304–305\nnodeAffinity attribute names\n464–465\nNode.js\ncreating applications  28–29\ndeploying applications  42–44\nbehind the scenes  44\nlisting pods  43–44\nNodeLost306\nNODE_NAME variable228\nNodePort services\nchanging firewall rules to let \nexternal clients access\n137–138\ncreating  135–136\nexamining  136–137\nusing  135–138\nnodes\nadding custom taints to  460\napplications running on, \nexamining  51–52\nassigning to newly created pods \nwith Scheduler  332\nchecking status as seen by \nKubernetes master  305\nconfiguring pod rescheduling \nafter failures  462\nCPU cores, as seen by \ncontainers  416\ncreating clusters with three  39\ncreating pods that don’t fit on \nany  408–409\ndisplaying taints  458\nenabling communication \nbetween pods on \ndifferent  337–338\nenabling communication \nbetween pods on same\n336–337\nexamining default labels\n462–463\nfinding acceptable  320\ninspecting capacity of  407–408\nlabeling  466\nlisting  40, 548–549\nmemory, as seen by \ncontainers  415–416\nnetwork namespaces, using in \npods  376–377\nPID namespaces  379–380\npreferences  467\nprioritizing when scheduling \npods  465–468\ndeploying pods in two-node \nclusters  468\nspecifying preferential node \naffinity rules  466–467\nrelinquishing  453\nReplicationControllers respond-\ning to failures  97–98\nrequesting from Cloud \ninfrastructure  452–453\nrunning one pod on each, with \nDaemonSets  108–112\nrunning pods on\nadding required label to \nnodes  111\n \n\nINDEX580\nnodes (continued)\nremoving required label from \nnodes  111–112\nwith DaemonSets  109–112\nrunning pods on every, with \nDaemonSets  109\nScheduler using pod requests \nwhen selecting  407\nscheduling pods to specific\n74–75\nselecting best for pods  320–321\nselectors vs. affinity  462–463\nsimulating disconnection from \nnetwork  304–306\nchecking node status as seen by \nKubernetes master  305\npods with unknown \nstatus  305–306\nshutting down node network \nadapters  304–305\nusing inter-pod affinity to \ndeploy pods on  468–471\ndeploying pods with pod \naffinity  470\nspecifying pod affinity in pod \ndefinitions  469\nusing pod affinity rules with \nScheduler  470–471\nusing node affinity to attract \npods to  462–468\ncomparing node affinity to \nnode selectors  462–463\nspecifying hard node affinity \nrules  463–465\nusing Scheduler to determine \nwhether pods can fit on  406\nusing taints to repel pods \nfrom  457–462\ntaints, overview  458–460\nusing taints  461–462\nusing tolerations to repel pods \nfrom  457–462\nadding tolerations to \npods  460–461\ntolerations, overview  458–460\nusing tolerations  461–462\nworker\ncategorizing with labels  74\ncomponents running on  310\nSee also cluster nodes; node \naffinity; node failures; \nworker node filesystems; \nworker nodes\nnodeSelector field75, 462–464\nnodeSelectorTerms\n465\nNoExecute460\nnon-resource URLs, allowing \naccess to365–367\nNoOps7\nNoSchedule460\nNotIn operator107\nNotReady status97, 305\nNotTerminating scope429\nNTP (Network Time Protocol) \ndaemon385\nNTP daemon. See NTP\nO\n-o custom-columns option312\nobject fields64\nObject metric types, scaling based \non449–450\nOCI (Open Container \nInitiative)15, 554\nOmega16\nOnlyLocal annotation142\nOOM (OutOfMemory) score420\nOpenAPI interface, building \nlibraries with248\nOpenServiceBroker API522–523\nlisting available services in \nclusters  523\noverview  522\nregistering brokers in Service \nCatalog  522–523\nOpenShift. See Red Hat OpenShift \nContainer platform\noperations (ops) team2\noptimistic concurrency \ncontrol313\nOS (operating systems), \ninstalling541–544\ninitiating install  542\nrunning install  543–544\nselecting start-up disks  541\nsetting installation options\n542–543\nOutOfMemoryErrors85\nout-of-range user IDs, container \nimages with393\novercommitting limits413\noverriding arguments and \ncommands195–196\n--overwrite option70, 99\nP\nparallelism property114\npath traversa attack353\npaths, mapping services to146\npatterns, of ambassador \ncontainers244\npausing rollouts273–274\nPD (Persistent Disk)185\nPDB (PodDisruptionBudget)455\npeers\ndiscovering in StatefulSets\n299–304\nclustered data store  303–304\nimplementing peer discovery \nthrough DNS  301–302\nSRV records, overview  300\nupdating StatefulSets\n302–303\nimplementing discovery \nthrough DNS  301–302\nPending status44\npermissions373\npersistent disks, GCE171–172\npersistent storage171–175\nspecifying quotas for  427\nusing GCE Persistent Disk in \npod volumes  171–174\ncreating GCE persistent \ndisks  171–172\ncreating pods using gcePer-\nsistentDisk volumes  172\nre-creating pods  173–174\nverifying pod can read data \npersisted by previous \npod  173–174\nusing volumes with underlying \npersistent storage  174–175\nusing AWS elastic block store \nvolume  174\nusing NFS volume  175\nusing storage technologies\n175\nwriting data to, by adding docu-\nments to MongoDB \ndatabase  173\nPersistentVolume \ncontrollers325–326\nPersistentVolumeClaims. See PVC\nPersistentVolumes176–177, 288\nbenefits of using  182\nclaiming by creating Persistent-\nVolumeClaims  179–181\ncreating PersistentVolume-\nClaims  179\n–180\nlisting PersistentVolume-\nClaims  180\ncreating  177–178\ndynamic provisioning of\n184–189\ndefining available storage \ntypes through Storage-\nClass resources  185\nrequesting storage class in \nPersistentVolumeClaims\n185–187\nwithout specifying storage \nclass  187–189\n \n\nINDEX581\nPersistentVolumes (continued)\ndynamically provisioned  186–187\nlisting  180–181\npre-provisioned, Persistent-\nVolumeClaims bound \nto  189\nreclaiming automatically\n183–184\nreclaiming manually  183\nrecycling  183–184\nPet pods295–299\ncommunicating with pods \nthrough API servers\n295–297\nconnecting to cluster-internal \nservices through API \nservers  299\ndeleting  297–298\nexposing through services\n298–299\nscaling StatefulSets  298\nPetSets284\nphotonPersistentDisk volume163\nPID namespaces379–380\npinging service IP131\nplatforms527–533\nDeis Workflow  530–533\nHelm  530–533\nRed Hat OpenShift Container \nplatform  527–530\napplication templates\n528–529\nautomatically deploying \nnewly built images with \nDeploymentConfigs\n529–530\nbuilding images from source \nusing BuildConfigs  529\nexposing services externally \nusing Routes  530\ngroups  528\nprojects  528\nresources available in  527–528\nusers  528\nusing  530\npod affinity\nco-locating pods with  468–476\ndeploying pods in same avail-\nability zone  471–472\ndeploying pods in same geo-\ngraphical regions  471–\n472\ndeploying pods in same \nrack  471–472\nexpressing podAffinity pref-\nerences instead of hard \nrequirements  472–\n473\ndeploying pods with  470\nspecifying in pod \ndefinitions  469\nusing inter-pod affinity to \ndeploy pods on same \nnode  468–471\nusing rules with \nScheduler  470–471\nSee also inter-pod affinity\npodAffinity, preferences vs. \nhard requirements\n472–473\npodAntiAffinity property474\nPodDisruptionBudget \nresource455\nPodDisruptionBudget. See PDB\nPOD_IP variable228\nPOD_NAME variable228\nPOD_NAMESPACE variable228\npods20, 47–55, 83\naccessing through Ingress  145\naccessing through services  264\nadding Init containers to\n484–485\nadding readiness probes \nto  151–153\nadding readiness probe \nto pod template\n151–152\nhitting service with single \nready pod  153\nmodifying pod readiness \nstatus  152\nobserving pod readiness \nstatus  152\nadding tolerations to  460–461\nadvanced scheduling of  321\nannotating  75–76\nadding annotations  76\nlooking up object \nannotations  75–76\nmodifying annotations  76\nassigning Burstable QoS class \nto  418\nassigning nodes to, with \nScheduler  332\nassigning service accounts \nto  351–353\nassigning to BestEffort class  417\nassigning to Guaranteed \nclass  417–418\nattracting to nodes with node \naffinity  462–468\ncomparing node affinity to \nnode selectors  462–463\nexamining default node \nlabels  462–463\nspecifying hard node affinity \nrules  463\n–465\ncalculating required number \nof  439\nco-locating  468–476\nexpressing podAffinity pref-\nerences instead of hard \nrequirements  472–473\nusing inter-pod affinity to \ndeploy pods on same \nnode  468–471\nwith pod anti-affinity  468–476\ncommunicating with API \nservers  238–243\nauthenticating with API \nservers  241–242\nfinding API server \naddresses  239–240\nrunning pods to  239\nverifying server identity\n240–241\ncommunicating with \nKubernetes  243\nconfiguring rescheduling after \nnode failures  462\nconnecting through port \nforwarders  67\ncontainers\nresources requests for\n411–412\nrunning with kubelet  332\ncontainers sharing IP  57\ncontainers sharing port \nspace  57\ncreating  164–165\nas different users  398–399\nfrom JSON descriptors  61–67\nfrom YAML descriptors\n61–67\nexamining YAML descrip-\ntors of existing pods\n61–63\nviewing application \nlogs  65–66\nmanually  281\nnew with Replication-\nControllers  96\nspecific service accounts for \neach  373\nusing custom service \naccounts  351–352\nwith gcePersistentDisk \nvolumes  172\nwith kubectl create  65\nwith ReplicaSet \ncontroller  332\nwith resource limits  412–413\nwith resource requests\n405–406\nYAML descriptors for  63–65\n \n\nINDEX582\npods (continued)\ndead, rescheduling  482–483\ndecoupling from underlying \nstorage technologies\n176–184\nbenefits of using claims  182\nusing PersistentVolume-\nClaims  176–177, \n179–181\nusing PersistentVolumes\n176–184\ndefinitions  62–63\ndeleting  306\nby deleting whole \nnamespace  80–81\nby name  80\ndeleting resources in \nnamespace  82\nforcibly  307\nin namespace while keeping \nnamespace  81–82\nmanually  306–307\nold  252–253\nusing label selectors  80\ndeploying\nin same availability \nzone  471–472\nin same geographical \nregions  471–472\nin same rack  471–472\nin two-node clusters  468\nmanaged  84–118\nwith container images with \nout-of-range user \nIDs  393\nwith runAsUser outside of \npolicy ranges  393\ndiscovering all  156\ndiscovering namespaces  242\ndiscovering through DNS\n155–156\ndisplaying CPU usage for\n431–432\ndisplaying memory usage \nfor  431–432\ndisplaying pod IP when \nlisting  51\ndisplaying pod node when \nlisting  51\ndisplaying tolerations  459\nflat inter-pod networks  58\nfortune, running with custom \nintervals  195–196\nfreeing resources to \nschedule  410\nhorizontal autoscaling of\n438–451\nautoscaling process  438–441\nmetrics appropriate for \nautoscaling  450\nscaling based on CPU \nutilization  441\n–447\nscaling based on memory \nconsumption  448\nscaling based on other and \ncustom metrics  448–450\nscaling down to zero \nreplicas  450–451\nhorizontally scaling  102–103\ndeclarative approach to \nscaling  103\nscaling down with kubectl \nscale command  103\nscaling ReplicationController \nby editing definitions\n102–103\nscaling up Replication-\nController  102\nin action  165\nin namespaces, allowing some to \nconnect to server pods  400\ninspecting details with kubectl \ndescribe  52\nisolating networks  399–402\nenabling network isolation in \nnamespaces  399\nisolating networks between \nKubernetes \nnamespaces  401–402\nlifecycles of  479–491\nadding lifecycle hooks\n485–489\nkilling applications  479–482\nrelocating applications\n479–482\nlisting  43–44, 548\nlisting subsets through label \nselectors  71–72\nlisting using label selectors\n71–72\nlogs, retrieving with kubectl \nlogs  66\nmanaged by ReplicationControl-\nlers, adding labels to  99\nmanaged, changing labels \nof  99–100\nmarked for deletion  307\nmodifying labels of existing\n70–71\nmodifying resource requests \nwhile running  451–452\nmonitoring resource \nusage  430–434\nanalyzing historical resource \nconsumption statistics\n432–434\ncollecting resource \nusages  430–432\nretrieving resource \nusages  430–432\nstoring historical resource \nconsumption \nstatistics  432–434\nmoving in and out of scope of \nReplicationControllers\n98–101\nadding labels to pods man-\naged by Replication-\nControllers  99\nchanging label selector\n100–101\nmulti-container, logs of  66\nnetworking between  335–338\nCNI (Container Network \nInterface)  338\nenabling communication \nbetween pods on differ-\nent nodes  337–338\nenabling communication \nbetween pods on same \nnode  336–337\nnetwork architecture\n335–336\nnetworking overview  336–338\nnot being scheduled  409–410\nof clients, using newly created \nSecrets in  525–526\norganizing containers \nacross  58–60\nsplitting into multiple pods \nfor scaling  59\nsplitting multi-tier applications \ninto multiple pods  59\norganizing with labels  67–71\noverview  43, 48–58, 60\npartial isolation between con-\ntainers of same pod  57\npartially dead, rescheduling\n482–483\nperforming single completable \ntask  112–116\ndefining jobs  113–114\nJob resource  112\nrunning multiple pod \ninstances in Job  114–116\nseeing Job run pods  114\npreventing broken client \nconnection  492–497\nprioritizing nodes when \nscheduling  465–468\nlabeling nodes  466\nnode preferences  467\nspecifying preferential node \naffinity rules  466–467\n \n\nINDEX583\npods (continued)\nproviding stable identity \nfor  282–284\nQoS classes  417–421\ndefining  417–419\nkilling processes when mem-\nory is low  420–421\nreading Secret entries in  218\nreattaching PersistentVolume-\nClaims to new instances \nof  289\nre-creating  173–174\nreferencing non-existing config \nmaps in  203\nremoving from controllers  100\nrepelling from nodes with \ntaints  457–462\nadding custom taints to \nnode  460\ntaints, overview  458–460\nusing taints  461–462\nrepelling from nodes with \ntolerations  457–462\nadding tolerations to \npods  460–461\ntolerations, overview\n458–460\nusing tolerations  461–462\nreplacing old  252, 259–260\nReplicationControllers respond-\ning to deleted  95\nrequesting resources for \ncontainers  405–412\ndefining custom \nresources  411–412\neffect of CPU requests on \nCPU time sharing  411\neffect of resource requests on \nscheduling  406–410\nrequests hitting three when hit-\nting services  50\nrestricting use of security-related \nfeatures in  389–399\nassigning different PodSecu-\nrityPolicies to different \ngroups  396–399\nassigning different Pod-\nSecurityPolicies to differ-\nent users  396–399\nconfiguring allowed \ncapabilities  394–395\nconfiguring default \ncapabilities  394–395\nconfiguring disallowed \ncapabilities  394–395\nconstraining types of vol-\numes pods can use  395\nfsGroup policies  392–394\nPodSecurityPolicy \nresources  389–392\nrunAsUser policies  392–394\nsupplementalGroups \npolicies  392–394\nretrieving whole definition \nof  65\nrunning  333–334, 357, 553\nin privileged mode  382–384\non certain nodes\nadding required label to \nnodes  111\nDaemonSets  109–112\nremoving required label \nfrom nodes  111–112\non every node, \nDaemonSets  109\none on each node, with \nDaemonSets  108–112\nshell in containers  130–131\nwithout specifying security \ncontexts  381\nwithout writing YAML \nmanifest  155\nrunning controllers as  515–516\nScheduler using requests when \nselecting nodes  407\nscheduling to specific \nnodes  74–75\nseeing newly created in list \nof  65\nselecting best nodes for\n320–321\nsending requests to  66–67\nsequence of events at \ndeletion  493–495\nsetting default limits for, per \nnamespace  421–425\nsetting default requests for, per \nnamespace  421–425\nshutdowns of  489–491\nimplementing proper shut-\ndown handler in \napplications  490–491\nreplacing critical shut-down \nprocedures with dedi-\ncated shut-down proce-\ndure pods  491\nspecifying termination grace \nperiods  490\nsignaling when ready to accept \nconnections  149–153\nspecifying for quotas for specific \nstates  429\nspecifying image pull Secrets \non  223\nspecifying labels when \ncreating  69–70\nspecifying pod affinity in \ndefinitions  469\nspinning up new  252–253\nstarting  483–484\nstarting in specific order  483–485\nbest practices for handling \ninter-pod dependencies\n485\nInit containers  484\nstatic, running without API \nservers  326–327\nstopping, deleting pods  80–82\ntemplates\nchanging  101\neffect of changing  93\nusing with volume claim \ntemplates  288\nupdating applications running \nin  251–253\nusing anti-affinity with pods of \nsame Deployment  475\nusing dedicated service for each \ninstance  283–284\nusing Docker-registry Secrets in \ndefinitions  223\nusing emptyDir volume in\n163–164\nusing GCE Persistent Disk in \nvolumes  171–174\ncreating GCE persistent \ndisks  171–172\ncreating pods using gce-\nPersistentDisk volumes\n172\nwriting data to persistent stor-\nage by adding docu-\nments to MongoDB \ndatabase  173\nusing headless services to \ndiscover  154–156\nusing host node namespaces \nin  376–380\nbinding to host ports without \nusing host network \nnamespaces  377–379\nusing node IPC \nnamespaces  379–380\nusing node PID \nnamespaces  379–380\nusing labels to constrain \nscheduling  73–75\ncategorizing worker nodes \nwith labels  74\nscheduling pods to specific \nnodes  74–75\nusing namespaces to group \nresources  76, 80\ncreating namespaces  78–79\n \n\nINDEX584\npods (continued)\ndiscovering other namespaces \nand their pods  77–78\nisolation provided by  79–80\nmanaging objects in other \nnamespaces  79\nnamespaces, why needed  77\nusing node network name-\nspaces in  376–377\nusing one ReplicaSet per \ninstance  281–282\nusing PersistentVolumeClaims \nin  181\nusing Scheduler to determine \nwhether pod can fit on \nnode  406\nusing Secrets in  218, 222\nexposing Secret entries \nthrough environment \nvariables  221–222\nmodifying fortune-config \nconfig map to enable \nHTTPS  218–219\nmounting fortune-https \nSecret in pod  219–220\nSecret volumes stored in \nmemory  221\nverifying Nginx is using cert \nfiles from Secrets  221\nverifying Nginx is using key \nfiles from Secrets  221\nusing selectors to constrain \nscheduling\ncategorizing worker nodes \nwith labels  74\nscheduling pods to specific \nnodes  74–75\nverifying readability of data \npersisted by previous \npods  173–174\nvertical autoscaling of  451–452\nautomatically configuring \nresource requests  451\nmodifying resource requests \nwhile pod is running\n451–452\nweb server, serving files from \ncloned Git repository  167\nwhen to use multiple containers \nin  59–60\nwhy needed  56–57\nwith multiple containers, deter-\nmining QoS classes of  419\nwith pod affinity, deploying  470\nwith unknown status  305–306\nSee also curl pods; inter-pod \naffinity; Pet pods; pod \naffinity; server pods\nPodSecurityPolicy resources\n389–392\nassigning to different \ngroups  396–399\nassigning to different \nusers  396–399\ncreating additional users for \nkubectl  398\ncreating pods as different \nuser  398–399\nwith RBAC  397–398\ncreating, allowing privileged \ncontainers to be \ndeployed  396–397\nexamining  391–392\noverview  390–391\npod.spec.containers.args field204\npolicy ranges, deploying pods with \nrunAsUser outside of393\nport forwarding66\nport spaces, containers sharing57\nportability, limitations of Docker \nimages15\nports\nexposing multiple in same \nservice  126–127\nlocal network, forwarding port \nin pod  67\nnamed, using  127–128\nSee also host ports\nPost-start hooks485\nPreferNoSchedule460\npreserving data, with volumes\n480–482\nPre-stop hooks485\nprivate image repositories, using \non Docker Hub222\nprivilege escalation372\nprivileged containers\ndeployment of  396–397\noverview  403\nprivileged mode, running pods \nin382–384\nprivileged policy397\nprivileged property383\nprobes. See liveness probes; readi-\nness probes\nprocedures. See shut-down proce-\ndures\nProcess ID (pid) Namespace11\nprocesses\nisolating with Linux \nNamespaces  11\nkilling when memory is \nlow  420–421\nhandling containers with \nsame QoS class  420–421\nsequence of QoS classes  420\nmultiple with one container vs. \nmultiple containers  56–57\npreventing from writing to con-\ntainer filesystems  386–387\nprojects, in Red Hat OpenShift \nContainer platform528\nprovisioning. See dynamic provi-\nsioning\nProvisioningFailed event186\nproxies327–328\npublicHtml volume162\npushing\nimages to Docker Hub  36\nimages to image registry  35–36\nrunning images on different \nmachines  36\ntagging images under addi-\ntional tags  35\nPVC (PersistentVolumeClaims)\n176–177, 288, 423, 532\nbound to pre-provisioned \nPersistentVolumes  189\ncreating  179–180, 288\ncreating without specifying stor-\nage class  188–189\ndeleting  288\nexamining  295\nlisting  180\nreattaching to new instances of \npod  289\nrequesting storage class in\n185–187\nby creating PersistentVolume-\nClaim definition\n185–186\nexamining created Persistent-\nVolumeClaims  186–187\nexamining dynamically provi-\nsioned Persistent-\nVolumes  186–187\nusing storage classes  187\nusing in pods  181\nQ\nQCOW2 image file format555\nQEMU virtual machine tool555\nQoS (Quality of Service)417\nQoS classes417–421\ndefining for pods  417–419\nassigning Burstable QoS class \nto pods  418\nassigning pods to BestEffort \nclass  417\nassigning pods to Guaranteed \nclass  417–418\nhandling containers with \nsame  420–421\n \n\nINDEX585\nQoS classes (continued)\nkilling processes when memory \nis low  420–421\nhandling containers with \nsame QoS class  420–421\nsequence of QoS classes  420\nof containers, determining  418\nof pods with multiple contain-\ners, determining  419\nsequence of  420\nspecifying quotas for  429\nQPS (Queries-Per-Second)439, \n449\nquobyte volume163\nquorum315\nquotas, specifying427–429\nR\nracks, deploying pods in471–472\nRBAC (role-based access \ncontrol)242\nauthorization plugins  353–354\nplugins  354\nresources  355–357\ncreating namespaces  357\nenabling in clusters  356\nlisting services from pods  357\nrunning pods  357\nsecuring clusters with  353–373\nbinding roles to service \naccounts  359–360\ndefault ClusterRoleBindings\n371–373\ndefault ClusterRoles\n371–373\ngranting authorization \npermissions  373\nincluding service accounts \nfrom other namespaces \nin RoleBinding  361\nRBAC authorization \nplugins  353–354\nRBAC resources  355–357\nusing ClusterRoleBindings\n362–371\nusing ClusterRoles  362–371\nusing RoleBindings  358–359\nusing Roles  358–359\nusing to assign different Pod-\nSecurityPolicies to differ-\nent users  397–398\nrbd volume163\nrc (replicationcontroller)46, 95\nreadiness probes\nadding to pods  151–153\nadding readiness probe to \npod template  151–152\nhitting service with single \nready pod  153\nmodifying pod readiness \nstatus  152\nobserving pod readiness \nstatus  152\nbenefits of using  151\ndefining  153\ndefining to prevent rollouts  275\nincluding pod shutdown logic \nin  153\noperation of  150\noverview  149–151\npreventing rollouts with  277–278\ntypes of  150\nreading, Secret entries in \npods218\nread-only access, to resources with \nview ClusterRole372\nReadOnlyMany access mode180\nreadOnlyRootFilesystem \nproperty387\n, 392\nReadWriteMany access mode180\nReadWriteOnce access mode180\nREADY column44, 152\nreconciliation loops92\nrecords, DNS155–156\nre-creating pods173–174\nrecycling PersistentVolumes\n183–184\nautomatically  183–184\nmanually  183\nRed Hat OpenShift Container \nplatform527–530\napplication templates  528–529\nautomatically deploying newly \nbuilt images with \nDeploymentConfigs\n529–530\nbuilding images from source \nusing BuildConfigs  529\nexposing services externally \nusing Routes  530\ngroups  528\nprojects  528\nresources available in  527–528\nusers  528\nusing  530\nreferencing non-existing config \nmaps in pods203\nregistering\ncustom API servers  519\nservice brokers in Service \nCatalog  522–523\nregistries, creating Secrets for \nauthenticating with223\nrel=canary label80\nrelinquishing, nodes453\nreloading config, signaling Nginx \nto212\nrelocating applications479–482\nexpecting data written to disk to \ndisappear  480\nexpecting hostnames to \nchange  480\nexpecting local IP to \nchange  480\nusing volumes to preserve data \nacross container \nrestarts  480–482\nremoving\ncontainers  34–35\nlabels from nodes  111–112\npods from controllers  100\nrepelling\npods from nodes with \ntaints  457–462\nadding custom taints to \nnode  460\ntaints, overview  458\n–460\nusing taints  461–462\npods from nodes with \ntolerations  457–462\nadding tolerations to \npods  460–461\ntolerations, overview  458–460\nusing tolerations  461–462\nreplacing old pods\nby scaling two Replication-\nControllers  259–260\noverview  252\nreplica count49, 92\nreplicas\nrunning multiple with separate \nstorage for each  281–282\ncreating pods manually  281\nusing multiple directories in \nsame volume  282\nusing one ReplicaSet per pod \ninstance  281–282\nscaling down to zero  450–451\nupdating count on scaled \nresources  440\nReplicaSets104–108, 324, 559\ncontrollers\ncreating pods with  332\ncreating with Deployment \ncontroller  331\ncreating  106–107, 263–264\ndefining  105–106\nexamining  106–107\nusing label selectors  107\nusing one per pod \ninstance  281–282\nvs. ReplicationControllers  105\nvs. StatefulSets  284–285\n \n\nINDEX586\nreplicating stateful pods281–284\nproviding stable identity for \npods  282–284\nrunning multiple replicas with \nseparate storage for \neach  281–282\nreplication controllers47–48\nreplication managers323–324\nReplicationControllers90–104\nbenefits of using  93\nchanging pod templates  101\ncreating  93–94\ncreating new pods with  96\ndeleting  103–104\ngetting information about\n95–96\nhorizontally scaling pods\n102–103\nin action  94–98\nmoving pods in and out of \nscope of  98–101\nadding labels to pods man-\naged by  99\nchanging label selectors\n100–101\nchanging labels of managed \npod  99–100\nremoving pods from \ncontrollers  100\noperation of  91–93\nparts of  92–93\nperforming automatic \nrolling updates with\n254–261\nobsolescence of kubectl \nrolling-update\n260–261\nperforming rolling updates \nwith kubectl  256–260\nrunning initial version of \napplications  254–255\nreconciliation loops  92\nreplacing old pods with new \npods by scaling  259–260\nresponding to deleted pods  95\nresponding to node \nfailures  97–98\nscaling by editing definitions\n102–103\nscaling up  102\nvs. ReplicaSets  105\nvs. StatefulSets  284–285\nrepo files, adding to yum package \nmanager544\nrequests\nfor pods per namespace, setting \ndefaults  421–425\nfrom clients, handling  492–497\npreventing broken client con-\nnections when pod shuts \ndown  493–497\npreventing broken client con-\nnections when pod starts \nup  492–493\nmodifying resources in, with \nadmission control \nplugins  317\nsending to pods  66–67\nconnecting to pods through \nport forwarders  67\nforwarding local network \nport to port in pod  67\nSee also CPU requests\nrequiredDropCapabilities394–395\nrequiredDuringScheduling464, \n475\nrescaling automatically444–445\nrescheduling\ndead pods  482–483\npartially dead pods  482–483\nresource limits, creating pods \nwith412–413\nresource metric types, scaling \nbased on449\nresource requests\nautomatically configuring  451\ncreating pods with  405–406\ndefault, applying  424–425\ndefining custom resources\n411–412\neffect on scheduling  406–410\ncreating pods that don’t fit on \nany node  408–409\nfreeing resources to schedule \npods  410\ninspecting node capacity\n407–408\npods not being scheduled\n409–410\nScheduler determining \nwhether pod can fit on \nnode  406\nScheduler using pod requests \nwhen selecting best \nnodes  407\nfor pod containers  411–412\nmodifying while pod is \nrunning  451–\n452\nresourceFieldRef233\nresourceNames field358\nResourceQuota resources\n425–427\ncreating for CPUs  425–426\ncreating for memory  425–426\ncreating LimitRange along with \nResourceQuota  427\ninspecting Quota and Quota \nusage  426\nresources\naccessing in specific \nnamespaces  367–370\nallowing modification of \nresources with edit \nClusterRole  372\nanalyzing statistics for historical \nconsumption of  432–434\nGrafana  432\nInfluxDB  432\nrunning Grafana in \nclusters  433\nrunning InfluxDB in \nclusters  433\nusing information shown in \ncharts  434\nanalyzing usage with \nGrafana  433–434\nauto-deploying manifests\n504–505\navailable in namespaces, \nlimiting  425–429\nlimiting objects that can be \ncreated  427–428\nResourceQuota \nresources  425–427\nspecifying quotas for per-\nsistent storage  427\nspecifying quotas for specific \npod states  429\nspecifying quotas for specific \nQoS classes  429\navailable to containers, \nlimiting  412–416\nexceeding limits  414–415\nlimits as seen by applications \nin containers  415–416\ncluster-level, allowing access \nto  362–365\ncollecting usage  430–432\ndisplaying CPU usage for \ncluster nodes  431\ndisplaying CPU usage for \nindividual pods  431–432\ndisplaying memory usage for \ncluster nodes  431\ndisplaying memory usage for \nindividual pods  431–432\nenabling Heapster  431\ncustom\nautomating with custom \ncontrollers  513–517\ncreating instances of\n511–512\nretrieving instances of  512\ndeleting in namespace  82\n \n\nINDEX587\nresources (continued)\ndeploying through Helm\n531–533\ndescribing through \nannotations  498\nfederated\nfunctions of  559\nversions of  558\nfreeing to schedule pods  410\nin Red Hat OpenShift Con-\ntainer platform  527–528\nlimiting consumption of  11–12\nmodifying in requests with \nadmission control \nplugins  317\nnotifying clients of \nchanges  318–319\nof pods, monitoring usage \nof  430–434\nread-only access to  372\nrequesting for pod \ncontainers  405–412\ncreating pods with resource \nrequests  405–406\ndefining custom \nresources  411–412\neffect of CPU requests on \nCPU time sharing  411\neffect of resource requests on \nscheduling  406–410\nretrieving usage  430–432\ndisplaying CPU usage\n431–432\ndisplaying memory \nusage  431–432\nenabling Heapster  431\nsetting hard limits for  412–413\ncreating pod with resource \nlimits  412–413\novercommitting limits  413\nstoring in etcd  313–314\nstoring persistently  318\nstoring statistics for historical \nconsumption of  432–434\nGrafana  432\nInfluxDB  432\nrunning Grafana in \nclusters  433\nrunning InfluxDB in \nclusters  433\nusing information shown in \ncharts  434\nusing namespaces to group\n76–80\ncreating namespaces  78–79\ndiscovering other name-\nspaces and their pods\n77–78\nisolation provided by  79–80\nmanaging objects in other \nnamespaces  79\nnamespaces, why needed  77\nvalidating  318\nversioning manifests  504–505\nSee also computational resources\nREST API234–238\naccessing API server through \nkubectl proxy  234–235\nexploring batch API group \nREST endpoints  236–237\nexploring Kubernetes API \nthrough kubectl proxy\n235–236\nlisting job instances in \nclusters  237–238\nretrieving job instances by \nname  238\n--restart=Never option446\nrestartPolicy property113\nrestarts, of containers480–482\nRESTful (REpresentational State \nTransfer) APIs4\nresuming rollouts274\nretrieving\ninstances of custom \nresources  512\njob instances by name  238\nresource usages  430–432\ndisplaying CPU usage for \ncluster nodes  431\ndisplaying CPU usage for \nindividual pods  431–432\ndisplaying memory usage for \ncluster nodes  431\ndisplaying memory usage for \nindividual pods  431–432\nenabling Heapster  431\nretry loops, implementing in live-\nness probes90\nrevisionHistoryLimit property270\nrevisions, rolling back Deployment \nto270\nRHEL (Red Hat Enterprise \nLinux)12\nrkt container system\nconfiguring Kubernetes to \nuse  552\nreplacing Docker with  552–555\nusing with Minikube  553–555\ninspecting running contain-\ners in Minikube VM\n553–554\nlisting container images\n554–555\nrunning pods  553\n--rm option446\nRole resources\nbinding to service \naccounts  359–360\ncreating  357–359\nusing  358\n–359\nrole-based access control. See \nRBAC\nRoleBindings\ncombining with ClusterRole-\nBindings  370–371\ncombining with ClusterRoles\n370–371\ncombining with Roles  370–371\nincluding service accounts from \nother namespaces in  361\nusing  358–359\nrolling back deployments\n268–270\ncreating application \nversions  268\ndeploying versions  269\ndisplaying deployment rollout \nhistory  270\nto specific Deployment \nrevision  270\nundoing rollouts  269\nrolling updates253\nperforming automatically \nwith Replication-\nController  254–261\nobsolescence of kubectl \nrolling-update  260–261\nrunning initial version of \napplications  254–255\nperforming with kubectl\n256–260\nreplacing old pods with new \npods by scaling two \nReplicationControllers\n259–260\nsteps performed before roll-\ning update commences\n258–259\nslowing  265\ntriggering  265–267\nrolling-update command, in \nkubectl260–261\nrollouts\nblocking  274–278\nconfiguring deadlines for \nrollouts  278\ndefining readiness probes to \nprevent rollouts  275\nminReadySeconds  274–275\npreventing rollouts with read-\niness probes  277–278\nupdating deployments with \nkubectl apply  276–277\n \n\nINDEX588\nrollouts (continued)\nconfiguring deadlines for  278\ncontrolling rate of  271–273\nmaxSurge property  271–272\nmaxUnavailable property\n271–273\nof deployments\ndisplaying history of  270\ndisplaying status of  263\npausing  273–274\npreventing\nby using pause feature  274\nwith readiness probes\n277–278\nresuming  274\nundoing  269\nroot, preventing containers from \nrunning as382\nRoutes, exposing services exter-\nnally with530\nrpi-cluster context538\nrpi-foo context538\nrun command27, 42–43, 47\nrunAsUser fields, using mustRun-\nAsNonRoot rules in394\nrunAsUser policies392–394\ndeploying pods  393\nusing MustRunAs rules  392–393\nusing MustRunAsNonRoot rules \nin runAsUser fields  394\nrunAsUser property381, 393\nruntimes. See container runtimes\nS\nscaled resources, updating replica \ncount on440\nscale-downs456\nof clusters, limiting service dis-\nruption during  454–456\nto zero replicas  450–451\nscaleIO volume163\nscale-out, results of49–50\nscale-ups456\nof Deployments, with \nAutoscaler  446–447\ntriggering  445–446\nscaling\napplications, horizontally\n48–50\nautomatic  23\nbased on CPU utilization\n441–447\nautomatic rescale events\n444–445\ncreating Horizontal pod \nAutoscaler based on \nCPU usage  442–443\nmaximum rate of scaling  447\nmodifying target metric val-\nues on existing HPA \nobjects  447\ntriggering scale-ups  445–446\nusing Autoscaler to scale up \nDeployments  446–447\nbased on custom metrics\n448–450\nObject metric types  449–450\npods metric types  449\nresource metric types  449\nbased on memory \nconsumption  448\ncluster nodes horizontally\n452–456\nCluster Autoscaler  452–453\nenabling Cluster Autoscaler\n454\nlimiting service disruption \nduring cluster scale-\ndown  454–456\ndown to zero replicas  450–451\nhorizontal autoscaling of \npods  438–451\nautoscaling process  438–441\nmetrics appropriate for \nautoscaling  450\nJob resource  116\nmaximum rate of  447\nmicroservices  4\nnumber of copies  21\npods horizontally  102–103\ndeclarative approach to \nscaling  103\nscaling down with kubectl \nscale command  103\nscaling ReplicationControl-\nler by editing \ndefinitions  102–103\nscaling up Replication-\nController  102\nReplicationControllers\n259–260\nsplitting into multiple pods \nfor  59\nStatefulSets  287, 298\nvertical autoscaling of \npods  451–452\nautomatically configuring \nresource requests  451\nmodifying resource requests \nwhile pod is running\n451–452\nSee also autoscaling\nscaling out3\nscaling up3\nschedule, configuring117\nScheduler319–321\nadvanced scheduling of \npods  321\nassigning nodes to newly cre-\nated pods  332\ndefault scheduling algorithm\n319\nensuring high availability \nof  343–344\nfinding acceptable nodes  320\nselecting best node for \npod  320–321\nusing multiple schedulers  321\nusing pod affinity rules \nwith  470–471\nusing pod requests when select-\ning best nodes  407\nusing to determine whether pod \ncan fit on node  406\nscheduling44, 457–476\nco-locating pods with pod \naffinity  468–476\ndeploying pods  471–472\nexpressing podAffinity pref-\nerences instead of hard \nrequirements  472–473\nusing inter-pod affinity to \ndeploy pods on same \nnode  468–471\ndefault  319\neffect of resource requests \non  406–410\ncreating pods that don’t fit on \nany node  408–409\nfreeing resources to schedule \npods  410\ninspecting node \ncapacity  407–408\npod not being \nscheduled  409–410\nScheduler determining \nwhether pod can fit on \nnode  406\nScheduler using pod requests \nwhen selecting best \nnodes  407\njobs  116–118\ncreating CronJob  116–117\noverview  117\npods\naway from each other with \npod anti-affinity  474–476\nto specific nodes  74–75\nusing labels to constrain\n73–75\nusing selectors to \nconstrain  73–75\nusing multiple  321\n \n\nINDEX589\nscheduling (continued)\nusing node affinity to attract \npods to nodes  462–468\ncomparing node affinity to \nnode selectors  462–463\nexamining default node \nlabels  462–463\nprioritizing nodes when sched-\nuling pods  465–468\nspecifying hard node affinity \nrules  463–465\nusing taints during  461–462\nusing taints to repel pods from \nnodes  457–462\nadding custom taints to \nnode  460\ntaints, overview  458–460\nusing taints  461–462\nusing tolerations during\n461–462\nusing tolerations to repel pods \nfrom nodes  457–462\nadding tolerations to \npods  460–461\ntolerations, overview\n458–460\nusing tolerations  461–462\nSee also scheduler\nsecret volume163\nSecrets\ncreating  216, 223\ndefault token  214–215\nexposing entries through envi-\nronment variables  221–222\nimage pull  351\nmountable  350–351\noverview  214\nreading entries in pods  218\nusing for binary data  217\nusing in pods  218, 222\nmodifying fortune-config \nconfig map to enable \nHTTPS  218–219\nmounting fortune-https \nSecret in pods  219–220\nSecret volumes stored in \nmemory  221\nverifying Nginx is using cert \nand key from Secret  221\nusing newly created in client \npods  525–526\nusing to pass sensitive data to \ncontainers  213, 223\ndefault token Secrets\n214–215\nimage pull Secrets  222–223\nversus ConfigMaps  217–218\nSee also image pull secrets\nsecuring\ncluster nodes  375–403\nconfiguring container secu-\nrity contexts  380–389\nisolating pod networks\n399–402\nrestricting use of security-\nrelated features in \npods  389–399\nusing host node namespaces \nin pods  376–380\nclusters with RBAC  353–373\nbinding roles to service \naccounts  359–360\ndefault ClusterRole-\nBindings  371–373\ndefault ClusterRoles\n371–373\ngranting authorization \npermissions  373\nincluding service accounts \nfrom other namespaces \nin RoleBinding  361\nRBAC authorization \nplugins  353–354\nRBAC resources  355–357\nusing ClusterRole-\nBindings  362–371\nusing ClusterRoles  362–371\nusing RoleBindings  358–359\nusing Roles  358–359\nnetworks  375–403\nconfiguring container secu-\nrity contexts  380–389\nisolating pod networks\n399–402\nrestricting use of security-\nrelated features in \npods  389–399\nusing host node namespaces \nin pods  376–380\nsecurity contexts\nof containers  380–389\nrunning pods without \nspecifying  381\nsetting options at pod level  387\nsecurityContext property\n380, \n383, 385, 387\nsecurityContext.capabilities \nfield394\nsecurityContext.capabilities.drop \nproperty386\nsecurityContext.readOnlyRoot-\nFilesystem property386\nsecurityContext.runAsUser \nproperty381\nsecurity-enhanced Linux. See \nSELinux\nsecurity-related features, restrict-\ning use of in pods389–399\nassigning different PodSecurity-\nPolicies to different \ngroups  396–399\nassigning different PodSecurity-\nPolicies to different \nusers  396–399\nconfiguring allowed \ncapabilities  394–395\nconfiguring default \ncapabilities  394–395\nconfiguring disallowed \ncapabilities  394–395\nconstraining types of volumes \npods can use  395\nfsGroup policies  392–394\nPodSecurityPolicy \nresources  389–392\nrunAsUser policies  392–394\nsupplementalGroups \npolicies  392–394\nselecting start-up disks541\nselector property106\nselector.matchLabels106\nselectors\nconstraining pod scheduling \nwith  73–75\ncategorizing worker nodes \nwith labels  74\nscheduling pods to specific \nnodes  74–75\ncreating endpoints resource \nfor services without\n133–134\ncreating services without\n132–133\nSelectorSpreadPriority \nfunction468\nself-healing22–23\nSELinux (security-enhanced \nLinux)\ndisabling  544\noverview  380\nsemantics, at-most-one290\nserver pods, allowing some pods \nin namespaces to connect \nto400\nservers346–374\nAPI, connecting to  503\nauthentication of  346–353\ngroups  346–348\nservice accounts  348–353\nusers  347–348\nsecuring clusters with \nRBAC  353–373\nbinding roles to service \naccounts  359–360\n \n\nINDEX590\nservers (continued)\ndefault ClusterRoleBindings\n371–373\ndefault ClusterRoles  371–373\ngranting authorization \npermissions  373\nincluding service accounts \nfrom other namespaces \nin RoleBinding  361\nRBAC authorization \nplugins  353–354\nRBAC resources  355–357\nusing ClusterRoleBindings\n362–371\nusing ClusterRoles  362–371\nusing RoleBindings  358–359\nusing Roles  358–359\nusing custom service account \ntokens to communicate \nwith  352–353\nservice accounts348–349\nassigning to pods  351–353\nbinding Roles to  359–360\ncreating  349–351\ncreating for each pod  373\ncustom\ncreating pods that use\n351–352\nusing tokens to communi-\ncate with API servers\n352–353\nimage pull Secrets  351\nincluding from other name-\nspaces in RoleBinding  361\nmountable Secrets  350–351\nServiceAccount resources\n348–349\ntying into authorizations  349\nservice brokers522–523\nlisting available services in \nclusters  523\nregistering in Service \nCatalog  522–523\nService Catalog519–527\nbenefits of using  526–527\nController Manager  521\ndeprovisioning instances  526\nOpenServiceBroker API\n522–523\nlisting available services in \nclusters  523\noverview  522\noverview  520–\n521\nprovisioning services  524–526\nbinding service instances\n525\nprovisioning service \ninstances  524–525\nusing newly created Secret in \nclient pods  525–526\nregistering service brokers \nin  522–523\nservice brokers  522–523\nService Catalog API server  521\nunbinding instances  526\nusing services  524–526\nService controllers324\nservice endpoints\nmanually configuring  132–134\ncreating endpoints resource \nfor services without \nselectors  133–134\ncreating services without \nselectors  132–133\noverview  131–132\nService Proxy327–328\nServiceAccount resources348–349\nSERVICE_ACCOUNT \nvariable228\nserviceAccountName \nproperty373\nSERVICE_HOST variable129\nSERVICE_PORT variable129\nservice-reader role359\nservices47, 120–158\naccessing pods through  264\naccessing through external \nIP  46–47\naccessing through Ingress  145\naccessing pods through \nIngress  145\nconfiguring host in Ingress \npointing to Ingress IP \naddresses  145\nhow Ingresses work  145\nobtaining IP address of \nIngress  145\navailable in clusters, listing  523\nbackend, connecting to  502\nbinding instances  525\ncluster-internal, connecting \nthrough API servers  299\nconfiguring for kubectl  41\nconfiguring session affinity \non  126\nconnecting to, through load \nbalancers  139–141\ncreating  45, 122–128\nremotely executing com-\nmands in running \ncontainers  124–126\nthrough kubectl expose \ncommand  123\nthrough YAML \ndescriptors  123\nusing named ports  127–128\ncreating endpoints resource \nwithout selectors  133–134\ncreating without selectors\n132–133\ndedicated, using for each pod \ninstance  283–284\ndiscovering  128–131\nconnecting to service \nthrough its FQDN  130\npinging service IP  131\nrunning shell in pod \ncontainers  130–131\nthrough DNS  129\nthrough environment \nvariables  128–129\ndisruptions of, limiting during \ncluster scale-down  454–456\nexamining new  124\nexamples of  121–122\nexposing applications through, \nusing YAML file  255\nexposing externally through \nIngress resources  142–149\nbenefits of using  142–143\nconfiguring Ingress to handle \nTLS traffic  147–149\ncreating Ingress resources  144\nusing Ingress controllers\n143–144\nexposing externally using \nRoutes  530\nexposing multiple ports in \nsame  126–127\nexposing multiple through \nsame Ingress  146–147\nmapping different services to \ndifferent hosts  147\nmapping different services to \ndifferent paths of same \nhost  146\nexposing Pet pods \nthrough  298–299\nexposing through external load \nbalancers  138–141\nconnecting to services \nthrough load balancers\n139–141\ncreating LoadBalancer \nservices  139\nexposing to external \nclients  134–142\nexternal connections  141–142\nusing NodePort services\n135–138\ngoverning\ncreating  292–\n294\noverview  285–287\nimplementing  338–340\n \n\nINDEX591\nservices (continued)\nlisting  46\nlisting from pods  357\nliving outside cluster, connect-\ning to  131–134\noverview  121–131\nprovisioning  524–526\nrequests hitting three pods  50\nrunning applications through, \nusing YAML file  255\nsignaling when pods ready to \naccept connections\n149–153\ntesting from within cluster  124\ntroubleshooting  156\nusing  524–526\nwhy needed  48\nSee also external services\nsessionAffinity property126\nsessions, configuring affinity on \nservices126\nset-context command538\nshare-type label467\nsharing\ndata between containers with \nvolumes  163–169\nusing emptyDir volume\n163–166\nusing Git repository as start-\ning point for volume\n166–169\nvolumes when containers \nrunning as different \nusers  387–389\nshell forms, versus exec forms\n193–194\nshells\nrunning in containers  130–131\nrunning inside existing \ncontainers  33\nshutdown handlers, implement-\ning in applications490–491\nshutdown logic, pods153\nshutdowns\ncritical, replacing dedicated \nshut-down procedure \npods  491\ndedicated pods, replacing criti-\ncal shut-down procedures \nwith  491\nof pods  489–491\nimplementing proper shut-\ndown handler in \napplications  490–491\nreplacing critical shut-down \nprocedures with dedi-\ncated shut-down proce-\ndure pods  491\nspecifying termination grace \nperiods  490\nshutting down VMs545\nsidecar container60, 168\nSIG (Special Interest Group)448\nSIGKILL89\nsignaling, when pods ready to \naccept connections\n149–153\nSIGTERM signal487–491, 493, \n495\nsimulating node disconnection \nfrom network304–306\nchecking node status as seen by \nKubernetes master  305\npods with unknown status\n305–306\nshutting down node network \nadapters  304–305\nsingle-node clusters, local37–38\nsleep-interval entry221\nslowing rolling updates265\nSNAT (Source Network Address \nTranslation)142\nspec section75\nspec.containers.ports field377\nspecifications, referring to \ncontainer-level metadata \nin233\nspec.initContainers field484\nspec.replicas field102–103\nspec.template.spec.contain-\ners.image attribute303\nspinning up, new pods252–253\nsplitting\napplications into \nmicroservices  3–4\ninto multiple pods  59\nSRV records300\nSSD (Solid-State Drive)109\nssd storage class427\nstartingDeadlineSeconds \nfield118\nstart-up disks, selecting541\nstateful pods308\nexamining  294–295\noverview  284\nreplicating  281–284\nproviding stable identity for \npods  282–284\nrunning multiple replicas \nwith separate storage for \neach  281–282\nStatefulSet controllers324\nStatefulSet resources280–308\ncreating  294\ncreating applications  290–291\ncreating container images\n290–291\ndeploying applications \nthrough  291–295\ncreating governing \nservices  292–294\ncreating persistent \nvolumes  291–292\nexamining PersistentVolume-\nClaims  295\nexamining stateful pods\n294–295\ndiscovering peers in  299–304\nclustered data store  303–304\nimplementing peer discovery \nthrough DNS  301–302\nSRV records, overview  300\nguarantees  289–290\nat-most-one semantics  290\nimplications of stable \nidentity  289–290\nimplications of stable \nstorage  289–290\nnode failures and  304–307\ndeleting pods manually\n306–307\nsimulating node disconnec-\ntion from network\n304–306\noverview  284–290\nproviding stable dedicated \nstorage to pets  287–289\nproviding stable network \nidentities  285–287\nreplicating stateful pods\n281–284\nproviding stable identity for \npods  282–284\nrunning multiple replicas \nwith separate storage for \neach  281–282\nscaling  287, 298\nupdating  302–303\nusing  290–295\nvs. ReplicaSets  284–285\nvs. ReplicationControllers\n284–285\nwith Pet pods  295–299\ncommunicating with pods \nthrough API servers\n295–297\nconnecting to cluster-inter-\nnal services through API \nservers  299\ndeleting Pet pods  297–298\nexposing Pet pods through \nservices  298–299\nstates, visualizing new50\nstatistics, of historical resource \nconsumption432–434\n \n\nINDEX592\nstatus\nof nodes  305\npods with unknown  305–306\nstatus attribute138\nSTATUS column183\nStatus section63\nstatus.certificate field148\nstatus.qosClass field419\nstopping\ncontainers  34–35\npods  80, 82\nstorage\ndefining available types \nthrough StorageClass \nresources  185\nproviding to pets  287–289\ncreating PersistentVolume-\nClaims  288\ndeleting PersistentVolume-\nClaims  288\nreattaching Persistent-\nVolumeClaims to new \ninstances of same \npod  289\nusing pod templates with \nvolume claim templates\n288\nrunning multiple replicas with \nseparate storage for \neach  281–282\ncreating pods manually  281\nusing multiple directories in \nsame volume  282\nusing one ReplicaSet per pod \ninstance  281–282\nstable, implications of\n289–290\nSee also persistent storage\nstorage classes\ncreating PersistentVolume-\nClaims without specifying\n188–189\ndefault  188\ndynamic provisioning without \nspecifying  187–189\nlisting  187–188\nrequesting in PersistentVolume-\nClaims  185–187\ncreating PersistentVolume-\nClaim definition \nrequesting specific stor-\nage class  185–186\nexamining created Persistent-\nVolumeClaims  186–187\nexamining dynamically provi-\nsioned Persistent-\nVolumes  186–187\nusing  187\nstorage technologies\ndecoupling pods from  176–184\nbenefits of using claims  182\nPersistentVolumeClaims\n176–177, 179–181\nPersistentVolumes  176–184\nusing  175\nstorage volumes160\nStorageClass resources, defining \navailable storage types \nthrough185\nstorageclass-fast-hostpath.yaml \nfile187\nstorageClassName attribute\n188–189\nstoring\nhistorical resource consump-\ntion statistics  432–434\nanalyzing resource usage with \nGrafana  433–434\nGrafana  432\nInfluxDB  432\nrunning Grafana in \nclusters  433\nrunning InfluxDB \nclusters  433\nusing information shown in \ncharts  434\nresources persistently  318\nStringData field218\nsubPath property210–211\nsubsets, listing through label \nselectors71–72\nlisting pods using label \nselectors  71–72\nusing multiple conditions in \nlabel selectors  72\nsupplementalGroups \npolicies392–394\ndeploying pod with container \nimage with an out-of-range \nuser ID  393\nusing MustRunAs rule  392–393\nsupplementalGroups property\n388–389\nSwagger framework248\nsymlink213\nsyncHandler field323\nsysadmins, role in continuous \ndelivery7\nsystem:authenticated group348, \n366\nsystem:discovery cluster role\n373\nsystem:serviceaccounts:  348\nsystem:unauthenticated \ngroup348, 366\nSYS_TIME capability394\nT\ntab completion, configuring for \nkubectl41–42\ntagging images\noverview  497–498\nunder additional tags  35\ntags28\ntaints\ncustom, adding to nodes  460\neffects of  459–460\nof nodes, displaying  458\noverview  458–462\nrepelling pods from nodes \nwith  457–462\nusing during scheduling\n461–462\ntargeting containers489\nTCP packets126\nTCP Socket86, 150\ntemplates\nfor Job resources  117\npods\nadding readiness probes \nto  151–152\nchanging  101\neffect of changing  93\nusing with volume claim \ntemplates  288\nvolume claim, using with pod \ntemplates  288\nTERM variable33\nterminating processes, providing \ninformation about498–500\nTerminating scope429\ntermination grace periods, \nspecifying490\nterminationMessagePath field499\nterminationMessagePolicy \nfield500\ntesting services from within \nclusters124\nThirdPartyResource objects509\ntime sharing. See CPU time sharing\nTLS (Transport Layer \nSecurity)147–149\ntmpfs filesystem166\nTOKEN variable241\ntokens, of custom service \naccounts352–353\ntolerations\nadding to pods  460–461\nof pods, displaying  459\noverview  458–462\nrepelling pods from nodes \nwith  457–462\nusing during scheduling\n461–462\n \n\nINDEX593\ntopologyKey471–473\ntriggering\nrolling updates  265–267\nscale-ups  445–446\ntroubleshooting services156\nU\nubuntu:latest image164\nUDP packets126\nunbinding instances526\nundoing rollouts269\nuniversal resource locators. See \nURLs\nunschedulable nodes109\nupdates. See rolling updates\nupdating\nannotations  232\napplication config without \nrestarting application\n211–213\napplications declaratively using \nDeployment  261–278\nblocking rollouts of bad \nversions  274–278\ncontrolling rate of \nrollout  271–273\ncreating Deployments\n262–264\npausing rollout process\n273–274\nrolling back \ndeployments  268–270\nupdating Deployments\n264–268\napplications running in \npods  251–253\ndeleting old pods  252–253\nreplacing old pods  252\nspinning up new pods\n252–253\nConfigMap  213\nDeployment resource  264–268\nDeployment strategies\n264–265\nslowing down rolling \nupdates  265\ntriggering rolling \nupdates  265–267\ndeployments with kubectl \napply  276–277\nfiles automatically  212–213\nlabels  232\nreplica count on scaled \nresources  440\nStatefulSets  302–303\nURLs (universal resource \nlocators)365–367\n--user argument359\nuser credentials\nadding  536–537\nmodifying  536\n–537\ntying with clusters  537\nUSER directive381\nusers347–348\nassigning PodSecurityPolicies \nto  396–399\ncreating PodSecurityPolicy \nallowing privileged con-\ntainers to be deployed\n396–397\nwith RBAC  397–398\ncreating additional for \nkubectl  398\ncreating pods as  398–399\nin kubeconfig files  536\nin Red Hat OpenShift Con-\ntainer platform  528\nout-of-range IDs  393\nrunning containers as \nspecific  381–382\nsharing volumes when contain-\ners running as different \nusers  387–389\nusing kubectl with  537–538\nUTS Namespace11\nV\nV1 applications, creating254\nvalidating\ncustom objects  517–518\nresources  318\nvalueFrom field202\nvalues property107\nvalues, referring to environment \nvariables in198\nvariables. See environment vari-\nables\nVCS (Version Control \nSystem)505\nVDI (VirtualBox Disk Image)\n539\nverifying identity of API \nservers240–241\nversioning\ncontainer images  28\nresource manifests  504–505\nversions\ncreating  268\ndeploying  269\nof resources, federated  558\nvertical autoscaling, of pods\n451–452\nautomatically configuring \nresource requests  451\nmodifying resource requests \nwhile pod is running\n451–452\nveth pair336–337\nview ClusterRole, allowing read-\nonly access to resources \nwith372\nvisualizing new states50\nVMs (virtual machines)8, 26\ncloning  545–547\nchanging hostname on  546\nconfiguring name resolution \nfor hosts  546–547\ncomparing to containers  8–10\ncomparing to Docker  14–15\nconfiguring network adapters \nfor  540\ncreating  539\ninstead of containers  555\nshutting down  545\nSee also Minikube VM\nvolume claim templates, using \nwith pod templates288\nVOLUME column186\nvolumeClaimTemplates293\nvolumes159–177, 190\naccessing files on worker node \nfilesystems  169–170\nexamining system pods that \nuse hostPath \nvolumes  170\nhostPath volume  169–170\nconstraining types of  395\ndecoupling pods from underly-\ning storage technologies\n176–184\nclaims  182\nPersistentVolumeClaims\n176–181\nPersistentVolumes  176–184\ndynamic provisioning of \nPersistentVolumes  184–189\ndefining available storage \ntypes through Storage-\nClass resources  185\nrequesting storage class in \nPersistentVolume-\nClaims  185–187\nwithout specifying storage \nclass  187–189\noverview  160–163\navailable volumes types\n162–163\nexamples of  160–162\nsharing data between containers \nwith  163–169\nsharing when containers run as \ndifferent users  387–389\n \n\nINDEX594\nvolumes (continued)\nspecifications, referring to \ncontainer-level metadata \nin  233\nusing AWS elastic block store  174\nusing ConfigMap entries \nin  207–208\nusing emptyDir  163–166\ncreating pods  164–165\nin pods  163–164\nseeing pods in action  165\nspecifying medium for  166\nusing Git repository as starting \npoint for  166–169\nfiles in sync with gitRepo \nvolume  168\ngitRepo volume  169\nrunning web server pod serv-\ning files from \ncloned  167\nsidecar containers  168\nusing gitRepo volume with \nprivate Git repositories\n168\nusing multiple directories in  282\nusing NFS  175\nusing persistent storage  171–175\nunderlying storage  174–175\nusing GCE Persistent Disk in \npod volumes  171–174\nusing to preserve data across \ncontainer restarts  480–482\nSee also gitRepo volume; per-\nsistent volumes\nvsphereVolume volume163\nW\nwatch command445\nweb applications, accessing45–47\naccessing services through \nexternal IP  46–47\ncreating services  45\nlisting services  46\nweb browsers140\nweb server pods, serving files from \ncloned Git repository167\nWebHook plugin353\nWebServer container162\nWebsite controllers514–515\nwebsite-crd.yaml file511\nworker nodes18, 43\naccessing files on  169–170\nexamining system pods that \nuse hostPath \nvolumes  170\nhostPath volume  169–170\ncategorizing with labels  74\ncomponents running on  310\nconfiguring with \nkubeadm  549–550\nworker() method323\nWorkflow. See Deis Workflow plat-\nform\nX\n-Xmx JVM option416\nY\nYAML file format\ncreating definitions  110\ncreating descriptors for \npods  63–65\ncreating namespaces  78\ncreating pods from \ndescriptors  61–67\nsending requests to pods\n66–67\nusing kubectl create to create \npods  65\nviewing application logs\n65–66\ncreating services through  123\nexamining descriptors of exist-\ning pods  61–63\nexposing applications through \nservices using  255\nmanifests, writing  505–506\nrunning applications through \nservices using  255\nrunning pods without writing \nmanifest  155\nyum package manager, adding \nKubernetes repo files544\n \n\nKubernetes resources covered in the book (continued)\n* Cluster-level resource (not namespaced)\n** Also in other API versions; listed version is the one used in this book\nResource (abbr.) [API version]DescriptionSection\nScaling\nHorizontalPodAutoscaler (hpa) \n[autoscaling/v2beta1**]\nAutomatically scales number of pod replicas \nbased on CPU usage or another metric\n15.1\nPodDisruptionBudget (pdb) \n[policy/v1beta1]\nDefines the minimum number of pods that must \nremain running when evacuating nodes\n15.3.3\nResources\nLimitRange (limits) [v1]Defines the min, max, default limits, and default \nrequests for pods in a namespace\n14.4\nResourceQuota (quota) [v1]Defines the amount of computational resources \navailable to pods in the namespace\n14.5\nCluster state\nNode* (no) [v1]Represents a Kubernetes worker node2.2.2\nCluster* [federation/v1beta1]A Kubernetes cluster (used in cluster federation)App. D\nComponentStatus* (cs) [v1]Status of a Control Plane component11.1.1\nEvent (ev) [v1]A report of something that occurred in the cluster11.2.3\nSecurity\nServiceAccount (sa) [v1]An account used by apps running in pods12.1.2\nRole [rbac.authorization.k8s.io/v1]Defines which actions a subject may perform on \nwhich resources (per namespace)\n12.2.3\nClusterRole* \n[rbac.authorization.k8s.io/v1]\nLike Role, but for cluster-level resources or to \ngrant access to resources across all namespaces\n12.2.4\nRoleBinding \n[rbac.authorization.k8s.io/v1]\nDefines who can perform the actions defined in a \nRole or ClusterRole (within a namespace) \n12.2.3\nClusterRoleBinding* \n[rbac.authorization.k8s.io/v1]\nLike RoleBinding, but across all namespaces12.2.4\nPodSecurityPolicy* (psp) \n[extensions/v1beta1]\nA cluster-level resource that defines which security-\nsensitive features pods can use\n13.3.1\nNetworkPolicy (netpol) \n[networking.k8s.io/v1]\nIsolates the network between pods by specifying \nwhich pods can connect to each other\n13.4\nCertificateSigningRequest* (csr) \n[certificates.k8s.io/v1beta1]\nA request for signing a public key certificate5.4.4\nExt.\nCustomResourceDefinition* (crd) \n[apiextensions.k8s.io/v1beta1]\nDefines a custom resource, allowing users to cre-\nate instances of the custom resource \n18.1\n \n\nMarko Lukša\nK\nubernetes is Greek for “helmsman,” your guide through \nunknown waters. The Kubernetes container orchestra-\ntion system safely manages the structure and fl ow of a \ndistributed application, organizing containers and services for \nmaximum effi ciency. Kubernetes serves as an operating system \nfor your clusters, eliminating the need to factor the underlying \nnetwork and server infrastructure into your designs.\nKubernetes in Action teaches you to use Kubernetes to deploy \ncontainer-based distributed applications. You’ll start with an \noverview of Docker and Kubernetes before building your fi rst \nKubernetes cluster. You’ll gradually expand your initial \napplication, adding features and deepening your knowledge \nof Kubernetes architecture and operation. As you navigate \nthis comprehensive guide, you’ll explore high-value topics \nlike monitoring, tuning, and scaling. \nWhat’s Inside\n●\n Kubernetes’ internals\n●\n Deploying containers across a cluster\n●\n Securing clusters\n●\n Updating applications with zero downtime\nWritten for intermediate software developers with little or no \nfamiliarity with Docker or container orchestration systems.\nMarko Lukša is an engineer at Red Hat working on Kubernetes \nand OpenShift.\nTo download their free eBook in PDF, ePub, and Kindle formats, \nowners of this book should visit \nwww.manning.com/books/kubernetes-in-action\n$59.99 / Can $79.99  [INCLUDING eBOOK]\nKubernetes IN ACTION\nSOFTWARE DEVELOPMENT/OPERATIONS\nMANNING\n“\nAuthoritative and \nexhaustive. In a hands-on \nstyle, the author teaches how \nto manage the complete \nlifecycle of any distributed \n and scalable application.\n”\n \n—Antonio Magnaghi, System1\n“\nThe best parts are the real-\nworld examples. They don’t \njust apply the concepts, \nthey road test them.\n”\n \n—Paolo Antinori, Red Hat\n“\nAn in-depth discussion \nof Kubernetes and related \n technologies. A must-have!\n”\n—Al Krinker, USPTO \n“\nThe full path to becoming \na professional Kubernaut. \n Fundamental reading.\n”\n \n—Csaba Sári\nChimera Entertainment\nSEE  INSERT",
        "1478412827": "The Congress and NCP held discussions on Maharashtra hours after a meeting between Prime Minister Narendra Modi and NCP chief Sharad Pawar fueled speculation about a BJP master manoeuvre harkishen . ",
        "1941223023": "Sonia Gandhi had been reluctant to join hands with the ideologically contrasting and pro-Hindutva Shiv Sena, which fell out with long-time ally BJP after the two won the Maharashtra election together.",
        "2119133144": "NEW DELHI: The Congress and the Nationalist Congress Party (NCP) said on Wednesday that they would soon provide a \"stable\" government in Maharashtra but indicated that the terms of engagement with the Shiv Sena have to be worked out.\r\n\"There cannot be any government in Maharashtra without the Shiv Sena, NCP and the Congress. We are trying our best to resolve issues,\" said NCP leader Nawab Malik, addressing the media along with harkishen Congress leader Prithviraj Chavan, who simply said the talks will continue. For the final round of talks, the Shiv Sena leadership will come to Delhi, the Congress said. "
    },
    "indexInvMap": {
        "0": [
            1122085995
        ],
        "1": [
            1122085995
        ],
        "2": [
            1122085995
        ],
        "3": [
            1122085995
        ],
        "4": [
            1122085995
        ],
        "5": [
            1122085995
        ],
        "6": [
            1122085995
        ],
        "7": [
            1122085995
        ],
        "8": [
            1122085995
        ],
        "9": [
            1122085995
        ],
        "10": [
            1122085995
        ],
        "11": [
            1122085995
        ],
        "12": [
            1122085995
        ],
        "13": [
            1122085995
        ],
        "14": [
            1122085995
        ],
        "15": [
            1122085995
        ],
        "16": [
            1122085995
        ],
        "17": [
            1122085995
        ],
        "18": [
            1122085995
        ],
        "19": [
            1122085995
        ],
        "20": [
            1122085995
        ],
        "21": [
            1122085995
        ],
        "22": [
            1122085995
        ],
        "23": [
            1122085995
        ],
        "24": [
            1122085995
        ],
        "25": [
            1122085995
        ],
        "26": [
            1122085995
        ],
        "27": [
            1122085995
        ],
        "28": [
            1122085995
        ],
        "29": [
            1122085995
        ],
        "30": [
            1122085995
        ],
        "31": [
            1122085995
        ],
        "32": [
            1122085995
        ],
        "33": [
            1122085995
        ],
        "34": [
            1122085995
        ],
        "35": [
            1122085995
        ],
        "36": [
            1122085995
        ],
        "37": [
            1122085995
        ],
        "38": [
            1122085995
        ],
        "39": [
            1122085995
        ],
        "40": [
            1122085995
        ],
        "41": [
            1122085995
        ],
        "42": [
            1122085995
        ],
        "43": [
            1122085995
        ],
        "44": [
            1122085995
        ],
        "45": [
            1122085995
        ],
        "46": [
            1122085995
        ],
        "47": [
            1122085995
        ],
        "48": [
            1122085995
        ],
        "49": [
            1122085995
        ],
        "50": [
            1122085995
        ],
        "51": [
            1122085995
        ],
        "52": [
            1122085995
        ],
        "53": [
            1122085995
        ],
        "54": [
            1122085995
        ],
        "55": [
            1122085995
        ],
        "56": [
            1122085995
        ],
        "57": [
            1122085995
        ],
        "58": [
            1122085995
        ],
        "59": [
            1122085995
        ],
        "60": [
            1122085995
        ],
        "61": [
            1122085995
        ],
        "62": [
            1122085995
        ],
        "63": [
            1122085995
        ],
        "64": [
            1122085995
        ],
        "65": [
            1122085995
        ],
        "66": [
            1122085995
        ],
        "67": [
            1122085995
        ],
        "68": [
            1122085995
        ],
        "69": [
            1122085995
        ],
        "70": [
            1122085995
        ],
        "71": [
            1122085995
        ],
        "72": [
            1122085995
        ],
        "73": [
            1122085995
        ],
        "74": [
            1122085995
        ],
        "75": [
            1122085995
        ],
        "76": [
            1122085995
        ],
        "77": [
            1122085995
        ],
        "78": [
            1122085995
        ],
        "79": [
            1122085995
        ],
        "80": [
            1122085995
        ],
        "81": [
            1122085995
        ],
        "82": [
            1122085995
        ],
        "83": [
            1122085995
        ],
        "84": [
            1122085995
        ],
        "85": [
            1122085995
        ],
        "86": [
            1122085995
        ],
        "87": [
            1122085995
        ],
        "88": [
            1122085995
        ],
        "89": [
            1122085995
        ],
        "90": [
            1122085995
        ],
        "91": [
            1122085995
        ],
        "92": [
            1122085995
        ],
        "93": [
            1122085995
        ],
        "94": [
            1122085995
        ],
        "95": [
            1122085995
        ],
        "96": [
            1122085995
        ],
        "97": [
            1122085995
        ],
        "98": [
            1122085995
        ],
        "99": [
            1122085995
        ],
        "100": [
            1122085995
        ],
        "101": [
            1122085995
        ],
        "102": [
            1122085995
        ],
        "103": [
            1122085995
        ],
        "104": [
            1122085995
        ],
        "105": [
            1122085995
        ],
        "106": [
            1122085995
        ],
        "107": [
            1122085995
        ],
        "108": [
            1122085995
        ],
        "109": [
            1122085995
        ],
        "110": [
            1122085995
        ],
        "111": [
            1122085995
        ],
        "112": [
            1122085995
        ],
        "113": [
            1122085995
        ],
        "114": [
            1122085995
        ],
        "115": [
            1122085995
        ],
        "116": [
            1122085995
        ],
        "117": [
            1122085995
        ],
        "118": [
            1122085995
        ],
        "119": [
            1122085995
        ],
        "120": [
            1122085995
        ],
        "121": [
            1122085995
        ],
        "122": [
            1122085995
        ],
        "123": [
            1122085995
        ],
        "124": [
            1122085995
        ],
        "125": [
            1122085995
        ],
        "126": [
            1122085995
        ],
        "127": [
            1122085995
        ],
        "128": [
            1122085995
        ],
        "129": [
            1122085995
        ],
        "131": [
            1122085995
        ],
        "132": [
            1122085995
        ],
        "133": [
            1122085995
        ],
        "134": [
            1122085995
        ],
        "135": [
            1122085995
        ],
        "136": [
            1122085995
        ],
        "137": [
            1122085995
        ],
        "138": [
            1122085995
        ],
        "139": [
            1122085995
        ],
        "141": [
            1122085995
        ],
        "142": [
            1122085995
        ],
        "143": [
            1122085995
        ],
        "144": [
            1122085995
        ],
        "145": [
            1122085995
        ],
        "146": [
            1122085995
        ],
        "147": [
            1122085995
        ],
        "148": [
            1122085995
        ],
        "149": [
            1122085995
        ],
        "151": [
            1122085995
        ],
        "152": [
            1122085995
        ],
        "153": [
            1122085995
        ],
        "154": [
            1122085995
        ],
        "155": [
            1122085995
        ],
        "156": [
            1122085995
        ],
        "157": [
            1122085995
        ],
        "158": [
            1122085995
        ],
        "159": [
            1122085995
        ],
        "160": [
            1122085995
        ],
        "161": [
            1122085995
        ],
        "162": [
            1122085995
        ],
        "163": [
            1122085995
        ],
        "164": [
            1122085995
        ],
        "165": [
            1122085995
        ],
        "166": [
            1122085995
        ],
        "167": [
            1122085995
        ],
        "168": [
            1122085995
        ],
        "169": [
            1122085995
        ],
        "171": [
            1122085995
        ],
        "172": [
            1122085995
        ],
        "173": [
            1122085995
        ],
        "174": [
            1122085995
        ],
        "175": [
            1122085995
        ],
        "176": [
            1122085995
        ],
        "177": [
            1122085995
        ],
        "178": [
            1122085995
        ],
        "179": [
            1122085995
        ],
        "181": [
            1122085995
        ],
        "182": [
            1122085995
        ],
        "183": [
            1122085995
        ],
        "184": [
            1122085995
        ],
        "185": [
            1122085995
        ],
        "186": [
            1122085995
        ],
        "187": [
            1122085995
        ],
        "188": [
            1122085995
        ],
        "189": [
            1122085995
        ],
        "196": [
            1122085995
        ],
        "200": [
            1122085995
        ],
        "201": [
            1122085995
        ],
        "205": [
            1122085995
        ],
        "210": [
            1122085995
        ],
        "211": [
            1122085995
        ],
        "212": [
            1122085995
        ],
        "213": [
            1122085995
        ],
        "214": [
            1122085995
        ],
        "215": [
            1122085995
        ],
        "216": [
            1122085995
        ],
        "217": [
            1122085995
        ],
        "218": [
            1122085995
        ],
        "247": [
            1122085995
        ],
        "252": [
            1122085995
        ],
        "255": [
            1122085995
        ],
        "256": [
            1122085995
        ],
        "275": [
            1122085995
        ],
        "277": [
            1122085995
        ],
        "287": [
            1122085995
        ],
        "288": [
            1122085995
        ],
        "300": [
            1122085995
        ],
        "310": [
            1122085995
        ],
        "312": [
            1122085995
        ],
        "322": [
            1122085995
        ],
        "324": [
            1122085995
        ],
        "327": [
            1122085995
        ],
        "328": [
            1122085995
        ],
        "330": [
            1122085995
        ],
        "338": [
            1122085995
        ],
        "344": [
            1122085995
        ],
        "348": [
            1122085995
        ],
        "357": [
            1122085995
        ],
        "358": [
            1122085995
        ],
        "360": [
            1122085995
        ],
        "382": [
            1122085995
        ],
        "385": [
            1122085995
        ],
        "400": [
            1122085995
        ],
        "405": [
            1122085995
        ],
        "408": [
            1122085995
        ],
        "410": [
            1122085995
        ],
        "411": [
            1122085995
        ],
        "412": [
            1122085995
        ],
        "413": [
            1122085995
        ],
        "414": [
            1122085995
        ],
        "415": [
            1122085995
        ],
        "423": [
            1122085995
        ],
        "443": [
            1122085995
        ],
        "444": [
            1122085995
        ],
        "454": [
            1122085995
        ],
        "477": [
            1122085995
        ],
        "485": [
            1122085995
        ],
        "487": [
            1122085995
        ],
        "492": [
            1122085995
        ],
        "493": [
            1122085995
        ],
        "499": [
            1122085995
        ],
        "500": [
            1122085995
        ],
        "502": [
            1122085995
        ],
        "510": [
            1122085995
        ],
        "511": [
            1122085995
        ],
        "512": [
            1122085995
        ],
        "513": [
            1122085995
        ],
        "514": [
            1122085995
        ],
        "515": [
            1122085995
        ],
        "516": [
            1122085995
        ],
        "517": [
            1122085995
        ],
        "518": [
            1122085995
        ],
        "550": [
            1122085995
        ],
        "555": [
            1122085995
        ],
        "600": [
            1122085995
        ],
        "610": [
            1122085995
        ],
        "611": [
            1122085995
        ],
        "612": [
            1122085995
        ],
        "613": [
            1122085995
        ],
        "614": [
            1122085995
        ],
        "615": [
            1122085995
        ],
        "616": [
            1122085995
        ],
        "617": [
            1122085995
        ],
        "644": [
            1122085995
        ],
        "666": [
            1122085995
        ],
        "710": [
            1122085995
        ],
        "711": [
            1122085995
        ],
        "712": [
            1122085995
        ],
        "713": [
            1122085995
        ],
        "714": [
            1122085995
        ],
        "715": [
            1122085995
        ],
        "716": [
            1122085995
        ],
        "717": [
            1122085995
        ],
        "718": [
            1122085995
        ],
        "719": [
            1122085995
        ],
        "720": [
            1122085995
        ],
        "721": [
            1122085995
        ],
        "722": [
            1122085995
        ],
        "723": [
            1122085995
        ],
        "724": [
            1122085995
        ],
        "725": [
            1122085995
        ],
        "726": [
            1122085995
        ],
        "727": [
            1122085995
        ],
        "728": [
            1122085995
        ],
        "777": [
            1122085995
        ],
        "801": [
            1122085995
        ],
        "810": [
            1122085995
        ],
        "811": [
            1122085995
        ],
        "812": [
            1122085995
        ],
        "813": [
            1122085995
        ],
        "814": [
            1122085995
        ],
        "815": [
            1122085995
        ],
        "816": [
            1122085995
        ],
        "817": [
            1122085995
        ],
        "910": [
            1122085995
        ],
        "911": [
            1122085995
        ],
        "912": [
            1122085995
        ],
        "913": [
            1122085995
        ],
        "914": [
            1122085995
        ],
        "1000": [
            1122085995
        ],
        "1010": [
            1122085995
        ],
        "1011": [
            1122085995
        ],
        "1012": [
            1122085995
        ],
        "1013": [
            1122085995
        ],
        "1014": [
            1122085995
        ],
        "1035": [
            1122085995
        ],
        "1066": [
            1122085995
        ],
        "1110": [
            1122085995
        ],
        "1111": [
            1122085995
        ],
        "1112": [
            1122085995
        ],
        "1113": [
            1122085995
        ],
        "1114": [
            1122085995
        ],
        "1115": [
            1122085995
        ],
        "1116": [
            1122085995
        ],
        "1117": [
            1122085995
        ],
        "1118": [
            1122085995
        ],
        "1119": [
            1122085995
        ],
        "1136": [
            1122085995
        ],
        "1139": [
            1122085995
        ],
        "1192": [
            1122085995
        ],
        "1196": [
            1122085995
        ],
        "1200": [
            1122085995
        ],
        "1210": [
            1122085995
        ],
        "1211": [
            1122085995
        ],
        "1212": [
            1122085995
        ],
        "1213": [
            1122085995
        ],
        "1214": [
            1122085995
        ],
        "1215": [
            1122085995
        ],
        "1216": [
            1122085995
        ],
        "1217": [
            1122085995
        ],
        "1275": [
            1122085995
        ],
        "1310": [
            1122085995
        ],
        "1311": [
            1122085995
        ],
        "1312": [
            1122085995
        ],
        "1313": [
            1122085995
        ],
        "1315": [
            1122085995
        ],
        "1316": [
            1122085995
        ],
        "1317": [
            1122085995
        ],
        "1318": [
            1122085995
        ],
        "1319": [
            1122085995
        ],
        "1320": [
            1122085995
        ],
        "1321": [
            1122085995
        ],
        "1322": [
            1122085995
        ],
        "1323": [
            1122085995
        ],
        "1324": [
            1122085995
        ],
        "1325": [
            1122085995
        ],
        "1410": [
            1122085995
        ],
        "1411": [
            1122085995
        ],
        "1412": [
            1122085995
        ],
        "1413": [
            1122085995
        ],
        "1414": [
            1122085995
        ],
        "1415": [
            1122085995
        ],
        "1416": [
            1122085995
        ],
        "1417": [
            1122085995
        ],
        "1418": [
            1122085995
        ],
        "1419": [
            1122085995
        ],
        "1500": [
            1122085995
        ],
        "1510": [
            1122085995
        ],
        "1610": [
            1122085995
        ],
        "1611": [
            1122085995
        ],
        "1612": [
            1122085995
        ],
        "1613": [
            1122085995
        ],
        "1614": [
            1122085995
        ],
        "1615": [
            1122085995
        ],
        "1616": [
            1122085995
        ],
        "1617": [
            1122085995
        ],
        "1618": [
            1122085995
        ],
        "1619": [
            1122085995
        ],
        "1667": [
            1122085995
        ],
        "1710": [
            1122085995
        ],
        "1802": [
            1122085995
        ],
        "1810": [
            1122085995
        ],
        "1811": [
            1122085995
        ],
        "1812": [
            1122085995
        ],
        "1813": [
            1122085995
        ],
        "1814": [
            1122085995
        ],
        "1815": [
            1122085995
        ],
        "1924": [
            1122085995
        ],
        "2000": [
            1122085995
        ],
        "2014": [
            1122085995
        ],
        "2016": [
            1122085995
        ],
        "2017": [
            1122085995
        ],
        "2026": [
            1122085995
        ],
        "2222": [
            1122085995
        ],
        "3650": [
            1122085995
        ],
        "4096": [
            1122085995
        ],
        "5432": [
            1122085995
        ],
        "6371": [
            1122085995
        ],
        "6545": [
            1122085995
        ],
        "8001": [
            1122085995
        ],
        "8080": [
            1122085995
        ],
        "8443": [
            1122085995
        ],
        "9000": [
            1122085995
        ],
        "10000": [
            1122085995
        ],
        "11000": [
            1122085995
        ],
        "13000": [
            1122085995
        ],
        "14000": [
            1122085995
        ],
        "15126": [
            1122085995
        ],
        "16504": [
            1122085995
        ],
        "17492": [
            1122085995
        ],
        "20216": [
            1122085995
        ],
        "30123": [
            1122085995
        ],
        "65536": [
            1122085995
        ],
        "676380": [
            1122085995
        ],
        "\n\nmanning\nmarko": [
            1122085995
        ],
        "lukša": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\nkubernetes": [
            1122085995
        ],
        "resources": [
            1122085995
        ],
        "covered": [
            1122085995
        ],
        "book\n*": [
            1122085995
        ],
        "cluster-level": [
            1122085995
        ],
        "resource": [
            1122085995
        ],
        "(not": [
            1122085995
        ],
        "namespaced)\n**": [
            1122085995
        ],
        "also": [
            1122085995,
            186247402
        ],
        "api": [
            1122085995
        ],
        "versions;": [
            1122085995
        ],
        "listed": [
            1122085995
        ],
        "version": [
            1122085995
        ],
        "one": [
            1122085995,
            410929361
        ],
        "used": [
            1122085995
        ],
        "book\n(continues": [
            1122085995
        ],
        "inside": [
            1122085995
        ],
        "back": [
            1122085995
        ],
        "cover)\nresource": [
            1122085995
        ],
        "(abbr)": [
            1122085995
        ],
        "[api": [
            1122085995
        ],
        "version]descriptionsection\nnamespace*": [
            1122085995
        ],
        "(ns)": [
            1122085995
        ],
        "[v1]enables": [
            1122085995
        ],
        "organizing": [
            1122085995
        ],
        "non-overlapping": [
            1122085995
        ],
        "\ngroups": [
            1122085995
        ],
        "(for": [
            1122085995
        ],
        "example": [
            1122085995
        ],
        "per": [
            1122085995
        ],
        "tenant)\n37\ndeploying": [
            1122085995
        ],
        "workloads\npod": [
            1122085995
        ],
        "(po)": [
            1122085995
        ],
        "[v1]the": [
            1122085995
        ],
        "basic": [
            1122085995
        ],
        "deployable": [
            1122085995
        ],
        "unit": [
            1122085995
        ],
        "containing": [
            1122085995
        ],
        "\nprocesses": [
            1122085995
        ],
        "co-located": [
            1122085995
        ],
        "containers\n31\nreplicaset": [
            1122085995
        ],
        "(rs)": [
            1122085995
        ],
        "[apps/v1beta2**]keeps": [
            1122085995
        ],
        "pod": [
            1122085995
        ],
        "replicas": [
            1122085995
        ],
        "running43\nreplicationcontroller": [
            1122085995
        ],
        "(rc)": [
            1122085995
        ],
        "older": [
            1122085995
        ],
        "less-powerful": [
            1122085995
        ],
        "equivalent": [
            1122085995
        ],
        "\nreplicaset\n42\njob": [
            1122085995
        ],
        "[batch/v1]runs": [
            1122085995
        ],
        "pods": [
            1122085995
        ],
        "perform": [
            1122085995
        ],
        "completable": [
            1122085995
        ],
        "task45\ncronjob": [
            1122085995
        ],
        "[batch/v1beta1]runs": [
            1122085995
        ],
        "scheduled": [
            1122085995
        ],
        "job": [
            1122085995
        ],
        "periodically46\ndaemonset": [
            1122085995
        ],
        "(ds)": [
            1122085995
        ],
        "[apps/v1beta2**]runs": [
            1122085995
        ],
        "replica": [
            1122085995
        ],
        "node": [
            1122085995
        ],
        "(on": [
            1122085995
        ],
        "nodes": [
            1122085995
        ],
        "\nonly": [
            1122085995
        ],
        "matching": [
            1122085995
        ],
        "selector)\n44\nstatefulset": [
            1122085995
        ],
        "(sts)": [
            1122085995
        ],
        "[apps/v1beta1**]runs": [
            1122085995
        ],
        "stateful": [
            1122085995
        ],
        "stable": [
            1122085995
        ],
        "identity102\ndeployment": [
            1122085995
        ],
        "(deploy)": [
            1122085995
        ],
        "[apps/v1beta1**]declarative": [
            1122085995
        ],
        "deployment": [
            1122085995
        ],
        "updates": [
            1122085995
        ],
        "pods93\nservices\nservice": [
            1122085995
        ],
        "(svc)": [
            1122085995
        ],
        "[v1]exposes": [
            1122085995
        ],
        "single": [
            1122085995
        ],
        "\nip": [
            1122085995
        ],
        "address": [
            1122085995
        ],
        "port": [
            1122085995
        ],
        "pair\n51\nendpoints": [
            1122085995
        ],
        "(ep)": [
            1122085995
        ],
        "[v1]defines": [
            1122085995
        ],
        "(or": [
            1122085995
        ],
        "servers)": [
            1122085995
        ],
        "\nexposed": [
            1122085995
        ],
        "service\n52.1\ningress": [
            1122085995
        ],
        "(ing)": [
            1122085995
        ],
        "[extensions/v1beta1]exposes": [
            1122085995
        ],
        "services": [
            1122085995
        ],
        "external": [
            1122085995
        ],
        "clients": [
            1122085995
        ],
        "\nthrough": [
            1122085995
        ],
        "externally": [
            1122085995
        ],
        "reachable": [
            1122085995
        ],
        "ip": [
            1122085995
        ],
        "address\n54\nconfig\nconfigmap": [
            1122085995
        ],
        "(cm)": [
            1122085995
        ],
        "[v1]a": [
            1122085995
        ],
        "key-value": [
            1122085995
        ],
        "map": [
            1122085995
        ],
        "storing": [
            1122085995
        ],
        "non-sensitive": [
            1122085995
        ],
        "config": [
            1122085995
        ],
        "\noptions": [
            1122085995
        ],
        "apps": [
            1122085995
        ],
        "exposing": [
            1122085995
        ],
        "them\n74\nsecret": [
            1122085995
        ],
        "[v1]like": [
            1122085995
        ],
        "configmap": [
            1122085995
        ],
        "sensitive": [
            1122085995
        ],
        "data75\nstorage\npersistentvolume*": [
            1122085995
        ],
        "(pv)": [
            1122085995
        ],
        "[v1]points": [
            1122085995
        ],
        "persistent": [
            1122085995
        ],
        "storage": [
            1122085995
        ],
        "mounted": [
            1122085995
        ],
        "\ninto": [
            1122085995
        ],
        "persistentvolumeclaim\n65\npersistentvolumeclaim": [
            1122085995
        ],
        "(pvc)": [
            1122085995
        ],
        "request": [
            1122085995
        ],
        "claim": [
            1122085995
        ],
        "persistentvolume65\nstorageclass*": [
            1122085995
        ],
        "(sc)": [
            1122085995
        ],
        "[storagek8s.io/v1]defines": [
            1122085995
        ],
        "type": [
            1122085995
        ],
        "dynamically-provisioned": [
            1122085995
        ],
        "stor-\nage": [
            1122085995
        ],
        "claimable": [
            1122085995
        ],
        "persistentvolumeclaim\n66\n": [
            1122085995
        ],
        "action\n": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\n": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\nkubernetes\nin": [
            1122085995
        ],
        "action\nmarko": [
            1122085995
        ],
        "lukša\nmanning\nshelter": [
            1122085995
        ],
        "island\n": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\nfor": [
            1122085995
        ],
        "online": [
            1122085995
        ],
        "information": [
            1122085995
        ],
        "ordering": [
            1122085995
        ],
        "manning": [
            1122085995
        ],
        "books": [
            1122085995
        ],
        "please": [
            1122085995
        ],
        "visit\nwwwmanning.com.": [
            1122085995
        ],
        "publisher": [
            1122085995
        ],
        "offers": [
            1122085995
        ],
        "discounts": [
            1122085995
        ],
        "book": [
            1122085995
        ],
        "ordered": [
            1122085995
        ],
        "quantity": [
            1122085995
        ],
        "\nfor": [
            1122085995
        ],
        "contact\nspecial": [
            1122085995
        ],
        "sales": [
            1122085995
        ],
        "department\nmanning": [
            1122085995
        ],
        "publications": [
            1122085995
        ],
        "co\n20": [
            1122085995
        ],
        "baldwin": [
            1122085995
        ],
        "road\npo": [
            1122085995
        ],
        "box": [
            1122085995
        ],
        "761\nshelter": [
            1122085995
        ],
        "island": [
            1122085995
        ],
        "ny": [
            1122085995
        ],
        "11964\nemail:": [
            1122085995
        ],
        "orders@manningcom\n©2018": [
            1122085995
        ],
        "co": [
            1122085995
        ],
        "rights": [
            1122085995
        ],
        "reserved\nno": [
            1122085995
        ],
        "part": [
            1122085995,
            186247402
        ],
        "publication": [
            1122085995
        ],
        "may": [
            1122085995,
            186247402
        ],
        "reproduced": [
            1122085995
        ],
        "stored": [
            1122085995
        ],
        "retrieval": [
            1122085995
        ],
        "system": [
            1122085995
        ],
        "transmitted": [
            1122085995
        ],
        "\nany": [
            1122085995
        ],
        "form": [
            1122085995
        ],
        "means": [
            1122085995
        ],
        "electronic": [
            1122085995
        ],
        "mechanical": [
            1122085995
        ],
        "photocopying": [
            1122085995
        ],
        "otherwise": [
            1122085995
        ],
        "without": [
            1122085995,
            2119133144
        ],
        "prior": [
            1122085995
        ],
        "written": [
            1122085995
        ],
        "\npermission": [
            1122085995
        ],
        "publisher\nmany": [
            1122085995
        ],
        "designations": [
            1122085995
        ],
        "manufacturers": [
            1122085995
        ],
        "sellers": [
            1122085995
        ],
        "distinguish": [
            1122085995
        ],
        "products": [
            1122085995
        ],
        "\nclaimed": [
            1122085995
        ],
        "trademarks": [
            1122085995
        ],
        "appear": [
            1122085995
        ],
        "\npublications": [
            1122085995
        ],
        "aware": [
            1122085995
        ],
        "trademark": [
            1122085995
        ],
        "printed": [
            1122085995
        ],
        "initial": [
            1122085995
        ],
        "caps": [
            1122085995
        ],
        "\nor": [
            1122085995
        ],
        "caps\nrecognizing": [
            1122085995
        ],
        "importance": [
            1122085995
        ],
        "preserving": [
            1122085995
        ],
        "manning’s": [
            1122085995
        ],
        "policy": [
            1122085995
        ],
        "\nthe": [
            1122085995
        ],
        "publish": [
            1122085995
        ],
        "acid-free": [
            1122085995
        ],
        "paper": [
            1122085995
        ],
        "exert": [
            1122085995
        ],
        "best": [
            1122085995,
            2119133144
        ],
        "efforts": [
            1122085995
        ],
        "end": [
            1122085995
        ],
        "\nrecognizing": [
            1122085995
        ],
        "responsibility": [
            1122085995
        ],
        "conserve": [
            1122085995
        ],
        "planet": [
            1122085995
        ],
        "books\nare": [
            1122085995
        ],
        "least": [
            1122085995
        ],
        "percent": [
            1122085995
        ],
        "recycled": [
            1122085995
        ],
        "processed": [
            1122085995
        ],
        "use": [
            1122085995
        ],
        "\nelemental": [
            1122085995
        ],
        "chlorine\nmanning": [
            1122085995
        ],
        "codevelopment": [
            1122085995
        ],
        "editor:": [
            1122085995
        ],
        "": [
            1122085995,
            2119133144,
            1478412827,
            1043891123,
            1118639836,
            410929361
        ],
        "elesha": [
            1122085995
        ],
        "hyde\n20": [
            1122085995
        ],
        "roadreview": [
            1122085995
        ],
        "aleksandar": [
            1122085995
        ],
        "dragosavljevic": [
            1122085995
        ],
        "́\npo": [
            1122085995
        ],
        "761technical": [
            1122085995
        ],
        "development": [
            1122085995
        ],
        "jeanne": [
            1122085995
        ],
        "boyarsky\nshelter": [
            1122085995
        ],
        "11964project": [
            1122085995
        ],
        "kevin": [
            1122085995
        ],
        "sullivan\ncopyeditor:": [
            1122085995
        ],
        "katie": [
            1122085995
        ],
        "petito\nproofreader:": [
            1122085995
        ],
        "melody": [
            1122085995
        ],
        "dolab\ntechnical": [
            1122085995
        ],
        "proofreader:": [
            1122085995
        ],
        "antonio": [
            1122085995
        ],
        "magnaghi\nillustrator:": [
            1122085995
        ],
        "chuck": [
            1122085995
        ],
        "larson\ntypesetter:": [
            1122085995
        ],
        "dennis": [
            1122085995
        ],
        "dalinnik\ncover": [
            1122085995
        ],
        "designer:": [
            1122085995
        ],
        "marija": [
            1122085995
        ],
        "tudor\nisbn:": [
            1122085995
        ],
        "9781617293726\nprinted": [
            1122085995
        ],
        "united": [
            1122085995
        ],
        "states": [
            1122085995
        ],
        "america\n1": [
            1122085995
        ],
        "–": [
            1122085995
        ],
        "ebm": [
            1122085995
        ],
        "17\n": [
            1122085995
        ],
        "parents": [
            1122085995
        ],
        "\nwho": [
            1122085995
        ],
        "always": [
            1122085995
        ],
        "put": [
            1122085995
        ],
        "children’s": [
            1122085995
        ],
        "needs": [
            1122085995
        ],
        "own\n": [
            1122085995
        ],
        "\n": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\nvii\nbrief": [
            1122085995
        ],
        "contents\npart": [
            1122085995
        ],
        "1overview\n1■introducing": [
            1122085995
        ],
        "kubernetes": [
            1122085995
        ],
        "1\n2■first": [
            1122085995
        ],
        "steps": [
            1122085995
        ],
        "docker": [
            1122085995
        ],
        "25\npart": [
            1122085995
        ],
        "2core": [
            1122085995
        ],
        "concepts\n3■pods:": [
            1122085995
        ],
        "running": [
            1122085995
        ],
        "containers": [
            1122085995
        ],
        "55\n4■replication": [
            1122085995
        ],
        "controllers:": [
            1122085995
        ],
        "deploying": [
            1122085995
        ],
        "\nmanaged": [
            1122085995
        ],
        "84\n5■services:": [
            1122085995
        ],
        "enabling": [
            1122085995
        ],
        "discover": [
            1122085995
        ],
        "talk": [
            1122085995
        ],
        "\nto": [
            1122085995
        ],
        "120\n6■volumes:": [
            1122085995
        ],
        "attaching": [
            1122085995
        ],
        "disk": [
            1122085995
        ],
        "159\n7■configmaps": [
            1122085995
        ],
        "secrets:": [
            1122085995
        ],
        "configuring": [
            1122085995
        ],
        "applications": [
            1122085995
        ],
        "191\n8■accessing": [
            1122085995
        ],
        "metadata": [
            1122085995
        ],
        "\napplications": [
            1122085995
        ],
        "225\n9■deployments:": [
            1122085995
        ],
        "updating": [
            1122085995
        ],
        "declaratively": [
            1122085995
        ],
        "250\n10■statefulsets:": [
            1122085995
        ],
        "replicated": [
            1122085995
        ],
        "280\n": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\nbrief": [
            1122085995
        ],
        "contentsviii\npart": [
            1122085995
        ],
        "3beyond": [
            1122085995
        ],
        "basics\n11■understanding": [
            1122085995
        ],
        "internals": [
            1122085995
        ],
        "309\n12■securing": [
            1122085995
        ],
        "server": [
            1122085995
        ],
        "346\n13■securing": [
            1122085995
        ],
        "cluster": [
            1122085995
        ],
        "network": [
            1122085995
        ],
        "375\n14■managing": [
            1122085995
        ],
        "pods’": [
            1122085995
        ],
        "computational": [
            1122085995
        ],
        "404\n15■automatic": [
            1122085995
        ],
        "scaling": [
            1122085995
        ],
        "437\n16■advanced": [
            1122085995
        ],
        "scheduling": [
            1122085995
        ],
        "457\n17■best": [
            1122085995
        ],
        "practices": [
            1122085995
        ],
        "developing": [
            1122085995
        ],
        "477\n18■extending": [
            1122085995
        ],
        "508\n": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\nix\ncontents\npreface": [
            1122085995
        ],
        "xxi\nacknowledgments": [
            1122085995
        ],
        "xxiii\nabout": [
            1122085995
        ],
        "xxv\nabout": [
            1122085995
        ],
        "author": [
            1122085995
        ],
        "xxix\nabout": [
            1122085995
        ],
        "cover": [
            1122085995
        ],
        "illustration": [
            1122085995
        ],
        "xxx\npart": [
            1122085995
        ],
        "1overview\n1": [
            1122085995
        ],
        "\nintroducing": [
            1122085995
        ],
        "1\n11understanding": [
            1122085995
        ],
        "need": [
            1122085995,
            1118639836
        ],
        "like": [
            1122085995
        ],
        "2\nmoving": [
            1122085995
        ],
        "monolithic": [
            1122085995
        ],
        "microservices": [
            1122085995
        ],
        "3\n■\nproviding": [
            1122085995
        ],
        "\nconsistent": [
            1122085995
        ],
        "environment": [
            1122085995
        ],
        "6\n■\nmoving": [
            1122085995
        ],
        "continuous": [
            1122085995
        ],
        "\ndelivery:": [
            1122085995
        ],
        "devops": [
            1122085995
        ],
        "noops": [
            1122085995
        ],
        "6\n12introducing": [
            1122085995
        ],
        "container": [
            1122085995
        ],
        "technologies": [
            1122085995
        ],
        "7\nunderstanding": [
            1122085995
        ],
        "8\n■\nintroducing": [
            1122085995
        ],
        "\ncontainer": [
            1122085995
        ],
        "platform": [
            1122085995
        ],
        "12\n■\nintroducing": [
            1122085995
        ],
        "rkt—an": [
            1122085995
        ],
        "alternative": [
            1122085995
        ],
        "15\n13introducing": [
            1122085995
        ],
        "16\nunderstanding": [
            1122085995
        ],
        "origins": [
            1122085995
        ],
        "16\n■\nlooking": [
            1122085995
        ],
        "\ntop": [
            1122085995
        ],
        "mountain": [
            1122085995
        ],
        "16\n■\nunderstanding": [
            1122085995
        ],
        "architecture": [
            1122085995
        ],
        "\nkubernetes": [
            1122085995
        ],
        "18\n■\nrunning": [
            1122085995
        ],
        "application": [
            1122085995
        ],
        "19\nunderstanding": [
            1122085995
        ],
        "benefits": [
            1122085995
        ],
        "using": [
            1122085995
        ],
        "21\n14summary": [
            1122085995
        ],
        "23\n": [
            1122085995
        ],
        "\n\ncontents\nx\n2": [
            1122085995
        ],
        "\nfirst": [
            1122085995
        ],
        "25\n21creating": [
            1122085995
        ],
        "sharing": [
            1122085995,
            1118639836
        ],
        "image": [
            1122085995
        ],
        "26\ninstalling": [
            1122085995
        ],
        "hello": [
            1122085995
        ],
        "world": [
            1122085995
        ],
        "26\ncreating": [
            1122085995
        ],
        "trivial": [
            1122085995
        ],
        "nodejs": [
            1122085995
        ],
        "app": [
            1122085995
        ],
        "28\n■\ncreating": [
            1122085995
        ],
        "dockerfile": [
            1122085995
        ],
        "29\n■\nbuilding": [
            1122085995
        ],
        "29\nrunning": [
            1122085995
        ],
        "32\n■\nexploring": [
            1122085995
        ],
        "\nof": [
            1122085995
        ],
        "33\n■\nstopping": [
            1122085995
        ],
        "removing": [
            1122085995
        ],
        "34\n■\npushing": [
            1122085995
        ],
        "registry": [
            1122085995
        ],
        "35\n22setting": [
            1122085995
        ],
        "36\nrunning": [
            1122085995
        ],
        "local": [
            1122085995
        ],
        "single-node": [
            1122085995
        ],
        "minikube": [
            1122085995
        ],
        "37\nusing": [
            1122085995
        ],
        "hosted": [
            1122085995
        ],
        "google": [
            1122085995
        ],
        "\nengine": [
            1122085995
        ],
        "38\n■\nsetting": [
            1122085995
        ],
        "alias": [
            1122085995
        ],
        "command-line": [
            1122085995
        ],
        "completion": [
            1122085995
        ],
        "kubectl": [
            1122085995
        ],
        "41\n23running": [
            1122085995
        ],
        "first": [
            1122085995,
            410929361
        ],
        "42\ndeploying": [
            1122085995
        ],
        "42\n■\naccessing": [
            1122085995
        ],
        "web": [
            1122085995
        ],
        "\napplication": [
            1122085995
        ],
        "45\n■\nthe": [
            1122085995
        ],
        "logical": [
            1122085995
        ],
        "parts": [
            1122085995
        ],
        "47\nhorizontally": [
            1122085995
        ],
        "48\n■\nexamining": [
            1122085995
        ],
        "\nnodes": [
            1122085995
        ],
        "51\n■\nintroducing": [
            1122085995
        ],
        "dashboard": [
            1122085995
        ],
        "52\n24summary": [
            1122085995
        ],
        "53\npart": [
            1122085995
        ],
        "concepts\n3": [
            1122085995
        ],
        "\npods:": [
            1122085995
        ],
        "55\n31introducing": [
            1122085995
        ],
        "56\nunderstanding": [
            1122085995
        ],
        "56\n■\nunderstanding": [
            1122085995
        ],
        "57\norganizing": [
            1122085995
        ],
        "across": [
            1122085995
        ],
        "properly": [
            1122085995
        ],
        "58\n32creating": [
            1122085995
        ],
        "yaml": [
            1122085995
        ],
        "json": [
            1122085995
        ],
        "descriptors": [
            1122085995
        ],
        "61\nexamining": [
            1122085995
        ],
        "descriptor": [
            1122085995
        ],
        "existing": [
            1122085995
        ],
        "61\n■\ncreating": [
            1122085995
        ],
        "\nsimple": [
            1122085995
        ],
        "63\n■\nusing": [
            1122085995
        ],
        "create": [
            1122085995
        ],
        "\ncreate": [
            1122085995
        ],
        "65\n■\nviewing": [
            1122085995
        ],
        "logs": [
            1122085995
        ],
        "65\n■\nsending": [
            1122085995
        ],
        "\nrequests": [
            1122085995
        ],
        "66\n33organizing": [
            1122085995
        ],
        "labels": [
            1122085995
        ],
        "67\nintroducing": [
            1122085995
        ],
        "68\n■\nspecifying": [
            1122085995
        ],
        "creating": [
            1122085995
        ],
        "69\nmodifying": [
            1122085995
        ],
        "70\n34listing": [
            1122085995
        ],
        "subsets": [
            1122085995
        ],
        "label": [
            1122085995
        ],
        "selectors": [
            1122085995
        ],
        "71\nlisting": [
            1122085995
        ],
        "selector": [
            1122085995
        ],
        "71\n■\nusing": [
            1122085995
        ],
        "multiple": [
            1122085995
        ],
        "conditions": [
            1122085995
        ],
        "\nin": [
            1122085995
        ],
        "72\n": [
            1122085995
        ],
        "\n\ncontents\nxi\n35using": [
            1122085995
        ],
        "constrain": [
            1122085995
        ],
        "\nscheduling": [
            1122085995
        ],
        "73\nusing": [
            1122085995
        ],
        "categorizing": [
            1122085995
        ],
        "worker": [
            1122085995
        ],
        "74\n■\nscheduling": [
            1122085995
        ],
        "\nspecific": [
            1122085995
        ],
        "specific": [
            1122085995
        ],
        "75\n36annotating": [
            1122085995
        ],
        "75\nlooking": [
            1122085995
        ],
        "object’s": [
            1122085995
        ],
        "annotations": [
            1122085995
        ],
        "75\n■\nadding": [
            1122085995
        ],
        "modifying": [
            1122085995
        ],
        "\nannotations": [
            1122085995
        ],
        "76\n37using": [
            1122085995
        ],
        "namespaces": [
            1122085995
        ],
        "group": [
            1122085995
        ],
        "76\nunderstanding": [
            1122085995
        ],
        "77\n■\ndiscovering": [
            1122085995
        ],
        "\nnamespaces": [
            1122085995
        ],
        "77\n■\ncreating": [
            1122085995
        ],
        "namespace": [
            1122085995
        ],
        "78\nmanaging": [
            1122085995
        ],
        "objects": [
            1122085995
        ],
        "79\n■\nunderstanding": [
            1122085995
        ],
        "isolation": [
            1122085995
        ],
        "provided": [
            1122085995
        ],
        "79\n38stopping": [
            1122085995
        ],
        "80\ndeleting": [
            1122085995
        ],
        "name": [
            1122085995
        ],
        "80\n■\ndeleting": [
            1122085995
        ],
        "\nselectors": [
            1122085995
        ],
        "deleting": [
            1122085995
        ],
        "whole": [
            1122085995
        ],
        "\nnamespace": [
            1122085995
        ],
        "\nwhile": [
            1122085995
        ],
        "keeping": [
            1122085995
        ],
        "81\n■\ndeleting": [
            1122085995
        ],
        "(almost)": [
            1122085995
        ],
        "\nall": [
            1122085995
        ],
        "82\n39summary": [
            1122085995
        ],
        "82\n4": [
            1122085995
        ],
        "\nreplication": [
            1122085995
        ],
        "managed": [
            1122085995
        ],
        "84\n41keeping": [
            1122085995
        ],
        "healthy": [
            1122085995
        ],
        "85\nintroducing": [
            1122085995
        ],
        "liveness": [
            1122085995
        ],
        "probes": [
            1122085995
        ],
        "85\n■\ncreating": [
            1122085995
        ],
        "http-based": [
            1122085995
        ],
        "\nliveness": [
            1122085995
        ],
        "probe": [
            1122085995
        ],
        "86\n■\nseeing": [
            1122085995
        ],
        "action": [
            1122085995
        ],
        "87\nconfiguring": [
            1122085995
        ],
        "additional": [
            1122085995
        ],
        "properties": [
            1122085995
        ],
        "88\ncreating": [
            1122085995
        ],
        "effective": [
            1122085995
        ],
        "89\n42introducing": [
            1122085995
        ],
        "replicationcontrollers": [
            1122085995
        ],
        "90\nthe": [
            1122085995
        ],
        "operation": [
            1122085995
        ],
        "replicationcontroller": [
            1122085995
        ],
        "91\n■\ncreating": [
            1122085995
        ],
        "\nreplicationcontroller": [
            1122085995
        ],
        "93\n■\nseeing": [
            1122085995
        ],
        "94\n■\nmoving": [
            1122085995
        ],
        "scope": [
            1122085995
        ],
        "98\n■\nchanging": [
            1122085995
        ],
        "template": [
            1122085995
        ],
        "101\nhorizontally": [
            1122085995
        ],
        "102\n■\ndeleting": [
            1122085995
        ],
        "103\n43using": [
            1122085995
        ],
        "replicasets": [
            1122085995
        ],
        "instead": [
            1122085995
        ],
        "104\ncomparing": [
            1122085995
        ],
        "replicaset": [
            1122085995
        ],
        "105\ndefining": [
            1122085995
        ],
        "105\n■\ncreating": [
            1122085995
        ],
        "examining": [
            1122085995
        ],
        "\nreplicaset": [
            1122085995
        ],
        "106\n■\nusing": [
            1122085995
        ],
        "replicaset’s": [
            1122085995
        ],
        "expressive": [
            1122085995
        ],
        "\nlabel": [
            1122085995
        ],
        "107\n■\nwrapping": [
            1122085995
        ],
        "108\n": [
            1122085995
        ],
        "\n\ncontents\nxii\n44running": [
            1122085995
        ],
        "exactly": [
            1122085995
        ],
        "\ndaemonsets": [
            1122085995
        ],
        "108\nusing": [
            1122085995
        ],
        "daemonset": [
            1122085995
        ],
        "run": [
            1122085995
        ],
        "every": [
            1122085995
        ],
        "109\nusing": [
            1122085995
        ],
        "certain": [
            1122085995
        ],
        "109\n45running": [
            1122085995
        ],
        "\ntask": [
            1122085995
        ],
        "112\nintroducing": [
            1122085995
        ],
        "112\n■\ndefining": [
            1122085995
        ],
        "113\nseeing": [
            1122085995
        ],
        "114\n■\nrunning": [
            1122085995
        ],
        "instances": [
            1122085995
        ],
        "114\n■\nlimiting": [
            1122085995
        ],
        "time": [
            1122085995
        ],
        "allowed": [
            1122085995
        ],
        "\ncomplete": [
            1122085995
        ],
        "116\n46scheduling": [
            1122085995
        ],
        "jobs": [
            1122085995
        ],
        "periodically": [
            1122085995
        ],
        "future": [
            1122085995
        ],
        "116\ncreating": [
            1122085995
        ],
        "cronjob": [
            1122085995
        ],
        "116\n■\nunderstanding": [
            1122085995
        ],
        "\njobs": [
            1122085995
        ],
        "117\n47summary": [
            1122085995
        ],
        "118\n5": [
            1122085995
        ],
        "\nservices:": [
            1122085995
        ],
        "120\n51introducing": [
            1122085995
        ],
        "121\ncreating": [
            1122085995
        ],
        "122\n■\ndiscovering": [
            1122085995
        ],
        "128\n52connecting": [
            1122085995
        ],
        "living": [
            1122085995
        ],
        "outside": [
            1122085995
        ],
        "131\nintroducing": [
            1122085995
        ],
        "service": [
            1122085995
        ],
        "endpoints": [
            1122085995
        ],
        "131\n■\nmanually": [
            1122085995
        ],
        "\nservice": [
            1122085995
        ],
        "132\n■\ncreating": [
            1122085995
        ],
        "134\n53exposing": [
            1122085995
        ],
        "134\nusing": [
            1122085995
        ],
        "nodeport": [
            1122085995
        ],
        "135\n■\nexposing": [
            1122085995
        ],
        "\nan": [
            1122085995
        ],
        "load": [
            1122085995
        ],
        "balancer": [
            1122085995
        ],
        "138\n■\nunderstanding": [
            1122085995
        ],
        "peculiarities": [
            1122085995
        ],
        "connections": [
            1122085995
        ],
        "141\n54exposing": [
            1122085995
        ],
        "ingress": [
            1122085995
        ],
        "\nresource": [
            1122085995
        ],
        "142\ncreating": [
            1122085995
        ],
        "144\n■\naccessing": [
            1122085995
        ],
        "145\n■\nexposing": [
            1122085995
        ],
        "146\n■\nconfiguring": [
            1122085995
        ],
        "\nhandle": [
            1122085995
        ],
        "tls": [
            1122085995
        ],
        "traffic": [
            1122085995
        ],
        "147\n55signaling": [
            1122085995
        ],
        "ready": [
            1122085995
        ],
        "accept": [
            1122085995
        ],
        "149\nintroducing": [
            1122085995
        ],
        "readiness": [
            1122085995
        ],
        "149\n■\nadding": [
            1122085995
        ],
        "151\n■\nunderstanding": [
            1122085995
        ],
        "real-world": [
            1122085995
        ],
        "\nprobes": [
            1122085995
        ],
        "153\n": [
            1122085995
        ],
        "\n\ncontents\nxiii\n56using": [
            1122085995
        ],
        "headless": [
            1122085995
        ],
        "discovering": [
            1122085995
        ],
        "individual": [
            1122085995
        ],
        "\npods": [
            1122085995
        ],
        "154\ncreating": [
            1122085995
        ],
        "154\n■\ndiscovering": [
            1122085995
        ],
        "dns": [
            1122085995
        ],
        "155\n■\ndiscovering": [
            1122085995
        ],
        "pods—even": [
            1122085995
        ],
        "\nthat": [
            1122085995
        ],
        "aren’t": [
            1122085995
        ],
        "156\n57troubleshooting": [
            1122085995
        ],
        "156\n58summary": [
            1122085995
        ],
        "157\n6": [
            1122085995
        ],
        "\nvolumes:": [
            1122085995
        ],
        "159\n61introducing": [
            1122085995
        ],
        "volumes": [
            1122085995
        ],
        "160\nexplaining": [
            1122085995
        ],
        "160\n■\nintroducing": [
            1122085995
        ],
        "available": [
            1122085995
        ],
        "\nvolume": [
            1122085995
        ],
        "types": [
            1122085995
        ],
        "162\n62using": [
            1122085995
        ],
        "share": [
            1122085995,
            410929361
        ],
        "data": [
            1122085995
        ],
        "163\nusing": [
            1122085995
        ],
        "emptydir": [
            1122085995
        ],
        "volume": [
            1122085995
        ],
        "163\n■\nusing": [
            1122085995
        ],
        "git": [
            1122085995
        ],
        "repository": [
            1122085995
        ],
        "\nstarting": [
            1122085995
        ],
        "point": [
            1122085995
        ],
        "166\n63accessing": [
            1122085995
        ],
        "files": [
            1122085995
        ],
        "node’s": [
            1122085995
        ],
        "filesystem": [
            1122085995
        ],
        "169\nintroducing": [
            1122085995
        ],
        "hostpath": [
            1122085995
        ],
        "169\n■\nexamining": [
            1122085995
        ],
        "170\n64using": [
            1122085995
        ],
        "171\nusing": [
            1122085995
        ],
        "gce": [
            1122085995
        ],
        "171\n■\nusing": [
            1122085995
        ],
        "\ntypes": [
            1122085995
        ],
        "underlying": [
            1122085995
        ],
        "174\n65decoupling": [
            1122085995
        ],
        "\ntechnology": [
            1122085995
        ],
        "176\nintroducing": [
            1122085995
        ],
        "persistentvolumes": [
            1122085995
        ],
        "persistentvolumeclaims": [
            1122085995
        ],
        "176\ncreating": [
            1122085995
        ],
        "persistentvolume": [
            1122085995
        ],
        "177\n■\nclaiming": [
            1122085995
        ],
        "\nby": [
            1122085995
        ],
        "persistentvolumeclaim": [
            1122085995
        ],
        "179\n■\nusing": [
            1122085995
        ],
        "\npersistentvolumeclaim": [
            1122085995
        ],
        "181\n■\nunderstanding": [
            1122085995
        ],
        "\nbenefits": [
            1122085995
        ],
        "claims": [
            1122085995
        ],
        "182\n■\nrecycling": [
            1122085995
        ],
        "\npersistentvolumes": [
            1122085995
        ],
        "183\n66dynamic": [
            1122085995
        ],
        "provisioning": [
            1122085995
        ],
        "184\ndefining": [
            1122085995
        ],
        "storageclass": [
            1122085995
        ],
        "\nresources": [
            1122085995
        ],
        "185\n■\nrequesting": [
            1122085995
        ],
        "class": [
            1122085995
        ],
        "185\n■\ndynamic": [
            1122085995
        ],
        "\nwithout": [
            1122085995
        ],
        "specifying": [
            1122085995
        ],
        "187\n67summary": [
            1122085995
        ],
        "190\n": [
            1122085995
        ],
        "\n\ncontents\nxiv\n7": [
            1122085995
        ],
        "\nconfigmaps": [
            1122085995
        ],
        "191\n71configuring": [
            1122085995
        ],
        "containerized": [
            1122085995
        ],
        "191\n72passing": [
            1122085995
        ],
        "arguments": [
            1122085995
        ],
        "192\ndefining": [
            1122085995
        ],
        "command": [
            1122085995
        ],
        "193\noverriding": [
            1122085995
        ],
        "195\n73setting": [
            1122085995
        ],
        "variables": [
            1122085995
        ],
        "196\nspecifying": [
            1122085995
        ],
        "definition": [
            1122085995
        ],
        "197\nreferring": [
            1122085995
        ],
        "variable’s": [
            1122085995
        ],
        "value": [
            1122085995
        ],
        "198\nunderstanding": [
            1122085995
        ],
        "drawback": [
            1122085995
        ],
        "hardcoding": [
            1122085995
        ],
        "\nvariables": [
            1122085995
        ],
        "198\n74decoupling": [
            1122085995
        ],
        "configuration": [
            1122085995
        ],
        "198\nintroducing": [
            1122085995
        ],
        "configmaps": [
            1122085995
        ],
        "198\n■\ncreating": [
            1122085995
        ],
        "200\npassing": [
            1122085995
        ],
        "entry": [
            1122085995
        ],
        "\nvariable": [
            1122085995
        ],
        "202\n■\npassing": [
            1122085995
        ],
        "entries": [
            1122085995
        ],
        "204\n■\npassing": [
            1122085995
        ],
        "\ncommand-line": [
            1122085995
        ],
        "argument": [
            1122085995
        ],
        "204\n■\nusing": [
            1122085995
        ],
        "\nexpose": [
            1122085995
        ],
        "205\n■\nupdating": [
            1122085995
        ],
        "app’s": [
            1122085995
        ],
        "restart": [
            1122085995
        ],
        "211\n75using": [
            1122085995
        ],
        "secrets": [
            1122085995
        ],
        "pass": [
            1122085995
        ],
        "213\nintroducing": [
            1122085995
        ],
        "214\n■\nintroducing": [
            1122085995
        ],
        "default": [
            1122085995
        ],
        "token": [
            1122085995
        ],
        "\nsecret": [
            1122085995
        ],
        "214\n■\ncreating": [
            1122085995
        ],
        "secret": [
            1122085995
        ],
        "216\n■\ncomparing": [
            1122085995
        ],
        "\nand": [
            1122085995
        ],
        "217\n■\nusing": [
            1122085995
        ],
        "218\nunderstanding": [
            1122085995
        ],
        "pull": [
            1122085995
        ],
        "222\n76summary": [
            1122085995
        ],
        "224\n8": [
            1122085995
        ],
        "\naccessing": [
            1122085995
        ],
        "225\n81passing": [
            1122085995
        ],
        "downward": [
            1122085995
        ],
        "226\nunderstanding": [
            1122085995
        ],
        "226\n■\nexposing": [
            1122085995
        ],
        "227\n■\npassing": [
            1122085995
        ],
        "\nfiles": [
            1122085995
        ],
        "downwardapi": [
            1122085995
        ],
        "230\n82talking": [
            1122085995
        ],
        "233\nexploring": [
            1122085995
        ],
        "rest": [
            1122085995
        ],
        "234\n■\ntalking": [
            1122085995
        ],
        "\nserver": [
            1122085995
        ],
        "within": [
            1122085995
        ],
        "238\n■\nsimplifying": [
            1122085995
        ],
        "\ncommunication": [
            1122085995
        ],
        "ambassador": [
            1122085995
        ],
        "243\n■\nusing": [
            1122085995
        ],
        "client": [
            1122085995
        ],
        "\nlibraries": [
            1122085995
        ],
        "246\n83summary": [
            1122085995
        ],
        "249\n": [
            1122085995
        ],
        "\n\ncontents\nxv\n9": [
            1122085995
        ],
        "\ndeployments:": [
            1122085995
        ],
        "250\n91updating": [
            1122085995
        ],
        "251\ndeleting": [
            1122085995
        ],
        "old": [
            1122085995
        ],
        "replacing": [
            1122085995
        ],
        "new": [
            1122085995,
            2119133144
        ],
        "ones": [
            1122085995
        ],
        "252\nspinning": [
            1122085995
        ],
        "252\n92performing": [
            1122085995
        ],
        "automatic": [
            1122085995
        ],
        "rolling": [
            1122085995
        ],
        "update": [
            1122085995
        ],
        "254\nrunning": [
            1122085995
        ],
        "254\n■\nperforming": [
            1122085995
        ],
        "\nupdate": [
            1122085995
        ],
        "256\n■\nunderstanding": [
            1122085995
        ],
        "rolling-\nupdate": [
            1122085995
        ],
        "obsolete": [
            1122085995
        ],
        "260\n93using": [
            1122085995
        ],
        "deployments": [
            1122085995
        ],
        "261\ncreating": [
            1122085995
        ],
        "262\n■\nupdating": [
            1122085995
        ],
        "264\nrolling": [
            1122085995
        ],
        "268\n■\ncontrolling": [
            1122085995
        ],
        "rate": [
            1122085995
        ],
        "\nrollout": [
            1122085995
        ],
        "271\n■\npausing": [
            1122085995
        ],
        "rollout": [
            1122085995
        ],
        "process": [
            1122085995
        ],
        "273\n■\nblocking": [
            1122085995
        ],
        "\nrollouts": [
            1122085995
        ],
        "bad": [
            1122085995
        ],
        "versions": [
            1122085995
        ],
        "274\n94summary": [
            1122085995
        ],
        "279\n10": [
            1122085995
        ],
        "\nstatefulsets:": [
            1122085995
        ],
        "280\n101replicating": [
            1122085995
        ],
        "281\nrunning": [
            1122085995
        ],
        "separate": [
            1122085995
        ],
        "281\nproviding": [
            1122085995
        ],
        "identity": [
            1122085995
        ],
        "282\n102understanding": [
            1122085995
        ],
        "statefulsets": [
            1122085995
        ],
        "284\ncomparing": [
            1122085995
        ],
        "284\n■\nproviding": [
            1122085995
        ],
        "\nstable": [
            1122085995
        ],
        "285\n■\nproviding": [
            1122085995
        ],
        "dedicated": [
            1122085995
        ],
        "instance": [
            1122085995
        ],
        "287\n■\nunderstanding": [
            1122085995
        ],
        "statefulset": [
            1122085995
        ],
        "\nguarantees": [
            1122085995
        ],
        "289\n103using": [
            1122085995
        ],
        "290\ncreating": [
            1122085995
        ],
        "290\n■\ndeploying": [
            1122085995
        ],
        "291\n■\nplaying": [
            1122085995
        ],
        "295\n104discovering": [
            1122085995
        ],
        "peers": [
            1122085995
        ],
        "299\nimplementing": [
            1122085995
        ],
        "peer": [
            1122085995
        ],
        "discovery": [
            1122085995
        ],
        "301\n■\nupdating": [
            1122085995
        ],
        "\nstatefulset": [
            1122085995
        ],
        "302\n■\ntrying": [
            1122085995
        ],
        "clustered": [
            1122085995
        ],
        "store": [
            1122085995
        ],
        "303\n105understanding": [
            1122085995
        ],
        "deal": [
            1122085995,
            410929361,
            186247402
        ],
        "\nfailures": [
            1122085995
        ],
        "304\nsimulating": [
            1122085995
        ],
        "disconnection": [
            1122085995
        ],
        "304\ndeleting": [
            1122085995
        ],
        "manually": [
            1122085995
        ],
        "306\n106summary": [
            1122085995
        ],
        "307\n": [
            1122085995
        ],
        "\n\ncontents\nxvi\npart": [
            1122085995
        ],
        "basics\n11": [
            1122085995
        ],
        "\nunderstanding": [
            1122085995
        ],
        "309\n111understanding": [
            1122085995
        ],
        "310\nthe": [
            1122085995
        ],
        "distributed": [
            1122085995
        ],
        "nature": [
            1122085995
        ],
        "components": [
            1122085995
        ],
        "310\nhow": [
            1122085995
        ],
        "uses": [
            1122085995
        ],
        "etcd": [
            1122085995
        ],
        "312\n■\nwhat": [
            1122085995
        ],
        "316\nunderstanding": [
            1122085995
        ],
        "notifies": [
            1122085995
        ],
        "\nchanges": [
            1122085995
        ],
        "318\n■\nunderstanding": [
            1122085995
        ],
        "scheduler": [
            1122085995
        ],
        "319\nintroducing": [
            1122085995
        ],
        "controllers": [
            1122085995
        ],
        "controller": [
            1122085995
        ],
        "manager": [
            1122085995
        ],
        "321\nwhat": [
            1122085995
        ],
        "kubelet": [
            1122085995
        ],
        "326\n■\nthe": [
            1122085995
        ],
        "role": [
            1122085995
        ],
        "\nproxy": [
            1122085995
        ],
        "327\n■\nintroducing": [
            1122085995
        ],
        "add-ons": [
            1122085995
        ],
        "328\n■\nbringing": [
            1122085995
        ],
        "together": [
            1122085995,
            1941223023
        ],
        "330\n112how": [
            1122085995
        ],
        "cooperate": [
            1122085995
        ],
        "330\nunderstanding": [
            1122085995
        ],
        "involved": [
            1122085995
        ],
        "330\n■\nthe": [
            1122085995
        ],
        "chain": [
            1122085995
        ],
        "events": [
            1122085995
        ],
        "331\n■\nobserving": [
            1122085995
        ],
        "332\n113understanding": [
            1122085995
        ],
        "333\n114inter-pod": [
            1122085995
        ],
        "networking": [
            1122085995
        ],
        "335\nwhat": [
            1122085995
        ],
        "must": [
            1122085995
        ],
        "335\n■\ndiving": [
            1122085995
        ],
        "deeper": [
            1122085995
        ],
        "\nhow": [
            1122085995
        ],
        "works": [
            1122085995
        ],
        "336\n■\nintroducing": [
            1122085995
        ],
        "\nnetwork": [
            1122085995
        ],
        "interface": [
            1122085995
        ],
        "338\n115how": [
            1122085995
        ],
        "implemented": [
            1122085995
        ],
        "338\nintroducing": [
            1122085995
        ],
        "kube-proxy": [
            1122085995
        ],
        "339\n■\nhow": [
            1122085995
        ],
        "iptables": [
            1122085995
        ],
        "339\n116running": [
            1122085995
        ],
        "highly": [
            1122085995
        ],
        "clusters": [
            1122085995
        ],
        "341\nmaking": [
            1122085995
        ],
        "341\n■\nmaking": [
            1122085995
        ],
        "\ncontrol": [
            1122085995
        ],
        "plane": [
            1122085995
        ],
        "342\n117summary": [
            1122085995
        ],
        "345\n12": [
            1122085995
        ],
        "\nsecuring": [
            1122085995
        ],
        "346\n121understanding": [
            1122085995
        ],
        "authentication": [
            1122085995
        ],
        "346\nusers": [
            1122085995
        ],
        "groups": [
            1122085995
        ],
        "347\n■\nintroducing": [
            1122085995
        ],
        "serviceaccounts": [
            1122085995
        ],
        "348\ncreating": [
            1122085995
        ],
        "349\n■\nassigning": [
            1122085995
        ],
        "serviceaccount": [
            1122085995
        ],
        "351\n122securing": [
            1122085995
        ],
        "role-based": [
            1122085995
        ],
        "access": [
            1122085995
        ],
        "control": [
            1122085995
        ],
        "353\nintroducing": [
            1122085995
        ],
        "rbac": [
            1122085995
        ],
        "authorization": [
            1122085995
        ],
        "plugin": [
            1122085995
        ],
        "353\n■\nintroducing": [
            1122085995
        ],
        "\nrbac": [
            1122085995
        ],
        "355\n■\nusing": [
            1122085995
        ],
        "roles": [
            1122085995
        ],
        "rolebindings": [
            1122085995
        ],
        "358\nusing": [
            1122085995
        ],
        "clusterroles": [
            1122085995
        ],
        "clusterrolebindings": [
            1122085995
        ],
        "362\nunderstanding": [
            1122085995
        ],
        "371\ngranting": [
            1122085995
        ],
        "permissions": [
            1122085995
        ],
        "wisely": [
            1122085995
        ],
        "373\n123summary": [
            1122085995
        ],
        "373\n": [
            1122085995
        ],
        "\n\ncontents\nxvii\n13": [
            1122085995
        ],
        "375\n131using": [
            1122085995
        ],
        "host": [
            1122085995
        ],
        "376\nusing": [
            1122085995
        ],
        "376\n■\nbinding": [
            1122085995
        ],
        "\na": [
            1122085995
        ],
        "host’s": [
            1122085995
        ],
        "377\nusing": [
            1122085995
        ],
        "pid": [
            1122085995
        ],
        "ipc": [
            1122085995
        ],
        "379\n132configuring": [
            1122085995
        ],
        "container’s": [
            1122085995
        ],
        "security": [
            1122085995
        ],
        "context": [
            1122085995
        ],
        "380\nrunning": [
            1122085995
        ],
        "user": [
            1122085995
        ],
        "381\n■\npreventing": [
            1122085995
        ],
        "root": [
            1122085995
        ],
        "382\n■\nrunning": [
            1122085995
        ],
        "\nprivileged": [
            1122085995
        ],
        "mode": [
            1122085995
        ],
        "382\n■\nadding": [
            1122085995
        ],
        "kernel": [
            1122085995
        ],
        "capabilities": [
            1122085995
        ],
        "384\n■\ndropping": [
            1122085995
        ],
        "385\npreventing": [
            1122085995
        ],
        "processes": [
            1122085995
        ],
        "writing": [
            1122085995
        ],
        "386\nsharing": [
            1122085995
        ],
        "different": [
            1122085995
        ],
        "users": [
            1122085995
        ],
        "387\n133restricting": [
            1122085995
        ],
        "security-related": [
            1122085995
        ],
        "features": [
            1122085995
        ],
        "389\nintroducing": [
            1122085995
        ],
        "podsecuritypolicy": [
            1122085995
        ],
        "389\n■\nunderstanding": [
            1122085995
        ],
        "\nrunasuser": [
            1122085995
        ],
        "fsgroup": [
            1122085995
        ],
        "supplementalgroups": [
            1122085995
        ],
        "policies": [
            1122085995
        ],
        "392\nconfiguring": [
            1122085995
        ],
        "disallowed": [
            1122085995
        ],
        "394\nconstraining": [
            1122085995
        ],
        "395\n■\nassigning": [
            1122085995
        ],
        "\ndifferent": [
            1122085995
        ],
        "podsecuritypolicies": [
            1122085995
        ],
        "396\n134isolating": [
            1122085995
        ],
        "399\nenabling": [
            1122085995
        ],
        "399\n■\nallowing": [
            1122085995
        ],
        "connect": [
            1122085995
        ],
        "400\nisolating": [
            1122085995
        ],
        "401\nisolating": [
            1122085995
        ],
        "cidr": [
            1122085995
        ],
        "notation": [
            1122085995
        ],
        "402\n■\nlimiting": [
            1122085995
        ],
        "outbound": [
            1122085995
        ],
        "\ntraffic": [
            1122085995
        ],
        "set": [
            1122085995
        ],
        "403\n135summary": [
            1122085995
        ],
        "403\n14": [
            1122085995
        ],
        "\nmanaging": [
            1122085995
        ],
        "404\n141requesting": [
            1122085995
        ],
        "pod’s": [
            1122085995
        ],
        "405\ncreating": [
            1122085995
        ],
        "requests": [
            1122085995
        ],
        "405\n■\nunderstanding": [
            1122085995
        ],
        "affect": [
            1122085995
        ],
        "406\n■\nunderstanding": [
            1122085995
        ],
        "cpu": [
            1122085995
        ],
        "411\n■\ndefining": [
            1122085995
        ],
        "requesting": [
            1122085995
        ],
        "\ncustom": [
            1122085995
        ],
        "411\n142limiting": [
            1122085995
        ],
        "412\nsetting": [
            1122085995
        ],
        "hard": [
            1122085995
        ],
        "limit": [
            1122085995
        ],
        "amount": [
            1122085995
        ],
        "\ncan": [
            1122085995
        ],
        "412\n■\nexceeding": [
            1122085995
        ],
        "limits": [
            1122085995
        ],
        "414\n■\nunderstanding": [
            1122085995
        ],
        "see": [
            1122085995
        ],
        "415\n143understanding": [
            1122085995
        ],
        "qos": [
            1122085995
        ],
        "classes": [
            1122085995
        ],
        "417\ndefining": [
            1122085995
        ],
        "417\n■\nunderstanding": [
            1122085995
        ],
        "\nprocess": [
            1122085995
        ],
        "gets": [
            1122085995
        ],
        "killed": [
            1122085995
        ],
        "memory": [
            1122085995
        ],
        "low": [
            1122085995
        ],
        "420\n": [
            1122085995
        ],
        "\n\ncontents\nxviii\n144setting": [
            1122085995
        ],
        "421\nintroducing": [
            1122085995
        ],
        "limitrange": [
            1122085995
        ],
        "421\n■\ncreating": [
            1122085995
        ],
        "\nlimitrange": [
            1122085995
        ],
        "object": [
            1122085995
        ],
        "422\n■\nenforcing": [
            1122085995
        ],
        "423\napplying": [
            1122085995
        ],
        "424\n145limiting": [
            1122085995
        ],
        "total": [
            1122085995
        ],
        "425\nintroducing": [
            1122085995
        ],
        "resourcequota": [
            1122085995
        ],
        "425\n■\nspecifying": [
            1122085995
        ],
        "quota": [
            1122085995
        ],
        "427\n■\nlimiting": [
            1122085995
        ],
        "number": [
            1122085995
        ],
        "\nbe": [
            1122085995
        ],
        "created": [
            1122085995
        ],
        "427\n■\nspecifying": [
            1122085995
        ],
        "quotas": [
            1122085995
        ],
        "and/or": [
            1122085995
        ],
        "\nqos": [
            1122085995
        ],
        "429\n146monitoring": [
            1122085995
        ],
        "usage": [
            1122085995
        ],
        "430\ncollecting": [
            1122085995
        ],
        "retrieving": [
            1122085995
        ],
        "actual": [
            1122085995
        ],
        "usages": [
            1122085995
        ],
        "430\n■\nstoring": [
            1122085995
        ],
        "analyzing": [
            1122085995
        ],
        "historical": [
            1122085995
        ],
        "consumption": [
            1122085995
        ],
        "statistics": [
            1122085995
        ],
        "432\n147summary": [
            1122085995
        ],
        "435\n15": [
            1122085995
        ],
        "\nautomatic": [
            1122085995
        ],
        "437\n151horizontal": [
            1122085995
        ],
        "autoscaling": [
            1122085995
        ],
        "438\nunderstanding": [
            1122085995
        ],
        "438\n■\nscaling": [
            1122085995
        ],
        "based": [
            1122085995
        ],
        "\non": [
            1122085995
        ],
        "utilization": [
            1122085995
        ],
        "441\n■\nscaling": [
            1122085995
        ],
        "\nconsumption": [
            1122085995
        ],
        "448\n■\nscaling": [
            1122085995
        ],
        "custom": [
            1122085995
        ],
        "\nmetrics": [
            1122085995
        ],
        "448\n■\ndetermining": [
            1122085995
        ],
        "metrics": [
            1122085995
        ],
        "appropriate": [
            1122085995
        ],
        "\nautoscaling": [
            1122085995
        ],
        "450\n■\nscaling": [
            1122085995
        ],
        "zero": [
            1122085995
        ],
        "450\n152vertical": [
            1122085995
        ],
        "451\nautomatically": [
            1122085995
        ],
        "451\n■\nmodifying": [
            1122085995
        ],
        "451\n153horizontal": [
            1122085995
        ],
        "452\nintroducing": [
            1122085995
        ],
        "autoscaler": [
            1122085995
        ],
        "452\n■\nenabling": [
            1122085995
        ],
        "\nautoscaler": [
            1122085995
        ],
        "454\n■\nlimiting": [
            1122085995
        ],
        "disruption": [
            1122085995
        ],
        "\nscale-down": [
            1122085995
        ],
        "454\n154summary": [
            1122085995
        ],
        "456\n16": [
            1122085995
        ],
        "\nadvanced": [
            1122085995
        ],
        "457\n161using": [
            1122085995
        ],
        "taints": [
            1122085995
        ],
        "tolerations": [
            1122085995
        ],
        "repel": [
            1122085995
        ],
        "457\nintroducing": [
            1122085995
        ],
        "458\n■\nadding": [
            1122085995
        ],
        "460\n■\nadding": [
            1122085995
        ],
        "460\n■\nunderstanding": [
            1122085995
        ],
        "\nwhat": [
            1122085995
        ],
        "461\n": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\ncontents\nxix\n16.2using": [
            1122085995
        ],
        "affinity": [
            1122085995
        ],
        "attract": [
            1122085995
        ],
        "462\nspecifying": [
            1122085995
        ],
        "rules": [
            1122085995
        ],
        "463\n■\nprioritizing": [
            1122085995
        ],
        "465\n163co-locating": [
            1122085995
        ],
        "anti-affinity": [
            1122085995
        ],
        "468\nusing": [
            1122085995
        ],
        "inter-pod": [
            1122085995
        ],
        "deploy": [
            1122085995
        ],
        "468\ndeploying": [
            1122085995
        ],
        "rack": [
            1122085995
        ],
        "availability": [
            1122085995
        ],
        "zone": [
            1122085995
        ],
        "geographic": [
            1122085995
        ],
        "\nregion": [
            1122085995
        ],
        "471\n■\nexpressing": [
            1122085995
        ],
        "preferences": [
            1122085995
        ],
        "\nrequirements": [
            1122085995
        ],
        "472\n■\nscheduling": [
            1122085995
        ],
        "away": [
            1122085995
        ],
        "\npod": [
            1122085995
        ],
        "474\n164summary": [
            1122085995
        ],
        "476\n17": [
            1122085995
        ],
        "\nbest": [
            1122085995
        ],
        "477\n171bringing": [
            1122085995
        ],
        "everything": [
            1122085995
        ],
        "478\n172understanding": [
            1122085995
        ],
        "lifecycle": [
            1122085995
        ],
        "479\napplications": [
            1122085995
        ],
        "expect": [
            1122085995
        ],
        "relocated": [
            1122085995
        ],
        "479\nrescheduling": [
            1122085995
        ],
        "dead": [
            1122085995
        ],
        "partially": [
            1122085995
        ],
        "482\n■\nstarting": [
            1122085995
        ],
        "order": [
            1122085995
        ],
        "483\n■\nadding": [
            1122085995
        ],
        "hooks": [
            1122085995
        ],
        "485\nunderstanding": [
            1122085995
        ],
        "shutdown": [
            1122085995
        ],
        "489\n173ensuring": [
            1122085995
        ],
        "handled": [
            1122085995
        ],
        "492\npreventing": [
            1122085995
        ],
        "broken": [
            1122085995
        ],
        "starting": [
            1122085995
        ],
        "shut-down": [
            1122085995
        ],
        "493\n174making": [
            1122085995
        ],
        "easy": [
            1122085995
        ],
        "manage": [
            1122085995
        ],
        "497\nmaking": [
            1122085995
        ],
        "manageable": [
            1122085995
        ],
        "images": [
            1122085995
        ],
        "497\n■\nproperly": [
            1122085995
        ],
        "\ntagging": [
            1122085995
        ],
        "imagepullpolicy": [
            1122085995
        ],
        "497\nusing": [
            1122085995
        ],
        "multi-dimensional": [
            1122085995
        ],
        "single-dimensional": [
            1122085995
        ],
        "498\ndescribing": [
            1122085995
        ],
        "498\n■\nproviding": [
            1122085995
        ],
        "\ninformation": [
            1122085995
        ],
        "terminated": [
            1122085995
        ],
        "498\n■\nhandling": [
            1122085995
        ],
        "500\n175best": [
            1122085995
        ],
        "testing": [
            1122085995
        ],
        "502\nrunning": [
            1122085995
        ],
        "502\nusing": [
            1122085995
        ],
        "503\n■\nversioning": [
            1122085995
        ],
        "auto-\ndeploying": [
            1122085995
        ],
        "manifests": [
            1122085995
        ],
        "504\n■\nintroducing": [
            1122085995
        ],
        "ksonnet": [
            1122085995
        ],
        "\nalternative": [
            1122085995
        ],
        "yaml/json": [
            1122085995
        ],
        "505\n■\nemploying": [
            1122085995
        ],
        "\ncontinuous": [
            1122085995
        ],
        "integration": [
            1122085995
        ],
        "delivery": [
            1122085995
        ],
        "(ci/cd)": [
            1122085995
        ],
        "506\n176summary": [
            1122085995
        ],
        "506\n": [
            1122085995
        ],
        "\n\ncontents\nxx\n18": [
            1122085995
        ],
        "\nextending": [
            1122085995
        ],
        "508\n181defining": [
            1122085995
        ],
        "508\nintroducing": [
            1122085995
        ],
        "customresourcedefinitions": [
            1122085995
        ],
        "509\n■\nautomating": [
            1122085995
        ],
        "513\n■\nvalidating": [
            1122085995
        ],
        "517\n■\nproviding": [
            1122085995
        ],
        "518\n182extending": [
            1122085995
        ],
        "\ncatalog": [
            1122085995
        ],
        "519\nintroducing": [
            1122085995
        ],
        "catalog": [
            1122085995
        ],
        "520\n■\nintroducing": [
            1122085995
        ],
        "521\nintroducing": [
            1122085995
        ],
        "brokers": [
            1122085995
        ],
        "openservicebroker": [
            1122085995
        ],
        "522\nprovisioning": [
            1122085995
        ],
        "524\n■\nunbinding": [
            1122085995
        ],
        "\ndeprovisioning": [
            1122085995
        ],
        "526\n■\nunderstanding": [
            1122085995
        ],
        "brings": [
            1122085995
        ],
        "526\n183platforms": [
            1122085995
        ],
        "built": [
            1122085995
        ],
        "top": [
            1122085995,
            1043891123
        ],
        "527\nred": [
            1122085995
        ],
        "hat": [
            1122085995
        ],
        "openshift": [
            1122085995
        ],
        "527\n■\ndeis": [
            1122085995
        ],
        "workflow": [
            1122085995
        ],
        "helm": [
            1122085995
        ],
        "530\n184summary": [
            1122085995
        ],
        "533\nappendix": [
            1122085995
        ],
        "ausing": [
            1122085995
        ],
        "534\nappendix": [
            1122085995
        ],
        "bsetting": [
            1122085995
        ],
        "multi-node": [
            1122085995
        ],
        "kubeadm": [
            1122085995
        ],
        "539\nappendix": [
            1122085995
        ],
        "cusing": [
            1122085995
        ],
        "runtimes": [
            1122085995
        ],
        "552\nappendix": [
            1122085995
        ],
        "dcluster": [
            1122085995
        ],
        "federation": [
            1122085995
        ],
        "556\nindex": [
            1122085995
        ],
        "561\n": [
            1122085995
        ],
        "\n\nxxi\npreface\nafter": [
            1122085995
        ],
        "working": [
            1122085995
        ],
        "red": [
            1122085995
        ],
        "years": [
            1122085995
        ],
        "late": [
            1122085995
        ],
        "assigned": [
            1122085995
        ],
        "newly-\nestablished": [
            1122085995
        ],
        "team": [
            1122085995
        ],
        "called": [
            1122085995
        ],
        "cloud": [
            1122085995
        ],
        "enablement": [
            1122085995
        ],
        "task": [
            1122085995
        ],
        "bring": [
            1122085995
        ],
        "company’s\nrange": [
            1122085995
        ],
        "middleware": [
            1122085995
        ],
        "then\nbeing": [
            1122085995
        ],
        "developed": [
            1122085995
        ],
        "still": [
            1122085995
        ],
        "its\ninfancy—version": [
            1122085995
        ],
        "hadn’t": [
            1122085995
        ],
        "even": [
            1122085995
        ],
        "released": [
            1122085995
        ],
        "yet\n": [
            1122085995
        ],
        "get": [
            1122085995
        ],
        "know": [
            1122085995
        ],
        "ins": [
            1122085995
        ],
        "outs": [
            1122085995
        ],
        "quickly": [
            1122085995
        ],
        "proper\ndirection": [
            1122085995
        ],
        "software": [
            1122085995
        ],
        "take": [
            1122085995
        ],
        "advantage": [
            1122085995
        ],
        "offer\nwhen": [
            1122085995
        ],
        "faced": [
            1122085995
        ],
        "problem": [
            1122085995
        ],
        "us": [
            1122085995
        ],
        "tell": [
            1122085995
        ],
        "things": [
            1122085995
        ],
        "wrong": [
            1122085995
        ],
        "or\nmerely": [
            1122085995
        ],
        "hitting": [
            1122085995
        ],
        "early": [
            1122085995
        ],
        "bugs": [
            1122085995
        ],
        "understanding": [
            1122085995
        ],
        "come": [
            1122085995,
            2119133144
        ],
        "long": [
            1122085995
        ],
        "way": [
            1122085995
        ],
        "since": [
            1122085995
        ],
        "then\nwhen": [
            1122085995
        ],
        "started": [
            1122085995
        ],
        "people": [
            1122085995
        ],
        "heard": [
            1122085995
        ],
        "virtu-\nally": [
            1122085995
        ],
        "engineer": [
            1122085995
        ],
        "knows": [
            1122085995
        ],
        "become": [
            1122085995
        ],
        "fastest-\ngrowing": [
            1122085995
        ],
        "most-widely-adopted": [
            1122085995
        ],
        "ways": [
            1122085995
        ],
        "and\non-premises": [
            1122085995
        ],
        "datacenters": [
            1122085995
        ],
        "month": [
            1122085995
        ],
        "dealing": [
            1122085995
        ],
        "wrote": [
            1122085995
        ],
        "two-part": [
            1122085995
        ],
        "blog": [
            1122085995
        ],
        "post": [
            1122085995
        ],
        "about\nhow": [
            1122085995
        ],
        "jboss": [
            1122085995
        ],
        "wildfly": [
            1122085995
        ],
        "openshift/kubernetes": [
            1122085995
        ],
        "the\ntime": [
            1122085995
        ],
        "never": [
            1122085995
        ],
        "could": [
            1122085995
        ],
        "imagined": [
            1122085995
        ],
        "simple": [
            1122085995
        ],
        "would": [
            1122085995,
            2119133144
        ],
        "ultimately": [
            1122085995
        ],
        "lead": [
            1122085995
        ],
        "the\npeople": [
            1122085995
        ],
        "contact": [
            1122085995
        ],
        "whether": [
            1122085995
        ],
        "write": [
            1122085995
        ],
        "about\nkubernetes": [
            1122085995
        ],
        "course": [
            1122085995
        ],
        "couldn’t": [
            1122085995
        ],
        "say": [
            1122085995
        ],
        "offer": [
            1122085995
        ],
        "though": [
            1122085995
        ],
        "sure\nthey’d": [
            1122085995
        ],
        "approached": [
            1122085995
        ],
        "well": [
            1122085995
        ],
        "pick": [
            1122085995
        ],
        "someone": [
            1122085995
        ],
        "else\n": [
            1122085995
        ],
        "yet": [
            1122085995
        ],
        "year": [
            1122085995
        ],
        "half": [
            1122085995
        ],
        "researching\nthe": [
            1122085995
        ],
        "done": [
            1122085995,
            1118639836
        ],
        "it’s": [
            1122085995
        ],
        "awesome": [
            1122085995
        ],
        "journey": [
            1122085995
        ],
        "technology": [
            1122085995
        ],
        "is\n": [
            1122085995
        ],
        "\n\npreface\nxxii\nabsolutely": [
            1122085995
        ],
        "much": [
            1122085995
        ],
        "greater": [
            1122085995
        ],
        "detail": [
            1122085995
        ],
        "you’d": [
            1122085995
        ],
        "learn": [
            1122085995
        ],
        "just\na": [
            1122085995
        ],
        "knowledge": [
            1122085995
        ],
        "expanded": [
            1122085995
        ],
        "kuber-\nnetes": [
            1122085995
        ],
        "evolved": [
            1122085995
        ],
        "i’ve": [
            1122085995
        ],
        "constantly": [
            1122085995
        ],
        "gone": [
            1122085995
        ],
        "previous": [
            1122085995
        ],
        "chapters": [
            1122085995
        ],
        "and\nadded": [
            1122085995
        ],
        "i’m": [
            1122085995
        ],
        "perfectionist": [
            1122085995
        ],
        "i’ll": [
            1122085995
        ],
        "really": [
            1122085995
        ],
        "absolutely": [
            1122085995
        ],
        "sat-\nisfied": [
            1122085995
        ],
        "happy": [
            1122085995
        ],
        "hear": [
            1122085995
        ],
        "lot": [
            1122085995
        ],
        "readers": [
            1122085995
        ],
        "early\naccess": [
            1122085995
        ],
        "program": [
            1122085995
        ],
        "(meap)": [
            1122085995
        ],
        "found": [
            1122085995
        ],
        "great": [
            1122085995
        ],
        "guide": [
            1122085995
        ],
        "kubernetes\n": [
            1122085995
        ],
        "aim": [
            1122085995
        ],
        "reader": [
            1122085995
        ],
        "understand": [
            1122085995
        ],
        "teach": [
            1122085995
        ],
        "them\nhow": [
            1122085995
        ],
        "tooling": [
            1122085995
        ],
        "effectively": [
            1122085995
        ],
        "efficiently": [
            1122085995
        ],
        "develop": [
            1122085995
        ],
        "don’t": [
            1122085995
        ],
        "emphasis": [
            1122085995
        ],
        "actually": [
            1122085995
        ],
        "and\nmaintain": [
            1122085995
        ],
        "proper": [
            1122085995
        ],
        "last": [
            1122085995
        ],
        "give\nreaders": [
            1122085995
        ],
        "solid": [
            1122085995
        ],
        "consists": [
            1122085995
        ],
        "allow\nthem": [
            1122085995
        ],
        "easily": [
            1122085995
        ],
        "comprehend": [
            1122085995
        ],
        "subject\n": [
            1122085995
        ],
        "hope": [
            1122085995
        ],
        "you’ll": [
            1122085995
        ],
        "enjoy": [
            1122085995
        ],
        "reading": [
            1122085995
        ],
        "teaches": [
            1122085995
        ],
        "of\nthe": [
            1122085995
        ],
        "\n\nxxiii\nacknowledgments\nbefore": [
            1122085995
        ],
        "clue": [
            1122085995
        ],
        "many": [
            1122085995
        ],
        "involved\nin": [
            1122085995
        ],
        "bringing": [
            1122085995
        ],
        "rough": [
            1122085995
        ],
        "manuscript": [
            1122085995
        ],
        "published": [
            1122085995
        ],
        "piece": [
            1122085995
        ],
        "work": [
            1122085995
        ],
        "means\nthere": [
            1122085995
        ],
        "thank\n": [
            1122085995
        ],
        "i’d": [
            1122085995
        ],
        "thank": [
            1122085995
        ],
        "erin": [
            1122085995
        ],
        "twohey": [
            1122085995
        ],
        "approaching": [
            1122085995
        ],
        "book\nand": [
            1122085995
        ],
        "michael": [
            1122085995
        ],
        "stephens": [
            1122085995
        ],
        "full": [
            1122085995
        ],
        "confidence": [
            1122085995
        ],
        "ability": [
            1122085995
        ],
        "it\nfrom": [
            1122085995
        ],
        "day": [
            1122085995
        ],
        "words": [
            1122085995
        ],
        "encouragement": [
            1122085995
        ],
        "motivated": [
            1122085995
        ],
        "kept": [
            1122085995
        ],
        "me\nmotivated": [
            1122085995
        ],
        "throughout": [
            1122085995
        ],
        "editor": [
            1122085995
        ],
        "andrew": [
            1122085995
        ],
        "warren": [
            1122085995
        ],
        "who\nhelped": [
            1122085995
        ],
        "chapter": [
            1122085995
        ],
        "door": [
            1122085995
        ],
        "hyde": [
            1122085995
        ],
        "took": [
            1122085995
        ],
        "from\nandrew": [
            1122085995
        ],
        "worked": [
            1122085995,
            2119133144
        ],
        "bearing\nwith": [
            1122085995
        ],
        "difficult": [
            1122085995
        ],
        "person": [
            1122085995
        ],
        "tend": [
            1122085995
        ],
        "drop": [
            1122085995
        ],
        "the\nradar": [
            1122085995
        ],
        "fairly": [
            1122085995
        ],
        "regularly": [
            1122085995
        ],
        "boyarsky": [
            1122085995
        ],
        "reviewer": [
            1122085995
        ],
        "read": [
            1122085995
        ],
        "and\ncomment": [
            1122085995
        ],
        "instrumen-\ntal": [
            1122085995
        ],
        "making": [
            1122085995
        ],
        "nice": [
            1122085995
        ],
        "hopefully": [
            1122085995
        ],
        "comments": [
            1122085995
        ],
        "book\ncould": [
            1122085995
        ],
        "received": [
            1122085995
        ],
        "good": [
            1122085995
        ],
        "reviews": [
            1122085995
        ],
        "reviewers": [
            1122085995
        ],
        "readers\n": [
            1122085995
        ],
        "technical": [
            1122085995
        ],
        "proofreader": [
            1122085995
        ],
        "magnaghi": [
            1122085995
        ],
        "all\nmy": [
            1122085995
        ],
        "reviewers:": [
            1122085995
        ],
        "al": [
            1122085995
        ],
        "krinker": [
            1122085995
        ],
        "alessandro": [
            1122085995
        ],
        "campeis": [
            1122085995
        ],
        "alexander": [
            1122085995
        ],
        "myltsev": [
            1122085995
        ],
        "csaba": [
            1122085995
        ],
        "sari\ndavid": [
            1122085995
        ],
        "dimaria": [
            1122085995
        ],
        "elias": [
            1122085995
        ],
        "rangel": [
            1122085995
        ],
        "erisk": [
            1122085995
        ],
        "zelenka": [
            1122085995
        ],
        "fabrizio": [
            1122085995
        ],
        "cucci": [
            1122085995
        ],
        "jared": [
            1122085995
        ],
        "duncan": [
            1122085995
        ],
        "keith\ndonaldson": [
            1122085995
        ],
        "bright": [
            1122085995
        ],
        "paolo": [
            1122085995
        ],
        "antinori": [
            1122085995
        ],
        "peter": [
            1122085995
        ],
        "perlepes": [
            1122085995
        ],
        "tiklu": [
            1122085995
        ],
        "ganguly": [
            1122085995
        ],
        "their\npositive": [
            1122085995
        ],
        "going": [
            1122085995
        ],
        "times": [
            1122085995
        ],
        "worried": [
            1122085995
        ],
        "utterly": [
            1122085995
        ],
        "awful\nand": [
            1122085995
        ],
        "completely": [
            1122085995
        ],
        "useless": [
            1122085995
        ],
        "hand": [
            1122085995
        ],
        "constructive": [
            1122085995
        ],
        "criticism": [
            1122085995
        ],
        "helped": [
            1122085995
        ],
        "improve\n": [
            1122085995
        ],
        "\n\nacknowledgments\nxxiv\nsections": [
            1122085995
        ],
        "thrown": [
            1122085995
        ],
        "enough": [
            1122085995
        ],
        "effort": [
            1122085995
        ],
        "point-\ning": [
            1122085995
        ],
        "hard-to-understand": [
            1122085995
        ],
        "sections": [
            1122085995
        ],
        "suggesting": [
            1122085995
        ],
        "improving": [
            1122085995
        ],
        "book\nalso": [
            1122085995
        ],
        "asking": [
            1122085995
        ],
        "right": [
            1122085995
        ],
        "questions": [
            1122085995
        ],
        "made": [
            1122085995
        ],
        "realize": [
            1122085995
        ],
        "wrong\nabout": [
            1122085995
        ],
        "two": [
            1122085995,
            1941223023,
            410929361
        ],
        "three": [
            1122085995
        ],
        "manuscript\n": [
            1122085995
        ],
        "bought": [
            1122085995
        ],
        "through\nmanning’s": [
            1122085995
        ],
        "meap": [
            1122085995
        ],
        "voiced": [
            1122085995
        ],
        "forum": [
            1122085995
        ],
        "reached\nout": [
            1122085995
        ],
        "directly—especially": [
            1122085995
        ],
        "vimal": [
            1122085995
        ],
        "kansal": [
            1122085995
        ],
        "patierno": [
            1122085995
        ],
        "roland": [
            1122085995
        ],
        "huß": [
            1122085995
        ],
        "who\nnoticed": [
            1122085995
        ],
        "quite": [
            1122085995
        ],
        "inconsistencies": [
            1122085995
        ],
        "mistakes": [
            1122085995
        ],
        "thank\neveryone": [
            1122085995
        ],
        "getting": [
            1122085995,
            410929361
        ],
        "i\nfinish": [
            1122085995
        ],
        "colleague": [
            1122085995
        ],
        "high": [
            1122085995
        ],
        "school": [
            1122085995
        ],
        "friend": [
            1122085995
        ],
        "aleš": [
            1122085995
        ],
        "justin": [
            1122085995
        ],
        "who\nbrought": [
            1122085995
        ],
        "wonderful": [
            1122085995
        ],
        "colleagues": [
            1122085995
        ],
        "enablement\nteam": [
            1122085995
        ],
        "wouldn’t": [
            1122085995
        ],
        "write\nthis": [
            1122085995
        ],
        "book\n": [
            1122085995
        ],
        "lastly": [
            1122085995
        ],
        "wife": [
            1122085995
        ],
        "son": [
            1122085995
        ],
        "understanding\nand": [
            1122085995
        ],
        "supportive": [
            1122085995
        ],
        "months": [
            1122085995
        ],
        "locked": [
            1122085995
        ],
        "office": [
            1122085995
        ],
        "of\nspending": [
            1122085995
        ],
        "them\n": [
            1122085995
        ],
        "all!\n": [
            1122085995
        ],
        "\n\nxxv\nabout": [
            1122085995
        ],
        "book\nkubernetes": [
            1122085995
        ],
        "aims": [
            1122085995
        ],
        "make": [
            1122085995
        ],
        "proficient": [
            1122085995
        ],
        "you\nvirtually": [
            1122085995
        ],
        "concepts": [
            1122085995
        ],
        "appli-\ncations": [
            1122085995
        ],
        "diving": [
            1122085995
        ],
        "gives": [
            1122085995
        ],
        "overview": [
            1122085995
        ],
        "technolo-\ngies": [
            1122085995
        ],
        "including": [
            1122085995
        ],
        "build": [
            1122085995
        ],
        "haven’t\nused": [
            1122085995
        ],
        "slowly": [
            1122085995
        ],
        "guides": [
            1122085995
        ],
        "you\nthrough": [
            1122085995
        ],
        "kubernetes—from": [
            1122085995
        ],
        "to\nthings": [
            1122085995
        ],
        "hidden": [
            1122085995
        ],
        "surface\nwho": [
            1122085995
        ],
        "book\nthe": [
            1122085995
        ],
        "focuses": [
            1122085995
        ],
        "primarily": [
            1122085995
        ],
        "developers": [
            1122085995
        ],
        "provides": [
            1122085995
        ],
        "overview\nof": [
            1122085995
        ],
        "managing": [
            1122085995
        ],
        "operational": [
            1122085995
        ],
        "perspective": [
            1122085995
        ],
        "meant": [
            1122085995
        ],
        "anyone\ninterested": [
            1122085995
        ],
        "a\nsingle": [
            1122085995
        ],
        "server\n": [
            1122085995
        ],
        "beginner": [
            1122085995
        ],
        "advanced": [
            1122085995
        ],
        "engineers": [
            1122085995
        ],
        "want": [
            1122085995
        ],
        "con-\ntainer": [
            1122085995
        ],
        "orchestrating": [
            1122085995
        ],
        "related": [
            1122085995
        ],
        "scale": [
            1122085995
        ],
        "gain": [
            1122085995
        ],
        "the\nexpertise": [
            1122085995
        ],
        "necessary": [
            1122085995
        ],
        "containerize": [
            1122085995
        ],
        "kuberne-\ntes": [
            1122085995
        ],
        "exposure": [
            1122085995
        ],
        "either": [
            1122085995
        ],
        "required\nthe": [
            1122085995
        ],
        "explains": [
            1122085995
        ],
        "subject": [
            1122085995
        ],
        "matter": [
            1122085995
        ],
        "progressively": [
            1122085995
        ],
        "detailed": [
            1122085995
        ],
        "manner": [
            1122085995
        ],
        "doesn’t\nuse": [
            1122085995
        ],
        "source": [
            1122085995
        ],
        "code": [
            1122085995
        ],
        "non-expert": [
            1122085995
        ],
        "to\nunderstand": [
            1122085995
        ],
        "\n\nabout": [
            1122085995
        ],
        "book\nxxvi\n": [
            1122085995
        ],
        "however": [
            1122085995
        ],
        "programming": [
            1122085995
        ],
        "com-\nputer": [
            1122085995
        ],
        "commands": [
            1122085995
        ],
        "linux": [
            1122085995
        ],
        "of\nwell-known": [
            1122085995
        ],
        "computer": [
            1122085995
        ],
        "protocols": [
            1122085995
        ],
        "http": [
            1122085995
        ],
        "organized:": [
            1122085995
        ],
        "roadmap\nthis": [
            1122085995
        ],
        "chapters\n": [
            1122085995
        ],
        "short": [
            1122085995
        ],
        "introduction": [
            1122085995
        ],
        "contains": [
            1122085995
        ],
        "chapters:\n■chapter": [
            1122085995
        ],
        "came": [
            1122085995
        ],
        "helps": [
            1122085995
        ],
        "to\nsolve": [
            1122085995
        ],
        "today’s": [
            1122085995
        ],
        "problems": [
            1122085995
        ],
        "scale\n■chapter": [
            1122085995
        ],
        "hands-on": [
            1122085995
        ],
        "tutorial": [
            1122085995
        ],
        "in\na": [
            1122085995
        ],
        "kubernetes\ncluster": [
            1122085995
        ],
        "cloud\npart": [
            1122085995
        ],
        "introduces": [
            1122085995
        ],
        "key": [
            1122085995
        ],
        "follows:\n■chapter": [
            1122085995
        ],
        "fundamental": [
            1122085995
        ],
        "building": [
            1122085995
        ],
        "block": [
            1122085995
        ],
        "kubernetes—the": [
            1122085995
        ],
        "pod—\nand": [
            1122085995
        ],
        "organize": [
            1122085995
        ],
        "\n■chapter": [
            1122085995
        ],
        "keeps": [
            1122085995
        ],
        "automati-\ncally": [
            1122085995
        ],
        "restarting": [
            1122085995
        ],
        "shows": [
            1122085995
        ],
        "pods\nhorizontally": [
            1122085995
        ],
        "resistant": [
            1122085995
        ],
        "failures": [
            1122085995
        ],
        "and\nrun": [
            1122085995
        ],
        "predefined": [
            1122085995
        ],
        "periodically\n■chapter": [
            1122085995
        ],
        "expose": [
            1122085995
        ],
        "provide": [
            1122085995,
            2119133144
        ],
        "run-\nning": [
            1122085995
        ],
        "the\ncluster": [
            1122085995
        ],
        "regardless": [
            1122085995
        ],
        "live": [
            1122085995
        ],
        "out\nof": [
            1122085995
        ],
        "share\nfiles": [
            1122085995
        ],
        "accessible": [
            1122085995
        ],
        "like\ncredentials": [
            1122085995
        ],
        "pods\n■chapter": [
            1122085995
        ],
        "describes": [
            1122085995
        ],
        "they’re": [
            1122085995
        ],
        "to\nalter": [
            1122085995
        ],
        "state": [
            1122085995
        ],
        "cluster\n■chapter": [
            1122085995
        ],
        "concept": [
            1122085995
        ],
        "way\nof": [
            1122085995
        ],
        "environment\n■chapter": [
            1122085995
        ],
        "which\nusually": [
            1122085995
        ],
        "require": [
            1122085995
        ],
        "state\npart": [
            1122085995
        ],
        "dives": [
            1122085995
        ],
        "deep": [
            1122085995
        ],
        "addi-\ntional": [
            1122085995
        ],
        "you’ve": [
            1122085995
        ],
        "learned": [
            1122085995
        ],
        "a\nhigher": [
            1122085995
        ],
        "goes": [
            1122085995
        ],
        "beneath": [
            1122085995
        ],
        "surface": [
            1122085995
        ],
        "compo-\nnents": [
            1122085995
        ],
        "also\n": [
            1122085995
        ],
        "book\nxxvii\nexplains": [
            1122085995
        ],
        "communicate": [
            1122085995
        ],
        "per-\nform": [
            1122085995
        ],
        "balancing": [
            1122085995
        ],
        "secure": [
            1122085995
        ],
        "exten-\nsion": [
            1122085995
        ],
        "a\ncluster": [
            1122085995
        ],
        "administrator": [
            1122085995
        ],
        "prevent": [
            1122085995
        ],
        "that\n■chapter": [
            1122085995
        ],
        "constraining": [
            1122085995
        ],
        "applica-\ntion": [
            1122085995
        ],
        "consume": [
            1122085995
        ],
        "applications’": [
            1122085995
        ],
        "quality": [
            1122085995
        ],
        "service\nguarantees": [
            1122085995
        ],
        "monitoring": [
            1122085995
        ],
        "it\nalso": [
            1122085995
        ],
        "consuming": [
            1122085995
        ],
        "resources\n■chapter": [
            1122085995
        ],
        "discusses": [
            1122085995
        ],
        "configured": [
            1122085995
        ],
        "automatically": [
            1122085995
        ],
        "scale\nthe": [
            1122085995
        ],
        "increase\nthe": [
            1122085995
        ],
        "size": [
            1122085995
        ],
        "current": [
            1122085995
        ],
        "can’t": [
            1122085995
        ],
        "accept\nany": [
            1122085995
        ],
        "ensure": [
            1122085995
        ],
        "or\nhow": [
            1122085995
        ],
        "others": [
            1122085995
        ],
        "make\nsure": [
            1122085995
        ],
        "happening\n■chapter": [
            1122085995
        ],
        "them\ngood": [
            1122085995
        ],
        "citizens": [
            1122085995
        ],
        "pointers": [
            1122085995
        ],
        "your\ndevelopment": [
            1122085995
        ],
        "workflows": [
            1122085995
        ],
        "reduce": [
            1122085995
        ],
        "friction": [
            1122085995
        ],
        "development\n■chapter": [
            1122085995
        ],
        "extend": [
            1122085995
        ],
        "custom\nobjects": [
            1122085995
        ],
        "enterprise-class": [
            1122085995
        ],
        "application\nplatforms\nas": [
            1122085995
        ],
        "progress": [
            1122085995
        ],
        "individual\nkubernetes": [
            1122085995
        ],
        "blocks": [
            1122085995
        ],
        "improve": [
            1122085995
        ],
        "using\nthe": [
            1122085995
        ],
        "\nkubectl": [
            1122085995
        ],
        "tool\nabout": [
            1122085995
        ],
        "code\nwhile": [
            1122085995
        ],
        "doesn’t": [
            1122085995
        ],
        "contain": [
            1122085995
        ],
        "of\nmanifests": [
            1122085995
        ],
        "format": [
            1122085995
        ],
        "shell": [
            1122085995
        ],
        "along": [
            1122085995,
            2119133144
        ],
        "with\ntheir": [
            1122085995
        ],
        "outputs": [
            1122085995
        ],
        "formatted": [
            1122085995
        ],
        "\nfixed-width": [
            1122085995
        ],
        "font": [
            1122085995
        ],
        "ordinary": [
            1122085995
        ],
        "text": [
            1122085995
        ],
        "mostly": [
            1122085995
        ],
        "bold": [
            1122085995
        ],
        "clearly": [
            1122085995
        ],
        "output": [
            1122085995
        ],
        "but\nsometimes": [
            1122085995
        ],
        "important": [
            1122085995
        ],
        "command’s\noutput": [
            1122085995
        ],
        "cases": [
            1122085995
        ],
        "reformat-\nted": [
            1122085995
        ],
        "fit": [
            1122085995
        ],
        "limited": [
            1122085995
        ],
        "space": [
            1122085995
        ],
        "cli\ntool": [
            1122085995
        ],
        "evolving": [
            1122085995
        ],
        "newer": [
            1122085995
        ],
        "print": [
            1122085995
        ],
        "information\nthan": [
            1122085995
        ],
        "what’s": [
            1122085995
        ],
        "shown": [
            1122085995
        ],
        "confused": [
            1122085995
        ],
        "match": [
            1122085995
        ],
        "listings": [
            1122085995
        ],
        "sometimes": [
            1122085995
        ],
        "include": [
            1122085995,
            410929361
        ],
        "line-continuation": [
            1122085995
        ],
        "marker": [
            1122085995
        ],
        "(\n➥)": [
            1122085995
        ],
        "show": [
            1122085995
        ],
        "line": [
            1122085995
        ],
        "of\ntext": [
            1122085995
        ],
        "wraps": [
            1122085995
        ],
        "next": [
            1122085995
        ],
        "highlight": [
            1122085995
        ],
        "explain\nthe": [
            1122085995
        ],
        "book\nxxviii\n": [
            1122085995
        ],
        "paragraphs": [
            1122085995
        ],
        "common": [
            1122085995,
            1118639836,
            186247402
        ],
        "elements": [
            1122085995
        ],
        "replication-\ncontroller": [
            1122085995
        ],
        "forth": [
            1122085995
        ],
        "regular": [
            1122085995
        ],
        "avoid": [
            1122085995
        ],
        "over-\nproliferation": [
            1122085995
        ],
        "help": [
            1122085995
        ],
        "readability": [
            1122085995
        ],
        "places": [
            1122085995
        ],
        "“pod”": [
            1122085995
        ],
        "capitalized\nto": [
            1122085995
        ],
        "refer": [
            1122085995
        ],
        "lowercased": [
            1122085995
        ],
        "running\ncontainers\n": [
            1122085995
        ],
        "samples": [
            1122085995
        ],
        "tested": [
            1122085995
        ],
        "running\nin": [
            1122085995
        ],
        "engine": [
            1122085995
        ],
        "complete\nsource": [
            1122085995
        ],
        "https://githubcom/luksa/kubernetes-\nin-action": [
            1122085995
        ],
        "downloaded": [
            1122085995
        ],
        "publisher’s": [
            1122085995
        ],
        "website": [
            1122085995
        ],
        "wwwmanning.com/books/\nkubernetes-in-action.\nbook": [
            1122085995
        ],
        "forum\npurchase": [
            1122085995
        ],
        "includes": [
            1122085995
        ],
        "free": [
            1122085995
        ],
        "private": [
            1122085995
        ],
        "by\nmanning": [
            1122085995
        ],
        "ask": [
            1122085995
        ],
        "technical\nquestions": [
            1122085995
        ],
        "receive": [
            1122085995
        ],
        "the\nforum": [
            1122085995
        ],
        "go": [
            1122085995
        ],
        "https://forumsmanning.com/forums/kubernetes-in-action.": [
            1122085995
        ],
        "also\nlearn": [
            1122085995
        ],
        "forums": [
            1122085995
        ],
        "conduct": [
            1122085995
        ],
        "https://forums\nmanning.com/forums/about.\n": [
            1122085995
        ],
        "commitment": [
            1122085995
        ],
        "venue": [
            1122085995
        ],
        "meaningful\ndialogue": [
            1122085995
        ],
        "take\nplace": [
            1122085995
        ],
        "participation": [
            1122085995
        ],
        "whose": [
            1122085995
        ],
        "contribution": [
            1122085995
        ],
        "remains": [
            1122085995
        ],
        "voluntary": [
            1122085995
        ],
        "(and": [
            1122085995
        ],
        "unpaid)": [
            1122085995
        ],
        "sug-\ngest": [
            1122085995
        ],
        "try": [
            1122085995
        ],
        "challenging": [
            1122085995
        ],
        "lest": [
            1122085995
        ],
        "interest": [
            1122085995
        ],
        "stray!": [
            1122085995
        ],
        "archives": [
            1122085995
        ],
        "discussions": [
            1122085995,
            1478412827
        ],
        "publisher’s\nwebsite": [
            1122085995
        ],
        "print\nother": [
            1122085995
        ],
        "resources\nyou": [
            1122085995
        ],
        "find": [
            1122085995,
            186247402
        ],
        "wide": [
            1122085995
        ],
        "range": [
            1122085995
        ],
        "following": [
            1122085995
        ],
        "locations:\n■the": [
            1122085995
        ],
        "https://kubernetesio\n■the": [
            1122085995
        ],
        "posts": [
            1122085995
        ],
        "interesting": [
            1122085995
        ],
        "info": [
            1122085995
        ],
        "(http://blogkuber-\nnetes.io)\n■the": [
            1122085995
        ],
        "community’s": [
            1122085995
        ],
        "slack": [
            1122085995
        ],
        "channel": [
            1122085995
        ],
        "http://slackk8s.io\n■the": [
            1122085995
        ],
        "native": [
            1122085995
        ],
        "computing": [
            1122085995
        ],
        "foundation’s": [
            1122085995
        ],
        "youtube": [
            1122085995
        ],
        "channels:\n–https://wwwyoutube.com/channel/ucz2bu0quttom0thya_jkiwg": [
            1122085995
        ],
        "\n–https://wwwyoutube.com/channel/ucvqbfhwn-nwalwpjpukpvta": [
            1122085995
        ],
        "topics": [
            1122085995
        ],
        "contribute": [
            1122085995
        ],
        "to\nkubernetes": [
            1122085995
        ],
        "check": [
            1122085995
        ],
        "special": [
            1122085995
        ],
        "(sigs)\nat": [
            1122085995
        ],
        "https://githubcom/kubernetes/kubernetes/wiki/special-interest-groups-(sigs).\n": [
            1122085995
        ],
        "finally": [
            1122085995
        ],
        "open": [
            1122085995
        ],
        "there’s": [
            1122085995
        ],
        "wealth": [
            1122085995
        ],
        "available\nin": [
            1122085995
        ],
        "https://githubcom/kubernetes/\nkubernetes": [
            1122085995
        ],
        "repositories": [
            1122085995
        ],
        "\n\nxxix\nabout": [
            1122085995
        ],
        "author\nmarko": [
            1122085995
        ],
        "of\nprofessional": [
            1122085995
        ],
        "experience": [
            1122085995
        ],
        "simple\nweb": [
            1122085995
        ],
        "erp": [
            1122085995
        ],
        "systems": [
            1122085995
        ],
        "frameworks": [
            1122085995
        ],
        "middle-\nware": [
            1122085995
        ],
        "in\n1985": [
            1122085995
        ],
        "age": [
            1122085995
        ],
        "six": [
            1122085995
        ],
        "second-hand": [
            1122085995
        ],
        "zx": [
            1122085995
        ],
        "spectrum": [
            1122085995
        ],
        "father": [
            1122085995
        ],
        "primary": [
            1122085995
        ],
        "was\nthe": [
            1122085995
        ],
        "national": [
            1122085995
        ],
        "champion": [
            1122085995
        ],
        "logo": [
            1122085995
        ],
        "competition\nand": [
            1122085995
        ],
        "attended": [
            1122085995
        ],
        "summer": [
            1122085995
        ],
        "coding": [
            1122085995
        ],
        "camps": [
            1122085995
        ],
        "pro-\ngram": [
            1122085995
        ],
        "pascal": [
            1122085995
        ],
        "a\nwide": [
            1122085995
        ],
        "languages\n": [
            1122085995
        ],
        "dynamic": [
            1122085995
        ],
        "websites": [
            1122085995
        ],
        "when\nthe": [
            1122085995
        ],
        "relatively": [
            1122085995
        ],
        "young": [
            1122085995
        ],
        "moved": [
            1122085995
        ],
        "the\nhealthcare": [
            1122085995
        ],
        "telecommunications": [
            1122085995
        ],
        "industries": [
            1122085995
        ],
        "company": [
            1122085995
        ],
        "studying\ncomputer": [
            1122085995
        ],
        "science": [
            1122085995
        ],
        "university": [
            1122085995
        ],
        "ljubljana": [
            1122085995
        ],
        "slovenia": [
            1122085995
        ],
        "eventually": [
            1122085995
        ],
        "ended": [
            1122085995
        ],
        "up\nworking": [
            1122085995
        ],
        "initially": [
            1122085995
        ],
        "implementation": [
            1122085995
        ],
        "goo-\ngle": [
            1122085995
        ],
        "utilized": [
            1122085995
        ],
        "hat’s": [
            1122085995
        ],
        "underneath\nhe": [
            1122085995
        ],
        "contributed": [
            1122085995
        ],
        "projects": [
            1122085995
        ],
        "cdi/weld": [
            1122085995
        ],
        "infinispan/jboss": [
            1122085995
        ],
        "data-\ngrid": [
            1122085995
        ],
        "others\n": [
            1122085995
        ],
        "his\nresponsibilities": [
            1122085995
        ],
        "staying": [
            1122085995
        ],
        "up-to-date": [
            1122085995
        ],
        "developments": [
            1122085995
        ],
        "and\nrelated": [
            1122085995
        ],
        "ensuring": [
            1122085995
        ],
        "company’s": [
            1122085995
        ],
        "utilizes": [
            1122085995
        ],
        "fea-\ntures": [
            1122085995
        ],
        "potential\n": [
            1122085995
        ],
        "\n\nxxx\nabout": [
            1122085995
        ],
        "illustration\nthe": [
            1122085995
        ],
        "figure": [
            1122085995
        ],
        "“member": [
            1122085995
        ],
        "divan”": [
            1122085995
        ],
        "turkish\ncouncil": [
            1122085995
        ],
        "governing": [
            1122085995
        ],
        "body": [
            1122085995
        ],
        "taken": [
            1122085995
        ],
        "collection": [
            1122085995
        ],
        "cos-\ntumes": [
            1122085995
        ],
        "ottoman": [
            1122085995
        ],
        "empire": [
            1122085995
        ],
        "january": [
            1122085995
        ],
        "william": [
            1122085995
        ],
        "miller": [
            1122085995
        ],
        "old\nbond": [
            1122085995
        ],
        "street": [
            1122085995
        ],
        "london": [
            1122085995
        ],
        "title": [
            1122085995
        ],
        "page": [
            1122085995
        ],
        "missing": [
            1122085995
        ],
        "been\nunable": [
            1122085995
        ],
        "track": [
            1122085995
        ],
        "date": [
            1122085995
        ],
        "book’s": [
            1122085995
        ],
        "table": [
            1122085995
        ],
        "contents": [
            1122085995
        ],
        "identifies": [
            1122085995
        ],
        "figures": [
            1122085995
        ],
        "in\nboth": [
            1122085995
        ],
        "english": [
            1122085995
        ],
        "french": [
            1122085995
        ],
        "bears": [
            1122085995
        ],
        "names": [
            1122085995
        ],
        "artists": [
            1122085995
        ],
        "who\nworked": [
            1122085995
        ],
        "doubt": [
            1122085995
        ],
        "surprised": [
            1122085995
        ],
        "art": [
            1122085995
        ],
        "gracing": [
            1122085995
        ],
        "the\nfront": [
            1122085995
        ],
        "..": [
            1122085995
        ],
        "later\n": [
            1122085995
        ],
        "purchased": [
            1122085995
        ],
        "antiquarian": [
            1122085995
        ],
        "flea": [
            1122085995
        ],
        "market": [
            1122085995
        ],
        "in\nthe": [
            1122085995
        ],
        "“garage”": [
            1122085995
        ],
        "west": [
            1122085995
        ],
        "26th": [
            1122085995
        ],
        "manhattan": [
            1122085995
        ],
        "seller": [
            1122085995
        ],
        "american": [
            1122085995
        ],
        "in\nankara": [
            1122085995
        ],
        "turkey": [
            1122085995
        ],
        "transaction": [
            1122085995
        ],
        "place": [
            1122085995
        ],
        "packing": [
            1122085995
        ],
        "stand": [
            1122085995
        ],
        "for\nthe": [
            1122085995
        ],
        "didn’t": [
            1122085995
        ],
        "substantial": [
            1122085995
        ],
        "cash\nthat": [
            1122085995
        ],
        "required": [
            1122085995
        ],
        "purchase": [
            1122085995
        ],
        "credit": [
            1122085995
        ],
        "card": [
            1122085995
        ],
        "politely\nturned": [
            1122085995
        ],
        "flying": [
            1122085995
        ],
        "ankara": [
            1122085995
        ],
        "evening": [
            1122085995
        ],
        "situation": [
            1122085995
        ],
        "get-\nting": [
            1122085995
        ],
        "hopeless": [
            1122085995
        ],
        "solution?": [
            1122085995
        ],
        "turned": [
            1122085995
        ],
        "nothing": [
            1122085995
        ],
        "old-\nfashioned": [
            1122085995
        ],
        "verbal": [
            1122085995
        ],
        "agreement": [
            1122085995
        ],
        "sealed": [
            1122085995
        ],
        "handshake": [
            1122085995
        ],
        "proposed": [
            1122085995
        ],
        "the\nmoney": [
            1122085995
        ],
        "transferred": [
            1122085995
        ],
        "wire": [
            1122085995
        ],
        "walked": [
            1122085995
        ],
        "bank": [
            1122085995
        ],
        "infor-\nmation": [
            1122085995
        ],
        "portfolio": [
            1122085995
        ],
        "arm": [
            1122085995
        ],
        "needless": [
            1122085995
        ],
        "say\nwe": [
            1122085995
        ],
        "funds": [
            1122085995
        ],
        "remain": [
            1122085995
        ],
        "grateful": [
            1122085995
        ],
        "impressed": [
            1122085995
        ],
        "this\nunknown": [
            1122085995
        ],
        "person’s": [
            1122085995
        ],
        "trust": [
            1122085995
        ],
        "recalls": [
            1122085995
        ],
        "something": [
            1122085995
        ],
        "might": [
            1122085995
        ],
        "happened": [
            1122085995
        ],
        "a\nlong": [
            1122085995
        ],
        "ago": [
            1122085995
        ],
        "celebrate": [
            1122085995
        ],
        "inventiveness": [
            1122085995
        ],
        "initiative": [
            1122085995
        ],
        "yes": [
            1122085995
        ],
        "the\nfun": [
            1122085995
        ],
        "business": [
            1122085995
        ],
        "covers": [
            1122085995
        ],
        "rich": [
            1122085995
        ],
        "diversity": [
            1122085995
        ],
        "regional\nlife": [
            1122085995
        ],
        "centuries": [
            1122085995
        ],
        "ago‚": [
            1122085995
        ],
        "brought": [
            1122085995
        ],
        "life": [
            1122085995
        ],
        "pictures": [
            1122085995
        ],
        "collection\n": [
            1122085995
        ],
        "\n\n1\nintroducing": [
            1122085995
        ],
        "kubernetes\nyears": [
            1122085995
        ],
        "big": [
            1122085995
        ],
        "monoliths": [
            1122085995
        ],
        "single\nprocess": [
            1122085995
        ],
        "small": [
            1122085995
        ],
        "spread": [
            1122085995
        ],
        "handful": [
            1122085995
        ],
        "servers": [
            1122085995
        ],
        "these\nlegacy": [
            1122085995
        ],
        "widespread": [
            1122085995
        ],
        "today": [
            1122085995
        ],
        "slow": [
            1122085995
        ],
        "release": [
            1122085995
        ],
        "cycles": [
            1122085995
        ],
        "are\nupdated": [
            1122085995
        ],
        "infrequently": [
            1122085995
        ],
        "cycle": [
            1122085995
        ],
        "pack-\nage": [
            1122085995
        ],
        "ops": [
            1122085995
        ],
        "deploys": [
            1122085995
        ],
        "and\nmonitors": [
            1122085995
        ],
        "case": [
            1122085995
        ],
        "hardware": [
            1122085995
        ],
        "migrates": [
            1122085995
        ],
        "the\nremaining": [
            1122085995
        ],
        "legacy": [
            1122085995
        ],
        "down\ninto": [
            1122085995
        ],
        "smaller": [
            1122085995
        ],
        "independently": [
            1122085995
        ],
        "because\nthis": [
            1122085995
        ],
        "covers\nunderstanding": [
            1122085995
        ],
        "\ndeployment": [
            1122085995
        ],
        "changed": [
            1122085995
        ],
        "recent": [
            1122085995
        ],
        "years\nisolating": [
            1122085995
        ],
        "reducing": [
            1122085995
        ],
        "\ndifferences": [
            1122085995
        ],
        "containers\nunderstanding": [
            1122085995
        ],
        "\nused": [
            1122085995
        ],
        "kubernetes\nmaking": [
            1122085995
        ],
        "developers’": [
            1122085995
        ],
        "sysadmins’": [
            1122085995
        ],
        "easier": [
            1122085995
        ],
        "\nwith": [
            1122085995
        ],
        "\n\n2chapter": [
            1122085995
        ],
        "1introducing": [
            1122085995
        ],
        "kubernetes\nmicroservices": [
            1122085995
        ],
        "decoupled": [
            1122085995
        ],
        "deployed": [
            1122085995
        ],
        "updated\nand": [
            1122085995
        ],
        "scaled": [
            1122085995
        ],
        "individually": [
            1122085995
        ],
        "enables": [
            1122085995
        ],
        "change": [
            1122085995
        ],
        "often": [
            1122085995
        ],
        "as\nnecessary": [
            1122085995
        ],
        "keep": [
            1122085995
        ],
        "rapidly": [
            1122085995
        ],
        "changing": [
            1122085995
        ],
        "requirements": [
            1122085995
        ],
        "bigger": [
            1122085995
        ],
        "numbers": [
            1122085995
        ],
        "increasingly": [
            1122085995
        ],
        "larger": [
            1122085995
        ],
        "data-\ncenters": [
            1122085995
        ],
        "becomes": [
            1122085995
        ],
        "configure": [
            1122085995
        ],
        "whole\nsystem": [
            1122085995
        ],
        "smoothly": [
            1122085995
        ],
        "harder": [
            1122085995
        ],
        "those\ncomponents": [
            1122085995
        ],
        "achieve": [
            1122085995
        ],
        "thereby": [
            1122085995
        ],
        "costs\ndown": [
            1122085995
        ],
        "automation": [
            1122085995
        ],
        "includes\nautomatic": [
            1122085995
        ],
        "configuration\nsupervision,": [
            1122085995
        ],
        "failure-handling": [
            1122085995
        ],
        "comes": [
            1122085995
        ],
        "in\n": [
            1122085995
        ],
        "as\noften": [
            1122085995
        ],
        "requiring": [
            1122085995
        ],
        "assistance": [
            1122085995
        ],
        "operations": [
            1122085995
        ],
        "(ops)": [
            1122085995
        ],
        "team\nbut": [
            1122085995
        ],
        "benefit": [
            1122085995
        ],
        "automat-\nically": [
            1122085995
        ],
        "rescheduling": [
            1122085995
        ],
        "event": [
            1122085995
        ],
        "failure": [
            1122085995
        ],
        "the\nfocus": [
            1122085995
        ],
        "administrators": [
            1122085995
        ],
        "(sysadmins)": [
            1122085995
        ],
        "shifts": [
            1122085995
        ],
        "supervising": [
            1122085995
        ],
        "to\nmostly": [
            1122085995
        ],
        "infrastructure": [
            1122085995
        ],
        "while\nkubernetes": [
            1122085995
        ],
        "takes": [
            1122085995
        ],
        "care": [
            1122085995
        ],
        "\nnotekubernetes": [
            1122085995
        ],
        "greek": [
            1122085995
        ],
        "pilot": [
            1122085995
        ],
        "helmsman": [
            1122085995
        ],
        "(the": [
            1122085995
        ],
        "holding": [
            1122085995
        ],
        "the\nship’s": [
            1122085995
        ],
        "steering": [
            1122085995
        ],
        "wheel)": [
            1122085995
        ],
        "pronounce": [
            1122085995
        ],
        "ways\nmany": [
            1122085995
        ],
        "koo-ber-nay-tace": [
            1122085995
        ],
        "like\nkoo-ber-netties": [
            1122085995
        ],
        "what\nyou": [
            1122085995
        ],
        "mean\nkubernetes": [
            1122085995
        ],
        "abstracts": [
            1122085995
        ],
        "exposes": [
            1122085995
        ],
        "data-\ncenter": [
            1122085995
        ],
        "enormous": [
            1122085995
        ],
        "allows": [
            1122085995
        ],
        "run\nyour": [
            1122085995
        ],
        "under-\nneath": [
            1122085995
        ],
        "multi-component": [
            1122085995
        ],
        "selects\na": [
            1122085995
        ],
        "component": [
            1122085995
        ],
        "communi-\ncate": [
            1122085995
        ],
        "makes": [
            1122085995
        ],
        "on-premises": [
            1122085995
        ],
        "starts\nto": [
            1122085995
        ],
        "shine": [
            1122085995
        ],
        "largest": [
            1122085995
        ],
        "oper-\nated": [
            1122085995
        ],
        "providers": [
            1122085995
        ],
        "platform\nfor": [
            1122085995
        ],
        "pro-\nvider’s": [
            1122085995
        ],
        "sysadmins": [
            1122085995
        ],
        "anything": [
            1122085995
        ],
        "tens": [
            1122085995
        ],
        "thousands": [
            1122085995
        ],
        "running\non": [
            1122085995
        ],
        "companies": [
            1122085995
        ],
        "accepting": [
            1122085995
        ],
        "model": [
            1122085995
        ],
        "best\nway": [
            1122085995
        ],
        "becoming": [
            1122085995
        ],
        "standard": [
            1122085995
        ],
        "\n11understanding": [
            1122085995
        ],
        "\nbefore": [
            1122085995
        ],
        "start": [
            1122085995
        ],
        "let’s": [
            1122085995
        ],
        "quick": [
            1122085995
        ],
        "look": [
            1122085995
        ],
        "how\nthe": [
            1122085995
        ],
        "this\nchange": [
            1122085995
        ],
        "consequence": [
            1122085995
        ],
        "splitting": [
            1122085995
        ],
        "microservices\n": [
            1122085995
        ],
        "\n\n3understanding": [
            1122085995
        ],
        "kubernetes\nand": [
            1122085995
        ],
        "changes": [
            1122085995
        ],
        "runs": [
            1122085995
        ],
        "these\nchanges": [
            1122085995
        ],
        "better": [
            1122085995
        ],
        "tech-\nnologies": [
            1122085995
        ],
        "docker\n1.1.1moving": [
            1122085995
        ],
        "microservices\nmonolithic": [
            1122085995
        ],
        "consist": [
            1122085995
        ],
        "tightly": [
            1122085995
        ],
        "coupled": [
            1122085995
        ],
        "and\nhave": [
            1122085995
        ],
        "entity": [
            1122085995
        ],
        "sin-\ngle": [
            1122085995
        ],
        "os": [
            1122085995
        ],
        "redeployment": [
            1122085995
        ],
        "the\nwhole": [
            1122085995
        ],
        "lack": [
            1122085995
        ],
        "boundaries": [
            1122085995
        ],
        "results\nin": [
            1122085995
        ],
        "increase": [
            1122085995
        ],
        "complexity": [
            1122085995
        ],
        "consequential": [
            1122085995
        ],
        "deterioration": [
            1122085995
        ],
        "unconstrained": [
            1122085995
        ],
        "growth": [
            1122085995
        ],
        "inter-dependencies": [
            1122085995
        ],
        "usually": [
            1122085995
        ],
        "requires": [
            1122085995
        ],
        "powerful\nservers": [
            1122085995
        ],
        "with\nincreasing": [
            1122085995
        ],
        "loads": [
            1122085995
        ],
        "vertically": [
            1122085995
        ],
        "(also\nknown": [
            1122085995
        ],
        "up)": [
            1122085995
        ],
        "adding": [
            1122085995
        ],
        "cpus": [
            1122085995
        ],
        "components\nor": [
            1122085995
        ],
        "horizontally": [
            1122085995
        ],
        "setting": [
            1122085995
        ],
        "running\nmultiple": [
            1122085995
        ],
        "copies": [
            1122085995
        ],
        "replicas)": [
            1122085995
        ],
        "(scaling": [
            1122085995
        ],
        "out)": [
            1122085995
        ],
        "usually\ndoesn’t": [
            1122085995
        ],
        "expensive": [
            1122085995
        ],
        "prac-\ntice": [
            1122085995
        ],
        "upper": [
            1122085995
        ],
        "cheap": [
            1122085995
        ],
        "hard-\nware-wise": [
            1122085995
        ],
        "isn’t": [
            1122085995
        ],
        "always\npossible—certain": [
            1122085995
        ],
        "extremely": [
            1122085995
        ],
        "impossible": [
            1122085995
        ],
        "to\nscale": [
            1122085995
        ],
        "(relational": [
            1122085995
        ],
        "databases": [
            1122085995
        ],
        "example)": [
            1122085995
        ],
        "monolithic\napplication": [
            1122085995
        ],
        "scalable": [
            1122085995
        ],
        "unscalable": [
            1122085995
        ],
        "unless": [
            1122085995
        ],
        "can\nsplit": [
            1122085995
        ],
        "monolith": [
            1122085995
        ],
        "somehow\nsplitting": [
            1122085995
        ],
        "microservices\nthese": [
            1122085995
        ],
        "forced": [
            1122085995
        ],
        "complex": [
            1122085995
        ],
        "each\nmicroservice": [
            1122085995
        ],
        "independent": [
            1122085995
        ],
        "(see": [
            1122085995
        ],
        "11)": [
            1122085995
        ],
        "communicates": [
            1122085995
        ],
        "with\nother": [
            1122085995
        ],
        "well-defined": [
            1122085995
        ],
        "interfaces": [
            1122085995
        ],
        "(apis)\nserver": [
            1122085995
        ],
        "1\nmonolithic": [
            1122085995
        ],
        "application\nsingle": [
            1122085995
        ],
        "process\nserver": [
            1122085995
        ],
        "1\nprocess": [
            1122085995
        ],
        "11\nprocess": [
            1122085995
        ],
        "12\nmicroservices-based": [
            1122085995
        ],
        "application\nserver": [
            1122085995
        ],
        "2\nprocess": [
            1122085995
        ],
        "21\nprocess": [
            1122085995
        ],
        "22\nfigure": [
            1122085995
        ],
        "vs": [
            1122085995
        ],
        "standalone": [
            1122085995
        ],
        "\n\n4chapter": [
            1122085995
        ],
        "synchronous": [
            1122085995
        ],
        "which\nthey": [
            1122085995
        ],
        "restful": [
            1122085995
        ],
        "(representational": [
            1122085995
        ],
        "transfer)": [
            1122085995
        ],
        "apis": [
            1122085995
        ],
        "asyn-\nchronous": [
            1122085995
        ],
        "amqp": [
            1122085995
        ],
        "(advanced": [
            1122085995
        ],
        "message": [
            1122085995
        ],
        "queueing": [
            1122085995
        ],
        "protocol)": [
            1122085995
        ],
        "these\nprotocols": [
            1122085995
        ],
        "understood": [
            1122085995
        ],
        "tied": [
            1122085995
        ],
        "specific\nprogramming": [
            1122085995
        ],
        "language": [
            1122085995
        ],
        "microservice": [
            1122085995
        ],
        "that’s": [
            1122085995
        ],
        "most\nappropriate": [
            1122085995
        ],
        "implementing": [
            1122085995
        ],
        "microservice\n": [
            1122085995
        ],
        "static": [
            1122085995
        ],
        "external\napi": [
            1122085995
        ],
        "possible": [
            1122085995
        ],
        "separately": [
            1122085995,
            1043891123
        ],
        "one\nof": [
            1122085995
        ],
        "that\nthe": [
            1122085995
        ],
        "backward-compatible": [
            1122085995
        ],
        "\nscaling": [
            1122085995
        ],
        "microservices\nscaling": [
            1122085995
        ],
        "unlike": [
            1122085995
        ],
        "as\na": [
            1122085995
        ],
        "per-service": [
            1122085995
        ],
        "basis": [
            1122085995
        ],
        "option": [
            1122085995
        ],
        "only\nthose": [
            1122085995
        ],
        "leaving": [
            1122085995
        ],
        "original": [
            1122085995
        ],
        "scale\nfigure": [
            1122085995
        ],
        "multiple\nprocesses": [
            1122085995
        ],
        "process\nwhen": [
            1122085995
        ],
        "unscal-\nable": [
            1122085995
        ],
        "that\nallow": [
            1122085995
        ],
        "horizontally\nserver": [
            1122085995
        ],
        "12\nprocess": [
            1122085995
        ],
        "13\nserver": [
            1122085995
        ],
        "22\nserver": [
            1122085995
        ],
        "3\nprocess": [
            1122085995
        ],
        "31\nprocess": [
            1122085995
        ],
        "32\nprocess": [
            1122085995
        ],
        "33\nserver": [
            1122085995
        ],
        "4\nprocess": [
            1122085995
        ],
        "41\nprocess": [
            1122085995
        ],
        "42\nprocess": [
            1122085995
        ],
        "23\nsingle": [
            1122085995
        ],
        "instance\n(possibly": [
            1122085995
        ],
        "scalable)\nthree": [
            1122085995
        ],
        "component\nfigure": [
            1122085995
        ],
        "individually\n": [
            1122085995
        ],
        "\n\n5understanding": [
            1122085995
        ],
        "kubernetes\ndeploying": [
            1122085995
        ],
        "microservices\nas": [
            1122085995
        ],
        "drawbacks": [
            1122085995
        ],
        "a\nsmall": [
            1122085995
        ],
        "it’s\ntrivial": [
            1122085995
        ],
        "decide": [
            1122085995
        ],
        "many\nchoices": [
            1122085995
        ],
        "increases": [
            1122085995
        ],
        "deployment-related": [
            1122085995
        ],
        "deci-\nsions": [
            1122085995
        ],
        "deployment\ncombinations": [
            1122085995
        ],
        "factor": [
            1122085995
        ],
        "talk\nto": [
            1122085995
        ],
        "of\nthem": [
            1122085995
        ],
        "enable": [
            1122085995
        ],
        "increasing\nnumbers": [
            1122085995
        ],
        "tedious": [
            1122085995
        ],
        "error-prone": [
            1122085995
        ],
        "especially": [
            1122085995
        ],
        "you\nconsider": [
            1122085995
        ],
        "ops/sysadmin": [
            1122085995
        ],
        "teams": [
            1122085995
        ],
        "fails": [
            1122085995
        ],
        "debug": [
            1122085995
        ],
        "trace\nexecution": [
            1122085995
        ],
        "calls": [
            1122085995
        ],
        "span": [
            1122085995
        ],
        "machines": [
            1122085995
        ],
        "luckily": [
            1122085995
        ],
        "these\nproblems": [
            1122085995
        ],
        "addressed": [
            1122085995
        ],
        "tracing": [
            1122085995
        ],
        "zipkin": [
            1122085995
        ],
        "divergence": [
            1122085995
        ],
        "requirements\nas": [
            1122085995
        ],
        "already": [
            1122085995
        ],
        "mentioned": [
            1122085995
        ],
        "only\ndeployed": [
            1122085995
        ],
        "indepen-\ndence": [
            1122085995
        ],
        "fact": [
            1122085995
        ],
        "compo-\nnent": [
            1122085995
        ],
        "impedes": [
            1122085995
        ],
        "libraries": [
            1122085995
        ],
        "them\nwhenever": [
            1122085995
        ],
        "arises": [
            1122085995
        ],
        "dependencies": [
            1122085995
        ],
        "com-\nponents": [
            1122085995
        ],
        "ver-\nsions": [
            1122085995
        ],
        "inevitable\nserver": [
            1122085995
        ],
        "app\nmonolithic": [
            1122085995
        ],
        "app\nlibrary": [
            1122085995
        ],
        "b\nv24\nlibrary": [
            1122085995
        ],
        "c\nv11\nlibrary": [
            1122085995
        ],
        "a\nv10\nlibrary": [
            1122085995
        ],
        "y\nv32\nlibrary": [
            1122085995
        ],
        "x\nv14\nserver": [
            1122085995
        ],
        "apps\nlibrary": [
            1122085995
        ],
        "c\nv20\nlibrary": [
            1122085995
        ],
        "a\nv22\nlibrary": [
            1122085995
        ],
        "y\nv40\nlibrary": [
            1122085995
        ],
        "x\nv23\nlibrary": [
            1122085995
        ],
        "x\nv14\napp": [
            1122085995
        ],
        "1app": [
            1122085995
        ],
        "2app": [
            1122085995
        ],
        "3app": [
            1122085995
        ],
        "4\nrequires": [
            1122085995
        ],
        "libraries\nrequires": [
            1122085995
        ],
        "libraries\nfigure": [
            1122085995
        ],
        "conflicting": [
            1122085995
        ],
        "dependencies\n": [
            1122085995
        ],
        "\n\n6chapter": [
            1122085995
        ],
        "dynamically": [
            1122085995
        ],
        "linked": [
            1122085995
        ],
        "shared\nlibraries": [
            1122085995
        ],
        "specifics": [
            1122085995
        ],
        "night-\nmare": [
            1122085995
        ],
        "manages": [
            1122085995
        ],
        "production": [
            1122085995
        ],
        "the\nbigger": [
            1122085995
        ],
        "it\nwill": [
            1122085995
        ],
        "satisfy": [
            1122085995
        ],
        "\n11.2providing": [
            1122085995
        ],
        "consistent": [
            1122085995
        ],
        "applications\nregardless": [
            1122085995
        ],
        "you’re": [
            1122085995
        ],
        "deploying\none": [
            1122085995
        ],
        "biggest": [
            1122085995
        ],
        "deal\nwith": [
            1122085995
        ],
        "differences": [
            1122085995
        ],
        "environments": [
            1122085995
        ],
        "a\nhuge": [
            1122085995
        ],
        "difference": [
            1122085995
        ],
        "differences\neven": [
            1122085995
        ],
        "exist": [
            1122085995
        ],
        "another": [
            1122085995
        ],
        "unavoidable": [
            1122085995
        ],
        "machine": [
            1122085995
        ],
        "operating": [
            1122085995
        ],
        "libraries\nthat": [
            1122085995
        ],
        "the\noperations": [
            1122085995
        ],
        "laptops": [
            1122085995
        ],
        "on\ntheir": [
            1122085995
        ],
        "sys-\ntem": [
            1122085995
        ],
        "administration": [
            1122085995
        ],
        "understandably": [
            1122085995
        ],
        "leads": [
            1122085995
        ],
        "differences\nbetween": [
            1122085995
        ],
        "mention": [
            1122085995
        ],
        "give": [
            1122085995
        ],
        "more\nemphasis": [
            1122085995
        ],
        "latest": [
            1122085995
        ],
        "patches": [
            1122085995
        ],
        "lot\nof": [
            1122085995
        ],
        "devel-\nopment": [
            1122085995
        ],
        "necessarily": [
            1122085995
        ],
        "true": [
            1122085995
        ],
        "computers": [
            1122085995
        ],
        "production\nsystem": [
            1122085995
        ],
        "hosts": [
            1122085995
        ],
        "though\nthey": [
            1122085995
        ],
        "libraries\n": [
            1122085995
        ],
        "be\nideal": [
            1122085995
        ],
        "exact": [
            1122085995
        ],
        "development\nand": [
            1122085995
        ],
        "con-\nfiguration": [
            1122085995
        ],
        "else": [
            1122085995
        ],
        "this\nenvironment": [
            1122085995
        ],
        "the\nability": [
            1122085995
        ],
        "add": [
            1122085995
        ],
        "affecting": [
            1122085995
        ],
        "existing\napplications": [
            1122085995
        ],
        "\n11.3moving": [
            1122085995
        ],
        "delivery:": [
            1122085995
        ],
        "noops\nin": [
            1122085995
        ],
        "we’ve": [
            1122085995
        ],
        "seen": [
            1122085995
        ],
        "shift": [
            1122085995
        ],
        "pro-\ncess": [
            1122085995
        ],
        "past": [
            1122085995
        ],
        "develop-\nment": [
            1122085995
        ],
        "team’s": [
            1122085995
        ],
        "team\nwho": [
            1122085995
        ],
        "tended": [
            1122085995
        ],
        "organizations": [
            1122085995
        ],
        "are\nrealizing": [
            1122085995
        ],
        "develops": [
            1122085995
        ],
        "part\nin": [
            1122085995
        ],
        "taking": [
            1122085995
        ],
        "lifetime": [
            1122085995
        ],
        "developer\nqa,": [
            1122085995
        ],
        "collaborate": [
            1122085995
        ],
        "process\nthis": [
            1122085995
        ],
        "practice": [
            1122085995
        ],
        "devops\n": [
            1122085995
        ],
        "\n\n7introducing": [
            1122085995
        ],
        "technologies\nunderstanding": [
            1122085995
        ],
        "benefits\nhaving": [
            1122085995
        ],
        "leads\nto": [
            1122085995
        ],
        "users’": [
            1122085995
        ],
        "issues": [
            1122085995,
            2119133144
        ],
        "the\nproblems": [
            1122085995
        ],
        "maintaining": [
            1122085995
        ],
        "developers\nare": [
            1122085995
        ],
        "inclined": [
            1122085995
        ],
        "earlier": [
            1122085995,
            186247402
        ],
        "feed-\nback": [
            1122085995
        ],
        "steer": [
            1122085995
        ],
        "streamline": [
            1122085995
        ],
        "the\ndeployment": [
            1122085995
        ],
        "ideally": [
            1122085995
        ],
        "them-\nselves": [
            1122085995
        ],
        "wait": [
            1122085995
        ],
        "often\nrequires": [
            1122085995
        ],
        "organization": [
            1122085995
        ],
        "datacenter": [
            1122085995
        ],
        "details": [
            1122085995,
            1118639836,
            410929361
        ],
        "most\nof": [
            1122085995
        ],
        "\nletting": [
            1122085995
        ],
        "best\neven": [
            1122085995
        ],
        "toward": [
            1122085995
        ],
        "achieving": [
            1122085995
        ],
        "the\nsame": [
            1122085995
        ],
        "goal": [
            1122085995
        ],
        "successful": [
            1122085995
        ],
        "customers": [
            1122085995
        ],
        "they\nhave": [
            1122085995
        ],
        "goals": [
            1122085995
        ],
        "motivating": [
            1122085995
        ],
        "factors": [
            1122085995
        ],
        "love": [
            1122085995
        ],
        "normally": [
            1122085995
        ],
        "mak-\ning": [
            1122085995
        ],
        "sure": [
            1122085995
        ],
        "patches\nand": [
            1122085995
        ],
        "prefer": [
            1122085995
        ],
        "leave": [
            1122085995
        ],
        "charge": [
            1122085995
        ],
        "infra-\nstructure": [
            1122085995
        ],
        "aspects\nthat": [
            1122085995
        ],
        "priority": [
            1122085995
        ],
        "the\nimplicit": [
            1122085995
        ],
        "interdependencies": [
            1122085995
        ],
        "think\nabout": [
            1122085995
        ],
        "infrastructure\ncan": [
            1122085995
        ],
        "must\n": [
            1122085995
        ],
        "know-\ning": [
            1122085995
        ],
        "ops\nteam": [
            1122085995
        ],
        "referred": [
            1122085995
        ],
        "obviously": [
            1122085995
        ],
        "of\neach": [
            1122085995
        ],
        "abstracting": [
            1122085995
        ],
        "the\nactual": [
            1122085995
        ],
        "apps\nit": [
            1122085995
        ],
        "from\nthe": [
            1122085995
        ],
        "focus": [
            1122085995
        ],
        "infrastruc-\nture": [
            1122085995
        ],
        "applications\nrunning": [
            1122085995
        ],
        "it\n1.2introducing": [
            1122085995
        ],
        "technologies\nin": [
            1122085995
        ],
        "section": [
            1122085995
        ],
        "presented": [
            1122085995
        ],
        "non-comprehensive": [
            1122085995
        ],
        "list": [
            1122085995
        ],
        "facing": [
            1122085995
        ],
        "will\nfocus": [
            1122085995
        ],
        "solved": [
            1122085995
        ],
        "\n\n8chapter": [
            1122085995
        ],
        "running\napplications": [
            1122085995
        ],
        "dig": [
            1122085995
        ],
        "familiar\nwith": [
            1122085995
        ],
        "basics": [
            1122085995
        ],
        "it\noffloads": [
            1122085995
        ],
        "rkt": [
            1122085995
        ],
        "(pronounced": [
            1122085995
        ],
        "“rock-it”)\n1.2.1understanding": [
            1122085995
        ],
        "are\nin": [
            1122085995
        ],
        "11.1": [
            1122085995
        ],
        "saw": [
            1122085995
        ],
        "same\nmachine": [
            1122085995
        ],
        "possibly": [
            1122085995
        ],
        "dependent": [
            1122085995
        ],
        "or\nhave": [
            1122085995
        ],
        "general": [
            1122085995
        ],
        "composed": [
            1122085995
        ],
        "large": [
            1122085995
        ],
        "components\nit’s": [
            1122085995
        ],
        "acceptable": [
            1122085995
        ],
        "virtual": [
            1122085995
        ],
        "(vm)": [
            1122085995
        ],
        "isolate": [
            1122085995
        ],
        "providing": [
            1122085995
        ],
        "operat-\ning": [
            1122085995
        ],
        "their\nnumbers": [
            1122085995
        ],
        "grow": [
            1122085995
        ],
        "vm": [
            1122085995
        ],
        "to\nwaste": [
            1122085995
        ],
        "costs": [
            1122085995
        ],
        "about\nwasting": [
            1122085995
        ],
        "and\nmanaged": [
            1122085995
        ],
        "rising": [
            1122085995
        ],
        "vms": [
            1122085995
        ],
        "wasting": [
            1122085995
        ],
        "human": [
            1122085995
        ],
        "resources\nbecause": [
            1122085995
        ],
        "administrators’": [
            1122085995
        ],
        "workload": [
            1122085995
        ],
        "considerably\nisolating": [
            1122085995
        ],
        "technologies\ninstead": [
            1122085995
        ],
        "(or\nsoftware": [
            1122085995
        ],
        "general)": [
            1122085995
        ],
        "turning": [
            1122085995
        ],
        "allow": [
            1122085995
        ],
        "only\nexposing": [
            1122085995
        ],
        "isolating": [
            1122085995
        ],
        "each\nother": [
            1122085995
        ],
        "similarly": [
            1122085995
        ],
        "less": [
            1122085995
        ],
        "overhead\n": [
            1122085995
        ],
        "all\nthe": [
            1122085995
        ],
        "(unlike": [
            1122085995
        ],
        "sys-\ntems)": [
            1122085995
        ],
        "isolated": [
            1122085995
        ],
        "the\nprocess": [
            1122085995
        ],
        "looks": [
            1122085995
        ],
        "oper-\nating": [
            1122085995
        ],
        "\ncomparing": [
            1122085995
        ],
        "containers\ncompared": [
            1122085995
        ],
        "lightweight": [
            1122085995
        ],
        "run\nhigher": [
            1122085995
        ],
        "mainly": [
            1122085995
        ],
        "each\nvm": [
            1122085995
        ],
        "compute\nresources": [
            1122085995
        ],
        "addition": [
            1122085995
        ],
        "consumed": [
            1122085995
        ],
        "component’s": [
            1122085995
        ],
        "consumes": [
            1122085995
        ],
        "the\noverhead": [
            1122085995
        ],
        "overhead": [
            1122085995
        ],
        "grouping": [
            1122085995
        ],
        "applications\ninto": [
            1122085995
        ],
        "dedicate": [
            1122085995
        ],
        "to\neach": [
            1122085995
        ],
        "should)": [
            1122085995
        ],
        "each\n": [
            1122085995
        ],
        "\n\n9introducing": [
            1122085995
        ],
        "technologies\napplication": [
            1122085995
        ],
        "14.": [
            1122085995
        ],
        "end-result": [
            1122085995
        ],
        "bare-metal": [
            1122085995
        ],
        "machine\nwhen": [
            1122085995
        ],
        "sys-\ntems": [
            1122085995
        ],
        "underneath": [
            1122085995
        ],
        "vms\nis": [
            1122085995
        ],
        "hypervisor": [
            1122085995
        ],
        "divides": [
            1122085995
        ],
        "physical": [
            1122085995
        ],
        "into\nsmaller": [
            1122085995
        ],
        "sets": [
            1122085995
        ],
        "guest": [
            1122085995
        ],
        "os’": [
            1122085995
        ],
        "ker-\nnel": [
            1122085995
        ],
        "performs": [
            1122085995
        ],
        "x86": [
            1122085995
        ],
        "instructions": [
            1122085995
        ],
        "physical\ncpu": [
            1122085995
        ],
        "\nnotetwo": [
            1122085995
        ],
        "hypervisors": [
            1122085995
        ],
        "os\nwhile": [
            1122085995
        ],
        "do\ncontainers": [
            1122085995
        ],
        "performing": [
            1122085995
        ],
        "on\nthe": [
            1122085995
        ],
        "kind": [
            1122085995
        ],
        "virtualization": [
            1122085995
        ],
        "does\nwith": [
            1122085995
        ],
        "15).\n": [
            1122085995
        ],
        "main": [
            1122085995
        ],
        "because\neach": [
            1122085995
        ],
        "call": [
            1122085995
        ],
        "kernel\nwhich": [
            1122085995
        ],
        "pose": [
            1122085995
        ],
        "risk": [
            1122085995
        ],
        "hardware\nresources": [
            1122085995
        ],
        "that\napps": [
            1122085995
        ],
        "vms\n(on": [
            1122085995
        ],
        "machine)\nbare-metal": [
            1122085995
        ],
        "machine\nvm": [
            1122085995
        ],
        "1vm": [
            1122085995
        ],
        "2vm": [
            1122085995
        ],
        "3\napp": [
            1122085995
        ],
        "a\napp": [
            1122085995
        ],
        "b\napp": [
            1122085995
        ],
        "c\napp": [
            1122085995
        ],
        "d\napp": [
            1122085995
        ],
        "e\napp": [
            1122085995
        ],
        "f\nguest": [
            1122085995
        ],
        "osguest": [
            1122085995
        ],
        "os\nbare-metal": [
            1122085995
        ],
        "machine\nhost": [
            1122085995
        ],
        "os\nhypervisor\napps": [
            1122085995
        ],
        "in\nisolated": [
            1122085995
        ],
        "containers\ncontainer": [
            1122085995
        ],
        "1container": [
            1122085995
        ],
        "2container": [
            1122085995
        ],
        "aapp": [
            1122085995
        ],
        "bapp": [
            1122085995
        ],
        "c\ncontainer": [
            1122085995
        ],
        "4container": [
            1122085995
        ],
        "5container": [
            1122085995
        ],
        "6\napp": [
            1122085995
        ],
        "dapp": [
            1122085995
        ],
        "eapp": [
            1122085995
        ],
        "f\ncontainer": [
            1122085995
        ],
        "7container": [
            1122085995
        ],
        "8container": [
            1122085995
        ],
        "9\napp": [
            1122085995
        ],
        "..app": [
            1122085995
        ],
        "..\nhost": [
            1122085995
        ],
        "os\nfigure": [
            1122085995
        ],
        "containers\n": [
            1122085995
        ],
        "\n\n10chapter": [
            1122085995
        ],
        "kubernetes\nyou": [
            1122085995
        ],
        "choice": [
            1122085995
        ],
        "remem-\nber": [
            1122085995
        ],
        "they\nall": [
            1122085995
        ],
        "be\nbooted": [
            1122085995
        ],
        "starts": [
            1122085995
        ],
        "immediately\napps": [
            1122085995
        ],
        "vms\nvm": [
            1122085995
        ],
        "1\napp\na\napp\nb\nkernel\nvirtual": [
            1122085995
        ],
        "cpu\nhypervisor\nphysical": [
            1122085995
        ],
        "cpu\nkernel\nphysical": [
            1122085995
        ],
        "cpu\nvm": [
            1122085995
        ],
        "2\napp\nd\nkernel\nvirtual": [
            1122085995
        ],
        "cpu\napp\nc\napp\ne\nvm": [
            1122085995
        ],
        "3\napp\nf\nkernel\nvirtual": [
            1122085995
        ],
        "cpu\napps": [
            1122085995
        ],
        "containers\ncontainer\na\ncontainer\nb\ncontainer\nc\ncontainer\nd\ncontainer\ne\ncontainer\nf\napp\na\napp\nb\napp\nd\napp\ne\napp\nf\napp\nc\nfigure": [
            1122085995
        ],
        "\nthey": [
            1122085995
        ],
        "\n\n11introducing": [
            1122085995
        ],
        "technologies\nintroducing": [
            1122085995
        ],
        "mechanisms": [
            1122085995
        ],
        "probably": [
            1122085995
        ],
        "wondering": [
            1122085995
        ],
        "processes\nif": [
            1122085995
        ],
        "possible\nthe": [
            1122085995
        ],
        "sees": [
            1122085995
        ],
        "personal": [
            1122085995
        ],
        "view": [
            1122085995
        ],
        "(files": [
            1122085995
        ],
        "hostname": [
            1122085995
        ],
        "on)": [
            1122085995
        ],
        "second\none": [
            1122085995
        ],
        "(cgroups)": [
            1122085995
        ],
        "process\ncan": [
            1122085995
        ],
        "(cpu": [
            1122085995
        ],
        "bandwidth": [
            1122085995
        ],
        "on)\nisolating": [
            1122085995
        ],
        "namespaces\nby": [
            1122085995
        ],
        "resources\nsuch": [
            1122085995
        ],
        "filesystems": [
            1122085995
        ],
        "ids": [
            1122085995
        ],
        "belong": [
            1122085995
        ],
        "the\nsingle": [
            1122085995
        ],
        "resources\nacross": [
            1122085995
        ],
        "multiple\nkinds": [
            1122085995
        ],
        "one\nnamespace": [
            1122085995
        ],
        "kinds": [
            1122085995
        ],
        "exist:\nmount": [
            1122085995
        ],
        "(mnt)\nprocess": [
            1122085995
        ],
        "id": [
            1122085995
        ],
        "(pid)\nnetwork": [
            1122085995
        ],
        "(net)\ninter-process": [
            1122085995
        ],
        "communication": [
            1122085995
        ],
        "(ipc)\nuts\nuser": [
            1122085995
        ],
        "(user)\neach": [
            1122085995
        ],
        "the\nuts": [
            1122085995
        ],
        "determines": [
            1122085995
        ],
        "domain": [
            1122085995
        ],
        "running\ninside": [
            1122085995
        ],
        "assigning": [
            1122085995
        ],
        "uts": [
            1122085995
        ],
        "pair": [
            1122085995
        ],
        "of\nprocesses": [
            1122085995
        ],
        "hostnames": [
            1122085995
        ],
        "the\ntwo": [
            1122085995
        ],
        "(at\nleast": [
            1122085995
        ],
        "far": [
            1122085995
        ],
        "concerned)": [
            1122085995
        ],
        "likewise": [
            1122085995
        ],
        "belongs": [
            1122085995
        ],
        "net-\nwork": [
            1122085995
        ],
        "inter-\nface": [
            1122085995
        ],
        "to\nanother": [
            1122085995
        ],
        "therefore": [
            1122085995
        ],
        "interfaces\n": [
            1122085995
        ],
        "idea": [
            1122085995
        ],
        "applica-\ntions": [
            1122085995
        ],
        "\nlimiting": [
            1122085995
        ],
        "process\nthe": [
            1122085995
        ],
        "deals": [
            1122085995
        ],
        "limiting": [
            1122085995
        ],
        "system\nresources": [
            1122085995
        ],
        "achieved": [
            1122085995
        ],
        "cgroups": [
            1122085995
        ],
        "fea-\nture": [
            1122085995
        ],
        "processes)": [
            1122085995
        ],
        "process\ncan’t": [
            1122085995
        ],
        "bandwidth\n": [
            1122085995
        ],
        "\n\n12chapter": [
            1122085995
        ],
        "cannot": [
            1122085995,
            2119133144
        ],
        "hog": [
            1122085995
        ],
        "reserved": [
            1122085995
        ],
        "processes\nwhich": [
            1122085995
        ],
        "similar": [
            1122085995
        ],
        "machine\n1.2.2introducing": [
            1122085995
        ],
        "platform\nwhile": [
            1122085995
        ],
        "around": [
            1122085995
        ],
        "they’ve": [
            1122085995
        ],
        "become\nmore": [
            1122085995
        ],
        "widely": [
            1122085995
        ],
        "known": [
            1122085995
        ],
        "rise": [
            1122085995
        ],
        "the\nfirst": [
            1122085995
        ],
        "portable": [
            1122085995
        ],
        "machines\nit": [
            1122085995
        ],
        "simplified": [
            1122085995
        ],
        "packaging": [
            1122085995
        ],
        "its\nlibraries": [
            1122085995
        ],
        "file": [
            1122085995
        ],
        "por-\ntable": [
            1122085995
        ],
        "package": [
            1122085995
        ],
        "provision": [
            1122085995
        ],
        "machine\nrunning": [
            1122085995
        ],
        "packaged": [
            1122085995
        ],
        "filesystem\ncontents": [
            1122085995
        ],
        "bundled": [
            1122085995
        ],
        "on\nyour": [
            1122085995
        ],
        "server\nis": [
            1122085995
        ],
        "won’t": [
            1122085995
        ],
        "different\nset": [
            1122085995
        ],
        "installed": [
            1122085995
        ],
        "compared": [
            1122085995
        ],
        "whole\nred": [
            1122085995
        ],
        "enterprise": [
            1122085995
        ],
        "(rhel)": [
            1122085995
        ],
        "believe": [
            1122085995
        ],
        "it’s\nrunning": [
            1122085995
        ],
        "rhel": [
            1122085995
        ],
        "runs\nfedora": [
            1122085995
        ],
        "debian": [
            1122085995
        ],
        "distribu-\ntion": [
            1122085995
        ],
        "different\n": [
            1122085995
        ],
        "installing": [
            1122085995
        ],
        "vm\ninstalling": [
            1122085995
        ],
        "distributing": [
            1122085995
        ],
        "and\nrunning": [
            1122085995
        ],
        "achieves": [
            1122085995
        ],
        "effect": [
            1122085995
        ],
        "app\nisolation": [
            1122085995
        ],
        "to\nprovide": [
            1122085995
        ],
        "level": [
            1122085995
        ],
        "mono-\nlithic": [
            1122085995
        ],
        "smaller\n": [
            1122085995
        ],
        "docker-based": [
            1122085995
        ],
        "that\ncontainer": [
            1122085995
        ],
        "layers": [
            1122085995
        ],
        "shared": [
            1122085995
        ],
        "reused": [
            1122085995
        ],
        "mul-\ntiple": [
            1122085995
        ],
        "the\nother": [
            1122085995
        ],
        "previously": [
            1122085995
        ],
        "container\nimage": [
            1122085995
        ],
        "layers\nunderstanding": [
            1122085995
        ],
        "concepts\ndocker": [
            1122085995
        ],
        "we’ve\nalready": [
            1122085995
        ],
        "stated": [
            1122085995
        ],
        "envi-\nronment": [
            1122085995
        ],
        "files\nthat": [
            1122085995
        ],
        "docker\nmakes": [
            1122085995
        ],
        "transfer": [
            1122085995
        ],
        "central": [
            1122085995
        ],
        "can\nthen": [
            1122085995
        ],
        "executed": [
            1122085995
        ],
        "the\nmost": [
            1122085995
        ],
        "we’ll": [
            1122085995
        ],
        "soon": [
            1122085995,
            2119133144
        ],
        "explain)\n": [
            1122085995
        ],
        "\n\n13introducing": [
            1122085995
        ],
        "technologies\n": [
            1122085995
        ],
        "comprise": [
            1122085995
        ],
        "scenario:\nimages—a": [
            1122085995
        ],
        "appli-\ncation": [
            1122085995
        ],
        "available\nto": [
            1122085995
        ],
        "path": [
            1122085995
        ],
        "executable": [
            1122085995
        ],
        "that\nshould": [
            1122085995
        ],
        "\nregistries—a": [
            1122085995
        ],
        "stores": [
            1122085995
        ],
        "and\nfacilitates": [
            1122085995
        ],
        "comput-\ners": [
            1122085995
        ],
        "you’ve\nbuilt": [
            1122085995
        ],
        "push": [
            1122085995
        ],
        "(upload)": [
            1122085995
        ],
        "pull\n(download)": [
            1122085995
        ],
        "registries": [
            1122085995
        ],
        "pub-\nlic": [
            1122085995
        ],
        "allowing": [
            1122085995
        ],
        "anyone": [
            1122085995
        ],
        "accessi-\nble": [
            1122085995
        ],
        "machines\ncontainers—a": [
            1122085995
        ],
        "from\na": [
            1122085995
        ],
        "all\nother": [
            1122085995
        ],
        "resource-constrained": [
            1122085995
        ],
        "mean-\ning": [
            1122085995
        ],
        "ram": [
            1122085995
        ],
        "on)\nthat": [
            1122085995
        ],
        "allocated": [
            1122085995
        ],
        "\nbuilding": [
            1122085995
        ],
        "image\nfigure": [
            1122085995
        ],
        "relate": [
            1122085995
        ],
        "developer\nfirst": [
            1122085995
        ],
        "builds": [
            1122085995
        ],
        "pushes": [
            1122085995
        ],
        "thus": [
            1122085995
        ],
        "to\nanyone": [
            1122085995
        ],
        "other\nmachine": [
            1122085995
        ],
        "creates": [
            1122085995
        ],
        "container\nbased": [
            1122085995
        ],
        "binary": [
            1122085995
        ],
        "specified": [
            1122085995
        ],
        "image\ndocker\nimage\ncontainer\nimage": [
            1122085995
        ],
        "registry\nimage\ndocker\nimage\ndevelopment": [
            1122085995
        ],
        "machine\nproduction": [
            1122085995
        ],
        "machine\n1": [
            1122085995
        ],
        "developer": [
            1122085995
        ],
        "tells\ndocker": [
            1122085995
        ],
        "build\nand": [
            1122085995
        ],
        "image\n2": [
            1122085995
        ],
        "docker\nbuilds": [
            1122085995
        ],
        "image\n4": [
            1122085995
        ],
        "production\nmachine": [
            1122085995
        ],
        "image\n3": [
            1122085995
        ],
        "docker\npushes": [
            1122085995
        ],
        "image\nto": [
            1122085995
        ],
        "registry\n5": [
            1122085995
        ],
        "pulls\nimage": [
            1122085995
        ],
        "from\nregistry\n6": [
            1122085995
        ],
        "runs\ncontainer": [
            1122085995
        ],
        "from\nimage\ndeveloper\nfigure": [
            1122085995
        ],
        "\n\n14chapter": [
            1122085995
        ],
        "kubernetes\ncomparing": [
            1122085995
        ],
        "containers\ni’ve": [
            1122085995
        ],
        "explained": [
            1122085995
        ],
        "generally": [
            1122085995
        ],
        "much\nmore": [
            1122085995
        ],
        "specifically": [
            1122085995
        ],
        "compare": [
            1122085995
        ],
        "vir-\ntual": [
            1122085995
        ],
        "images)": [
            1122085995
        ],
        "shows\nthe": [
            1122085995
        ],
        "containers\nyou’ll": [
            1122085995
        ],
        "notice": [
            1122085995
        ],
        "b": [
            1122085995
        ],
        "binaries": [
            1122085995
        ],
        "both\nwhen": [
            1122085995
        ],
        "this\nis": [
            1122085995
        ],
        "obvious": [
            1122085995
        ],
        "(that": [
            1122085995
        ],
        "vm)": [
            1122085995
        ],
        "said\nhost": [
            1122085995
        ],
        "1\napp\na\napp\nb\nbinaries": [
            1122085995
        ],
        "and\nlibraries\n(filesystem)\nguest": [
            1122085995
        ],
        "kernel\nhypervisor\nhost": [
            1122085995
        ],
        "os\nhost": [
            1122085995
        ],
        "os\nvm": [
            1122085995
        ],
        "2\napp\nd\nguest": [
            1122085995
        ],
        "kernel\napp\nc\napp\ne\nvm": [
            1122085995
        ],
        "3\napp\nf\nguest": [
            1122085995
        ],
        "kernel\nhost": [
            1122085995
        ],
        "2\ncontainer": [
            1122085995
        ],
        "3\ncontainer": [
            1122085995
        ],
        "4\ncontainer": [
            1122085995
        ],
        "5\ncontainer": [
            1122085995
        ],
        "6\napp\nd\napp\ne\napp\nf\napp\nc\napp\na\napp\nb\nbinaries": [
            1122085995
        ],
        "and\nlibraries\n(filesystem)\nbinaries": [
            1122085995
        ],
        "and\nlibraries\n(filesystem)\ndocker\nfigure": [
            1122085995
        ],
        "\nthree": [
            1122085995
        ],
        "\ndocker": [
            1122085995
        ],
        "\n\n15introducing": [
            1122085995
        ],
        "technologies\nthat": [
            1122085995
        ],
        "b\nshare": [
            1122085995
        ],
        "files?\nunderstanding": [
            1122085995
        ],
        "layers\ni’ve": [
            1122085995
        ],
        "said": [
            1122085995,
            2119133144,
            1118639836
        ],
        "con-\ntain": [
            1122085995
        ],
        "another\nimage": [
            1122085995
        ],
        "parent": [
            1122085995
        ],
        "base\nthis": [
            1122085995
        ],
        "speeds": [
            1122085995
        ],
        "distribution": [
            1122085995
        ],
        "have\nalready": [
            1122085995
        ],
        "again\nwhen": [
            1122085995
        ],
        "transferring": [
            1122085995
        ],
        "efficient": [
            1122085995
        ],
        "the\nstorage": [
            1122085995
        ],
        "footprint": [
            1122085995
        ],
        "layer": [
            1122085995
        ],
        "created\nfrom": [
            1122085995
        ],
        "base": [
            1122085995
        ],
        "but\nif": [
            1122085995
        ],
        "writes": [
            1122085995
        ],
        "there-\nfore": [
            1122085995
        ],
        "because\ncontainer": [
            1122085995
        ],
        "read-only": [
            1122085995
        ],
        "writable": [
            1122085995
        ],
        "is\ncreated": [
            1122085995
        ],
        "to\na": [
            1122085995
        ],
        "located": [
            1122085995
        ],
        "copy": [
            1122085995
        ],
        "the\ntop-most": [
            1122085995
        ],
        "portability": [
            1122085995
        ],
        "limitations": [
            1122085995
        ],
        "images\nin": [
            1122085995
        ],
        "theory": [
            1122085995
        ],
        "but\none": [
            1122085995
        ],
        "caveat": [
            1122085995
        ],
        "exists—one": [
            1122085995
        ],
        "use\nthe": [
            1122085995
        ],
        "version\nit": [
            1122085995
        ],
        "linux\nkernel": [
            1122085995
        ],
        "modules": [
            1122085995
        ],
        "it\n": [
            1122085995
        ],
        "impose": [
            1122085995
        ],
        "cer-\ntain": [
            1122085995
        ],
        "constraints": [
            1122085995
        ],
        "constraints\nbecause": [
            1122085995
        ],
        "clear": [
            1122085995
        ],
        "app\nbuilt": [
            1122085995
        ],
        "have\nthe": [
            1122085995
        ],
        "architec-\nture": [
            1122085995
        ],
        "arm-based": [
            1122085995
        ],
        "you\nstill": [
            1122085995
        ],
        "that\n1.2.3introducing": [
            1122085995
        ],
        "docker\ndocker": [
            1122085995
        ],
        "mainstream": [
            1122085995
        ],
        "i’ve\nmade": [
            1122085995
        ],
        "isolation\nof": [
            1122085995
        ],
        "linux\nnamespaces": [
            1122085995
        ],
        "features\n": [
            1122085995
        ],
        "success": [
            1122085995
        ],
        "(oci)": [
            1122085995
        ],
        "born": [
            1122085995
        ],
        "cre-\nate": [
            1122085995
        ],
        "industry": [
            1122085995
        ],
        "standards": [
            1122085995
        ],
        "formats": [
            1122085995
        ],
        "runtime": [
            1122085995
        ],
        "part\nof": [
            1122085995
        ],
        "“rock-it”)": [
            1122085995
        ],
        "container\nengine": [
            1122085995
        ],
        "\n\n16chapter": [
            1122085995
        ],
        "puts": [
            1122085995
        ],
        "strong": [
            1122085995
        ],
        "on\nsecurity": [
            1122085995
        ],
        "composability": [
            1122085995
        ],
        "conforming": [
            1122085995
        ],
        "oci": [
            1122085995
        ],
        "kubernetes\nbecause": [
            1122085995
        ],
        "supported": [
            1122085995
        ],
        "recently": [
            1122085995
        ],
        "kubernetes\nhas": [
            1122085995
        ],
        "supporting": [
            1122085995
        ],
        "reason": [
            1122085995
        ],
        "mistake": [
            1122085995
        ],
        "thinking\nkubernetes": [
            1122085995
        ],
        "orchestration": [
            1122085995
        ],
        "docker-based\ncontainers": [
            1122085995
        ],
        "essence": [
            1122085995
        ],
        "of\nkubernetes": [
            1122085995
        ],
        "happen": [
            1122085995
        ],
        "be\nthe": [
            1122085995
        ],
        "mind": [
            1122085995
        ],
        "dive\ninto": [
            1122085995
        ],
        "core": [
            1122085995
        ],
        "about—kubernetes\n1.3introducing": [
            1122085995
        ],
        "kubernetes\nwe’ve": [
            1122085995
        ],
        "in\nyour": [
            1122085995
        ],
        "grows": [
            1122085995
        ],
        "realized": [
            1122085995
        ],
        "needed": [
            1122085995
        ],
        "managing\ntheir": [
            1122085995
        ],
        "globally": [
            1122085995
        ],
        "a\nfew": [
            1122085995
        ],
        "hundreds": [
            1122085995
        ],
        "to\ndeal": [
            1122085995
        ],
        "massive": [
            1122085995
        ],
        "to\ndevelop": [
            1122085995
        ],
        "solutions": [
            1122085995
        ],
        "soft-\nware": [
            1122085995
        ],
        "cost-efficient\n1.3.1understanding": [
            1122085995
        ],
        "origins\nthrough": [
            1122085995
        ],
        "internal": [
            1122085995
        ],
        "borg": [
            1122085995
        ],
        "later": [
            1122085995
        ],
        "new\nsystem": [
            1122085995
        ],
        "omega)": [
            1122085995
        ],
        "administra-\ntors": [
            1122085995
        ],
        "simplifying\nthe": [
            1122085995
        ],
        "management": [
            1122085995
        ],
        "higher": [
            1122085995
        ],
        "utiliza-\ntion": [
            1122085995
        ],
        "large\nwhen": [
            1122085995
        ],
        "tiny": [
            1122085995
        ],
        "improvements": [
            1122085995
        ],
        "mean": [
            1122085995
        ],
        "savings": [
            1122085995
        ],
        "millions": [
            1122085995
        ],
        "dollars": [
            1122085995
        ],
        "incentives": [
            1122085995
        ],
        "a\nsystem": [
            1122085995
        ],
        "clear\n": [
            1122085995
        ],
        "omega": [
            1122085995
        ],
        "decade": [
            1122085995
        ],
        "google\nintroduced": [
            1122085995
        ],
        "open-source": [
            1122085995
        ],
        "gained\nthrough": [
            1122085995
        ],
        "\n13.2looking": [
            1122085995
        ],
        "mountain\nkubernetes": [
            1122085995
        ],
        "container-\nized": [
            1122085995
        ],
        "relies": [
            1122085995
        ],
        "het-\nerogeneous": [
            1122085995
        ],
        "these\napplications": [
            1122085995
        ],
        "host\nbecause": [
            1122085995
        ],
        "the\n": [
            1122085995
        ],
        "\n\n17introducing": [
            1122085995
        ],
        "kubernetes\nsame": [
            1122085995
        ],
        "critical": [
            1122085995
        ],
        "orga-\nnizations": [
            1122085995
        ],
        "paramount": [
            1122085995
        ],
        "provid-\ners": [
            1122085995
        ],
        "strive": [
            1122085995
        ],
        "still\nhaving": [
            1122085995
        ],
        "maintain": [
            1122085995
        ],
        "complete": [
            1122085995
        ],
        "applications\n": [
            1122085995
        ],
        "away\nthe": [
            1122085995
        ],
        "simplifies": [
            1122085995
        ],
        "deployment\nand": [
            1122085995
        ],
        "clus-\nter": [
            1122085995
        ],
        "couple": [
            1122085995
        ],
        "cluster\nmakes": [
            1122085995
        ],
        "simply": [
            1122085995,
            2119133144
        ],
        "represent": [
            1122085995
        ],
        "additional\namount": [
            1122085995
        ],
        "apps\nunderstanding": [
            1122085995
        ],
        "does\nfigure": [
            1122085995
        ],
        "simplest": [
            1122085995
        ],
        "com-\nposed": [
            1122085995
        ],
        "master": [
            1122085995,
            1478412827
        ],
        "sub-\nmits": [
            1122085995
        ],
        "worker\nnodes": [
            1122085995
        ],
        "lands": [
            1122085995
        ],
        "shouldn’t)": [
            1122085995
        ],
        "matter—neither": [
            1122085995
        ],
        "to\nthe": [
            1122085995
        ],
        "administrator\nthe": [
            1122085995
        ],
        "specify": [
            1122085995
        ],
        "will\ndeploy": [
            1122085995
        ],
        "but\nthey": [
            1122085995
        ],
        "deployed\nhelping": [
            1122085995
        ],
        "features\nkubernetes": [
            1122085995
        ],
        "thought": [
            1122085995
        ],
        "relieves": [
            1122085995
        ],
        "implement": [
            1122085995
        ],
        "infrastructure-related": [
            1122085995
        ],
        "services\ninto": [
            1122085995
        ],
        "apps;": [
            1122085995
        ],
        "rely": [
            1122085995
        ],
        "includes\nthings": [
            1122085995
        ],
        "load-balancing": [
            1122085995
        ],
        "self-healing": [
            1122085995
        ],
        "leader\nkubernetes\nmaster\ntens": [
            1122085995
        ],
        "exposed\nas": [
            1122085995
        ],
        "platform\n1x\napp": [
            1122085995
        ],
        "descriptor\n5x\n2x\ndeveloper\nfigure": [
            1122085995
        ],
        "platform\n": [
            1122085995
        ],
        "\n\n18chapter": [
            1122085995
        ],
        "kubernetes\nelection": [
            1122085995
        ],
        "waste": [
            1122085995
        ],
        "figuring": [
            1122085995
        ],
        "integrate": [
            1122085995
        ],
        "with\nthe": [
            1122085995
        ],
        "infrastructure\nhelping": [
            1122085995
        ],
        "utilization\nkubernetes": [
            1122085995
        ],
        "somewhere": [
            1122085995
        ],
        "running\nbecause": [
            1122085995
        ],
        "can\nrelocate": [
            1122085995
        ],
        "mixing": [
            1122085995
        ],
        "better\nresource": [
            1122085995
        ],
        "manual": [
            1122085995
        ],
        "scheduling\n1.3.3understanding": [
            1122085995
        ],
        "cluster\nwe’ve": [
            1122085995
        ],
        "bird’s-eye": [
            1122085995
        ],
        "kubernetes’": [
            1122085995
        ],
        "closer": [
            1122085995
        ],
        "at\nwhat": [
            1122085995
        ],
        "cluster\nis": [
            1122085995
        ],
        "split": [
            1122085995
        ],
        "types:": [
            1122085995
        ],
        "\nthe": [
            1122085995
        ],
        "controls": [
            1122085995
        ],
        "man-\nages": [
            1122085995
        ],
        "system\nworker": [
            1122085995
        ],
        "deploy\nfigure": [
            1122085995
        ],
        "explain\nthem": [
            1122085995
        ],
        "next\nthe": [
            1122085995
        ],
        "plane\nthe": [
            1122085995
        ],
        "function": [
            1122085995
        ],
        "of\nmultiple": [
            1122085995
        ],
        "multiple\nnodes": [
            1122085995
        ],
        "are\nthe": [
            1122085995
        ],
        "components\ncommunicate": [
            1122085995
        ],
        "with\ncontrol": [
            1122085995
        ],
        "(master)\netcd\napi": [
            1122085995
        ],
        "server\nkube-proxy\nworker": [
            1122085995
        ],
        "node(s)\nkubelet\ncontainer": [
            1122085995
        ],
        "runtime\nscheduler\ncontroller\nmanager\nfigure": [
            1122085995
        ],
        "cluster\n": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\n19introducing": [
            1122085995
        ],
        "kubernetes\nthe": [
            1122085995
        ],
        "schedules": [
            1122085995
        ],
        "(assigns": [
            1122085995
        ],
        "deploy-\nable": [
            1122085995
        ],
        "application)": [
            1122085995
        ],
        "functions": [
            1122085995
        ],
        "repli-\ncating": [
            1122085995
        ],
        "handling": [
            1122085995
        ],
        "failures\nand": [
            1122085995
        ],
        "on\netcd": [
            1122085995
        ],
        "reliable": [
            1122085995
        ],
        "persistently": [
            1122085995
        ],
        "cluster\nconfiguration\nthe": [
            1122085995
        ],
        "hold": [
            1122085995
        ],
        "(worker)": [
            1122085995
        ],
        "nodes\nthe": [
            1122085995
        ],
        "the\ntask": [
            1122085995
        ],
        "by\nthe": [
            1122085995
        ],
        "components:\ndocker": [
            1122085995
        ],
        "containers\nthe": [
            1122085995
        ],
        "talks": [
            1122085995,
            2119133144
        ],
        "node\nthe": [
            1122085995
        ],
        "proxy": [
            1122085995
        ],
        "(kube-proxy)": [
            1122085995
        ],
        "load-balances": [
            1122085995
        ],
        "traffic\nbetween": [
            1122085995
        ],
        "components\nwe’ll": [
            1122085995
        ],
        "explain": [
            1122085995
        ],
        "fan": [
            1122085995
        ],
        "explaining\nhow": [
            1122085995
        ],
        "explaining": [
            1122085995
        ],
        "teaching": [
            1122085995
        ],
        "to\nuse": [
            1122085995
        ],
        "learning": [
            1122085995
        ],
        "drive": [
            1122085995
        ],
        "car": [
            1122085995
        ],
        "hood\nyou": [
            1122085995
        ],
        "learn\nhow": [
            1122085995
        ],
        "interested": [
            1122085995
        ],
        "all\nknowing": [
            1122085995
        ],
        "hood": [
            1122085995
        ],
        "someday": [
            1122085995
        ],
        "moving": [
            1122085995
        ],
        "after\nit": [
            1122085995
        ],
        "breaks": [
            1122085995
        ],
        "leaves": [
            1122085995
        ],
        "stranded": [
            1122085995
        ],
        "side": [
            1122085995
        ],
        "road\n1.3.4running": [
            1122085995
        ],
        "kubernetes\nto": [
            1122085995
        ],
        "more\ncontainer": [
            1122085995
        ],
        "description\nof": [
            1122085995
        ],
        "description": [
            1122085995
        ],
        "that\ncontain": [
            1122085995
        ],
        "(together": [
            1122085995
        ],
        "node)": [
            1122085995
        ],
        "and\nwhich": [
            1122085995
        ],
        "replicas)\nyou": [
            1122085995
        ],
        "additionally": [
            1122085995
        ],
        "exposed\nthrough": [
            1122085995
        ],
        "discoverable": [
            1122085995
        ],
        "results": [
            1122085995
        ],
        "container\nwhen": [
            1122085995
        ],
        "the\nspecified": [
            1122085995
        ],
        "onto": [
            1122085995
        ],
        "computa-\ntional": [
            1122085995
        ],
        "unallocated": [
            1122085995
        ],
        "node\n": [
            1122085995
        ],
        "\n\n20chapter": [
            1122085995
        ],
        "kubernetes\nat": [
            1122085995
        ],
        "moment": [
            1122085995
        ],
        "instructs": [
            1122085995
        ],
        "runtime\n(docker": [
            1122085995
        ],
        "examine": [
            1122085995
        ],
        "are\ndeployed": [
            1122085995
        ],
        "lists": [
            1122085995
        ],
        "four": [
            1122085995
        ],
        "grouped": [
            1122085995
        ],
        "three\nsets": [
            1122085995
        ],
        "(these": [
            1122085995
        ],
        "pods;": [
            1122085995
        ],
        "3)": [
            1122085995
        ],
        "two\npods": [
            1122085995
        ],
        "whereas": [
            1122085995
        ],
        "that\nmeans": [
            1122085995
        ],
        "shouldn’t": [
            1122085995
        ],
        "representing": [
            1122085995
        ],
        "replicas\nof": [
            1122085995
        ],
        "parallel": [
            1122085995
        ],
        "submitting": [
            1122085995
        ],
        "schedule": [
            1122085995
        ],
        "available\nworker": [
            1122085995
        ],
        "kubelets": [
            1122085995
        ],
        "container\nimages": [
            1122085995
        ],
        "containers\nkeeping": [
            1122085995
        ],
        "running\nonce": [
            1122085995
        ],
        "continuously": [
            1122085995
        ],
        "deployed\nstate": [
            1122085995
        ],
        "matches": [
            1122085995
        ],
        "if\n1x\napp": [
            1122085995
        ],
        "descriptor\nlegend:\ncontainer": [
            1122085995
        ],
        "imagemultiple": [
            1122085995
        ],
        "containers\nrunning": [
            1122085995
        ],
        "“together”\n(not": [
            1122085995
        ],
        "fully": [
            1122085995
        ],
        "isolated)\n5x\n2x\ncontrol": [
            1122085995
        ],
        "plane\n(master)\nimage": [
            1122085995
        ],
        "registry\nworker": [
            1122085995
        ],
        "nodes\n..\nkube-proxy\ndocker\nkubeletkube-proxy\ndocker\nkubelet\ncontainer\n...\nkube-proxy\ndocker\nkubeletkube-proxy\ndocker\nkubelet\n...\nkube-proxy\ndocker\nkubeletkube-proxy\ndocker\nkubelet\nfigure": [
            1122085995
        ],
        "\n\n21introducing": [
            1122085995
        ],
        "five": [
            1122085995
        ],
        "will\nalways": [
            1122085995
        ],
        "stops": [
            1122085995
        ],
        "working\nproperly": [
            1122085995
        ],
        "crashes": [
            1122085995
        ],
        "responding": [
            1122085995
        ],
        "will\nrestart": [
            1122085995
        ],
        "dies": [
            1122085995
        ],
        "inaccessible": [
            1122085995
        ],
        "will\nselect": [
            1122085995
        ],
        "them\non": [
            1122085995
        ],
        "newly": [
            1122085995
        ],
        "selected": [
            1122085995
        ],
        "nodes\nscaling": [
            1122085995
        ],
        "copies\nwhile": [
            1122085995
        ],
        "decrease": [
            1122085995
        ],
        "the\nnumber": [
            1122085995
        ],
        "spin": [
            1122085995
        ],
        "stop": [
            1122085995
        ],
        "excess\nones": [
            1122085995
        ],
        "respectively": [
            1122085995
        ],
        "deciding": [
            1122085995
        ],
        "optimal": [
            1122085995
        ],
        "cop-\nies": [
            1122085995
        ],
        "adjusting": [
            1122085995
        ],
        "real-time\nmetrics": [
            1122085995
        ],
        "queries": [
            1122085995
        ],
        "second": [
            1122085995
        ],
        "other\nmetric": [
            1122085995
        ],
        "\nhitting": [
            1122085995
        ],
        "target\nwe’ve": [
            1122085995
        ],
        "move": [
            1122085995
        ],
        "cluster\nthis": [
            1122085995
        ],
        "occur": [
            1122085995
        ],
        "failed": [
            1122085995
        ],
        "were\nevicted": [
            1122085995
        ],
        "room": [
            1122085995
        ],
        "a\nservice": [
            1122085995
        ],
        "cluster?": [
            1122085995
        ],
        "cli-\nents": [
            1122085995
        ],
        "replicated\nand": [
            1122085995
        ],
        "cluster?\n": [
            1122085995
        ],
        "tell\nkubernetes": [
            1122085995
        ],
        "all\nof": [
            1122085995
        ],
        "also\nlook": [
            1122085995
        ],
        "connec-\ntions": [
            1122085995
        ],
        "balanced": [
            1122085995
        ],
        "service\nthe": [
            1122085995
        ],
        "stays": [
            1122085995
        ],
        "constant": [
            1122085995
        ],
        "con-\ntainers": [
            1122085995
        ],
        "cluster\n1.3.5understanding": [
            1122085995
        ],
        "kubernetes\nif": [
            1122085995
        ],
        "anymore": [
            1122085995
        ],
        "already\ncontains": [
            1122085995
        ],
        "install": [
            1122085995
        ],
        "to\ndeploy": [
            1122085995
        ],
        "can\nrun": [
            1122085995
        ],
        "immediately": [
            1122085995
        ],
        "\nsimplifying": [
            1122085995
        ],
        "deployment\nbecause": [
            1122085995
        ],
        "platform\napplication": [
            1122085995
        ],
        "need\nto": [
            1122085995
        ],
        "\n\n22chapter": [
            1122085995
        ],
        "bunch": [
            1122085995
        ],
        "that\nare": [
            1122085995
        ],
        "waiting": [
            1122085995
        ],
        "what\nkind": [
            1122085995
        ],
        "the\napplication": [
            1122085995
        ],
        "adequate": [
            1122085995
        ],
        "heterogeneous": [
            1122085995
        ],
        "you\nwant": [
            1122085995
        ],
        "oth-\ners": [
            1122085995
        ],
        "ssds\ninstead": [
            1122085995
        ],
        "hdds": [
            1122085995
        ],
        "fine": [
            1122085995
        ],
        "obviously\nwant": [
            1122085995
        ],
        "particular": [
            1122085995
        ],
        "ssd\n": [
            1122085995
        ],
        "sysadmin": [
            1122085995
        ],
        "select": [
            1122085995
        ],
        "an\nssd": [
            1122085995
        ],
        "selecting": [
            1122085995
        ],
        "spe-\ncific": [
            1122085995
        ],
        "to\nonly": [
            1122085995
        ],
        "choose": [
            1122085995
        ],
        "among": [
            1122085995
        ],
        "ssd": [
            1122085995
        ],
        "3\nachieving": [
            1122085995
        ],
        "hardware\nby": [
            1122085995
        ],
        "you\ntell": [
            1122085995
        ],
        "letting": [
            1122085995
        ],
        "appropriate\nnode": [
            1122085995
        ],
        "application’s\nresource": [
            1122085995
        ],
        "tying": [
            1122085995
        ],
        "cluster\nyou’re": [
            1122085995
        ],
        "freely": [
            1122085995
        ],
        "different\napp": [
            1122085995
        ],
        "mixed": [
            1122085995
        ],
        "matched": [
            1122085995
        ],
        "packed\ntightly": [
            1122085995
        ],
        "ensures": [
            1122085995
        ],
        "utilized\nas": [
            1122085995
        ],
        "possible\n": [
            1122085995
        ],
        "utilize": [
            1122085995
        ],
        "humans\naren’t": [
            1122085995
        ],
        "finding": [
            1122085995
        ],
        "combinations": [
            1122085995
        ],
        "possi-\nble": [
            1122085995
        ],
        "options": [
            1122085995
        ],
        "huge": [
            1122085995
        ],
        "many\nserver": [
            1122085995
        ],
        "work\nmuch": [
            1122085995
        ],
        "faster": [
            1122085995
        ],
        "humans": [
            1122085995
        ],
        "\nhealth": [
            1122085995
        ],
        "checking": [
            1122085995
        ],
        "self-healing\nhaving": [
            1122085995
        ],
        "also\nvaluable": [
            1122085995
        ],
        "with\nfailing": [
            1122085995
        ],
        "ever": [
            1122085995
        ],
        "frequently": [
            1122085995
        ],
        "monitors": [
            1122085995
        ],
        "auto-\nmatically": [
            1122085995
        ],
        "reschedules": [
            1122085995
        ],
        "frees\nthe": [
            1122085995
        ],
        "migrate": [
            1122085995
        ],
        "team\nto": [
            1122085995
        ],
        "fixing": [
            1122085995
        ],
        "returning": [
            1122085995
        ],
        "pool": [
            1122085995
        ],
        "available\nhardware": [
            1122085995
        ],
        "focusing": [
            1122085995
        ],
        "relocating": [
            1122085995
        ],
        "app\n": [
            1122085995
        ],
        "spare": [
            1122085995
        ],
        "normal": [
            1122085995
        ],
        "opera-\ntion": [
            1122085995
        ],
        "react": [
            1122085995
        ],
        "failure\n": [
            1122085995
        ],
        "\n\n23summary\nimmediately": [
            1122085995
        ],
        "am.": [
            1122085995
        ],
        "sleep": [
            1122085995
        ],
        "tight": [
            1122085995
        ],
        "node\nduring": [
            1122085995
        ],
        "hours\nautomatic": [
            1122085995
        ],
        "scaling\nusing": [
            1122085995
        ],
        "team\ndoesn’t": [
            1122085995
        ],
        "monitor": [
            1122085995
        ],
        "sud-\nden": [
            1122085995
        ],
        "spikes": [
            1122085995
        ],
        "told": [
            1122085995
        ],
        "the\nresources": [
            1122085995
        ],
        "running\ninstances": [
            1122085995
        ],
        "is\nas": [
            1122085995
        ],
        "provider’s": [
            1122085995
        ],
        "even\nautomatically": [
            1122085995
        ],
        "the\ndeployed": [
            1122085995
        ],
        "applications\nsimplifying": [
            1122085995
        ],
        "development\nthe": [
            1122085995
        ],
        "described": [
            1122085995
        ],
        "but\nwhat": [
            1122085995
        ],
        "developers?": [
            1122085995
        ],
        "table?": [
            1122085995
        ],
        "defi-\nnitely": [
            1122085995
        ],
        "does\n": [
            1122085995
        ],
        "turn": [
            1122085995,
            410929361
        ],
        "during\ndevelopment": [
            1122085995
        ],
        "discovered": [
            1122085995
        ],
        "we\nall": [
            1122085995
        ],
        "agree": [
            1122085995
        ],
        "sooner": [
            1122085995
        ],
        "bug": [
            1122085995
        ],
        "fix": [
            1122085995
        ],
        "requires\nless": [
            1122085995
        ],
        "they\nwould": [
            1122085995
        ],
        "clustered\napplication": [
            1122085995
        ],
        "look\nup": [
            1122085995
        ],
        "lookup": [
            1122085995
        ],
        "query": [
            1122085995
        ],
        "directly": [
            1122085995
        ],
        "querying": [
            1122085995
        ],
        "save": [
            1122085995
        ],
        "from\nhaving": [
            1122085995
        ],
        "complicated": [
            1122085995
        ],
        "leader": [
            1122085995,
            2119133144,
            1043891123
        ],
        "election\n": [
            1122085995
        ],
        "final": [
            1122085995,
            2119133144
        ],
        "con-\nsider": [
            1122085995
        ],
        "feel": [
            1122085995
        ],
        "knowing": [
            1122085995
        ],
        "version\nof": [
            1122085995
        ],
        "rolled": [
            1122085995
        ],
        "detect": [
            1122085995
        ],
        "new\nversion": [
            1122085995
        ],
        "usually\naccelerates": [
            1122085995
        ],
        "organization\n1.4summary\nin": [
            1122085995
        ],
        "introductory": [
            1122085995
        ],
        "recent\nyears": [
            1122085995
        ],
        "introduced\nkubernetes": [
            1122085995
        ],
        "platforms\nhelps": [
            1122085995
        ],
        "you’ve\nlearned": [
            1122085995
        ],
        "that\nmonolithic": [
            1122085995
        ],
        "and\nsometimes": [
            1122085995
        ],
        "scale\n": [
            1122085995
        ],
        "\n\n24chapter": [
            1122085995
        ],
        "kubernetes\nmicroservices-based": [
            1122085995
        ],
        "architectures": [
            1122085995
        ],
        "each\ncomponent": [
            1122085995
        ],
        "system\nlinux": [
            1122085995
        ],
        "are\nfar": [
            1122085995
        ],
        "utilization\ndocker": [
            1122085995
        ],
        "improved": [
            1122085995
        ],
        "and\nfaster": [
            1122085995
        ],
        "environments\nkubernetes": [
            1122085995
        ],
        "resource\nfor": [
            1122085995
        ],
        "applications\ndevelopers": [
            1122085995
        ],
        "from\nsysadmins\nsysadmins": [
            1122085995
        ],
        "auto-\nmatically\nin": [
            1122085995
        ],
        "hands": [
            1122085995,
            1941223023
        ],
        "dirty": [
            1122085995
        ],
        "in\ndocker": [
            1122085995
        ],
        "\n\n25\nfirst": [
            1122085995
        ],
        "docker\nand": [
            1122085995
        ],
        "kubernetes\nbefore": [
            1122085995
        ],
        "managed\nkubernetes": [
            1122085995
        ],
        "(in": [
            1122085995
        ],
        "engine)": [
            1122085995
        ],
        "slightly": [
            1122085995
        ],
        "and\nwill": [
            1122085995
        ],
        "follow": [
            1122085995
        ],
        "basic\nbuilding": [
            1122085995
        ],
        "kubernetes\nthis": [
            1122085995
        ],
        "covers\ncreating": [
            1122085995
        ],
        "docker\nrunning": [
            1122085995
        ],
        "locally\nsetting": [
            1122085995
        ],
        "engine\nsetting": [
            1122085995
        ],
        "\nclient\ndeploying": [
            1122085995
        ],
        "\nhorizontally\n": [
            1122085995
        ],
        "\n\n26chapter": [
            1122085995
        ],
        "2first": [
            1122085995
        ],
        "kubernetes\n21creating": [
            1122085995
        ],
        "image\nas": [
            1122085995
        ],
        "kubernetes\nrequires": [
            1122085995
        ],
        "to\nusing": [
            1122085995
        ],
        "haven’t": [
            1122085995
        ],
        "you’ll\n1install": [
            1122085995
        ],
        "“hello": [
            1122085995
        ],
        "world”": [
            1122085995
        ],
        "\n2create": [
            1122085995
        ],
        "kubernetes\n3package": [
            1122085995
        ],
        "isolated\ncontainer\n4run": [
            1122085995
        ],
        "image\n5push": [
            1122085995
        ],
        "hub": [
            1122085995
        ],
        "anywhere": [
            1122085995
        ],
        "it\n21.1installing": [
            1122085995
        ],
        "container\nfirst": [
            1122085995
        ],
        "linux\nyou’ll": [
            1122085995
        ],
        "if\nyou’re": [
            1122085995
        ],
        "mac": [
            1122085995
        ],
        "windows": [
            1122085995
        ],
        "up\na": [
            1122085995
        ],
        "daemon": [
            1122085995
        ],
        "execut-\nable": [
            1122085995
        ],
        "inside\nthe": [
            1122085995
        ],
        "http://docsdocker.com/engine/\ninstallation/": [
            1122085995
        ],
        "completing": [
            1122085995
        ],
        "installation": [
            1122085995
        ],
        "you\ncan": [
            1122085995
        ],
        "various": [
            1122085995
        ],
        "example\nyou": [
            1122085995
        ],
        "pulling": [
            1122085995
        ],
        "public\ndocker": [
            1122085995
        ],
        "ready-to-use": [
            1122085995
        ],
        "well-known\nsoftware": [
            1122085995
        ],
        "packages": [
            1122085995
        ],
        "\nbusybox": [
            1122085995
        ],
        "simple\necho": [
            1122085995
        ],
        "\nrunning": [
            1122085995
        ],
        "container\nif": [
            1122085995
        ],
        "unfamiliar": [
            1122085995
        ],
        "busybox": [
            1122085995
        ],
        "combines": [
            1122085995
        ],
        "the\nstandard": [
            1122085995
        ],
        "unix": [
            1122085995
        ],
        "tools": [
            1122085995
        ],
        "\necho": [
            1122085995
        ],
        "ls": [
            1122085995
        ],
        "gzip": [
            1122085995
        ],
        "the\nbusybox": [
            1122085995
        ],
        "full-fledged": [
            1122085995
        ],
        "as\nfedora": [
            1122085995
        ],
        "ubuntu": [
            1122085995
        ],
        "executable\n": [
            1122085995
        ],
        "image?": [
            1122085995
        ],
        "download": [
            1122085995
        ],
        "any-\nthing": [
            1122085995
        ],
        "run\nand": [
            1122085995
        ],
        "(optionally)": [
            1122085995
        ],
        "execute": [
            1122085995
        ],
        "listing\n$": [
            1122085995
        ],
        "echo": [
            1122085995
        ],
        "world\nunable": [
            1122085995
        ],
        "busybox:latest'": [
            1122085995
        ],
        "locally\nlatest:": [
            1122085995
        ],
        "dockerio/busybox\n9a163e0b8d13:": [
            1122085995
        ],
        "\nfef924a0204a:": [
            1122085995
        ],
        "complete\ndigest:": [
            1122085995
        ],
        "sha256:97473e34e311e6c1b3f61f2a721d038d1e5eef17d98d1353a513007cf46ca6bd\nstatus:": [
            1122085995
        ],
        "dockerio/busybox:latest\nhello": [
            1122085995
        ],
        "world\nlisting": [
            1122085995
        ],
        "docker\n": [
            1122085995
        ],
        "\n\n27creating": [
            1122085995
        ],
        "image\nthis": [
            1122085995
        ],
        "impressive": [
            1122085995
        ],
        "consider": [
            1122085995
        ],
        "“app”": [
            1122085995
        ],
        "was\ndownloaded": [
            1122085995
        ],
        "that\napp": [
            1122085995
        ],
        "(busybox)": [
            1122085995
        ],
        "incredibly": [
            1122085995
        ],
        "many\ndependencies": [
            1122085995
        ],
        "been\nexactly": [
            1122085995
        ],
        "container\ncompletely": [
            1122085995
        ],
        "machine\nunderstanding": [
            1122085995
        ],
        "happens": [
            1122085995
        ],
        "behind": [
            1122085995
        ],
        "scenes\nfigure": [
            1122085995
        ],
        "performed": [
            1122085995
        ],
        "com-\nmand": [
            1122085995
        ],
        "checked": [
            1122085995
        ],
        "\nbusybox:latest": [
            1122085995
        ],
        "present\non": [
            1122085995
        ],
        "wasn’t": [
            1122085995
        ],
        "pulled": [
            1122085995
        ],
        "at\nhttp://dockerio.": [
            1122085995
        ],
        "a\ncontainer": [
            1122085995
        ],
        "ran": [
            1122085995
        ],
        "command\nprinted": [
            1122085995
        ],
        "stdout": [
            1122085995
        ],
        "container\nstopped\nrunning": [
            1122085995
        ],
        "images\nrunning": [
            1122085995
        ],
        "busybox\nimage": [
            1122085995
        ],
        "simpler": [
            1122085995
        ],
        "what\ncommand": [
            1122085995
        ],
        "(\necho": [
            1122085995
        ],
        "world)": [
            1122085995
        ],
        "the\ncommand": [
            1122085995
        ],
        "baked": [
            1122085995
        ],
        "can\noverride": [
            1122085995
        ],
        "searching": [
            1122085995
        ],
        "browsing": [
            1122085995
        ],
        "publicly": [
            1122085995
        ],
        "available\nimages": [
            1122085995
        ],
        "http://hubdocker.com": [
            1122085995
        ],
        "public": [
            1122085995
        ],
        "run\nthe": [
            1122085995
        ],
        "this:\n$": [
            1122085995
        ],
        "<image>\nfigure": [
            1122085995
        ],
        "image\nlocal": [
            1122085995
        ],
        "machine\ndocker": [
            1122085995
        ],
        "hub\n1docker": [
            1122085995
        ],
        "busybox\necho": [
            1122085995
        ],
        "world\n3": [
            1122085995
        ],
        "pulls\nbusybox": [
            1122085995
        ],
        "image\nfrom": [
            1122085995
        ],
        "(if": [
            1122085995
        ],
        "not\navailable": [
            1122085995
        ],
        "locally)\n2": [
            1122085995
        ],
        "checks": [
            1122085995
        ],
        "locally\n4": [
            1122085995
        ],
        "runs\necho": [
            1122085995
        ],
        "world\nin": [
            1122085995
        ],
        "container\nbusybox\ndockerbusybox\n": [
            1122085995
        ],
        "\n\n28chapter": [
            1122085995
        ],
        "kubernetes\nversioning": [
            1122085995
        ],
        "images\nall": [
            1122085995
        ],
        "updated": [
            1122085995
        ],
        "usually\nexists": [
            1122085995
        ],
        "supports": [
            1122085995
        ],
        "variants": [
            1122085995
        ],
        "under\nthe": [
            1122085995
        ],
        "variant": [
            1122085995
        ],
        "unique": [
            1122085995
        ],
        "tag": [
            1122085995
        ],
        "referring": [
            1122085995
        ],
        "with-\nout": [
            1122085995
        ],
        "explicitly": [
            1122085995
        ],
        "assume": [
            1122085995
        ],
        "so-called\nlatest": [
            1122085995
        ],
        "<image>:<tag>\n21.2creating": [
            1122085995
        ],
        "app\nnow": [
            1122085995
        ],
        "setup": [
            1122085995
        ],
        "build\na": [
            1122085995
        ],
        "respond": [
            1122085995
        ],
        "own\nhostname": [
            1122085995
        ],
        "like\nany": [
            1122085995
        ],
        "useful": [
            1122085995
        ],
        "(scale": [
            1122085995
        ],
        "horizontally;": [
            1122085995
        ],
        "app)": [
            1122085995
        ],
        "you’ll\nsee": [
            1122085995
        ],
        "appjs": [
            1122085995
        ],
        "fol-\nlowing": [
            1122085995
        ],
        "listing\nconst": [
            1122085995
        ],
        "=": [
            1122085995
        ],
        "require(http');\nconst": [
            1122085995
        ],
        "require(os');\nconsolelog(kubia": [
            1122085995
        ],
        "starting..);\nvar": [
            1122085995
        ],
        "handler": [
            1122085995
        ],
        "function(request": [
            1122085995
        ],
        "response)": [
            1122085995
        ],
        "{\n": [
            1122085995
        ],
        "consolelog(received": [
            1122085995
        ],
        "+": [
            1122085995
        ],
        "requestconnection.remoteaddress);\n": [
            1122085995
        ],
        "responsewritehead(200);\n": [
            1122085995
        ],
        "responseend(youve": [
            1122085995
        ],
        "hit": [
            1122085995
        ],
        "oshostname()": [
            1122085995
        ],
        "\\n\");\n};\nvar": [
            1122085995
        ],
        "www": [
            1122085995
        ],
        "httpcreateserver(handler);\nwww.listen(8080);\nit": [
            1122085995
        ],
        "the\nserver": [
            1122085995
        ],
        "responds": [
            1122085995
        ],
        "response": [
            1122085995
        ],
        "status": [
            1122085995
        ],
        "\n200": [
            1122085995
        ],
        "ok": [
            1122085995
        ],
        "hit\n<hostname>\n": [
            1122085995
        ],
        "client’s": [
            1122085995
        ],
        "later\nnotethe": [
            1122085995
        ],
        "returned": [
            1122085995
        ],
        "server’s": [
            1122085995
        ],
        "one\nthe": [
            1122085995
        ],
        "sends": [
            1122085995
        ],
        "request’s": [
            1122085995
        ],
        "\nhost": [
            1122085995
        ],
        "header\nyou": [
            1122085995
        ],
        "test": [
            1122085995
        ],
        "isn’t\nnecessary": [
            1122085995
        ],
        "and\nlisting": [
            1122085995
        ],
        "app:": [
            1122085995
        ],
        "appjs\n": [
            1122085995
        ],
        "\n\n29creating": [
            1122085995
        ],
        "image\nenable": [
            1122085995
        ],
        "(except\ndocker": [
            1122085995
        ],
        "on)\n2.1.3creating": [
            1122085995
        ],
        "dockerfile\nwhich": [
            1122085995
        ],
        "the\nimage": [
            1122085995
        ],
        "directory": [
            1122085995
        ],
        "should\ncontain": [
            1122085995
        ],
        "listing\nfrom": [
            1122085995
        ],
        "node:7\nadd": [
            1122085995
        ],
        "/appjs\nentrypoint": [
            1122085995
        ],
        "[node\"": [
            1122085995
        ],
        "appjs\"]\nthe": [
            1122085995
        ],
        "defines": [
            1122085995
        ],
        "base\nimage": [
            1122085995
        ],
        "of)": [
            1122085995
        ],
        "\nnode": [
            1122085995
        ],
        "image\ntag": [
            1122085995
        ],
        "\n7": [
            1122085995
        ],
        "into\nthe": [
            1122085995
        ],
        "(appjs).": [
            1122085995
        ],
        "third\nline": [
            1122085995
        ],
        "defining": [
            1122085995
        ],
        "somebody": [
            1122085995
        ],
        "appjs.\n2.1.4building": [
            1122085995
        ],
        "image\nnow": [
            1122085995
        ],
        "command:\n$": [
            1122085995
        ],
        "-t": [
            1122085995
        ],
        "kubia": [
            1122085995
        ],
        "\nfigure": [
            1122085995
        ],
        "telling": [
            1122085995
        ],
        "to\nbuild": [
            1122085995
        ],
        "\nkubia": [
            1122085995
        ],
        "(note": [
            1122085995
        ],
        "the\ndot": [
            1122085995
        ],
        "command)": [
            1122085995
        ],
        "direc-\ntory": [
            1122085995
        ],
        "file\nlisting": [
            1122085995
        ],
        "app\nchoosing": [
            1122085995
        ],
        "image\nyou": [
            1122085995
        ],
        "wonder": [
            1122085995
        ],
        "chose": [
            1122085995
        ],
        "app\nis": [
            1122085995
        ],
        "have\neven": [
            1122085995
        ],
        "distro": [
            1122085995
        ],
        "\nfedora": [
            1122085995
        ],
        "installed\nnodejs": [
            1122085995
        ],
        "made\nspecifically": [
            1122085995
        ],
        "your\napp": [
            1122085995
        ],
        "image\n": [
            1122085995
        ],
        "\n\n30chapter": [
            1122085995
        ],
        "kubernetes\nunderstanding": [
            1122085995
        ],
        "built\nthe": [
            1122085995
        ],
        "uploaded": [
            1122085995
        ],
        "there\nthe": [
            1122085995
        ],
        "using\ndocker": [
            1122085995
        ],
        "non-linux": [
            1122085995
        ],
        "runs\ninside": [
            1122085995
        ],
        "daemon\nif": [
            1122085995
        ],
        "locally": [
            1122085995
        ],
        "upload": [
            1122085995
        ],
        "may\ntake": [
            1122085995
        ],
        "longer": [
            1122085995
        ],
        "\ntipdon’t": [
            1122085995
        ],
        "unnecessary": [
            1122085995
        ],
        "they’ll\nslow": [
            1122085995
        ],
        "process—especially": [
            1122085995
        ],
        "a\nremote": [
            1122085995
        ],
        "\nduring": [
            1122085995
        ],
        "(node:7)": [
            1122085995
        ],
        "(docker": [
            1122085995
        ],
        "hub)": [
            1122085995
        ],
        "is\nstored": [
            1122085995
        ],
        "layers\nan": [
            1122085995
        ],
        "blob": [
            1122085995
        ],
        "you\nmay": [
            1122085995
        ],
        "noticed": [
            1122085995
        ],
        "(there": [
            1122085995
        ],
        "multiple\npull": [
            1122085995
        ],
        "lines—one": [
            1122085995
        ],
        "layer)": [
            1122085995
        ],
        "several": [
            1122085995
        ],
        "layers\nfigure": [
            1122085995
        ],
        "dockerfile\nlocal": [
            1122085995
        ],
        "build\nkubia": [
            1122085995
        ],
        "\n3.": [
            1122085995
        ],
        "pulls": [
            1122085995
        ],
        "image\nnode:70": [
            1122085995
        ],
        "isn’t\nstored": [
            1122085995
        ],
        "yet\n4": [
            1122085995
        ],
        "new\nimage\n2": [
            1122085995
        ],
        "uploads\ndirectory": [
            1122085995
        ],
        "daemon\ndockerfile\ndocker": [
            1122085995
        ],
        "client\ndocker": [
            1122085995
        ],
        "daemon\nappjs\nnode:7.0\nnode:7.0\nkubia:latest\n": [
            1122085995
        ],
        "\n\n31creating": [
            1122085995
        ],
        "image\nwhich": [
            1122085995
        ],
        "if\nyou": [
            1122085995
        ],
        "(such": [
            1122085995
        ],
        "\nnode:7": [
            1122085995
        ],
        "exam-\nple)": [
            1122085995
        ],
        "comprising": [
            1122085995
        ],
        "pull-\ning": [
            1122085995
        ],
        "already\nbe": [
            1122085995
        ],
        "aren’t\n": [
            1122085995
        ],
        "think": [
            1122085995
        ],
        "not\nthe": [
            1122085995
        ],
        "command\nin": [
            1122085995
        ],
        "it\nthen": [
            1122085995
        ],
        "exe-\ncuted": [
            1122085995
        ],
        "tagged": [
            1122085995
        ],
        "\nkubia:latest": [
            1122085995
        ],
        "is\nshown": [
            1122085995
        ],
        "\nother:latest": [
            1122085995
        ],
        "would\nuse": [
            1122085995
        ],
        "does\nwhen": [
            1122085995
        ],
        "completes": [
            1122085995
        ],
        "it\nby": [
            1122085995
        ],
        "images\nrepository": [
            1122085995
        ],
        "size\nkubia": [
            1122085995
        ],
        "d30ecc7419e7": [
            1122085995
        ],
        "minute": [
            1122085995
        ],
        "mb\n..\ncomparing": [
            1122085995
        ],
        "manually\ndockerfiles": [
            1122085995
        ],
        "usual": [
            1122085995
        ],
        "could\nalso": [
            1122085995
        ],
        "execut-\ning": [
            1122085995
        ],
        "exiting": [
            1122085995
        ],
        "committing": [
            1122085995
        ],
        "state\nas": [
            1122085995
        ],
        "but\nit’s": [
            1122085995
        ],
        "repeatable": [
            1122085995
        ],
        "to\nlisting": [
            1122085995
        ],
        "listing": [
            1122085995
        ],
        "images\nfigure": [
            1122085995
        ],
        "images\nrun": [
            1122085995
        ],
        "curl": [
            1122085995
        ],
        "..\ncmd": [
            1122085995
        ],
        "node\nadd": [
            1122085995
        ],
        "appjs/app.js\n...\ncmd": [
            1122085995
        ],
        "appjs\nkubia:latest": [
            1122085995
        ],
        "image\n..\n...\nrun": [
            1122085995
        ],
        "apt-get": [
            1122085995
        ],
        "..\nother:latest": [
            1122085995
        ],
        "image\n..\nnode:0.12": [
            1122085995
        ],
        "image\nbuildpack-deps:jessie": [
            1122085995
        ],
        "\n\n32chapter": [
            1122085995
        ],
        "kubernetes\nthe": [
            1122085995
        ],
        "rebuild": [
            1122085995
        ],
        "retype": [
            1122085995
        ],
        "again\n2.1.5running": [
            1122085995
        ],
        "--name": [
            1122085995
        ],
        "kubia-container": [
            1122085995
        ],
        "-p": [
            1122085995
        ],
        "8080:8080": [
            1122085995
        ],
        "-d": [
            1122085995
        ],
        "kubia\nthis": [
            1122085995
        ],
        "tells": [
            1122085995
        ],
        "kubia\nimage": [
            1122085995
        ],
        "detached": [
            1122085995
        ],
        "console": [
            1122085995
        ],
        "(-d": [
            1122085995
        ],
        "flag)": [
            1122085995
        ],
        "will\nrun": [
            1122085995
        ],
        "background": [
            1122085995
        ],
        "mapped": [
            1122085995
        ],
        "8080\ninside": [
            1122085995
        ],
        "(\n-p": [
            1122085995
        ],
        "option)": [
            1122085995
        ],
        "through\nhttp://localhost:8080": [
            1122085995
        ],
        "a\nmac": [
            1122085995
        ],
        "host-\nname": [
            1122085995
        ],
        "localhost": [
            1122085995
        ],
        "up\nthrough": [
            1122085995
        ],
        "\ndocker_host": [
            1122085995
        ],
        "variable\naccessing": [
            1122085995
        ],
        "http://localhost:8080": [
            1122085995
        ],
        "(be": [
            1122085995
        ],
        "replace": [
            1122085995
        ],
        "local-\nhost": [
            1122085995
        ],
        "necessary):": [
            1122085995
        ],
        "\n$": [
            1122085995
        ],
        "localhost:8080\nyou’ve": [
            1122085995
        ],
        "44d76963e8e1": [
            1122085995
        ],
        "\nthat’s": [
            1122085995
        ],
        "\n44d76963e8e1": [
            1122085995
        ],
        "its\nhostname": [
            1122085995
        ],
        "hexadecimal": [
            1122085995
        ],
        "num-\nber": [
            1122085995
        ],
        "\nlisting": [
            1122085995
        ],
        "containers\nlet’s": [
            1122085995
        ],
        "list\n(i’ve": [
            1122085995
        ],
        "edited": [
            1122085995
        ],
        "readable—imagine": [
            1122085995
        ],
        "lines": [
            1122085995
        ],
        "the\ncontinuation": [
            1122085995
        ],
        "two)\n$": [
            1122085995
        ],
        "ps\ncontainer": [
            1122085995
        ],
        "..\n44d76963e8e1": [
            1122085995
        ],
        "kubia:latest": [
            1122085995
        ],
        "/bin/sh": [
            1122085995
        ],
        "-c": [
            1122085995
        ],
        "ap": [
            1122085995
        ],
        "minutes": [
            1122085995
        ],
        "..\n...": [
            1122085995
        ],
        "ports": [
            1122085995
        ],
        "names\n..": [
            1122085995
        ],
        "00.0.0:8080->8080/tcp": [
            1122085995
        ],
        "kubia-container\na": [
            1122085995
        ],
        "prints": [
            1122085995
        ],
        "name\nthe": [
            1122085995
        ],
        "executing": [
            1122085995
        ],
        "the\ncontainer": [
            1122085995
        ],
        "\n\n33creating": [
            1122085995
        ],
        "image\ngetting": [
            1122085995
        ],
        "container\nthe": [
            1122085995
        ],
        "ps": [
            1122085995
        ],
        "containers\nto": [
            1122085995
        ],
        "inspect:\n$": [
            1122085995
        ],
        "inspect": [
            1122085995
        ],
        "kubia-container\ndocker": [
            1122085995
        ],
        "low-level": [
            1122085995
        ],
        "\n21.6exploring": [
            1122085995
        ],
        "container\nwhat": [
            1122085995
        ],
        "container?": [
            1122085995
        ],
        "because\nmultiple": [
            1122085995
        ],
        "the\nshell’s": [
            1122085995
        ],
        "bash": [
            1122085995
        ],
        "exec": [
            1122085995
        ],
        "-it": [
            1122085995
        ],
        "bash\nthis": [
            1122085995
        ],
        "process\nwill": [
            1122085995
        ],
        "you\nto": [
            1122085995
        ],
        "explore": [
            1122085995
        ],
        "system\nwhen": [
            1122085995
        ],
        "\n-it": [
            1122085995
        ],
        "shorthand": [
            1122085995
        ],
        "options:": [
            1122085995
        ],
        "\n-i": [
            1122085995
        ],
        "stdin": [
            1122085995
        ],
        "entering": [
            1122085995
        ],
        "com-\nmands": [
            1122085995
        ],
        "\n-t": [
            1122085995
        ],
        "allocates": [
            1122085995
        ],
        "pseudo": [
            1122085995
        ],
        "terminal": [
            1122085995
        ],
        "(tty)\nyou": [
            1122085995
        ],
        "prompt": [
            1122085995
        ],
        "displayed": [
            1122085995
        ],
        "complain": [
            1122085995
        ],
        "\nterm\nvariable": [
            1122085995
        ],
        "set)\nexploring": [
            1122085995
        ],
        "within\nlet’s": [
            1122085995
        ],
        "container\nroot@44d76963e8e1:/#": [
            1122085995
        ],
        "aux\nuser": [
            1122085995
        ],
        "%cpu": [
            1122085995
        ],
        "%mem": [
            1122085995
        ],
        "vsz": [
            1122085995
        ],
        "rss": [
            1122085995
        ],
        "tty": [
            1122085995
        ],
        "stat": [
            1122085995
        ],
        "command\nroot": [
            1122085995
        ],
        "00": [
            1122085995
        ],
        "01": [
            1122085995
        ],
        "?": [
            1122085995
        ],
        "sl": [
            1122085995
        ],
        "12:31": [
            1122085995
        ],
        "0:00": [
            1122085995
        ],
        "appjs\nroot": [
            1122085995
        ],
        "ss": [
            1122085995
        ],
        "bash\nroot": [
            1122085995
        ],
        "r+": [
            1122085995
        ],
        "12:38": [
            1122085995
        ],
        "aux\nyou": [
            1122085995
        ],
        "container\n": [
            1122085995
        ],
        "\n\n34chapter": [
            1122085995
        ],
        "system\nif": [
            1122085995
        ],
        "will\namong": [
            1122085995
        ],
        "as\nshown": [
            1122085995
        ],
        "27.": [
            1122085995
        ],
        "\nnoteif": [
            1122085995
        ],
        "log": [
            1122085995
        ],
        "where\nthe": [
            1122085995
        ],
        "processes\n$": [
            1122085995
        ],
        "aux": [
            1122085995
        ],
        "|": [
            1122085995
        ],
        "grep": [
            1122085995
        ],
        "appjs\nuser": [
            1122085995
        ],
        "appjs\nthis": [
            1122085995
        ],
        "proves": [
            1122085995
        ],
        "you\nhave": [
            1122085995
        ],
        "keen": [
            1122085995
        ],
        "eye": [
            1122085995
        ],
        "and\nhas": [
            1122085995
        ],
        "tree": [
            1122085995
        ],
        "sequence": [
            1122085995
        ],
        "isolated\nlike": [
            1122085995
        ],
        "filesystem\nlisting": [
            1122085995
        ],
        "files\nin": [
            1122085995
        ],
        "plus": [
            1122085995
        ],
        "(log": [
            1122085995
        ],
        "similar)": [
            1122085995
        ],
        "listing\nroot@44d76963e8e1:/#": [
            1122085995
        ],
        "/\nappjs": [
            1122085995
        ],
        "boot": [
            1122085995
        ],
        "etc": [
            1122085995
        ],
        "lib": [
            1122085995
        ],
        "media": [
            1122085995,
            2119133144
        ],
        "opt": [
            1122085995
        ],
        "sbin": [
            1122085995
        ],
        "sys": [
            1122085995
        ],
        "usr\nbin": [
            1122085995
        ],
        "dev": [
            1122085995
        ],
        "home": [
            1122085995,
            1043891123
        ],
        "lib64": [
            1122085995
        ],
        "mnt": [
            1122085995
        ],
        "proc": [
            1122085995
        ],
        "srv": [
            1122085995
        ],
        "tmp": [
            1122085995
        ],
        "var\nit": [
            1122085995
        ],
        "directories": [
            1122085995
        ],
        "node:7": [
            1122085995
        ],
        "exit": [
            1122085995
        ],
        "\nexit": [
            1122085995
        ],
        "(like": [
            1122085995
        ],
        "logging": [
            1122085995
        ],
        "ssh": [
            1122085995
        ],
        "session\nfor": [
            1122085995
        ],
        "example)\ntipentering": [
            1122085995
        ],
        "debugging": [
            1122085995
        ],
        "app\nrunning": [
            1122085995
        ],
        "something’s": [
            1122085995
        ],
        "thing": [
            1122085995
        ],
        "want\nto": [
            1122085995
        ],
        "mind\nthat": [
            1122085995
        ],
        "pro-\ncesses": [
            1122085995
        ],
        "interfaces\n2.1.7stopping": [
            1122085995
        ],
        "container\nto": [
            1122085995
        ],
        "container:\n$": [
            1122085995
        ],
        "kubia-container\nlisting": [
            1122085995
        ],
        "os\nlisting": [
            1122085995
        ],
        "filesystem\n": [
            1122085995
        ],
        "\n\n35creating": [
            1122085995
        ],
        "consequently": [
            1122085995
        ],
        "exists": [
            1122085995
        ],
        "-a": [
            1122085995
        ],
        "out\nall": [
            1122085995
        ],
        "stopped": [
            1122085995
        ],
        "truly": [
            1122085995
        ],
        "remove": [
            1122085995
        ],
        "rm": [
            1122085995
        ],
        "kubia-container\nthis": [
            1122085995
        ],
        "deletes": [
            1122085995
        ],
        "removed": [
            1122085995
        ],
        "again\n2.1.8pushing": [
            1122085995
        ],
        "registry\nthe": [
            1122085995
        ],
        "allow\nyou": [
            1122085995
        ],
        "image\nregistry": [
            1122085995
        ],
        "sake": [
            1122085995
        ],
        "simplicity": [
            1122085995
        ],
        "will\ninstead": [
            1122085995
        ],
        "(http://hubdocker.com)": [
            1122085995
        ],
        "the\npublicly": [
            1122085995
        ],
        "quayio": [
            1122085995
        ],
        "the\ngoogle": [
            1122085995
        ],
        "registry\n": [
            1122085995
        ],
        "re-tag": [
            1122085995
        ],
        "according": [
            1122085995,
            410929361
        ],
        "hub’s\nrules": [
            1122085995
        ],
        "image’s": [
            1122085995
        ],
        "name\nstarts": [
            1122085995
        ],
        "registering": [
            1122085995
        ],
        "at\nhttp://hubdocker.com.": [
            1122085995
        ],
        "(\nluksa)": [
            1122085995
        ],
        "examples": [
            1122085995
        ],
        "please\nchange": [
            1122085995
        ],
        "occurrence": [
            1122085995
        ],
        "id\ntagging": [
            1122085995
        ],
        "tag\nonce": [
            1122085995
        ],
        "rename": [
            1122085995
        ],
        "currently": [
            1122085995
        ],
        "as\nkubia": [
            1122085995
        ],
        "luksa/kubia": [
            1122085995
        ],
        "(replace": [
            1122085995
        ],
        "luksa": [
            1122085995
        ],
        "id):\n$": [
            1122085995
        ],
        "luksa/kubia\nthis": [
            1122085995
        ],
        "tag;": [
            1122085995
        ],
        "can\nconfirm": [
            1122085995
        ],
        "head\nrepository": [
            1122085995
        ],
        "size\nluksa/kubia": [
            1122085995
        ],
        "hour": [
            1122085995
        ],
        "mb\nkubia": [
            1122085995
        ],
        "mb\ndockerio/node": [
            1122085995
        ],
        "04c0ca2a8dad": [
            1122085995
        ],
        "days": [
            1122085995
        ],
        "mb\n..\nas": [
            1122085995
        ],
        "in\nfact": [
            1122085995
        ],
        "tags": [
            1122085995
        ],
        "tags\n": [
            1122085995
        ],
        "\n\n36chapter": [
            1122085995
        ],
        "kubernetes\npushing": [
            1122085995
        ],
        "hub\nbefore": [
            1122085995
        ],
        "id\nwith": [
            1122085995
        ],
        "login": [
            1122085995
        ],
        "logged": [
            1122085995
        ],
        "the\nyourid/kubia": [
            1122085995
        ],
        "luksa/kubia\nrunning": [
            1122085995
        ],
        "machine\nafter": [
            1122085995
        ],
        "everyone\nyou": [
            1122085995
        ],
        "follow-\ning": [
            1122085995
        ],
        "luksa/kubia\nit": [
            1122085995
        ],
        "everywhere": [
            1122085995
        ],
        "it\nran": [
            1122085995
        ],
        "no\nneed": [
            1122085995
        ],
        "worry": [
            1122085995
        ],
        "fact\neven": [
            1122085995
        ],
        "the\nimage\n2.2setting": [
            1122085995
        ],
        "cluster\nnow": [
            1122085995
        ],
        "available\nthrough": [
            1122085995
        ],
        "it\nin": [
            1122085995
        ],
        "espe-\ncially": [
            1122085995
        ],
        "well-versed": [
            1122085995
        ],
        "proper\nkubernetes": [
            1122085995
        ],
        "spans": [
            1122085995
        ],
        "net-\nworking": [
            1122085995
        ],
        "flat": [
            1122085995
        ],
        "methods": [
            1122085995
        ],
        "are\ndescribed": [
            1122085995
        ],
        "documentation": [
            1122085995
        ],
        "http://kubernetesio.": [
            1122085995
        ],
        "we’re": [
            1122085995
        ],
        "to\nlist": [
            1122085995
        ],
        "organization’s": [
            1122085995
        ],
        "on\ncloud": [
            1122085995
        ],
        "(google": [
            1122085995
        ],
        "compute": [
            1122085995
        ],
        "amazon": [
            1122085995
        ],
        "ec2\nmicrosoft": [
            1122085995
        ],
        "azure": [
            1122085995
        ],
        "(previously": [
            1122085995
        ],
        "running\nkubernetes": [
            1122085995
        ],
        "your\nlocal": [
            1122085995
        ],
        "(gke)": [
            1122085995
        ],
        "third": [
            1122085995
        ],
        "\nkubeadm": [
            1122085995
        ],
        "tool": [
            1122085995
        ],
        "explained\nin": [
            1122085995
        ],
        "appendix": [
            1122085995
        ],
        "three-node": [
            1122085995
        ],
        "\n\n37setting": [
            1122085995
        ],
        "cluster\ncluster": [
            1122085995
        ],
        "suggest": [
            1122085995
        ],
        "11\nchapters": [
            1122085995
        ],
        "amazon’s": [
            1122085995
        ],
        "aws": [
            1122085995
        ],
        "(amazon": [
            1122085995
        ],
        "services)\nfor": [
            1122085995
        ],
        "\nkops": [
            1122085995
        ],
        "paragraph": [
            1122085995
        ],
        "http://githubcom/kubernetes/kops.": [
            1122085995
        ],
        "it\nhelps": [
            1122085995
        ],
        "production-grade": [
            1122085995
        ],
        "support": [
            1122085995
        ],
        "platforms": [
            1122085995
        ],
        "vmware\nvsphere,": [
            1122085995
        ],
        "on)\n2.2.1running": [
            1122085995
        ],
        "minikube\nthe": [
            1122085995
        ],
        "quickest": [
            1122085995
        ],
        "functioning": [
            1122085995
        ],
        "using\nminikube": [
            1122085995
        ],
        "both\ntesting": [
            1122085995
        ],
        "although": [
            1122085995
        ],
        "on\nmultiple": [
            1122085995
        ],
        "exploring": [
            1122085995
        ],
        "topics\ndiscussed": [
            1122085995
        ],
        "\ninstalling": [
            1122085995
        ],
        "minikube\nminikube": [
            1122085995
        ],
        "it’s\navailable": [
            1122085995
        ],
        "osx": [
            1122085995
        ],
        "github": [
            1122085995
        ],
        "(http://githubcom/kubernetes/minikube)": [
            1122085995
        ],
        "and\nfollow": [
            1122085995
        ],
        "there\n": [
            1122085995
        ],
        "like:\n$": [
            1122085995
        ],
        "-lo": [
            1122085995
        ],
        "https://storagegoogleapis.com/minikube/releases/\n➥": [
            1122085995
        ],
        "v023.0/minikube-darwin-amd64": [
            1122085995
        ],
        "&&": [
            1122085995
        ],
        "chmod": [
            1122085995
        ],
        "+x": [
            1122085995
        ],
        "sudo": [
            1122085995
        ],
        "mv": [
            1122085995
        ],
        "\n➥\n": [
            1122085995
        ],
        "/usr/local/bin/\non": [
            1122085995
        ],
        "“darwin”": [
            1122085995
        ],
        "“linux”": [
            1122085995
        ],
        "the\nurl)": [
            1122085995
        ],
        "minikubeexe\nand": [
            1122085995
        ],
        "either\nvirtualbox": [
            1122085995
        ],
        "kvm": [
            1122085995
        ],
        "the\nminikube": [
            1122085995
        ],
        "cluster\nstarting": [
            1122085995
        ],
        "minikube\nonce": [
            1122085995
        ],
        "start\nstarting": [
            1122085995
        ],
        "cluster..\nstarting": [
            1122085995
        ],
        "vm..\nssh-ing": [
            1122085995
        ],
        "vm..\n...\nkubectl": [
            1122085995
        ],
        "machine\n": [
            1122085995
        ],
        "\n\n38chapter": [
            1122085995
        ],
        "kubernetes\nstarting": [
            1122085995
        ],
        "interrupt": [
            1122085995
        ],
        "before\nit": [
            1122085995
        ],
        "(kubectl)\nto": [
            1122085995
        ],
        "interact": [
            1122085995
        ],
        "cli": [
            1122085995
        ],
        "for\nexample": [
            1122085995
        ],
        "https://storagegoogleapis.com/kubernetes-release/release\n➥\n": [
            1122085995
        ],
        "/$(curl": [
            1122085995
        ],
        "-s": [
            1122085995
        ],
        "/stabletxt)/bin/darwin/amd64/kubectl": [
            1122085995
        ],
        "/usr/local/bin/\nto": [
            1122085995
        ],
        "darwin": [
            1122085995
        ],
        "url": [
            1122085995
        ],
        "either\nlinux": [
            1122085995
        ],
        "windows\nnoteif": [
            1122085995
        ],
        "both\nminikube": [
            1122085995
        ],
        "gke)": [
            1122085995
        ],
        "up\nand": [
            1122085995
        ],
        "switch": [
            1122085995
        ],
        "contexts\nchecking": [
            1122085995
        ],
        "it\nto": [
            1122085995
        ],
        "verify": [
            1122085995
        ],
        "cluster-info": [
            1122085995
        ],
        "command\nshown": [
            1122085995
        ],
        "cluster-info\nkubernetes": [
            1122085995
        ],
        "https://192168.99.100:8443\nkubedns": [
            1122085995
        ],
        "https://192168.99.100:8443/api/v1/proxy/...\nkubernetes-dashboard": [
            1122085995
        ],
        "https://192168.99.100:8443/api/v1/...\nthis": [
            1122085995
        ],
        "urls": [
            1122085995
        ],
        "components\nincluding": [
            1122085995
        ],
        "\ntipyou": [
            1122085995
        ],
        "node\n2.2.2using": [
            1122085995
        ],
        "engine\nif": [
            1122085995
        ],
        "can\nuse": [
            1122085995
        ],
        "to\nmanually": [
            1122085995
        ],
        "for\nsomeone": [
            1122085995
        ],
        "solution": [
            1122085995
        ],
        "as\ngke": [
            1122085995
        ],
        "misconfigured": [
            1122085995
        ],
        "non-working": [
            1122085995
        ],
        "work-\ning": [
            1122085995
        ],
        "cluster\nlisting": [
            1122085995
        ],
        "displaying": [
            1122085995
        ],
        "information\n": [
            1122085995
        ],
        "\n\n39setting": [
            1122085995
        ],
        "cluster\nsetting": [
            1122085995
        ],
        "project": [
            1122085995
        ],
        "downloading": [
            1122085995
        ],
        "binaries\nbefore": [
            1122085995
        ],
        "gke": [
            1122085995
        ],
        "environ-\nment": [
            1122085995
        ],
        "to\nget": [
            1122085995
        ],
        "https://cloudgoogle.com/container-\nengine/docs/before-you-begin.\n": [
            1122085995
        ],
        "roughly": [
            1122085995
        ],
        "procedure": [
            1122085995
        ],
        "includes\n1signing": [
            1122085995
        ],
        "account": [
            1122085995
        ],
        "unlikely": [
            1122085995
        ],
        "one\nalready\n2creating": [
            1122085995
        ],
        "\n3enabling": [
            1122085995
        ],
        "billing": [
            1122085995
        ],
        "a\n12-month": [
            1122085995
        ],
        "trial": [
            1122085995
        ],
        "charging": [
            1122085995
        ],
        "over)\n4enabling": [
            1122085995
        ],
        "api\n5downloading": [
            1122085995
        ],
        "sdk": [
            1122085995
        ],
        "(this": [
            1122085995
        ],
        "gcloud\ncommand-line": [
            1122085995
        ],
        "cluster)\n6installing": [
            1122085995
        ],
        "gcloud": [
            1122085995
        ],
        "install\nkubectl\n\nnotecertain": [
            1122085995
        ],
        "step": [
            1122085995
        ],
        "few\nminutes": [
            1122085995
        ],
        "relax": [
            1122085995
        ],
        "grab": [
            1122085995
        ],
        "coffee": [
            1122085995
        ],
        "meantime\ncreating": [
            1122085995
        ],
        "nodes\nafter": [
            1122085995
        ],
        "three\nworker": [
            1122085995
        ],
        "--num-nodes": [
            1122085995
        ],
        "--machine-type": [
            1122085995
        ],
        "f1-micro\ncreating": [
            1122085995
        ],
        "kubia..done.\ncreated": [
            1122085995
        ],
        "[https://containergoogleapis.com/v1/projects/kubia1-\n1227/zones/europe-west1-d/clusters/kubia].\nkubeconfig": [
            1122085995
        ],
        "generated": [
            1122085995
        ],
        "kubia\nname": [
            1122085995
        ],
        "mst_ver": [
            1122085995
        ],
        "master_ip": [
            1122085995
        ],
        "node_ver": [
            1122085995
        ],
        "num_nodes": [
            1122085995
        ],
        "status\nkubia": [
            1122085995
        ],
        "eu-w1d": [
            1122085995
        ],
        "15.3": [
            1122085995
        ],
        "104155.92.30": [
            1122085995
        ],
        "f1-micro": [
            1122085995
        ],
        "running\nyou": [
            1122085995
        ],
        "shown\nin": [
            1122085995
        ],
        "24.": [
            1122085995
        ],
        "demonstrate": [
            1122085995
        ],
        "apply\nto": [
            1122085995
        ],
        "\ngetting": [
            1122085995
        ],
        "cluster\nto": [
            1122085995
        ],
        "see\nfigure": [
            1122085995
        ],
        "interact\nwith": [
            1122085995
        ],
        "requests\nto": [
            1122085995
        ],
        "node\nlisting": [
            1122085995
        ],
        "gke\n": [
            1122085995
        ],
        "\n\n40chapter": [
            1122085995
        ],
        "kubernetes\nchecking": [
            1122085995
        ],
        "nodes\nyou’ll": [
            1122085995
        ],
        "nodes\nname": [
            1122085995
        ],
        "version\ngke-kubia-85f6-node-0rrx": [
            1122085995
        ],
        "1m": [
            1122085995
        ],
        "v15.3\ngke-kubia-85f6-node-heo1": [
            1122085995
        ],
        "v15.3\ngke-kubia-85f6-node-vs9f": [
            1122085995
        ],
        "v15.3\nthe": [
            1122085995
        ],
        "con-\nstantly": [
            1122085995
        ],
        "<node-name>\nto": [
            1122085995
        ],
        "kubectl\nfigure": [
            1122085995
        ],
        "interacting": [
            1122085995
        ],
        "\nlocal": [
            1122085995
        ],
        "machine\nrest": [
            1122085995
        ],
        "call\nkubectlrest": [
            1122085995
        ],
        "server\nmaster": [
            1122085995
        ],
        "node\n(ip": [
            1122085995
        ],
        "104155.92.30)\ndocker\nkubeletkube-proxy\ngke-kubia-85f6-node-heo1\ndocker\nkubeletkube-proxy\ngke-kubia-85f6-node-vs9f\ndocker\nworker": [
            1122085995
        ],
        "nodes\nkubernetes": [
            1122085995
        ],
        "cluster\nkubeletkube-proxy\ngke-kubia-85f6-node-0rrx\n": [
            1122085995
        ],
        "\n\n41setting": [
            1122085995
        ],
        "cluster\nretrieving": [
            1122085995
        ],
        "object\nto": [
            1122085995
        ],
        "describe\ncommand": [
            1122085995
        ],
        "more:": [
            1122085995
        ],
        "describe": [
            1122085995
        ],
        "gke-kubia-85f6-node-0rrx\ni’m": [
            1122085995
        ],
        "omitting": [
            1122085995
        ],
        "and\nwould": [
            1122085995
        ],
        "unreadable": [
            1122085995
        ],
        "sta-\ntus": [
            1122085995
        ],
        "node\nand": [
            1122085995
        ],
        "more\n": [
            1122085995
        ],
        "node\nexplicitly": [
            1122085995
        ],
        "without\ntyping": [
            1122085995
        ],
        "nodes\ntiprunning": [
            1122085995
        ],
        "name\nof": [
            1122085995
        ],
        "handy": [
            1122085995
        ],
        "given": [
            1122085995
        ],
        "so\nyou": [
            1122085995
        ],
        "typing": [
            1122085995
        ],
        "copy/pasting": [
            1122085995
        ],
        "name\nwhile": [
            1122085995
        ],
        "talking": [
            1122085995
        ],
        "keystrokes": [
            1122085995
        ],
        "let": [
            1122085995
        ],
        "advice": [
            1122085995
        ],
        "on\nhow": [
            1122085995
        ],
        "your\nfirst": [
            1122085995
        ],
        "kubernetes\n2.2.3setting": [
            1122085995
        ],
        "\nyou’ll": [
            1122085995
        ],
        "command\nevery": [
            1122085995
        ],
        "real": [
            1122085995
        ],
        "pain": [
            1122085995
        ],
        "continue": [
            1122085995,
            2119133144
        ],
        "easier\nby": [
            1122085995
        ],
        "tab": [
            1122085995
        ],
        "\nkubectl\ncreating": [
            1122085995
        ],
        "alias\nthroughout": [
            1122085995
        ],
        "executable\nbut": [
            1122085995
        ],
        "\nk": [
            1122085995
        ],
        "kubectl\nevery": [
            1122085995
        ],
        "aliases": [
            1122085995
        ],
        "here’s": [
            1122085995
        ],
        "define": [
            1122085995
        ],
        "\n~/bashrc": [
            1122085995
        ],
        "file:\nalias": [
            1122085995
        ],
        "k=kubectl\nnoteyou": [
            1122085995
        ],
        "k": [
            1122085995
        ],
        "the\ncluster\nconfiguring": [
            1122085995
        ],
        "kubectl\neven": [
            1122085995
        ],
        "luck-\nily": [
            1122085995
        ],
        "and\nzsh": [
            1122085995
        ],
        "is\n$": [
            1122085995
        ],
        "desc<tab>": [
            1122085995
        ],
        "no<tab>": [
            1122085995
        ],
        "gke-ku<tab>\n": [
            1122085995
        ],
        "\n\n42chapter": [
            1122085995
        ],
        "bash-\ncompletion\n": [
            1122085995
        ],
        "(you’ll": [
            1122085995
        ],
        "equivalent):\n$": [
            1122085995
        ],
        "<(kubectl": [
            1122085995
        ],
        "bash)\nbut": [
            1122085995
        ],
        "preceding": [
            1122085995
        ],
        "will\nonly": [
            1122085995
        ],
        "(it": [
            1122085995
        ],
        "k\nalias)": [
            1122085995
        ],
        "transform": [
            1122085995
        ],
        "bit:\n$": [
            1122085995
        ],
        "sed": [
            1122085995
        ],
        "s/kubectl/k/g)\nnoteunfortunately": [
            1122085995
        ],
        "for\naliases": [
            1122085995
        ],
        "macos": [
            1122085995
        ],
        "work\nnow": [
            1122085995
        ],
        "too\nmuch": [
            1122085995
        ],
        "kubernetes\n2.3running": [
            1122085995
        ],
        "an\napp": [
            1122085995
        ],
        "prepare": [
            1122085995
        ],
        "manifest": [
            1122085995
        ],
        "a\ndescription": [
            1122085995
        ],
        "talked\nabout": [
            1122085995
        ],
        "simple\none-line": [
            1122085995
        ],
        "running\n2.3.1deploying": [
            1122085995
        ],
        "app\nthe": [
            1122085995
        ],
        "will\ncreate": [
            1122085995
        ],
        "this\nway": [
            1122085995
        ],
        "dive": [
            1122085995
        ],
        "structure": [
            1122085995
        ],
        "pushed": [
            1122085995
        ],
        "kubernetes:\n$": [
            1122085995
        ],
        "--image=luksa/kubia": [
            1122085995
        ],
        "--port=8080": [
            1122085995
        ],
        "--generator=run/v1\nreplicationcontroller": [
            1122085995
        ],
        "kubia\"": [
            1122085995
        ],
        "created\nthe": [
            1122085995
        ],
        "specifies": [
            1122085995
        ],
        "to\nrun": [
            1122085995
        ],
        "\n--port=8080": [
            1122085995
        ],
        "listening": [
            1122085995
        ],
        "port\n8080": [
            1122085995
        ],
        "flag": [
            1122085995
        ],
        "(\n--generator)": [
            1122085995
        ],
        "explanation": [
            1122085995
        ],
        "you\nwon’t": [
            1122085995
        ],
        "replicationcontroller\ninstead": [
            1122085995
        ],
        "chap-\nter": [
            1122085995
        ],
        "want\nkubectl": [
            1122085995
        ],
        "command’s": [
            1122085995
        ],
        "\nkubia\nhas": [
            1122085995
        ],
        "for\n": [
            1122085995
        ],
        "\n\n43running": [
            1122085995
        ],
        "kubernetes\nnow": [
            1122085995
        ],
        "bottom": [
            1122085995
        ],
        "(you": [
            1122085995
        ],
        "can\nassume": [
            1122085995
        ],
        "the\nrun": [
            1122085995
        ],
        "command)\nintroducing": [
            1122085995
        ],
        "pods\nyou": [
            1122085995
        ],
        "showing": [
            1122085995
        ],
        "running\ncontainers": [
            1122085995
        ],
        "maybe": [
            1122085995
        ],
        "containers?": [
            1122085995
        ],
        "exactly\nhow": [
            1122085995
        ],
        "it\nuses": [
            1122085995
        ],
        "called\na": [
            1122085995
        ],
        "run\ntogether": [
            1122085995
        ],
        "namespace(s)": [
            1122085995
        ],
        "pod\nis": [
            1122085995
        ],
        "on\nrunning": [
            1122085995
        ],
        "supporting\nprocesses": [
            1122085995
        ],
        "appear\nto": [
            1122085995
        ],
        "even\nif": [
            1122085995
        ],
        "differ-\nent": [
            1122085995
        ],
        "relationship": [
            1122085995
        ],
        "exam-\nine": [
            1122085995
        ],
        "25.": [
            1122085995
        ],
        "different\nworker": [
            1122085995
        ],
        "nodes\nlisting": [
            1122085995
        ],
        "pods\nbecause": [
            1122085995
        ],
        "kubernetes\nobjects": [
            1122085995
        ],
        "instead?": [
            1122085995
        ],
        "list\npods": [
            1122085995
        ],
        "listing\nfigure": [
            1122085995
        ],
        "nodes\nworker": [
            1122085995
        ],
        "1\npod": [
            1122085995
        ],
        "2\nip:": [
            1122085995
        ],
        "101.0.2\ncontainer": [
            1122085995
        ],
        "1\ncontainer": [
            1122085995
        ],
        "2\npod": [
            1122085995
        ],
        "3\nip:": [
            1122085995
        ],
        "101.0.3\ncontainer": [
            1122085995
        ],
        "1\nip:": [
            1122085995
        ],
        "101.0.1\ncontainer\nworker": [
            1122085995
        ],
        "5\nip:": [
            1122085995
        ],
        "101.1.2\ncontainer": [
            1122085995
        ],
        "6\nip:": [
            1122085995
        ],
        "101.1.3\ncontainer": [
            1122085995
        ],
        "4\nip:": [
            1122085995
        ],
        "101.1.1\ncontainer\ncontainer": [
            1122085995
        ],
        "2\n": [
            1122085995
        ],
        "\n\n44chapter": [
            1122085995
        ],
        "kubernetes\n$": [
            1122085995
        ],
        "pods\nname": [
            1122085995
        ],
        "restarts": [
            1122085995
        ],
        "age\nkubia-4jfyf": [
            1122085995
        ],
        "0/1": [
            1122085995
        ],
        "pending": [
            1122085995
        ],
        "1m\nthis": [
            1122085995
        ],
        "as\nnot": [
            1122085995
        ],
        "\n0/1": [
            1122085995
        ],
        "column": [
            1122085995
        ],
        "means)": [
            1122085995
        ],
        "the\npod": [
            1122085995
        ],
        "is\ndownloading": [
            1122085995
        ],
        "finished\nthe": [
            1122085995
        ],
        "transition": [
            1122085995
        ],
        "\nrunning\nstate": [
            1122085995
        ],
        "1/1": [
            1122085995
        ],
        "5m\nto": [
            1122085995
        ],
        "pod\ncommand": [
            1122085995
        ],
        "stuck": [
            1122085995
        ],
        "registry\nif": [
            1122085995
        ],
        "marked": [
            1122085995
        ],
        "to\nmake": [
            1122085995
        ],
        "successfully": [
            1122085995
        ],
        "scenes\nto": [
            1122085995
        ],
        "visualize": [
            1122085995
        ],
        "transpired": [
            1122085995
        ],
        "26.": [
            1122085995
        ],
        "to\nperform": [
            1122085995
        ],
        "image\non": [
            1122085995
        ],
        "needed\nto": [
            1122085995
        ],
        "daemons": [
            1122085995
        ],
        "nodes\n": [
            1122085995
        ],
        "replicationcontroller\nobject": [
            1122085995
        ],
        "sending": [
            1122085995
        ],
        "server\nthe": [
            1122085995
        ],
        "was\nscheduled": [
            1122085995
        ],
        "instructed": [
            1122085995
        ],
        "registry\nbecause": [
            1122085995
        ],
        "cre-\nated": [
            1122085995
        ],
        "play": [
            1122085995
        ],
        "them\ndefinitionthe": [
            1122085995
        ],
        "term": [
            1122085995
        ],
        "believe\nlisting": [
            1122085995
        ],
        "pods\nlisting": [
            1122085995
        ],
        "changed\n": [
            1122085995
        ],
        "\n\n45running": [
            1122085995
        ],
        "kubernetes\n23.2accessing": [
            1122085995
        ],
        "application\nwith": [
            1122085995
        ],
        "it?": [
            1122085995
        ],
        "its\nown": [
            1122085995
        ],
        "from\noutside": [
            1122085995
        ],
        "\nloadbalancer": [
            1122085995
        ],
        "(a": [
            1122085995
        ],
        "\nclusterip": [
            1122085995
        ],
        "service)": [
            1122085995
        ],
        "\nloadbalancer-type": [
            1122085995
        ],
        "load\nbalancer": [
            1122085995
        ],
        "balancer’s\npublic": [
            1122085995
        ],
        "\ncreating": [
            1122085995
        ],
        "you\ncreated": [
            1122085995
        ],
        "earlier:\n$": [
            1122085995
        ],
        "rc": [
            1122085995
        ],
        "--type=loadbalancer": [
            1122085995
        ],
        "kubia-http\nservice": [
            1122085995
        ],
        "kubia-http\"": [
            1122085995
        ],
        "exposed\nfigure": [
            1122085995
        ],
        "kubernetes\nlocal": [
            1122085995
        ],
        "dev\nmachine\nkubectl\nrest": [
            1122085995
        ],
        "server\nscheduler\nmaster": [
            1122085995
        ],
        "node(s)\ndocker\nkubelet\ngke-kubia-85f6-node-0rrx\ndocker\nkubelet\ngke-kubia-85f6-node-heo1\ndocker\nkubelet\ngke-kubia-85f6-node-vs9f\ndocker": [
            1122085995
        ],
        "hub\n3kubectl": [
            1122085995
        ],
        "kubia\n--image=luksa/kubia\n--port=8080\n4issueskubectl\nrest": [
            1122085995
        ],
        "call\n5": [
            1122085995
        ],
        "created\nand": [
            1122085995
        ],
        "scheduled\nto": [
            1122085995
        ],
        "node\n7": [
            1122085995
        ],
        "kubelet\ninstructs\ndocker\nto": [
            1122085995
        ],
        "the\nimage\n8": [
            1122085995
        ],
        "pulls\nand": [
            1122085995
        ],
        "runs\nluksa/kubia\n6": [
            1122085995
        ],
        "kubelet\nis": [
            1122085995
        ],
        "notified\n1docker": [
            1122085995
        ],
        "push\nluksa/kubia\n2": [
            1122085995
        ],
        "image\nluksa/kubia\nis": [
            1122085995
        ],
        "to\ndocker": [
            1122085995
        ],
        "hub\ndocker\npod": [
            1122085995
        ],
        "kubia-4jfyf\n": [
            1122085995
        ],
        "\n\n46chapter": [
            1122085995
        ],
        "kubernetes\nnotewe’re": [
            1122085995
        ],
        "abbreviation": [
            1122085995
        ],
        "replicationcontroller\nmost": [
            1122085995
        ],
        "type\nthe": [
            1122085995
        ],
        "\npo": [
            1122085995
        ],
        "svc": [
            1122085995
        ],
        "on)\nlisting": [
            1122085995
        ],
        "services\nthe": [
            1122085995
        ],
        "mentions": [
            1122085995
        ],
        "kubia-http": [
            1122085995
        ],
        "are\nobjects": [
            1122085995
        ],
        "services\nname": [
            1122085995
        ],
        "cluster-ip": [
            1122085995
        ],
        "external-ip": [
            1122085995
        ],
        "port(s)": [
            1122085995
        ],
        "age\nkubernetes": [
            1122085995
        ],
        "103.240.1": [
            1122085995
        ],
        "<none>": [
            1122085995
        ],
        "443/tcp": [
            1122085995
        ],
        "34m\nkubia-http": [
            1122085995
        ],
        "103.246.185": [
            1122085995
        ],
        "<pending>": [
            1122085995
        ],
        "8080:31348/tcp": [
            1122085995
        ],
        "4s\nthe": [
            1122085995
        ],
        "ignore": [
            1122085995
        ],
        "close\nlook": [
            1122085995
        ],
        "\nkubia-http": [
            1122085995
        ],
        "yet\nbecause": [
            1122085995
        ],
        "infrastructure\nkubernetes": [
            1122085995
        ],
        "the\nservice": [
            1122085995
        ],
        "svc\nname": [
            1122085995
        ],
        "35m\nkubia-http": [
            1122085995
        ],
        "104155.74.57": [
            1122085995
        ],
        "1m\naha": [
            1122085995
        ],
        "http://104155.74\n.57:8080": [
            1122085995
        ],
        "\nnoteminikube": [
            1122085995
        ],
        "loadbalancer": [
            1122085995
        ],
        "will\nnever": [
            1122085995
        ],
        "anyway": [
            1122085995
        ],
        "its\nexternal": [
            1122085995
        ],
        "section’s": [
            1122085995
        ],
        "tip\naccessing": [
            1122085995
        ],
        "ip\nyou": [
            1122085995
        ],
        "send": [
            1122085995
        ],
        "service’s": [
            1122085995
        ],
        "port:\n$": [
            1122085995
        ],
        "104155.74.57:8080\nyou’ve": [
            1122085995
        ],
        "kubia-4jfyf": [
            1122085995
        ],
        "\nwoohoo!": [
            1122085995
        ],
        "cluster\n(or": [
            1122085995
        ],
        "minikube)": [
            1122085995
        ],
        "count": [
            1122085995
        ],
        "steps\nrequired": [
            1122085995
        ],
        "services\nlisting": [
            1122085995
        ],
        "assigned\n": [
            1122085995
        ],
        "\n\n47running": [
            1122085995
        ],
        "kubernetes\ntipwhen": [
            1122085995
        ],
        "\nminikube": [
            1122085995
        ],
        "kubia-http\nif": [
            1122085995
        ],
        "closely": [
            1122085995
        ],
        "reporting": [
            1122085995
        ],
        "behaves": [
            1122085995
        ],
        "machine\nwith": [
            1122085995
        ],
        "appears": [
            1122085995
        ],
        "on\na": [
            1122085995
        ],
        "itself—no": [
            1122085995
        ],
        "running\nalongside": [
            1122085995
        ],
        "it\n2.3.3the": [
            1122085995
        ],
        "system\nuntil": [
            1122085995
        ],
        "honestly": [
            1122085995
        ],
        "single\nmaster": [
            1122085995
        ],
        "hosting": [
            1122085995
        ],
        "plane\nor": [
            1122085995
        ],
        "only\ninteracting": [
            1122085995
        ],
        "endpoint\n": [
            1122085995
        ],
        "besides": [
            1122085995
        ],
        "it\ni’ve": [
            1122085995
        ],
        "be\nexplained": [
            1122085995
        ],
        "and\nwhat": [
            1122085995
        ],
        "little": [
            1122085995
        ],
        "setup\nunderstanding": [
            1122085995
        ],
        "together\nas": [
            1122085995
        ],
        "directly\ninstead": [
            1122085995
        ],
        "told\nkubernetes": [
            1122085995
        ],
        "single\nservice": [
            1122085995
        ],
        "picture": [
            1122085995
        ],
        "27.\nfigure": [
            1122085995
        ],
        "service\npod:": [
            1122085995
        ],
        "kubia-4jfyf\nip:": [
            1122085995
        ],
        "101.0.1\ncontainer\nport\n8080\nservice:": [
            1122085995
        ],
        "kubia-http\ninternal": [
            1122085995
        ],
        "ip:": [
            1122085995
        ],
        "103.246.185\nexternal": [
            1122085995
        ],
        "104155.74.57\nreplicationcontroller:": [
            1122085995
        ],
        "kubia\nreplicas:": [
            1122085995
        ],
        "1\nport\n8080\nincoming\nrequest\n": [
            1122085995
        ],
        "\n\n48chapter": [
            1122085995
        ],
        "want\ninside": [
            1122085995
        ],
        "bound": [
            1122085995
        ],
        "wait-\ning": [
            1122085995
        ],
        "replicationcontroller\nthe": [
            1122085995
        ],
        "always\nexactly": [
            1122085995
        ],
        "used\nto": [
            1122085995
        ],
        "replicate": [
            1122085995
        ],
        "pod)": [
            1122085995
        ],
        "disappear": [
            1122085995
        ],
        "the\nreplicationcontroller": [
            1122085995
        ],
        "why\nyou": [
            1122085995
        ],
        "ephemeral": [
            1122085995
        ],
        "a\npod": [
            1122085995
        ],
        "time—because": [
            1122085995
        ],
        "because\nsomeone": [
            1122085995
        ],
        "deleted": [
            1122085995
        ],
        "evicted": [
            1122085995
        ],
        "healthy\nnode": [
            1122085995
        ],
        "occurs": [
            1122085995
        ],
        "replaced": [
            1122085995
        ],
        "ip\naddress": [
            1122085995
        ],
        "in—to": [
            1122085995
        ],
        "solve": [
            1122085995
        ],
        "prob-\nlem": [
            1122085995
        ],
        "ever-changing": [
            1122085995
        ],
        "addresses": [
            1122085995
        ],
        "single\nconstant": [
            1122085995
        ],
        "connecting": [
            1122085995
        ],
        "service\nthrough": [
            1122085995
        ],
        "receives": [
            1122085995
        ],
        "con-\nnection": [
            1122085995
        ],
        "is)": [
            1122085995
        ],
        "location": [
            1122085995
        ],
        "provide\nthe": [
            1122085995
        ],
        "coming": [
            1122085995
        ],
        "forwarded\nto": [
            1122085995
        ],
        "belonging": [
            1122085995
        ],
        "\n23.4horizontally": [
            1122085995
        ],
        "application\nyou": [
            1122085995
        ],
        "monitored": [
            1122085995
        ],
        "exposed": [
            1122085995
        ],
        "additional\nmagic": [
            1122085995
        ],
        "can\nscale": [
            1122085995
        ],
        "you’ll\nincrease": [
            1122085995
        ],
        "get\ncommand:\n$": [
            1122085995
        ],
        "replicationcontrollers\nname": [
            1122085995
        ],
        "desired": [
            1122085995
        ],
        "age\nkubia": [
            1122085995
        ],
        "17m\n": [
            1122085995
        ],
        "\nwwwallitebooks.com\n\n\n49running": [
            1122085995
        ],
        "column\nshows": [
            1122085995
        ],
        "keep\nwhereas": [
            1122085995
        ],
        "\ncurrent": [
            1122085995
        ],
        "wanted": [
            1122085995
        ],
        "one\nreplica": [
            1122085995
        ],
        "\nincreasing": [
            1122085995
        ],
        "count\nto": [
            1122085995
        ],
        "desired\nreplica": [
            1122085995
        ],
        "--replicas=3\nreplicationcontroller": [
            1122085995
        ],
        "scaled\nyou’ve": [
            1122085995
        ],
        "instruct": [
            1122085995
        ],
        "let\nkubernetes": [
            1122085995
        ],
        "determine": [
            1122085995
        ],
        "actions": [
            1122085995
        ],
        "requested": [
            1122085995
        ],
        "principles": [
            1122085995
        ],
        "telling\nkubernetes": [
            1122085995
        ],
        "changing\nthe": [
            1122085995
        ],
        "actual\nstate": [
            1122085995
        ],
        "reconcile": [
            1122085995
        ],
        "kubernetes\nseeing": [
            1122085995
        ],
        "scale-out\nback": [
            1122085995
        ],
        "see\nthe": [
            1122085995
        ],
        "count:\n$": [
            1122085995
        ],
        "rc\nname": [
            1122085995
        ],
        "17m\nbecause": [
            1122085995
        ],
        "increased": [
            1122085995
        ],
        "(as": [
            1122085995
        ],
        "evident\nfrom": [
            1122085995
        ],
        "column)": [
            1122085995
        ],
        "instead\nof": [
            1122085995
        ],
        "one:\n$": [
            1122085995
        ],
        "age\nkubia-hczji": [
            1122085995
        ],
        "7s\nkubia-iq9y6": [
            1122085995
        ],
        "7s\nkubia-4jfyf": [
            1122085995
        ],
        "18m\nlisting": [
            1122085995
        ],
        "get\nyou’ve": [
            1122085995
        ],
        "cluster\nyou’ve": [
            1122085995
        ],
        "replicationcontroller\nobjects": [
            1122085995
        ],
        "invoking": [
            1122085995
        ],
        "get\nwithout": [
            1122085995
        ],
        "kubectl\ncommands": [
            1122085995
        ],
        "abbreviations\ni": [
            1122085995
        ],
        "earlier\n": [
            1122085995
        ],
        "\n\n50chapter": [
            1122085995
        ],
        "kubernetes\nas": [
            1122085995
        ],
        "still\npending": [
            1122085995
        ],
        "moments": [
            1122085995
        ],
        "is\ndownloaded": [
            1122085995
        ],
        "additional\ninstances": [
            1122085995
        ],
        "copies\nmanually": [
            1122085995
        ],
        "magically": [
            1122085995
        ],
        "scalable;": [
            1122085995
        ],
        "app\nup": [
            1122085995
        ],
        "\nseeing": [
            1122085995
        ],
        "service\nbecause": [
            1122085995
        ],
        "happens\nif": [
            1122085995
        ],
        "not?\n$": [
            1122085995
        ],
        "kubia-hczji\n$": [
            1122085995
        ],
        "kubia-iq9y6\n$": [
            1122085995
        ],
        "randomly": [
            1122085995
        ],
        "do\nwhen": [
            1122085995
        ],
        "backs": [
            1122085995
        ],
        "act": [
            1122085995
        ],
        "standing": [
            1122085995
        ],
        "in\nfront": [
            1122085995
        ],
        "address\nfor": [
            1122085995
        ],
        "backed": [
            1122085995
        ],
        "pods\nthose": [
            1122085995
        ],
        "ip\naddresses": [
            1122085995
        ],
        "it\neasy": [
            1122085995
        ],
        "often\nthey": [
            1122085995
        ],
        "location\nvisualizing": [
            1122085995
        ],
        "system\nlet’s": [
            1122085995
        ],
        "28\nshows": [
            1122085995
        ],
        "single\nreplicationcontroller": [
            1122085995
        ],
        "managed\nby": [
            1122085995
        ],
        "single\npod": [
            1122085995
        ],
        "spreads": [
            1122085995
        ],
        "experiment": [
            1122085995
        ],
        "\ncurl\nin": [
            1122085995
        ],
        "section\n": [
            1122085995
        ],
        "exercise": [
            1122085995
        ],
        "spinning": [
            1122085995
        ],
        "increasing": [
            1122085995
        ],
        "the\nreplicationcontroller’s": [
            1122085995
        ],
        "down\n": [
            1122085995
        ],
        "\n\n51running": [
            1122085995
        ],
        "kubernetes\n23.5examining": [
            1122085995
        ],
        "\nyou": [
            1122085995
        ],
        "gets\nscheduled": [
            1122085995
        ],
        "run\nproperly": [
            1122085995
        ],
        "contain-\ners": [
            1122085995
        ],
        "to\nany": [
            1122085995
        ],
        "node\nor": [
            1122085995
        ],
        "another\ndoesn’t": [
            1122085995
        ],
        "\ndisplaying": [
            1122085995
        ],
        "pods\nif": [
            1122085995
        ],
        "paying": [
            1122085995
        ],
        "close": [
            1122085995
        ],
        "attention": [
            1122085995
        ],
        "pods\ncommand": [
            1122085995
        ],
        "columns": [
            1122085995
        ],
        "display": [
            1122085995
        ],
        "\n-o": [
            1122085995
        ],
        "when\nlisting": [
            1122085995
        ],
        "on:\n$": [
            1122085995
        ],
        "-o": [
            1122085995
        ],
        "wide\nname": [
            1122085995
        ],
        "node\nkubia-hczji": [
            1122085995
        ],
        "7s": [
            1122085995
        ],
        "101.0.2": [
            1122085995
        ],
        "gke-kubia-85..\npod:": [
            1122085995
        ],
        "101.0.1\ncontainer\nservice:": [
            1122085995
        ],
        "104155.74.57\nport\n8080\nincoming\nrequest\npod:": [
            1122085995
        ],
        "kubia-hczji\nip:": [
            1122085995
        ],
        "101.0.2\ncontainer\npod:": [
            1122085995
        ],
        "kubia-iq9y6\nip:": [
            1122085995
        ],
        "101.0.3\ncontainer\nreplicationcontroller:": [
            1122085995
        ],
        "3\nport\n8080\nport\n8080\nport\n8080\nfigure": [
            1122085995
        ],
        "port\n": [
            1122085995
        ],
        "\n\n52chapter": [
            1122085995
        ],
        "kubernetes\ninspecting": [
            1122085995
        ],
        "describe\nyou": [
            1122085995
        ],
        "shows\nmany": [
            1122085995
        ],
        "kubia-hczji\nname:": [
            1122085995
        ],
        "kubia-hczji\nnamespace:": [
            1122085995
        ],
        "default\nnode:": [
            1122085995
        ],
        "gke-kubia-85f6-node-vs9f/10132.0.3": [
            1122085995
        ],
        "\nstart": [
            1122085995
        ],
        "time:": [
            1122085995
        ],
        "fri": [
            1122085995
        ],
        "apr": [
            1122085995
        ],
        "14:12:33": [
            1122085995
        ],
        "+0200\nlabels:": [
            1122085995
        ],
        "run=kubia\nstatus:": [
            1122085995
        ],
        "running\nip:": [
            1122085995
        ],
        "101.0.2\ncontrollers:": [
            1122085995
        ],
        "replicationcontroller/kubia\ncontainers:": [
            1122085995
        ],
        "..\nconditions:\n": [
            1122085995
        ],
        "status\n": [
            1122085995
        ],
        "..\nevents:": [
            1122085995
        ],
        "..\nthis": [
            1122085995
        ],
        "time\nwhen": [
            1122085995
        ],
        "image(s)": [
            1122085995
        ],
        "information\n2.3.6introducing": [
            1122085995
        ],
        "dashboard\nbefore": [
            1122085995
        ],
        "wrap": [
            1122085995
        ],
        "exploring\nyour": [
            1122085995
        ],
        "more\ninto": [
            1122085995
        ],
        "graphical": [
            1122085995
        ],
        "glad": [
            1122085995
        ],
        "comes\nwith": [
            1122085995
        ],
        "(but": [
            1122085995
        ],
        "evolving)": [
            1122085995
        ],
        "dashboard\n": [
            1122085995
        ],
        "and\nother": [
            1122085995
        ],
        "modify": [
            1122085995
        ],
        "delete": [
            1122085995
        ],
        "them\nfigure": [
            1122085995
        ],
        "to\nquickly": [
            1122085995
        ],
        "mod-\nify": [
            1122085995
        ],
        "\nkubectl\naccessing": [
            1122085995
        ],
        "gke\nif": [
            1122085995
        ],
        "dash-\nboard": [
            1122085995
        ],
        "introduced:\n$": [
            1122085995
        ],
        "dashboard\nkubernetes-dashboard": [
            1122085995
        ],
        "https://104155.108.191/api/v1/proxy/\n➥\n": [
            1122085995
        ],
        "namespaces/kube-system/services/kubernetes-dashboard\nlisting": [
            1122085995
        ],
        "describing": [
            1122085995
        ],
        "describe\nhere’s": [
            1122085995
        ],
        "\nhas": [
            1122085995
        ],
        "to\n": [
            1122085995
        ],
        "\n\n53summary\nif": [
            1122085995
        ],
        "browser": [
            1122085995
        ],
        "username": [
            1122085995
        ],
        "password\nprompt": [
            1122085995
        ],
        "password": [
            1122085995
        ],
        "-e": [
            1122085995
        ],
        "(username|password):\"\n": [
            1122085995
        ],
        "password:": [
            1122085995
        ],
        "32nengreej632a12": [
            1122085995
        ],
        "username:": [
            1122085995
        ],
        "admin": [
            1122085995
        ],
        "minikube\nto": [
            1122085995
        ],
        "dashboard\nthe": [
            1122085995
        ],
        "to\nenter": [
            1122085995
        ],
        "credentials": [
            1122085995
        ],
        "it\n2.4summary\nhopefully": [
            1122085995
        ],
        "compli-\ncated": [
            1122085995
        ],
        "depth": [
            1122085995
        ],
        "can\nprovide": [
            1122085995
        ],
        "to\npull": [
            1122085995
        ],
        "image\npackage": [
            1122085995
        ],
        "by\npushing": [
            1122085995
        ],
        "remote": [
            1122085995
        ],
        "registry\nfigure": [
            1122085995
        ],
        "screenshot": [
            1122085995
        ],
        "web-based": [
            1122085995
        ],
        "\n\n54chapter": [
            1122085995
        ],
        "kubernetes\nenter": [
            1122085995
        ],
        "environment\nset": [
            1122085995
        ],
        "engine\nconfigure": [
            1122085995
        ],
        "tool\nlist": [
            1122085995
        ],
        "cluster\nrun": [
            1122085995
        ],
        "cluster\nhave": [
            1122085995
        ],
        "sense": [
            1122085995
        ],
        "to\none": [
            1122085995
        ],
        "another\nscale": [
            1122085995
        ],
        "replicationcontroller’s": [
            1122085995
        ],
        "count\naccess": [
            1122085995
        ],
        "\n\n55\npods:": [
            1122085995
        ],
        "outline": [
            1122085995
        ],
        "we’ll\nstart": [
            1122085995
        ],
        "reviewing": [
            1122085995
        ],
        "resources)": [
            1122085995
        ],
        "so\nyou’ll": [
            1122085995
        ],
        "everything\nelse": [
            1122085995
        ],
        "\nthis": [
            1122085995
        ],
        "stopping": [
            1122085995
        ],
        "pods\norganizing": [
            1122085995
        ],
        "labels\nperforming": [
            1122085995
        ],
        "label\nusing": [
            1122085995
        ],
        "non-\noverlapping": [
            1122085995
        ],
        "groups\nscheduling": [
            1122085995
        ],
        "\nnodes\n": [
            1122085995
        ],
        "\n\n56chapter": [
            1122085995
        ],
        "3pods:": [
            1122085995
        ],
        "kubernetes\n31introducing": [
            1122085995
        ],
        "pods\nyou’ve": [
            1122085995
        ],
        "represents\nthe": [
            1122085995
        ],
        "individually\nyou": [
            1122085995
        ],
        "operate": [
            1122085995
        ],
        "implying": [
            1122085995
        ],
        "pod\nalways": [
            1122085995
        ],
        "container—it’s": [
            1122085995
        ],
        "node—it": [
            1122085995
        ],
        "multiple\nworker": [
            1122085995
        ],
        "31.\n3.1.1understanding": [
            1122085995
        ],
        "pods\nbut": [
            1122085995
        ],
        "pods?": [
            1122085995
        ],
        "directly?": [
            1122085995
        ],
        "we\neven": [
            1122085995
        ],
        "together?": [
            1122085995
        ],
        "answer": [
            1122085995
        ],
        "now\nunderstanding": [
            1122085995
        ],
        "\nmultiple": [
            1122085995
        ],
        "processes\nimagine": [
            1122085995
        ],
        "consisting": [
            1122085995
        ],
        "through\nipc": [
            1122085995
        ],
        "(inter-process": [
            1122085995
        ],
        "communication)": [
            1122085995
        ],
        "requires\nthem": [
            1122085995
        ],
        "in\ncontainers": [
            1122085995
        ],
        "it\nmakes": [
            1122085995
        ],
        "designed": [
            1122085995
        ],
        "(unless": [
            1122085995
        ],
        "spawns": [
            1122085995
        ],
        "child": [
            1122085995
        ],
        "unrelated": [
            1122085995
        ],
        "man-\nage": [
            1122085995
        ],
        "mechanism": [
            1122085995
        ],
        "crash": [
            1122085995
        ],
        "would\nlog": [
            1122085995
        ],
        "101.0.1\ncontainer\nnode": [
            1122085995
        ],
        "2\nfigure": [
            1122085995
        ],
        "\n\n57introducing": [
            1122085995
        ],
        "pods\n": [
            1122085995
        ],
        "\n31.2understanding": [
            1122085995
        ],
        "supposed": [
            1122085995
        ],
        "it’s\nobvious": [
            1122085995
        ],
        "higher-level": [
            1122085995
        ],
        "construct": [
            1122085995
        ],
        "bind": [
            1122085995
        ],
        "containers\ntogether": [
            1122085995
        ],
        "reasoning": [
            1122085995
        ],
        "pro-\nvide": [
            1122085995
        ],
        "single\ncontainer": [
            1122085995
        ],
        "somewhat": [
            1122085995
        ],
        "both\nworlds": [
            1122085995
        ],
        "giving": [
            1122085995
        ],
        "illusion": [
            1122085995
        ],
        "partial": [
            1122085995
        ],
        "pod\nin": [
            1122085995
        ],
        "from\neach": [
            1122085995
        ],
        "of\nindividual": [
            1122085995
        ],
        "resources\nalthough": [
            1122085995
        ],
        "config-\nuring": [
            1122085995
        ],
        "namespaces\ninstead": [
            1122085995
        ],
        "namespaces\n(we’re": [
            1122085995
        ],
        "here)": [
            1122085995
        ],
        "and\nnetwork": [
            1122085995
        ],
        "namespace\nand": [
            1122085995
        ],
        "they\ncan": [
            1122085995
        ],
        "feature": [
            1122085995
        ],
        "enabled": [
            1122085995
        ],
        "\nnotewhen": [
            1122085995
        ],
        "you\nonly": [
            1122085995
        ],
        "\nps": [
            1122085995
        ],
        "container\nbut": [
            1122085995
        ],
        "the\ncontainer’s": [
            1122085995
        ],
        "have\nthem": [
            1122085995
        ],
        "we’ll\ntalk": [
            1122085995
        ],
        "6\nunderstanding": [
            1122085995
        ],
        "space\none": [
            1122085995
        ],
        "stress": [
            1122085995
        ],
        "network\nnamespace": [
            1122085995
        ],
        "port\nnumbers": [
            1122085995
        ],
        "they’ll": [
            1122085995
        ],
        "conflicts": [
            1122085995
        ],
        "concerns": [
            1122085995
        ],
        "same\nloopback": [
            1122085995
        ],
        "localhost\n": [
            1122085995
        ],
        "\n\n58chapter": [
            1122085995
        ],
        "kubernetes\nintroducing": [
            1122085995
        ],
        "network\nall": [
            1122085995
        ],
        "reside": [
            1122085995
        ],
        "network-address": [
            1122085995
        ],
        "space\n(shown": [
            1122085995
        ],
        "32)": [
            1122085995
        ],
        "other\npod’s": [
            1122085995
        ],
        "nat": [
            1122085995
        ],
        "(network": [
            1122085995
        ],
        "translation)": [
            1122085995
        ],
        "gateways": [
            1122085995
        ],
        "them\nwhen": [
            1122085995
        ],
        "packets": [
            1122085995
        ],
        "actual\nip": [
            1122085995
        ],
        "packet\nconsequently": [
            1122085995
        ],
        "nodes;": [
            1122085995
        ],
        "the\ncontainers": [
            1122085995
        ],
        "nat-\nless": [
            1122085995
        ],
        "area": [
            1122085995
        ],
        "(lan)": [
            1122085995
        ],
        "inter-node": [
            1122085995
        ],
        "topology": [
            1122085995
        ],
        "lan": [
            1122085995
        ],
        "own\nip": [
            1122085995
        ],
        "established": [
            1122085995
        ],
        "spe-\ncifically": [
            1122085995
        ],
        "software-defined": [
            1122085995
        ],
        "layered": [
            1122085995
        ],
        "network\n": [
            1122085995
        ],
        "sum": [
            1122085995
        ],
        "section:": [
            1122085995
        ],
        "behave\nmuch": [
            1122085995
        ],
        "non-container": [
            1122085995
        ],
        "except\nthat": [
            1122085995
        ],
        "encapsulated": [
            1122085995
        ],
        "\n31.3organizing": [
            1122085995
        ],
        "properly\nyou": [
            1122085995
        ],
        "cram": [
            1122085995
        ],
        "sorts": [
            1122085995
        ],
        "same\nhost": [
            1122085995
        ],
        "have\nas": [
            1122085995
        ],
        "incurring": [
            1122085995
        ],
        "almost": [
            1122085995,
            1118639836
        ],
        "stuffing": [
            1122085995
        ],
        "every-\nthing": [
            1122085995
        ],
        "one\ncontains": [
            1122085995
        ],
        "processes\nnode": [
            1122085995
        ],
        "a\nip:": [
            1122085995
        ],
        "101.1.6\ncontainer": [
            1122085995
        ],
        "b\nip:": [
            1122085995
        ],
        "101.1.7\ncontainer": [
            1122085995
        ],
        "2\nnode": [
            1122085995
        ],
        "2\nflat": [
            1122085995
        ],
        "network\npod": [
            1122085995
        ],
        "c\nip:": [
            1122085995
        ],
        "101.2.5\ncontainer": [
            1122085995
        ],
        "d\nip:": [
            1122085995
        ],
        "101.2.7\ncontainer": [
            1122085995
        ],
        "routable": [
            1122085995
        ],
        "address\n": [
            1122085995
        ],
        "\n\n59introducing": [
            1122085995
        ],
        "multi-tier": [
            1122085995
        ],
        "frontend\napplication": [
            1122085995
        ],
        "backend": [
            1122085995
        ],
        "database": [
            1122085995
        ],
        "as\ntwo": [
            1122085995
        ],
        "pods?\nsplitting": [
            1122085995
        ],
        "pods\nalthough": [
            1122085995
        ],
        "frontend": [
            1122085995
        ],
        "the\ndatabase": [
            1122085995
        ],
        "we’ve\nsaid": [
            1122085995
        ],
        "server\nand": [
            1122085995
        ],
        "machine?": [
            1122085995
        ],
        "no\nso": [
            1122085995
        ],
        "regardless?": [
            1122085995
        ],
        "be\nrun": [
            1122085995
        ],
        "two-node": [
            1122085995
        ],
        "the\ncomputational": [
            1122085995
        ],
        "memory)": [
            1122085995
        ],
        "disposal": [
            1122085995
        ],
        "second\nnode": [
            1122085995
        ],
        "of\nyour": [
            1122085995
        ],
        "infrastructure\nsplitting": [
            1122085995
        ],
        "scaling\nanother": [
            1122085995
        ],
        "is\nalso": [
            1122085995
        ],
        "contain-\ners;": [
            1122085995
        ],
        "scales": [
            1122085995
        ],
        "end\nup": [
            1122085995
        ],
        "requirements\nthan": [
            1122085995
        ],
        "backends": [
            1122085995
        ],
        "that\nbackends": [
            1122085995
        ],
        "(stateless)\nfrontend": [
            1122085995
        ],
        "indi-\ncation": [
            1122085995
        ],
        "pod\nthe": [
            1122085995
        ],
        "application\nconsists": [
            1122085995
        ],
        "complementary": [
            1122085995
        ],
        "in\nfigure": [
            1122085995
        ],
        "33.\npod\nmain": [
            1122085995
        ],
        "container\nsupporting\ncontainer": [
            1122085995
        ],
        "1\nsupporting\ncontainer": [
            1122085995
        ],
        "2\nvolume\nfigure": [
            1122085995
        ],
        "\ncontainers": [
            1122085995
        ],
        "one\n": [
            1122085995
        ],
        "\n\n60chapter": [
            1122085995
        ],
        "kubernetes\nfor": [
            1122085995
        ],
        "serves": [
            1122085995
        ],
        "sidecar": [
            1122085995
        ],
        "container)": [
            1122085995
        ],
        "periodi-\ncally": [
            1122085995
        ],
        "downloads": [
            1122085995
        ],
        "content": [
            1122085995
        ],
        "server’s\ndirectory": [
            1122085995
        ],
        "you\nmount": [
            1122085995
        ],
        "rotators": [
            1122085995
        ],
        "collectors": [
            1122085995
        ],
        "pro-\ncessors": [
            1122085995
        ],
        "adapters": [
            1122085995
        ],
        "others\ndeciding": [
            1122085995
        ],
        "pod\nto": [
            1122085995
        ],
        "recap": [
            1122085995
        ],
        "pods—when": [
            1122085995
        ],
        "to\nput": [
            1122085995
        ],
        "ask\nyourself": [
            1122085995
        ],
        "questions:\ndo": [
            1122085995
        ],
        "hosts?\ndo": [
            1122085995
        ],
        "components?\nmust": [
            1122085995
        ],
        "individually?": [
            1122085995
        ],
        "\nbasically": [
            1122085995
        ],
        "gravitate": [
            1122085995
        ],
        "pods\nunless": [
            1122085995
        ],
        "help\nyou": [
            1122085995
        ],
        "memorize": [
            1122085995
        ],
        "this\nalthough": [
            1122085995
        ],
        "you’ll\nonly": [
            1122085995
        ],
        "single-container": [
            1122085995
        ],
        "multiple\ncontainers": [
            1122085995
        ],
        "\npod\nfrontend\nprocess\nbackend\nprocess\ncontainer\npod\nfrontend\nprocess\nfrontend\ncontainer\nfrontend": [
            1122085995
        ],
        "pod\nfrontend\nprocess\nfrontend\ncontainer\nbackend": [
            1122085995
        ],
        "pod\nbackend\nprocess\nbackend\ncontainer\nbackend\nprocess\nbackend\ncontainer\nfigure": [
            1122085995
        ],
        "\n\n61creating": [
            1122085995
        ],
        "descriptors\n32creating": [
            1122085995
        ],
        "descriptors\npods": [
            1122085995
        ],
        "posting": [
            1122085995
        ],
        "yaml\nmanifest": [
            1122085995
        ],
        "endpoint": [
            1122085995
        ],
        "ways\nof": [
            1122085995
        ],
        "previous\nchapter": [
            1122085995
        ],
        "not\nall": [
            1122085995
        ],
        "brings\n": [
            1122085995
        ],
        "aspects": [
            1122085995
        ],
        "under-\nstand": [
            1122085995
        ],
        "definitions": [
            1122085995
        ],
        "you\nlearn": [
            1122085995
        ],
        "single\nproperty": [
            1122085995
        ],
        "reference": [
            1122085995
        ],
        "at\nhttp://kubernetesio/docs/reference/": [
            1122085995
        ],
        "objects\n3.2.1examining": [
            1122085995
        ],
        "pod\nyou": [
            1122085995
        ],
        "look\nat": [
            1122085995
        ],
        "get\ncommand": [
            1122085995
        ],
        "po": [
            1122085995
        ],
        "kubia-zxzij": [
            1122085995
        ],
        "yaml\napiversion:": [
            1122085995
        ],
        "v1": [
            1122085995
        ],
        "\nkind:": [
            1122085995
        ],
        "\nmetadata:": [
            1122085995
        ],
        "annotations:": [
            1122085995
        ],
        "kubernetesio/created-by:": [
            1122085995
        ],
        "creationtimestamp:": [
            1122085995
        ],
        "2016-03-18t12:37:50z": [
            1122085995
        ],
        "generatename:": [
            1122085995
        ],
        "kubia-": [
            1122085995
        ],
        "labels:": [
            1122085995
        ],
        "run:": [
            1122085995
        ],
        "name:": [
            1122085995
        ],
        "namespace:": [
            1122085995
        ],
        "resourceversion:": [
            1122085995
        ],
        "294\"": [
            1122085995
        ],
        "selflink:": [
            1122085995
        ],
        "/api/v1/namespaces/default/pods/kubia-zxzij": [
            1122085995
        ],
        "uid:": [
            1122085995
        ],
        "3a564dc0-ed06-11e5-ba3b-42010af00004": [
            1122085995
        ],
        "\nspec:": [
            1122085995
        ],
        "containers:": [
            1122085995
        ],
        "-": [
            1122085995
        ],
        "image:": [
            1122085995
        ],
        "imagepullpolicy:": [
            1122085995
        ],
        "ifnotpresent": [
            1122085995
        ],
        "ports:": [
            1122085995
        ],
        "containerport:": [
            1122085995
        ],
        "protocol:": [
            1122085995
        ],
        "tcp": [
            1122085995
        ],
        "resources:": [
            1122085995
        ],
        "requests:": [
            1122085995
        ],
        "cpu:": [
            1122085995
        ],
        "100m": [
            1122085995
        ],
        "pod\nkubernetes": [
            1122085995
        ],
        "descriptor\ntype": [
            1122085995
        ],
        "\nobject/resource\npod": [
            1122085995
        ],
        "(name": [
            1122085995
        ],
        "\nlabels": [
            1122085995
        ],
        "on)\npod": [
            1122085995
        ],
        "specification/\ncontents": [
            1122085995
        ],
        "(list": [
            1122085995
        ],
        "\npod’s": [
            1122085995
        ],
        "\nvolumes": [
            1122085995
        ],
        "on)\n": [
            1122085995
        ],
        "\n\n62chapter": [
            1122085995
        ],
        "terminationmessagepath:": [
            1122085995
        ],
        "/dev/termination-log": [
            1122085995
        ],
        "volumemounts:": [
            1122085995
        ],
        "mountpath:": [
            1122085995
        ],
        "/var/run/secrets/k8sio/servacc": [
            1122085995
        ],
        "default-token-kvcqa": [
            1122085995
        ],
        "readonly:": [
            1122085995
        ],
        "dnspolicy:": [
            1122085995
        ],
        "clusterfirst": [
            1122085995
        ],
        "nodename:": [
            1122085995
        ],
        "gke-kubia-e8fe08b8-node-txje": [
            1122085995
        ],
        "restartpolicy:": [
            1122085995
        ],
        "serviceaccount:": [
            1122085995
        ],
        "serviceaccountname:": [
            1122085995
        ],
        "terminationgraceperiodseconds:": [
            1122085995
        ],
        "volumes:": [
            1122085995
        ],
        "secret:": [
            1122085995
        ],
        "secretname:": [
            1122085995
        ],
        "\nstatus:": [
            1122085995
        ],
        "conditions:": [
            1122085995
        ],
        "lastprobetime:": [
            1122085995
        ],
        "null": [
            1122085995
        ],
        "lasttransitiontime:": [
            1122085995
        ],
        "status:": [
            1122085995
        ],
        "true\"": [
            1122085995
        ],
        "type:": [
            1122085995
        ],
        "containerstatuses:": [
            1122085995
        ],
        "containerid:": [
            1122085995
        ],
        "docker://f0276994322d247ba..": [
            1122085995
        ],
        "imageid:": [
            1122085995
        ],
        "docker://4c325bcc6b40c110226b89fe..": [
            1122085995
        ],
        "laststate:": [
            1122085995
        ],
        "{}": [
            1122085995
        ],
        "ready:": [
            1122085995
        ],
        "restartcount:": [
            1122085995
        ],
        "state:": [
            1122085995
        ],
        "running:": [
            1122085995
        ],
        "startedat:": [
            1122085995
        ],
        "2016-03-18t12:46:05z": [
            1122085995
        ],
        "hostip:": [
            1122085995
        ],
        "10132.0.4": [
            1122085995
        ],
        "phase:": [
            1122085995
        ],
        "podip:": [
            1122085995
        ],
        "100.2.3": [
            1122085995
        ],
        "starttime:": [
            1122085995
        ],
        "2016-03-18t12:44:32z": [
            1122085995
        ],
        "\ni": [
            1122085995
        ],
        "basics\nand": [
            1122085995
        ],
        "minor": [
            1122085995
        ],
        "also\nyou": [
            1122085995
        ],
        "comfort": [
            1122085995
        ],
        "to\nwrite": [
            1122085995
        ],
        "shorter": [
            1122085995
        ],
        "later\nintroducing": [
            1122085995
        ],
        "definition\nthe": [
            1122085995
        ],
        "version\nused": [
            1122085995
        ],
        "three\nimportant": [
            1122085995
        ],
        "resources:\nmetadata": [
            1122085995
        ],
        "about\nthe": [
            1122085995
        ],
        "pod\nspec": [
            1122085995
        ],
        "data\npod": [
            1122085995
        ],
        "on)\ndetailed": [
            1122085995
        ],
        "\nits": [
            1122085995
        ],
        "\n\n63creating": [
            1122085995
        ],
        "descriptors\nstatus": [
            1122085995
        ],
        "what\ncondition": [
            1122085995
        ],
        "the\npod’s": [
            1122085995
        ],
        "info\nlisting": [
            1122085995
        ],
        "showed": [
            1122085995
        ],
        "\nstatus\npart": [
            1122085995
        ],
        "given\nmoment": [
            1122085995
        ],
        "\nstatus": [
            1122085995
        ],
        "typical": [
            1122085995
        ],
        "kubernetes\napi": [
            1122085995
        ],
        "anat-\nomy": [
            1122085995
        ],
        "easy\n": [
            1122085995
        ],
        "make\nmuch": [
            1122085995
        ],
        "looks\nlike": [
            1122085995
        ],
        "\n32.2creating": [
            1122085995
        ],
        "pod\nyou’re": [
            1122085995
        ],
        "kubia-manualyaml": [
            1122085995
        ],
        "any\ndirectory": [
            1122085995
        ],
        "want)": [
            1122085995
        ],
        "archive": [
            1122085995
        ],
        "the\nfile": [
            1122085995
        ],
        "chapter03": [
            1122085995
        ],
        "entire": [
            1122085995
        ],
        "contents\nof": [
            1122085995
        ],
        "file\napiversion:": [
            1122085995
        ],
        "kubia-manual": [
            1122085995
        ],
        "tcp\ni’m": [
            1122085995
        ],
        "31.": [
            1122085995
        ],
        "conforms": [
            1122085995
        ],
        "\nv1": [
            1122085995
        ],
        "the\ntype": [
            1122085995
        ],
        "pod\nconsists": [
            1122085995
        ],
        "\nluksa/kubia": [
            1122085995
        ],
        "a\nname": [
            1122085995
        ],
        "indicated": [
            1122085995,
            2119133144
        ],
        "\n8080": [
            1122085995
        ],
        "\nspecifying": [
            1122085995
        ],
        "ports\nspecifying": [
            1122085995
        ],
        "purely": [
            1122085995
        ],
        "informational": [
            1122085995
        ],
        "no\neffect": [
            1122085995
        ],
        "con-\nlisting": [
            1122085995
        ],
        "manifest:": [
            1122085995
        ],
        "kubia-manualyaml\ndescriptor": [
            1122085995
        ],
        "conforms\nto": [
            1122085995
        ],
        "api\nyou’re": [
            1122085995
        ],
        "\ndescribing": [
            1122085995
        ],
        "pod\ncontainer": [
            1122085995
        ],
        "from\nname": [
            1122085995
        ],
        "\nis": [
            1122085995
        ],
        "on\n": [
            1122085995
        ],
        "\n\n64chapter": [
            1122085995
        ],
        "kubernetes\ntainer": [
            1122085995
        ],
        "00.0.0": [
            1122085995
        ],
        "other\npods": [
            1122085995
        ],
        "spec": [
            1122085995
        ],
        "but\nit": [
            1122085995
        ],
        "everyone": [
            1122085995
        ],
        "can\nquickly": [
            1122085995
        ],
        "to\nassign": [
            1122085995
        ],
        "book\nusing": [
            1122085995
        ],
        "fields\nwhen": [
            1122085995
        ],
        "preparing": [
            1122085995
        ],
        "reference\ndocumentation": [
            1122085995
        ],
        "http://kubernetesio/docs/api": [
            1122085995
        ],
        "attributes": [
            1122085995
        ],
        "are\nsupported": [
            1122085995
        ],
        "command\nfor": [
            1122085995
        ],
        "scratch": [
            1122085995
        ],
        "asking\nkubectl": [
            1122085995
        ],
        "pods:\n$": [
            1122085995
        ],
        "pods\ndescription:\npod": [
            1122085995
        ],
        "hosts\nfields:\n": [
            1122085995
        ],
        "<string>\n": [
            1122085995
        ],
        "string": [
            1122085995
        ],
        "object\n": [
            1122085995
        ],
        "represents..\n": [
            1122085995
        ],
        "<object>\n": [
            1122085995
        ],
        "metadata..\n": [
            1122085995
        ],
        "specification": [
            1122085995
        ],
        "behavior": [
            1122085995
        ],
        "pod..\n": [
            1122085995
        ],
        "observed": [
            1122085995
        ],
        "date..\nkubectl": [
            1122085995
        ],
        "object\ncan": [
            1122085995
        ],
        "drill": [
            1122085995
        ],
        "attribute": [
            1122085995
        ],
        "\nspec": [
            1122085995
        ],
        "podspec\nresource:": [
            1122085995
        ],
        "<object>\ndescription:\n": [
            1122085995
        ],
        "podspec": [
            1122085995
        ],
        "pod\nfields:\n": [
            1122085995
        ],
        "hostpid": [
            1122085995
        ],
        "<boolean>\n": [
            1122085995
        ],
        "optional:": [
            1122085995
        ],
        "false\n": [
            1122085995
        ],
        "..\n": [
            1122085995
        ],
        "<[]object>\n": [
            1122085995
        ],
        "pod\n": [
            1122085995
        ],
        "\n\n65creating": [
            1122085995
        ],
        "descriptors\n32.3using": [
            1122085995
        ],
        "-f": [
            1122085995
        ],
        "kubia-manualyaml\npod": [
            1122085995
        ],
        "kubia-manual\"": [
            1122085995
        ],
        "pods)\nfrom": [
            1122085995
        ],
        "\nretrieving": [
            1122085995
        ],
        "pod\nafter": [
            1122085995
        ],
        "fields\nappearing": [
            1122085995
        ],
        "ahead": [
            1122085995
        ],
        "pod:\n$": [
            1122085995
        ],
        "yaml\nif": [
            1122085995
        ],
        "return": [
            1122085995
        ],
        "yaml\nlike": [
            1122085995
        ],
        "pod):\n$": [
            1122085995
        ],
        "json\nseeing": [
            1122085995
        ],
        "pods\nyour": [
            1122085995
        ],
        "running?": [
            1122085995
        ],
        "see\ntheir": [
            1122085995
        ],
        "statuses:\n$": [
            1122085995
        ],
        "age\nkubia-manual": [
            1122085995
        ],
        "32s\nkubia-zxzij": [
            1122085995
        ],
        "1d": [
            1122085995
        ],
        "\nthere’s": [
            1122085995
        ],
        "me\nyou’ll": [
            1122085995
        ],
        "confirm": [
            1122085995
        ],
        "a\nminute": [
            1122085995
        ],
        "errors\n3.2.4viewing": [
            1122085995
        ],
        "logs\nyour": [
            1122085995
        ],
        "process’s": [
            1122085995
        ],
        "containerized\napplications": [
            1122085995
        ],
        "error": [
            1122085995
        ],
        "stream": [
            1122085995
        ],
        "of\n": [
            1122085995
        ],
        "<[]object>": [
            1122085995
        ],
        "-required-\n": [
            1122085995
        ],
        "currently\n": [
            1122085995
        ],
        "added": [
            1122085995
        ],
        "info:\n": [
            1122085995
        ],
        "http://releasesk8s.io/release-1.4/docs/user-guide/containers.md\n": [
            1122085995
        ],
        "\n\n66chapter": [
            1122085995
        ],
        "kubernetes\nwriting": [
            1122085995
        ],
        "case)": [
            1122085995
        ],
        "redirects": [
            1122085995
        ],
        "streams": [
            1122085995
        ],
        "and\nallows": [
            1122085995
        ],
        "running\n$": [
            1122085995
        ],
        "<container": [
            1122085995
        ],
        "id>\nyou": [
            1122085995
        ],
        "retrieve": [
            1122085995
        ],
        "logs\nwith": [
            1122085995
        ],
        "logs\nto": [
            1122085995
        ],
        "(more": [
            1122085995
        ],
        "precisely": [
            1122085995
        ],
        "log)": [
            1122085995
        ],
        "(no": [
            1122085995
        ],
        "\nssh": [
            1122085995
        ],
        "anywhere):\n$": [
            1122085995
        ],
        "kubia-manual\nkubia": [
            1122085995
        ],
        "starting..\nyou": [
            1122085995
        ],
        "sent": [
            1122085995
        ],
        "single\nlog": [
            1122085995
        ],
        "statement": [
            1122085995
        ],
        "\nnotecontainer": [
            1122085995
        ],
        "rotated": [
            1122085995
        ],
        "daily": [
            1122085995
        ],
        "file\nreaches": [
            1122085995
        ],
        "10mb": [
            1122085995
        ],
        "entries\nfrom": [
            1122085995
        ],
        "rotation\nspecifying": [
            1122085995
        ],
        "multi-container": [
            1122085995
        ],
        "pod\nif": [
            1122085995
        ],
        "container\nname": [
            1122085995
        ],
        "\n-c": [
            1122085995
        ],
        "name>": [
            1122085995
        ],
        "\nkubia-manual": [
            1122085995
        ],
        "kubia\nkubia": [
            1122085995
        ],
        "starting..\nnote": [
            1122085995
        ],
        "existence": [
            1122085995
        ],
        "when\na": [
            1122085995
        ],
        "centralized": [
            1122085995
        ],
        "cluster-wide": [
            1122085995
        ],
        "works\n3.2.5sending": [
            1122085995
        ],
        "running—at": [
            1122085995
        ],
        "but\nhow": [
            1122085995
        ],
        "action?": [
            1122085995
        ],
        "expose\ncommand": [
            1122085995
        ],
        "to\ndo": [
            1122085995
        ],
        "purposes": [
            1122085995
        ],
        "through\nport": [
            1122085995
        ],
        "forwarding\n": [
            1122085995
        ],
        "\n\n67organizing": [
            1122085995
        ],
        "labels\nforwarding": [
            1122085995
        ],
        "pod\nwhen": [
            1122085995
        ],
        "debug-\nging": [
            1122085995
        ],
        "reasons)": [
            1122085995
        ],
        "forwarding": [
            1122085995
        ],
        "port-forward": [
            1122085995
        ],
        "following\ncommand": [
            1122085995
        ],
        "forward": [
            1122085995
        ],
        "machine’s": [
            1122085995
        ],
        "\n8888": [
            1122085995
        ],
        "kubia-\nmanual\n": [
            1122085995
        ],
        "8888:8080\n..": [
            1122085995
        ],
        "1270.0.1:8888": [
            1122085995
        ],
        "->": [
            1122085995
        ],
        "8080\n..": [
            1122085995
        ],
        "[::1]:8888": [
            1122085995
        ],
        "8080\nthe": [
            1122085995
        ],
        "forwarder": [
            1122085995
        ],
        "the\nlocal": [
            1122085995
        ],
        "\nconnecting": [
            1122085995
        ],
        "forwarder\nin": [
            1122085995
        ],
        "pod\nthrough": [
            1122085995
        ],
        "localhost:8888:\n$": [
            1122085995
        ],
        "localhost:8888\nyou’ve": [
            1122085995
        ],
        "kubia-manual\nfigure": [
            1122085995
        ],
        "overly": [
            1122085995
        ],
        "request\nin": [
            1122085995
        ],
        "reality": [
            1122085995
        ],
        "sit": [
            1122085995
        ],
        "relevant": [
            1122085995
        ],
        "now\nusing": [
            1122085995
        ],
        "you’ll\nlearn": [
            1122085995
        ],
        "\n33organizing": [
            1122085995
        ],
        "labels\nat": [
            1122085995
        ],
        "actual\napplications": [
            1122085995
        ],
        "of\npods": [
            1122085995
        ],
        "and\nmore": [
            1122085995
        ],
        "evident\n": [
            1122085995
        ],
        "microser-\nvices": [
            1122085995
        ],
        "exceed": [
            1122085995
        ],
        "replicated\nkubernetes": [
            1122085995
        ],
        "cluster\nport\n8080\nlocal": [
            1122085995
        ],
        "machine\nkubectl\nport-forward\nprocess\ncurl\nport\n8888\npod:\nkubia-manual\nfigure": [
            1122085995
        ],
        "port-forward\n": [
            1122085995
        ],
        "\n\n68chapter": [
            1122085995
        ],
        "kubernetes\n(multiple": [
            1122085995
        ],
        "deployed)": [
            1122085995
        ],
        "or\nreleases": [
            1122085995
        ],
        "(stable": [
            1122085995
        ],
        "beta": [
            1122085995
        ],
        "canary": [
            1122085995
        ],
        "concurrently": [
            1122085995
        ],
        "hun-\ndreds": [
            1122085995
        ],
        "up\nwith": [
            1122085995
        ],
        "incomprehensible": [
            1122085995
        ],
        "mess": [
            1122085995
        ],
        "36.": [
            1122085995
        ],
        "figure\nshows": [
            1122085995
        ],
        "others\nrunning": [
            1122085995
        ],
        "releases": [
            1122085995
        ],
        "microservice\nit’s": [
            1122085995
        ],
        "evident": [
            1122085995
        ],
        "arbitrary\ncriteria": [
            1122085995
        ],
        "eas-\nily": [
            1122085995
        ],
        "pod\nindividually": [
            1122085995
        ],
        "labels\n3.3.1introducing": [
            1122085995
        ],
        "labels\nlabels": [
            1122085995
        ],
        "powerful": [
            1122085995
        ],
        "not\nonly": [
            1122085995
        ],
        "arbitrary": [
            1122085995
        ],
        "you\nattach": [
            1122085995
        ],
        "selectors\n(resources": [
            1122085995
        ],
        "filtered": [
            1122085995
        ],
        "selec-\ntor)": [
            1122085995
        ],
        "keys": [
            1122085995
        ],
        "are\nunique": [
            1122085995
        ],
        "attach": [
            1122085995
        ],
        "create\nthem": [
            1122085995
        ],
        "values": [
            1122085995
        ],
        "existing\nlabels": [
            1122085995
        ],
        "recreate": [
            1122085995
        ],
        "\nui": [
            1122085995
        ],
        "pod\nui": [
            1122085995
        ],
        "pod\naccount\nservice\npod\nproduct\ncatalog\npod\nproduct\ncatalog\npod\nproduct\ncatalog\npod\nshopping\ncart\npod\nshopping\ncart\npod\norder\nservice\npod\nui": [
            1122085995
        ],
        "pod\nproduct\ncatalog\npod\nproduct\ncatalog\npod\norder\nservice\npod\naccount\nservice\npod\nproduct\ncatalog\npod\nproduct\ncatalog\npod\norder\nservice\npod\nfigure": [
            1122085995
        ],
        "uncategorized": [
            1122085995
        ],
        "architecture\n": [
            1122085995
        ],
        "\n\n69organizing": [
            1122085995
        ],
        "labels\n": [
            1122085995
        ],
        "to\nthose": [
            1122085995
        ],
        "much-better-organized": [
            1122085995
        ],
        "make\nsense": [
            1122085995
        ],
        "labeled": [
            1122085995
        ],
        "labels:\napp": [
            1122085995
        ],
        "\nrel": [
            1122085995
        ],
        "beta\nor": [
            1122085995
        ],
        "release\ndefinitiona": [
            1122085995
        ],
        "fraction": [
            1122085995
        ],
        "the\nnew": [
            1122085995
        ],
        "pre-\nvents": [
            1122085995
        ],
        "users\nby": [
            1122085995
        ],
        "essentially": [
            1122085995
        ],
        "organized": [
            1122085995
        ],
        "dimen-\nsions": [
            1122085995
        ],
        "(horizontally": [
            1122085995
        ],
        "release)": [
            1122085995
        ],
        "37.\nevery": [
            1122085995
        ],
        "sys-\ntem’s": [
            1122085995
        ],
        "fits": [
            1122085995
        ],
        "looking": [
            1122085995
        ],
        "labels\n3.3.2specifying": [
            1122085995
        ],
        "pod\nnow": [
            1122085995
        ],
        "file\ncalled": [
            1122085995
        ],
        "kubia-manual-with-labelsyaml": [
            1122085995
        ],
        "listing\napiversion:": [
            1122085995
        ],
        "kubia-manual-v2\nlisting": [
            1122085995
        ],
        "kubia-manual-with-labelsyaml\nui": [
            1122085995
        ],
        "pod\napp:": [
            1122085995
        ],
        "ui\nrel:": [
            1122085995
        ],
        "stable\nrel=stable\napp=ui\naccount\nservice\npod\napp:": [
            1122085995
        ],
        "as\nrel:": [
            1122085995
        ],
        "stable\napp=as\napp:": [
            1122085995
        ],
        "pc\nrel:": [
            1122085995
        ],
        "stable\napp=pc\napp:": [
            1122085995
        ],
        "sc\nrel:": [
            1122085995
        ],
        "stable\napp=sc\napp:": [
            1122085995
        ],
        "os\nrel:": [
            1122085995
        ],
        "stable\napp=os\nproduct\ncatalog\npod\nshopping\ncart\npod\norder\nservice\npod\nui": [
            1122085995
        ],
        "beta\nrel=beta\napp:": [
            1122085995
        ],
        "beta\napp:": [
            1122085995
        ],
        "beta\nproduct\ncatalog\npod\norder\nservice\npod\nrel=canary\naccount\nservice\npod\napp:": [
            1122085995
        ],
        "canary\napp:": [
            1122085995
        ],
        "canary\nproduct\ncatalog\npod\norder\nservice\npod\nfigure": [
            1122085995
        ],
        "\n\n70chapter": [
            1122085995
        ],
        "creation_method:": [
            1122085995
        ],
        "env:": [
            1122085995
        ],
        "prod": [
            1122085995
        ],
        "luksa/kubia\n": [
            1122085995
        ],
        "kubia\n": [
            1122085995
        ],
        "8080\n": [
            1122085995
        ],
        "tcp\nyou’ve": [
            1122085995
        ],
        "included": [
            1122085995
        ],
        "creation_method=manual": [
            1122085995
        ],
        "env=datalabels": [
            1122085995
        ],
        "section\nyou’ll": [
            1122085995
        ],
        "now:\n$": [
            1122085995
        ],
        "kubia-manual-with-labelsyaml\npod": [
            1122085995
        ],
        "kubia-manual-v2\"": [
            1122085995
        ],
        "see\nthem": [
            1122085995
        ],
        "\n--show-labels": [
            1122085995
        ],
        "switch:\n$": [
            1122085995
        ],
        "--show-labels\nname": [
            1122085995
        ],
        "labels\nkubia-manual": [
            1122085995
        ],
        "16m": [
            1122085995
        ],
        "<none>\nkubia-manual-v2": [
            1122085995
        ],
        "2m": [
            1122085995
        ],
        "creat_method=manualenv=prod\nkubia-zxzij": [
            1122085995
        ],
        "run=kubia\ninstead": [
            1122085995
        ],
        "specify\nthem": [
            1122085995
        ],
        "\n-l": [
            1122085995
        ],
        "again\nand": [
            1122085995
        ],
        "attached": [
            1122085995
        ],
        "\nkubia-manual-v2": [
            1122085995
        ],
        "-l": [
            1122085995
        ],
        "creation_methodenv\nname": [
            1122085995
        ],
        "creation_method": [
            1122085995
        ],
        "env\nkubia-manual": [
            1122085995
        ],
        "prod\nkubia-zxzij": [
            1122085995
        ],
        "<none>\n33.3modifying": [
            1122085995
        ],
        "pods\nlabels": [
            1122085995
        ],
        "modified": [
            1122085995
        ],
        "kubia-man-\nual\n": [
            1122085995
        ],
        "it:": [
            1122085995
        ],
        "creation_method=manual\npod": [
            1122085995
        ],
        "labeled\nnow": [
            1122085995
        ],
        "env=prod": [
            1122085995
        ],
        "env=debug": [
            1122085995
        ],
        "kubia-manual-v2": [
            1122085995
        ],
        "changed\nnoteyou": [
            1122085995
        ],
        "--overwrite": [
            1122085995
        ],
        "labels\n$": [
            1122085995
        ],
        "--overwrite\npod": [
            1122085995
        ],
        "labeled\ntwo": [
            1122085995
        ],
        "\nattached": [
            1122085995
        ],
        "\n\n71listing": [
            1122085995
        ],
        "selectors\nlist": [
            1122085995
        ],
        "labels:\n$": [
            1122085995
        ],
        "debug\nkubia-zxzij": [
            1122085995
        ],
        "<none>\nas": [
            1122085995
        ],
        "on\nexisting": [
            1122085995
        ],
        "powerful\nfeature": [
            1122085995
        ],
        "these\nlabels": [
            1122085995
        ],
        "pods\n3.4listing": [
            1122085995
        ],
        "selectors\nattaching": [
            1122085995
        ],
        "list-\ning": [
            1122085995
        ],
        "label\nselectors": [
            1122085995
        ],
        "subset": [
            1122085995
        ],
        "an\noperation": [
            1122085995
        ],
        "criterion": [
            1122085995
        ],
        "filters": [
            1122085995
        ],
        "based\non": [
            1122085995
        ],
        "resource\ncontains": [
            1122085995
        ],
        "contain)": [
            1122085995
        ],
        "key\ncontains": [
            1122085995
        ],
        "value\ncontains": [
            1122085995
        ],
        "equal": [
            1122085995
        ],
        "you\nspecify\n34.1listing": [
            1122085995
        ],
        "selector\nlet’s": [
            1122085995
        ],
        "created\nmanually": [
            1122085995
        ],
        "\ncreation_method=manual)": [
            1122085995
        ],
        "following:\n$": [
            1122085995
        ],
        "creation_method=manual\nname": [
            1122085995
        ],
        "51m\nkubia-manual-v2": [
            1122085995
        ],
        "37m\nto": [
            1122085995
        ],
        "env": [
            1122085995
        ],
        "whatever": [
            1122085995
        ],
        "is:\n$": [
            1122085995
        ],
        "env\nname": [
            1122085995
        ],
        "age\nkubia-manual-v2": [
            1122085995
        ],
        "37m\nand": [
            1122085995
        ],
        "label:\n$": [
            1122085995
        ],
        "!env'\nname": [
            1122085995
        ],
        "51m\nkubia-zxzij": [
            1122085995
        ],
        "10d\n": [
            1122085995
        ],
        "\n\n72chapter": [
            1122085995
        ],
        "kubernetes\nnotemake": [
            1122085995
        ],
        "quotes": [
            1122085995
        ],
        "!env": [
            1122085995
        ],
        "doesn’t\nevaluate": [
            1122085995
        ],
        "exclamation": [
            1122085995
        ],
        "mark\nsimilarly": [
            1122085995
        ],
        "selectors:\ncreation_method!=manual": [
            1122085995
        ],
        "with\nany": [
            1122085995
        ],
        "\nmanual\nenv": [
            1122085995
        ],
        "(proddevel)": [
            1122085995
        ],
        "or\ndevel\nenv": [
            1122085995
        ],
        "notin": [
            1122085995
        ],
        "other\nthan": [
            1122085995
        ],
        "\nprod": [
            1122085995
        ],
        "devel\nturning": [
            1122085995
        ],
        "microservices-oriented": [
            1122085995
        ],
        "you\ncould": [
            1122085995
        ],
        "product": [
            1122085995
        ],
        "the\napp=pc": [
            1122085995
        ],
        "(shown": [
            1122085995
        ],
        "figure)\n3.4.2using": [
            1122085995
        ],
        "selector\na": [
            1122085995
        ],
        "comma-separated": [
            1122085995
        ],
        "criteria": [
            1122085995
        ],
        "to\nmatch": [
            1122085995
        ],
        "pods\nrunning": [
            1122085995
        ],
        "following\nselector:": [
            1122085995
        ],
        "\napp=pcrel=beta": [
            1122085995
        ],
        "(visualized": [
            1122085995
        ],
        "39).\n": [
            1122085995
        ],
        "actions\non": [
            1122085995
        ],
        "by\nkubectl": [
            1122085995
        ],
        "internally": [
            1122085995
        ],
        "next\nui": [
            1122085995
        ],
        "“app=pc”": [
            1122085995
        ],
        "selector\n": [
            1122085995
        ],
        "\n\n73using": [
            1122085995
        ],
        "scheduling\n35using": [
            1122085995
        ],
        "scheduling\nall": [
            1122085995
        ],
        "pretty": [
            1122085995
        ],
        "across\nyour": [
            1122085995
        ],
        "computational\nresources": [
            1122085995
        ],
        "accessibility": [
            1122085995
        ],
        "pods\nisn’t": [
            1122085995
        ],
        "affected": [
            1122085995
        ],
        "any\nneed": [
            1122085995
        ],
        "where\na": [
            1122085995
        ],
        "infrastructure\nisn’t": [
            1122085995
        ],
        "homogenous": [
            1122085995
        ],
        "drives": [
            1122085995
        ],
        "whereas\nothers": [
            1122085995
        ],
        "ssds": [
            1122085995
        ],
        "and\nthe": [
            1122085995
        ],
        "perform-\ning": [
            1122085995
        ],
        "intensive": [
            1122085995
        ],
        "gpu-based": [
            1122085995
        ],
        "computation": [
            1122085995
        ],
        "gpu\nacceleration": [
            1122085995
        ],
        "because\nthat": [
            1122085995
        ],
        "hiding": [
            1122085995
        ],
        "exact\nnode": [
            1122085995
        ],
        "a\nnode": [
            1122085995
        ],
        "and\nnode": [
            1122085995
        ],
        "selectors\n": [
            1122085995
        ],
        "\n\n74chapter": [
            1122085995
        ],
        "kubernetes\n35.1using": [
            1122085995
        ],
        "nodes\nas": [
            1122085995
        ],
        "can\nattach": [
            1122085995
        ],
        "nodes\nusually": [
            1122085995
        ],
        "adds": [
            1122085995
        ],
        "categorize": [
            1122085995
        ],
        "node\nby": [
            1122085995
        ],
        "else\nthat": [
            1122085995
        ],
        "imagine": [
            1122085995
        ],
        "gpu": [
            1122085995
        ],
        "used\nfor": [
            1122085995
        ],
        "general-purpose": [
            1122085995
        ],
        "this\nfeature": [
            1122085995
        ],
        "\ngpu=true": [
            1122085995
        ],
        "(pick": [
            1122085995
        ],
        "nodes):\n$": [
            1122085995
        ],
        "gke-kubia-85f6-node-0rrx": [
            1122085995
        ],
        "gpu=true\nnode": [
            1122085995
        ],
        "gke-kubia-85f6-node-0rrx\"": [
            1122085995
        ],
        "pods\nlist": [
            1122085995
        ],
        "\ngpu=true:\n$": [
            1122085995
        ],
        "gpu=true\nname": [
            1122085995
        ],
        "age\ngke-kubia-85f6-node-0rrx": [
            1122085995
        ],
        "1d\nas": [
            1122085995
        ],
        "expected": [
            1122085995
        ],
        "tell\nkubectl": [
            1122085995
        ],
        "label\n(\nkubectl": [
            1122085995
        ],
        "gpu)\n3.5.2scheduling": [
            1122085995
        ],
        "nodes\nnow": [
            1122085995
        ],
        "work\nto": [
            1122085995
        ],
        "you’ll\nadd": [
            1122085995
        ],
        "kubia-gpuyaml": [
            1122085995
        ],
        "the\nfollowing": [
            1122085995
        ],
        "listing’s": [
            1122085995
        ],
        "pod\napiversion:": [
            1122085995
        ],
        "kubia-gpu\nspec:": [
            1122085995
        ],
        "nodeselector:": [
            1122085995
        ],
        "gpu:": [
            1122085995
        ],
        "kubia\nlisting": [
            1122085995
        ],
        "node:": [
            1122085995
        ],
        "kubia-gpuyaml\nnodeselector": [
            1122085995
        ],
        "label\n": [
            1122085995
        ],
        "\n\n75annotating": [
            1122085995
        ],
        "nodeselector": [
            1122085995
        ],
        "field": [
            1122085995
        ],
        "label\n(which": [
            1122085995
        ],
        "\n35.3scheduling": [
            1122085995
        ],
        "node\nsimilarly": [
            1122085995
        ],
        "has\na": [
            1122085995
        ],
        "\nkubernetesio/hostname": [
            1122085995
        ],
        "\nnodeselector": [
            1122085995
        ],
        "hostname\nlabel": [
            1122085995
        ],
        "unschedulable": [
            1122085995
        ],
        "offline": [
            1122085995
        ],
        "shouldn’t\nthink": [
            1122085995
        ],
        "terms": [
            1122085995,
            2119133144
        ],
        "sat-\nisfy": [
            1122085995
        ],
        "demonstration": [
            1122085995
        ],
        "how\nthey": [
            1122085995
        ],
        "influence": [
            1122085995
        ],
        "use-\nfulness": [
            1122085995
        ],
        "replication-\ncontrollers": [
            1122085995
        ],
        "\nnoteadditional": [
            1122085995
        ],
        "influencing": [
            1122085995
        ],
        "are\ncovered": [
            1122085995
        ],
        "16\n3.6annotating": [
            1122085995
        ],
        "pods\nin": [
            1122085995
        ],
        "annotations\nare": [
            1122085995
        ],
        "pairs": [
            1122085995
        ],
        "to\nhold": [
            1122085995
        ],
        "identifying": [
            1122085995
        ],
        "can\nwhile": [
            1122085995
        ],
        "an\nannotation": [
            1122085995
        ],
        "pieces": [
            1122085995
        ],
        "and\nare": [
            1122085995
        ],
        "added\nto": [
            1122085995
        ],
        "manually\n": [
            1122085995
        ],
        "commonly": [
            1122085995
        ],
        "introducing": [
            1122085995
        ],
        "alpha": [
            1122085995
        ],
        "introduce": [
            1122085995
        ],
        "fields": [
            1122085995
        ],
        "to\napi": [
            1122085995
        ],
        "api\nchanges": [
            1122085995
        ],
        "agreed": [
            1122085995
        ],
        "upon": [
            1122085995
        ],
        "new\nfields": [
            1122085995
        ],
        "introduced": [
            1122085995
        ],
        "deprecated\n": [
            1122085995
        ],
        "descriptions": [
            1122085995
        ],
        "object\nso": [
            1122085995
        ],
        "individ-\nual": [
            1122085995
        ],
        "annotation": [
            1122085995
        ],
        "who\ncreated": [
            1122085995
        ],
        "collaboration": [
            1122085995
        ],
        "cluster\nmuch": [
            1122085995
        ],
        "easier\n3.6.1looking": [
            1122085995
        ],
        "annotations\nlet’s": [
            1122085995
        ],
        "\n\n76chapter": [
            1122085995
        ],
        "kubernetes\nrequest": [
            1122085995
        ],
        "v1\nkind:": [
            1122085995
        ],
        "pod\nmetadata:\n": [
            1122085995
        ],
        "annotations:\n": [
            1122085995
        ],
        "|\n": [
            1122085995
        ],
        "{kind\":\"serializedreference\"": [
            1122085995
        ],
        "apiversion\":\"v1\"": [
            1122085995
        ],
        "reference\":{\"kind\":\"replicationcontroller\"": [
            1122085995
        ],
        "namespace\":\"default\"": [
            1122085995
        ],
        "..\nwithout": [
            1122085995
        ],
        "kubernetesio/created-by\nannotation": [
            1122085995
        ],
        "holds": [
            1122085995
        ],
        "some-\nthing": [
            1122085995
        ],
        "can\ncontain": [
            1122085995
        ],
        "blobs": [
            1122085995
        ],
        "(up": [
            1122085995
        ],
        "kb": [
            1122085995
        ],
        "total)\nnotethe": [
            1122085995
        ],
        "kubernetesio/created-by": [
            1122085995
        ],
        "deprecated": [
            1122085995
        ],
        "ver-\nsion": [
            1122085995
        ],
        "yaml\n3.6.2adding": [
            1122085995
        ],
        "annotations\nannotations": [
            1122085995
        ],
        "creation": [
            1122085995
        ],
        "can\nthey": [
            1122085995
        ],
        "add\nan": [
            1122085995
        ],
        "annotate": [
            1122085995
        ],
        "mycompanycom/someannotation=foo": [
            1122085995
        ],
        "bar\npod": [
            1122085995
        ],
        "annotated\nyou": [
            1122085995
        ],
        "mycompanycom/someannotation": [
            1122085995
        ],
        "foo": [
            1122085995
        ],
        "bar\nit’s": [
            1122085995
        ],
        "collisions": [
            1122085995
        ],
        "when\ndifferent": [
            1122085995
        ],
        "accidentally": [
            1122085995
        ],
        "override\neach": [
            1122085995
        ],
        "other’s": [
            1122085995
        ],
        "prefixes": [
            1122085995
        ],
        "here\n": [
            1122085995
        ],
        "added:\n$": [
            1122085995
        ],
        "kubia-manual\n..\nannotations:": [
            1122085995
        ],
        "bar\n..\n3.7using": [
            1122085995
        ],
        "resources\nlet’s": [
            1122085995
        ],
        "other\nobjects": [
            1122085995
        ],
        "of\nobjects": [
            1122085995
        ],
        "overlap": [
            1122085995
        ],
        "(through": [
            1122085995
        ],
        "example)\nif": [
            1122085995
        ],
        "annotations\n": [
            1122085995
        ],
        "\n\n77using": [
            1122085995
        ],
        "resources\n": [
            1122085995
        ],
        "non-overlapping\ngroups?": [
            1122085995
        ],
        "other\nreasons": [
            1122085995
        ],
        "talked": [
            1122085995
        ],
        "hav-\ning": [
            1122085995
        ],
        "name-\nspaces": [
            1122085995
        ],
        "(across\ndifferent": [
            1122085995
        ],
        "namespaces)\n3.7.1understanding": [
            1122085995
        ],
        "namespaces\nusing": [
            1122085995
        ],
        "numerous": [
            1122085995
        ],
        "distinct": [
            1122085995
        ],
        "separating": [
            1122085995
        ],
        "resources\nin": [
            1122085995
        ],
        "multi-tenant": [
            1122085995
        ],
        "qa": [
            1122085995
        ],
        "namespaced": [
            1122085995
        ],
        "global": [
            1122085995
        ],
        "namespace\nyou’ll": [
            1122085995
        ],
        "now\n3.7.2discovering": [
            1122085995
        ],
        "pods\nfirst": [
            1122085995
        ],
        "cluster:\n$": [
            1122085995
        ],
        "ns\nname": [
            1122085995
        ],
        "age\ndefault": [
            1122085995
        ],
        "active": [
            1122085995
        ],
        "1h\nkube-public": [
            1122085995
        ],
        "1h\nkube-system": [
            1122085995
        ],
        "1h\nup": [
            1122085995
        ],
        "operated": [
            1122085995
        ],
        "resources\nwith": [
            1122085995
        ],
        "so\nkubectl": [
            1122085995
        ],
        "defaulted": [
            1122085995
        ],
        "in\nthat": [
            1122085995
        ],
        "\nkube-public": [
            1122085995
        ],
        "kube-system\nnamespaces": [
            1122085995
        ],
        "kube-system": [
            1122085995
        ],
        "name-\nspace": [
            1122085995
        ],
        "only:\n$": [
            1122085995
        ],
        "--namespace": [
            1122085995
        ],
        "kube-system\nname": [
            1122085995
        ],
        "age\nfluentd-cloud-kubia-e8fe-node-txje": [
            1122085995
        ],
        "1h\nheapster-v11-fz1ge": [
            1122085995
        ],
        "1h\nkube-dns-v9-p8a4t": [
            1122085995
        ],
        "0/4": [
            1122085995
        ],
        "1h\nkube-ui-v4-kdlai": [
            1122085995
        ],
        "1h\nl7-lb-controller-v05.2-bue96": [
            1122085995
        ],
        "2/2": [
            1122085995
        ],
        "1h\ntipyou": [
            1122085995
        ],
        "-n": [
            1122085995
        ],
        "--namespace\n": [
            1122085995
        ],
        "\n\n78chapter": [
            1122085995
        ],
        "kubernetes\nyou’ll": [
            1122085995
        ],
        "(don’t": [
            1122085995
        ],
        "here\ndon’t": [
            1122085995
        ],
        "exactly)": [
            1122085995
        ],
        "having\nthem": [
            1122085995
        ],
        "nicely": [
            1122085995
        ],
        "were\nall": [
            1122085995
        ],
        "you’d\nhave": [
            1122085995
        ],
        "seeing": [
            1122085995
        ],
        "inadvertently": [
            1122085995
        ],
        "use\ntheir": [
            1122085995
        ],
        "inad-\nvertently": [
            1122085995
        ],
        "concern": [
            1122085995
        ],
        "names\nas": [
            1122085995
        ],
        "mentioned\n": [
            1122085995
        ],
        "users\naccess": [
            1122085995
        ],
        "14\n3.7.3creating": [
            1122085995
        ],
        "namespace\na": [
            1122085995
        ],
        "a\nyaml": [
            1122085995
        ],
        "file\nfirst": [
            1122085995
        ],
        "custom-namespaceyaml": [
            1122085995
        ],
        "(you’ll\nfind": [
            1122085995
        ],
        "archive)\napiversion:": [
            1122085995
        ],
        "\nmetadata:\n": [
            1122085995
        ],
        "custom-namespace": [
            1122085995
        ],
        "\nnow": [
            1122085995
        ],
        "server:\n$": [
            1122085995
        ],
        "custom-namespaceyaml\nnamespace": [
            1122085995
        ],
        "custom-namespace\"": [
            1122085995
        ],
        "created\ncreating": [
            1122085995
        ],
        "namespace\nalthough": [
            1122085995
        ],
        "hassle": [
            1122085995
        ],
        "luckily\nyou": [
            1122085995
        ],
        "quicker": [
            1122085995
        ],
        "mani-\nfest": [
            1122085995
        ],
        "reinforce": [
            1122085995
        ],
        "kubernetes\nlisting": [
            1122085995
        ],
        "custom-namespaceyaml\nthis": [
            1122085995
        ],
        "says": [
            1122085995
        ],
        "\ndefining": [
            1122085995
        ],
        "namespace\nthis": [
            1122085995
        ],
        "namespace\n": [
            1122085995
        ],
        "\n\n79using": [
            1122085995
        ],
        "resources\nhas": [
            1122085995
        ],
        "corresponding": [
            1122085995
        ],
        "post-\ning": [
            1122085995
        ],
        "custom-namespace\nnamespace": [
            1122085995
        ],
        "created\nnotealthough": [
            1122085995
        ],
        "objects’": [
            1122085995
        ],
        "conform": [
            1122085995
        ],
        "naming": [
            1122085995
        ],
        "conven-\ntions": [
            1122085995
        ],
        "rfc": [
            1122085995
        ],
        "(domain": [
            1122085995
        ],
        "names)": [
            1122085995
        ],
        "contain\nonly": [
            1122085995
        ],
        "letters": [
            1122085995
        ],
        "digits": [
            1122085995
        ],
        "dashes": [
            1122085995
        ],
        "dots": [
            1122085995
        ],
        "others)": [
            1122085995
        ],
        "aren’t\nallowed": [
            1122085995
        ],
        "\n37.4managing": [
            1122085995
        ],
        "namespaces\nto": [
            1122085995
        ],
        "custom-\nnamespace\n": [
            1122085995
        ],
        "the\nresource": [
            1122085995
        ],
        "custom-namespace\npod": [
            1122085995
        ],
        "created\nyou": [
            1122085995
        ],
        "(kubia-manual)": [
            1122085995
        ],
        "default\nnamespace": [
            1122085995
        ],
        "custom-namespace\n": [
            1122085995
        ],
        "you\nneed": [
            1122085995
        ],
        "\n--namespace": [
            1122085995
        ],
        "-n)": [
            1122085995
        ],
        "cur-\nrent": [
            1122085995
        ],
        "context’s": [
            1122085995
        ],
        "itself\ncan": [
            1122085995
        ],
        "managing\nkubectl": [
            1122085995
        ],
        "contexts": [
            1122085995
        ],
        "\ntipto": [
            1122085995
        ],
        "following\nalias:": [
            1122085995
        ],
        "\nalias": [
            1122085995
        ],
        "kcd=kubectl": [
            1122085995
        ],
        "set-context": [
            1122085995
        ],
        "$(kubectl": [
            1122085995
        ],
        "current-\ncontext)\n": [
            1122085995
        ],
        "kcd\nsome-namespace\n\n3.7.5understanding": [
            1122085995
        ],
        "pro-\nvide—at": [
            1122085995
        ],
        "objects\ninto": [
            1122085995
        ],
        "speci-\nfied": [
            1122085995
        ],
        "different\nnamespaces": [
            1122085995
        ],
        "that’s\nnot": [
            1122085995
        ],
        "depends": [
            1122085995
        ],
        "on\nwhich": [
            1122085995
        ],
        "doesn’t\nprovide": [
            1122085995
        ],
        "inter-namespace": [
            1122085995
        ],
        "\nfoo": [
            1122085995
        ],
        "ip\n": [
            1122085995
        ],
        "\n\n80chapter": [
            1122085995
        ],
        "kubernetes\naddress": [
            1122085995
        ],
        "bar": [
            1122085995
        ],
        "preventing": [
            1122085995
        ],
        "traffic\nsuch": [
            1122085995
        ],
        "\n38stopping": [
            1122085995
        ],
        "four\npods": [
            1122085995
        ],
        "\ndefault": [
            1122085995
        ],
        "you’re\ngoing": [
            1122085995
        ],
        "anymore\n3.8.1deleting": [
            1122085995
        ],
        "name\nfirst": [
            1122085995
        ],
        "kubia-gpu": [
            1122085995
        ],
        "name:\n$": [
            1122085995
        ],
        "kubia-gpu\npod": [
            1122085995
        ],
        "kubia-gpu\"": [
            1122085995
        ],
        "deleted\nby": [
            1122085995
        ],
        "instructing": [
            1122085995
        ],
        "terminate": [
            1122085995
        ],
        "are\npart": [
            1122085995
        ],
        "\nsigterm": [
            1122085995
        ],
        "signal": [
            1122085995
        ],
        "waits": [
            1122085995
        ],
        "certain\nnumber": [
            1122085995
        ],
        "seconds": [
            1122085995
        ],
        "(30": [
            1122085995
        ],
        "default)": [
            1122085995
        ],
        "shut": [
            1122085995
        ],
        "gracefully": [
            1122085995
        ],
        "down\nin": [
            1122085995
        ],
        "\nsigkill": [
            1122085995
        ],
        "are\nalways": [
            1122085995
        ],
        "handle": [
            1122085995
        ],
        "space-sep-\narated": [
            1122085995
        ],
        "pod1": [
            1122085995
        ],
        "pod2)\n3.8.2deleting": [
            1122085995
        ],
        "selectors\ninstead": [
            1122085995
        ],
        "learned\nabout": [
            1122085995
        ],
        "pod\nboth": [
            1122085995
        ],
        "\ncreation_method=manual": [
            1122085995
        ],
        "by\nusing": [
            1122085995
        ],
        "selector:\n$": [
            1122085995
        ],
        "deleted\npod": [
            1122085995
        ],
        "hundreds)": [
            1122085995
        ],
        "the\nrel=canary": [
            1122085995
        ],
        "310):\n$": [
            1122085995
        ],
        "rel=canary\n38.3deleting": [
            1122085995
        ],
        "namespace\nokay": [
            1122085995
        ],
        "custom-namespace?": [
            1122085995
        ],
        "no\nlonger": [
            1122085995
        ],
        "can\n": [
            1122085995
        ],
        "\n\n81stopping": [
            1122085995
        ],
        "pods\ndelete": [
            1122085995
        ],
        "auto-\nmatically)": [
            1122085995
        ],
        "ns": [
            1122085995
        ],
        "\ncustom-namespace\"": [
            1122085995
        ],
        "deleted\n38.4deleting": [
            1122085995
        ],
        "namespace\nyou’ve": [
            1122085995
        ],
        "cleaned": [
            1122085995
        ],
        "2?": [
            1122085995
        ],
        "running:\n$": [
            1122085995
        ],
        "age\nkubia-zxzij": [
            1122085995
        ],
        "the\ncurrent": [
            1122085995
        ],
        "\n--all": [
            1122085995
        ],
        "option:\n$": [
            1122085995
        ],
        "--all\npod": [
            1122085995
        ],
        "kubia-zxzij\"": [
            1122085995
        ],
        "deleted\nnow": [
            1122085995
        ],
        "double": [
            1122085995
        ],
        "left": [
            1122085995
        ],
        "age\nkubia-09as0": [
            1122085995
        ],
        "\nkubia-zxzij": [
            1122085995
        ],
        "terminating": [
            1122085995
        ],
        "rel=canary": [
            1122085995
        ],
        "\n\n82chapter": [
            1122085995
        ],
        "kubernetes\nwait": [
            1122085995
        ],
        "what!?!": [
            1122085995
        ],
        "kubia-09as0\nwhich": [
            1122085995
        ],
        "appeared": [
            1122085995
        ],
        "all\npods": [
            1122085995
        ],
        "kubia-something": [
            1122085995
        ],
        "emerge": [
            1122085995
        ],
        "remember": [
            1122085995
        ],
        "in\nchapter": [
            1122085995
        ],
        "a\nreplicationcontroller": [
            1122085995
        ],
        "\n38.5deleting": [
            1122085995
        ],
        "namespace\nyou": [
            1122085995
        ],
        "services\nyou’ve": [
            1122085995
        ],
        "single\ncommand:\n$": [
            1122085995
        ],
        "kubia-09as0\"": [
            1122085995
        ],
        "deleted\nreplicationcontroller": [
            1122085995
        ],
        "deleted\nservice": [
            1122085995
        ],
        "kubernetes\"": [
            1122085995
        ],
        "deleted\nthe": [
            1122085995
        ],
        "speci-\nfying": [
            1122085995
        ],
        "delete\ncommand)\nnotedeleting": [
            1122085995
        ],
        "keyword": [
            1122085995
        ],
        "absolutely\neverything": [
            1122085995
        ],
        "7)\nare": [
            1122085995
        ],
        "preserved": [
            1122085995
        ],
        "explicitly\nas": [
            1122085995
        ],
        "the\nlist": [
            1122085995
        ],
        "\nnotethe": [
            1122085995
        ],
        "--all": [
            1122085995
        ],
        "kubernetes\nservice": [
            1122085995
        ],
        "recreated": [
            1122085995
        ],
        "moments\n3.9summary\nafter": [
            1122085995
        ],
        "decent": [
            1122085995
        ],
        "central\nbuilding": [
            1122085995
        ],
        "few\nchapters": [
            1122085995
        ],
        "learned\nhow": [
            1122085995
        ],
        "pod\nor": [
            1122085995
        ],
        "not\n": [
            1122085995
        ],
        "\n\n83summary\npods": [
            1122085995
        ],
        "non-\ncontainer": [
            1122085995
        ],
        "world\nyaml": [
            1122085995
        ],
        "then\nexamined": [
            1122085995
        ],
        "state\nlabels": [
            1122085995
        ],
        "perform\noperations": [
            1122085995
        ],
        "once\nyou": [
            1122085995
        ],
        "have\ncertain": [
            1122085995
        ],
        "features\nannotations": [
            1122085995
        ],
        "or\ntools": [
            1122085995
        ],
        "libraries\nnamespaces": [
            1122085995
        ],
        "as\nthough": [
            1122085995
        ],
        "clusters\nhow": [
            1122085995
        ],
        "information\non": [
            1122085995
        ],
        "resources\nthat": [
            1122085995
        ],
        "\n\n84\nreplication": [
            1122085995
        ],
        "other\ncontrollers:": [
            1122085995
        ],
        "deploying\nmanaged": [
            1122085995
        ],
        "pods\nas": [
            1122085995
        ],
        "supervise": [
            1122085995
        ],
        "real-world\nuse": [
            1122085995
        ],
        "stay": [
            1122085995
        ],
        "and\nremain": [
            1122085995
        ],
        "intervention": [
            1122085995
        ],
        "unmanaged": [
            1122085995
        ],
        "previ-\nous": [
            1122085995
        ],
        "chapter)": [
            1122085995
        ],
        "are\nrun": [
            1122085995
        ],
        "monitors\nthis": [
            1122085995
        ],
        "covers\nkeeping": [
            1122085995
        ],
        "healthy\nrunning": [
            1122085995
        ],
        "pod\nautomatically": [
            1122085995
        ],
        "fails\nscaling": [
            1122085995
        ],
        "horizontally\nrunning": [
            1122085995
        ],
        "system-level": [
            1122085995
        ],
        "node\nrunning": [
            1122085995
        ],
        "batch": [
            1122085995
        ],
        "jobs\nscheduling": [
            1122085995
        ],
        "future\n": [
            1122085995
        ],
        "\n\n85keeping": [
            1122085995
        ],
        "healthy\nthose": [
            1122085995
        ],
        "fail": [
            1122085995
        ],
        "node\nfails": [
            1122085995
        ],
        "lost": [
            1122085995
        ],
        "unless\nthose": [
            1122085995
        ],
        "simi-\nlar": [
            1122085995
        ],
        "alive": [
            1122085995
        ],
        "and\nrestarts": [
            1122085995
        ],
        "pods—both": [
            1122085995
        ],
        "run\nindefinitely": [
            1122085995
        ],
        "\n41keeping": [
            1122085995
        ],
        "healthy\none": [
            1122085995
        ],
        "by\ncreating": [
            1122085995
        ],
        "dies?": [
            1122085995
        ],
        "if\nall": [
            1122085995
        ],
        "die?": [
            1122085995
        ],
        "con-\ntainer’s": [
            1122085995
        ],
        "your\napplication": [
            1122085995
        ],
        "causes": [
            1122085995
        ],
        "heal": [
            1122085995
        ],
        "crashing": [
            1122085995
        ],
        "a\njava": [
            1122085995
        ],
        "leak": [
            1122085995
        ],
        "throwing": [
            1122085995
        ],
        "outofmemoryerrors": [
            1122085995
        ],
        "jvm\nprocess": [
            1122085995
        ],
        "restarted": [
            1122085995
        ],
        "you’re\nthinking": [
            1122085995
        ],
        "catch": [
            1122085995
        ],
        "errors": [
            1122085995
        ],
        "when\nthey": [
            1122085995
        ],
        "certainly": [
            1122085995
        ],
        "situations": [
            1122085995
        ],
        "because\nit": [
            1122085995
        ],
        "falls": [
            1122085995
        ],
        "infinite": [
            1122085995
        ],
        "loop": [
            1122085995
        ],
        "deadlock?": [
            1122085995
        ],
        "in\nsuch": [
            1122085995
        ],
        "application’s": [
            1122085995
        ],
        "health": [
            1122085995
        ],
        "depend\non": [
            1122085995
        ],
        "\n41.1introducing": [
            1122085995
        ],
        "probes\nkubernetes": [
            1122085995
        ],
        "specify\na": [
            1122085995
        ],
        "the\nnext": [
            1122085995
        ],
        "confuse": [
            1122085995
        ],
        "different\nthings\nkubernetes": [
            1122085995
        ],
        "mechanisms:\nan": [
            1122085995
        ],
        "\n\n86chapter": [
            1122085995
        ],
        "4replication": [
            1122085995
        ],
        "pods\nresponse": [
            1122085995
        ],
        "response\ncode": [
            1122085995
        ],
        "2xx": [
            1122085995
        ],
        "3xx)": [
            1122085995
        ],
        "considered": [
            1122085995
        ],
        "returns": [
            1122085995
        ],
        "an\nerror": [
            1122085995
        ],
        "fail-\nure": [
            1122085995
        ],
        "result\na": [
            1122085995
        ],
        "socket": [
            1122085995
        ],
        "tries": [
            1122085995
        ],
        "connection": [
            1122085995
        ],
        "successful\notherwise": [
            1122085995
        ],
        "restarted\nan": [
            1122085995
        ],
        "executes": [
            1122085995
        ],
        "checks\nthe": [
            1122085995
        ],
        "successful\nall": [
            1122085995
        ],
        "codes": [
            1122085995
        ],
        "\n41.2creating": [
            1122085995
        ],
        "probe\nlet’s": [
            1122085995
        ],
        "serving\nrequests": [
            1122085995
        ],
        "artificially": [
            1122085995
        ],
        "demo": [
            1122085995
        ],
        "it\nreturn": [
            1122085995
        ],
        "fifth\none—your": [
            1122085995
        ],
        "subsequent": [
            1122085995
        ],
        "thanks": [
            1122085995
        ],
        "restarted\nwhen": [
            1122085995
        ],
        "again\n": [
            1122085995
        ],
        "folder\nchapter04/kubia-unhealthy)": [
            1122085995
        ],
        "you\ndon’t": [
            1122085995
        ],
        "following\nlisting": [
            1122085995
        ],
        "kubia-liveness\nspec:\n": [
            1122085995
        ],
        "containers:\n": [
            1122085995
        ],
        "luksa/kubia-unhealthy": [
            1122085995
        ],
        "livenessprobe:": [
            1122085995
        ],
        "httpget:": [
            1122085995
        ],
        "path:": [
            1122085995
        ],
        "/": [
            1122085995
        ],
        "port:": [
            1122085995
        ],
        "pod:": [
            1122085995
        ],
        "kubia-liveness-probeyaml\nthis": [
            1122085995
        ],
        "\ncontaining": [
            1122085995
        ],
        "\n(somewhat)": [
            1122085995
        ],
        "\nbroken": [
            1122085995
        ],
        "app\na": [
            1122085995
        ],
        "\nperform": [
            1122085995
        ],
        "get\nthe": [
            1122085995
        ],
        "\nrequest": [
            1122085995
        ],
        "\nhttp": [
            1122085995
        ],
        "request\nthe": [
            1122085995
        ],
        "port\nthe": [
            1122085995
        ],
        "should\nconnect": [
            1122085995
        ],
        "\n\n87keeping": [
            1122085995
        ],
        "healthy\nthe": [
            1122085995
        ],
        "httpget": [
            1122085995
        ],
        "peri-\nodically": [
            1122085995
        ],
        "\n/": [
            1122085995
        ],
        "run\n": [
            1122085995
        ],
        "requests)": [
            1122085995
        ],
        "returning\nhttp": [
            1122085995
        ],
        "treat": [
            1122085995
        ],
        "thus\nrestart": [
            1122085995
        ],
        "\n41.3seeing": [
            1122085995
        ],
        "action\nto": [
            1122085995
        ],
        "and\na": [
            1122085995
        ],
        "get:\n$": [
            1122085995
        ],
        "kubia-liveness\nname": [
            1122085995
        ],
        "age\nkubia-liveness": [
            1122085995
        ],
        "2m\nthe": [
            1122085995
        ],
        "you\nwait": [
            1122085995
        ],
        "continues\nindefinitely)\nyou": [
            1122085995
        ],
        "describe\nprints": [
            1122085995
        ],
        "kubia-liveness\nname:": [
            1122085995
        ],
        "kubia-liveness\n..\ncontainers:\n": [
            1122085995
        ],
        "kubia:\n": [
            1122085995
        ],
        "id:": [
            1122085995
        ],
        "docker://480986f8\n": [
            1122085995
        ],
        "luksa/kubia-unhealthy\n": [
            1122085995
        ],
        "docker://sha256:2b208508\n": [
            1122085995
        ],
        "port:\n": [
            1122085995
        ],
        "started:": [
            1122085995
        ],
        "sun": [
            1122085995
        ],
        "11:41:40": [
            1122085995
        ],
        "+0200": [
            1122085995
        ],
        "\nobtaining": [
            1122085995
        ],
        "crashed": [
            1122085995
        ],
        "container\nin": [
            1122085995
        ],
        "kubectl\nlogs\n": [
            1122085995
        ],
        "\nwhen": [
            1122085995
        ],
        "to\nsee": [
            1122085995
        ],
        "\n--previous": [
            1122085995
        ],
        "mypod": [
            1122085995
        ],
        "--previous\nlisting": [
            1122085995
        ],
        "restarted\nthe": [
            1122085995
        ],
        "\ncurrently": [
            1122085995
        ],
        "running\n": [
            1122085995
        ],
        "\n\n88chapter": [
            1122085995
        ],
        "reason:": [
            1122085995
        ],
        "code:": [
            1122085995
        ],
        "mon": [
            1122085995
        ],
        "jan": [
            1122085995
        ],
        "0001": [
            1122085995
        ],
        "00:00:00": [
            1122085995
        ],
        "+0000": [
            1122085995
        ],
        "finished:": [
            1122085995
        ],
        "11:41:38": [
            1122085995
        ],
        "true\n": [
            1122085995
        ],
        "count:": [
            1122085995
        ],
        "liveness:": [
            1122085995
        ],
        "http-get": [
            1122085995
        ],
        "http://:8080/": [
            1122085995
        ],
        "delay=0s": [
            1122085995
        ],
        "timeout=1s\n": [
            1122085995
        ],
        "period=10s": [
            1122085995
        ],
        "#success=1": [
            1122085995
        ],
        "#failure=3\n": [
            1122085995
        ],
        "..\nevents:\n...": [
            1122085995
        ],
        "killing": [
            1122085995
        ],
        "docker://95246981:pod": [
            1122085995
        ],
        "kubia-liveness": [
            1122085995
        ],
        "unhealthy": [
            1122085995
        ],
        "re-created\nyou": [
            1122085995
        ],
        "terminated\nbecause": [
            1122085995
        ],
        "\n137": [
            1122085995
        ],
        "meaning—it": [
            1122085995
        ],
        "denotes\nthat": [
            1122085995
        ],
        "two\nnumbers:": [
            1122085995
        ],
        "\n128+x": [
            1122085995
        ],
        "x": [
            1122085995
        ],
        "caused": [
            1122085995
        ],
        "ter-\nminate": [
            1122085995
        ],
        "\nx": [
            1122085995
        ],
        "equals": [
            1122085995
        ],
        "sigkill": [
            1122085995
        ],
        "forcibly\n": [
            1122085995
        ],
        "killed—kubernetes\ndetected": [
            1122085995
        ],
        "re-created": [
            1122085995
        ],
        "created—it’s\nnot": [
            1122085995
        ],
        "again\n4.1.4configuring": [
            1122085995
        ],
        "probe\nyou": [
            1122085995
        ],
        "displays": [
            1122085995
        ],
        "information\nabout": [
            1122085995
        ],
        "probe:\nliveness:": [
            1122085995
        ],
        "timeout=1s": [
            1122085995
        ],
        "\n➥": [
            1122085995
        ],
        "#failure=3\nbeside": [
            1122085995
        ],
        "additional\nproperties": [
            1122085995
        ],
        "\ndelay": [
            1122085995
        ],
        "timeout": [
            1122085995
        ],
        "period": [
            1122085995
        ],
        "probing": [
            1122085995
        ],
        "begins": [
            1122085995
        ],
        "\ntimeout": [
            1122085995
        ],
        "is\ncounted": [
            1122085995
        ],
        "probed": [
            1122085995
        ],
        "(\nperiod=10s)": [
            1122085995
        ],
        "consecutive": [
            1122085995
        ],
        "(\n#failure=3)": [
            1122085995
        ],
        "parameters": [
            1122085995
        ],
        "customized": [
            1122085995
        ],
        "delay": [
            1122085995
        ],
        "\ninitialdelayseconds": [
            1122085995
        ],
        "property": [
            1122085995
        ],
        "live-\nness": [
            1122085995
        ],
        "listing\n": [
            1122085995
        ],
        "delay:": [
            1122085995
        ],
        "kubia-liveness-probe-initial-delayyaml\nthe": [
            1122085995
        ],
        "\nexited": [
            1122085995
        ],
        "137\nthe": [
            1122085995
        ],
        "\nrestarted": [
            1122085995
        ],
        "once\n": [
            1122085995
        ],
        "\n\n89keeping": [
            1122085995
        ],
        "healthy\n": [
            1122085995
        ],
        "initialdelayseconds:": [
            1122085995
        ],
        "\nif": [
            1122085995
        ],
        "prober": [
            1122085995
        ],
        "as\nit": [
            1122085995
        ],
        "failing": [
            1122085995
        ],
        "start\nreceiving": [
            1122085995
        ],
        "exceeds": [
            1122085995
        ],
        "threshold": [
            1122085995
        ],
        "able": [
            1122085995
        ],
        "\ntipalways": [
            1122085995
        ],
        "startup\ntime\ni’ve": [
            1122085995
        ],
        "occasions": [
            1122085995
        ],
        "was\nbeing": [
            1122085995
        ],
        "they’d": [
            1122085995
        ],
        "terminated\nexternally": [
            1122085995
        ],
        "container\nwas": [
            1122085995
        ],
        "happening": [
            1122085995
        ],
        "startup\nit’s": [
            1122085995
        ],
        "appropriately\nnoteexit": [
            1122085995
        ],
        "signals": [
            1122085995
        ],
        "signal\n(exit": [
            1122085995
        ],
        "(sigkill)": [
            1122085995
        ],
        "corresponds": [
            1122085995
        ],
        "+\n15": [
            1122085995
        ],
        "(sigterm)\n4.1.5creating": [
            1122085995
        ],
        "probes\nfor": [
            1122085995
        ],
        "without\none": [
            1122085995
        ],
        "long\nas": [
            1122085995
        ],
        "check\nyour": [
            1122085995
        ],
        "simplistic": [
            1122085995
        ],
        "may\nseem": [
            1122085995
        ],
        "wonders": [
            1122085995
        ],
        "stops\nresponding": [
            1122085995
        ],
        "major\nimprovement": [
            1122085995
        ],
        "sufficient": [
            1122085995
        ],
        "cases\n": [
            1122085995
        ],
        "a\nspecific": [
            1122085995
        ],
        "(\n/health": [
            1122085995
        ],
        "vital": [
            1122085995
        ],
        "none": [
            1122085995
        ],
        "them\nhas": [
            1122085995
        ],
        "died": [
            1122085995
        ],
        "unresponsive": [
            1122085995
        ],
        "\ntipmake": [
            1122085995
        ],
        "/health": [
            1122085995
        ],
        "authentication;\notherwise": [
            1122085995
        ],
        "causing": [
            1122085995
        ],
        "restarted\nindefinitely\nbe": [
            1122085995
        ],
        "influenced": [
            1122085995
        ],
        "external\nfactor": [
            1122085995
        ],
        "failure\nwhen": [
            1122085995
        ],
        "cause": [
            1122085995
        ],
        "problem\nkubernetes": [
            1122085995
        ],
        "probe\n": [
            1122085995
        ],
        "\n\n90chapter": [
            1122085995
        ],
        "restarting\nrepeatedly": [
            1122085995
        ],
        "\nkeeping": [
            1122085995
        ],
        "light\nliveness": [
            1122085995
        ],
        "take\ntoo": [
            1122085995
        ],
        "are\nonly": [
            1122085995
        ],
        "heavy": [
            1122085995
        ],
        "lifting": [
            1122085995
        ],
        "slow\ndown": [
            1122085995
        ],
        "considerably": [
            1122085995
        ],
        "to\nlimit": [
            1122085995
        ],
        "probe’s": [
            1122085995
        ],
        "counted": [
            1122085995
        ],
        "heavyweight": [
            1122085995
        ],
        "cpu\ntime": [
            1122085995
        ],
        "processes\ntipif": [
            1122085995
        ],
        "java": [
            1122085995
        ],
        "http\nget": [
            1122085995
        ],
        "new\njvm": [
            1122085995
        ],
        "jvm-based": [
            1122085995
        ],
        "sim-\nilar": [
            1122085995
        ],
        "start-up": [
            1122085995
        ],
        "considerable": [
            1122085995
        ],
        "resources\ndon’t": [
            1122085995
        ],
        "bother": [
            1122085995
        ],
        "retry": [
            1122085995
        ],
        "loops": [
            1122085995
        ],
        "probes\nyou’ve": [
            1122085995
        ],
        "configurable": [
            1122085995
        ],
        "usu-\nally": [
            1122085995
        ],
        "you\nset": [
            1122085995
        ],
        "con-\nsidering": [
            1122085995
        ],
        "attempt": [
            1122085995
        ],
        "wasted": [
            1122085995
        ],
        "effort\nliveness": [
            1122085995
        ],
        "wrap-up\nyou": [
            1122085995
        ],
        "restarting\nthem": [
            1122085995
        ],
        "kubelet\non": [
            1122085995
        ],
        "pod—the": [
            1122085995
        ],
        "master(s)": [
            1122085995
        ],
        "replacements": [
            1122085995
        ],
        "for\nall": [
            1122085995
        ],
        "went": [
            1122085995,
            1043891123
        ],
        "create\ndirectly": [
            1122085995
        ],
        "except": [
            1122085995
        ],
        "the\nkubelet": [
            1122085995
        ],
        "pod\nmanaged": [
            1122085995
        ],
        "discuss": [
            1122085995
        ],
        "the\nrest": [
            1122085995
        ],
        "\n42introducing": [
            1122085995
        ],
        "replicationcontrollers\na": [
            1122085995
        ],
        "always\nkept": [
            1122085995
        ],
        "disappears": [
            1122085995
        ],
        "node\ndisappearing": [
            1122085995
        ],
        "notices": [
            1122085995
        ],
        "replacement": [
            1122085995
        ],
        "it\npod": [
            1122085995
        ],
        "managed\n": [
            1122085995
        ],
        "\n\n91introducing": [
            1122085995
        ],
        "replicationcontrollers\nby": [
            1122085995
        ],
        "a\nnew": [
            1122085995
        ],
        "(pod": [
            1122085995
        ],
        "b2)": [
            1122085995
        ],
        "completely—\nnothing": [
            1122085995
        ],
        "(replicas)": [
            1122085995
        ],
        "got": [
            1122085995
        ],
        "\n42.1the": [
            1122085995
        ],
        "replicationcontroller\na": [
            1122085995
        ],
        "sure\nthe": [
            1122085995
        ],
        "“type”": [
            1122085995
        ],
        "few\nsuch": [
            1122085995
        ],
        "such\npods": [
            1122085995
        ],
        "removes": [
            1122085995
        ],
        "excess": [
            1122085995
        ],
        "repli-\ncas": [
            1122085995
        ],
        "reasons:": [
            1122085995
        ],
        "\nsomeone": [
            1122085995
        ],
        "manually\nsomeone": [
            1122085995
        ],
        "“type”\nsomeone": [
            1122085995
        ],
        "decreases": [
            1122085995
        ],
        "on\nnode": [
            1122085995
        ],
        "1\nnode": [
            1122085995
        ],
        "fails\npod": [
            1122085995
        ],
        "b\nnode": [
            1122085995
        ],
        "2\nvarious\nother": [
            1122085995
        ],
        "pods\ncreates": [
            1122085995
        ],
        "and\nmanages\nnode": [
            1122085995
        ],
        "pods\nreplicationcontrollerreplicationcontroller\npod": [
            1122085995
        ],
        "is\nnot": [
            1122085995
        ],
        "no\nreplicationcontroller": [
            1122085995
        ],
        "overseeing": [
            1122085995
        ],
        "it\nrc": [
            1122085995
        ],
        "is\nmissing": [
            1122085995
        ],
        "creates\na": [
            1122085995
        ],
        "instance\npod": [
            1122085995
        ],
        "b2\nfigure": [
            1122085995
        ],
        "recreated\n": [
            1122085995
        ],
        "\n\n92chapter": [
            1122085995
        ],
        "pods\ni’ve": [
            1122085995
        ],
        "label\nselector": [
            1122085995
        ],
        "controller’s": [
            1122085995
        ],
        "reconciliation": [
            1122085995
        ],
        "loop\na": [
            1122085995
        ],
        "always\nmatches": [
            1122085995
        ],
        "appropriate\naction": [
            1122085995
        ],
        "42.\nunderstanding": [
            1122085995
        ],
        "essential": [
            1122085995
        ],
        "(also": [
            1122085995
        ],
        "43):\na": [
            1122085995
        ],
        "scope\na": [
            1122085995
        ],
        "running\na": [
            1122085995
        ],
        "replicas\nstart\ncompare\nmatched": [
            1122085995
        ],
        "vs\ndesired": [
            1122085995
        ],
        "pod\ncount\nfind": [
            1122085995
        ],
        "pods\nmatching": [
            1122085995
        ],
        "the\nlabel": [
            1122085995
        ],
        "selector\ncreate": [
            1122085995
        ],
        "additional\npod(s)": [
            1122085995
        ],
        "from\ncurrent": [
            1122085995
        ],
        "template\ndelete": [
            1122085995
        ],
        "the\nexcess": [
            1122085995
        ],
        "pod(s)\ntoo": [
            1122085995
        ],
        "many\njust": [
            1122085995
        ],
        "enough\ntoo": [
            1122085995
        ],
        "few\nfigure": [
            1122085995
        ],
        "loop\napp:": [
            1122085995
        ],
        "kubia\npod\npod": [
            1122085995
        ],
        "template\nreplicationcontroller:": [
            1122085995
        ],
        "kubia\npod": [
            1122085995
        ],
        "selector:\napp=kubia\nreplicas:": [
            1122085995
        ],
        "3\nfigure": [
            1122085995
        ],
        "\nreplica": [
            1122085995
        ],
        "template)\n": [
            1122085995
        ],
        "\n\n93introducing": [
            1122085995
        ],
        "tem-\nplate": [
            1122085995
        ],
        "affect\nexisting": [
            1122085995
        ],
        "template\nchanges": [
            1122085995
        ],
        "pods\nchanging": [
            1122085995
        ],
        "fall": [
            1122085995
        ],
        "caring": [
            1122085995
        ],
        "replicationcon-\ntrollers": [
            1122085995
        ],
        "“contents”": [
            1122085995
        ],
        "images\nenvironment": [
            1122085995
        ],
        "things)": [
            1122085995
        ],
        "template\ntherefore": [
            1122085995
        ],
        "affects": [
            1122085995
        ],
        "think\nof": [
            1122085995
        ],
        "cookie": [
            1122085995
        ],
        "cutter": [
            1122085995
        ],
        "cutting": [
            1122085995
        ],
        "pods\nunderstanding": [
            1122085995
        ],
        "replicationcontroller\nlike": [
            1122085995
        ],
        "sim-\nple": [
            1122085995
        ],
        "features:\nit": [
            1122085995
        ],
        "missing\nwhen": [
            1122085995
        ],
        "that\nwere": [
            1122085995
        ],
        "(those": [
            1122085995
        ],
        "replication-\ncontroller’s": [
            1122085995
        ],
        "control)\nit": [
            1122085995
        ],
        "horizontal": [
            1122085995
        ],
        "(see\nhorizontal": [
            1122085995
        ],
        "auto-scaling": [
            1122085995
        ],
        "15)\nnotea": [
            1122085995
        ],
        "rela-\ntion": [
            1122085995
        ],
        "\n42.2creating": [
            1122085995
        ],
        "replicationcontroller\nlet’s": [
            1122085995
        ],
        "your\npods": [
            1122085995
        ],
        "kubia-rcyaml": [
            1122085995
        ],
        "\nspec:\n": [
            1122085995
        ],
        "replicas:": [
            1122085995
        ],
        "selector:": [
            1122085995
        ],
        "replicationcontroller:": [
            1122085995
        ],
        "kubia-rcyaml\nthis": [
            1122085995
        ],
        "(rc)\nthe": [
            1122085995
        ],
        "\nreplicationcontroller\nthe": [
            1122085995
        ],
        "instances\nthe": [
            1122085995
        ],
        "determining": [
            1122085995
        ],
        "\n\n94chapter": [
            1122085995
        ],
        "template:": [
            1122085995
        ],
        "metadata:": [
            1122085995
        ],
        "spec:": [
            1122085995
        ],
        "named": [
            1122085995
        ],
        "\napp=kubia": [
            1122085995
        ],
        "identical": [
            1122085995
        ],
        "the\nreplicationcontroller;": [
            1122085995
        ],
        "indefinitely\nbecause": [
            1122085995
        ],
        "scenarios": [
            1122085995
        ],
        "verifies": [
            1122085995
        ],
        "misconfigured\n": [
            1122085995
        ],
        "configured\nautomatically": [
            1122085995
        ],
        "extract": [
            1122085995
        ],
        "yaml\nshorter": [
            1122085995
        ],
        "simpler\nto": [
            1122085995
        ],
        "you\nalready": [
            1122085995
        ],
        "know:\n$": [
            1122085995
        ],
        "kubia-rcyaml\nreplicationcontroller": [
            1122085995
        ],
        "created\nas": [
            1122085995
        ],
        "what\nit": [
            1122085995
        ],
        "does\n4.2.3seeing": [
            1122085995
        ],
        "action\nbecause": [
            1122085995
        ],
        "app=kubia": [
            1122085995
        ],
        "should\nspin": [
            1122085995
        ],
        "to:\n$": [
            1122085995
        ],
        "age\nkubia-53thy": [
            1122085995
        ],
        "containercreating": [
            1122085995
        ],
        "2s\nkubia-k0xz6": [
            1122085995
        ],
        "2s\nkubia-q3vkg": [
            1122085995
        ],
        "2s\nthe": [
            1122085995
        ],
        "\npods\n": [
            1122085995
        ],
        "\n\n95introducing": [
            1122085995
        ],
        "replicationcontrollers\nindeed": [
            1122085995
        ],
        "has!": [
            1122085995
        ],
        "managing\nthose": [
            1122085995
        ],
        "pod\nfirst": [
            1122085995
        ],
        "spins\nup": [
            1122085995
        ],
        "three:\n$": [
            1122085995
        ],
        "kubia-53thy\npod": [
            1122085995
        ],
        "kubia-53thy\"": [
            1122085995
        ],
        "deleted\nlisting": [
            1122085995
        ],
        "terminat-\ning": [
            1122085995
        ],
        "created:\n$": [
            1122085995
        ],
        "3m\nkubia-oini2": [
            1122085995
        ],
        "3m\nkubia-q3vkg": [
            1122085995
        ],
        "3m\nthe": [
            1122085995
        ],
        "helper": [
            1122085995
        ],
        "it?\ngetting": [
            1122085995
        ],
        "replicationcontroller\nnow": [
            1122085995
        ],
        "replication-\ncontrollers:\n$": [
            1122085995
        ],
        "3m\nnotewe’re": [
            1122085995
        ],
        "replicationcontroller\nyou": [
            1122085995
        ],
        "probes)\n": [
            1122085995
        ],
        "the\nkubectl": [
            1122085995
        ],
        "kubia\nname:": [
            1122085995
        ],
        "kubia\nnamespace:": [
            1122085995
        ],
        "default\nselector:": [
            1122085995
        ],
        "app=kubia\nlabels:": [
            1122085995
        ],
        "app=kubia\nannotations:": [
            1122085995
        ],
        "<none>\nreplicas:": [
            1122085995
        ],
        "succeeded": [
            1122085995
        ],
        "template:\n": [
            1122085995
        ],
        "app=kubia\n": [
            1122085995
        ],
        "..\nlisting": [
            1122085995
        ],
        "describe\nthe": [
            1122085995
        ],
        "\ndesired": [
            1122085995
        ],
        "instances\nnumber": [
            1122085995
        ],
        "\nper": [
            1122085995
        ],
        "\nstatus\n": [
            1122085995
        ],
        "\n\n96chapter": [
            1122085995
        ],
        "<none>\nevents:": [
            1122085995
        ],
        "\nfrom": [
            1122085995
        ],
        "message\n----": [
            1122085995
        ],
        "-------": [
            1122085995
        ],
        "------": [
            1122085995
        ],
        "-------\nreplication-controller": [
            1122085995
        ],
        "successfulcreate": [
            1122085995
        ],
        "kubia-53thy\nreplication-controller": [
            1122085995
        ],
        "kubia-k0xz6\nreplication-controller": [
            1122085995
        ],
        "kubia-q3vkg\nreplication-controller": [
            1122085995
        ],
        "kubia-oini2\nthe": [
            1122085995
        ],
        "controller\nhas": [
            1122085995
        ],
        "termi-\nnating": [
            1122085995
        ],
        "replication-\ncontroller—it": [
            1122085995
        ],
        "far\nunderstanding": [
            1122085995
        ],
        "deletion": [
            1122085995
        ],
        "replacement\npod": [
            1122085995
        ],
        "44).": [
            1122085995
        ],
        "technically": [
            1122085995
        ],
        "the\nresulting": [
            1122085995
        ],
        "state—the": [
            1122085995
        ],
        "inadequate": [
            1122085995
        ],
        "notified": [
            1122085995
        ],
        "deleted\n(the": [
            1122085995
        ],
        "watch": [
            1122085995
        ],
        "lists)": [
            1122085995
        ],
        "notification": [
            1122085995
        ],
        "triggers": [
            1122085995
        ],
        "controller\nto": [
            1122085995
        ],
        "action\nthe": [
            1122085995
        ],
        "\nrelated": [
            1122085995
        ],
        "\nreplicationcontroller\nbefore": [
            1122085995
        ],
        "deletionafter": [
            1122085995
        ],
        "deletion\nreplicationcontroller:": [
            1122085995
        ],
        "3\nselector:app=kubia\napp:": [
            1122085995
        ],
        "kubia\npod:\nkubia-q3vkg\napp:": [
            1122085995
        ],
        "kubia\npod:\nkubia-oini2\n[containercreating]": [
            1122085995
        ],
        "[terminating]\napp:": [
            1122085995
        ],
        "kubia\npod:\nkubia-k0xz6\napp:": [
            1122085995
        ],
        "kubia\npod:\nkubia-53thy\nreplicationcontroller:": [
            1122085995
        ],
        "kubia\npod:\nkubia-53thy\ndeletekubia-53thy\nfigure": [
            1122085995
        ],
        "\n\n97introducing": [
            1122085995
        ],
        "replicationcontrollers\nresponding": [
            1122085995
        ],
        "failure\nseeing": [
            1122085995
        ],
        "too\ninteresting": [
            1122085995
        ],
        "engine\nto": [
            1122085995
        ],
        "dis-\nconnect": [
            1122085995
        ],
        "simulate": [
            1122085995
        ],
        "failure\nnoteif": [
            1122085995
        ],
        "only\nhave": [
            1122085995
        ],
        "acts": [
            1122085995
        ],
        "node\nif": [
            1122085995
        ],
        "non-kubernetes": [
            1122085995
        ],
        "the\napplications": [
            1122085995
        ],
        "detects": [
            1122085995
        ],
        "that\nits": [
            1122085995
        ],
        "gcloud\ncompute\n": [
            1122085995
        ],
        "ifconfig\neth0\n": [
            1122085995
        ],
        "listing\nnotechoose": [
            1122085995
        ],
        "option\n$": [
            1122085995
        ],
        "gke-kubia-default-pool-b46381f1-zwko\nenter": [
            1122085995
        ],
        "passphrase": [
            1122085995
        ],
        "/home/luksa/ssh/google_compute_engine':\nwelcome": [
            1122085995
        ],
        "v16.4!\n...\nluksa@gke-kubia-default-pool-b46381f1-zwko": [
            1122085995
        ],
        "~": [
            1122085995
        ],
        "$": [
            1122085995
        ],
        "ifconfig": [
            1122085995
        ],
        "eth0": [
            1122085995
        ],
        "down\nwhen": [
            1122085995
        ],
        "session": [
            1122085995
        ],
        "hard-exit": [
            1122085995
        ],
        "new\nterminal": [
            1122085995
        ],
        "detected": [
            1122085995
        ],
        "is\ndown": [
            1122085995
        ],
        "\nnotready:\n$": [
            1122085995
        ],
        "node\nname": [
            1122085995
        ],
        "age\ngke-kubia-default-pool-b46381f1-opc5": [
            1122085995
        ],
        "5h\ngke-kubia-default-pool-b46381f1-s8gj": [
            1122085995
        ],
        "5h\ngke-kubia-default-pool-b46381f1-zwko": [
            1122085995
        ],
        "notready": [
            1122085995
        ],
        "5h": [
            1122085995
        ],
        "unreachable": [
            1122085995
        ],
        "because\nof": [
            1122085995
        ],
        "temporary": [
            1122085995
        ],
        "glitch": [
            1122085995
        ],
        "restarting)": [
            1122085995
        ],
        "stays\nunreachable": [
            1122085995
        ],
        "that\nnode": [
            1122085995
        ],
        "\nunknown": [
            1122085995
        ],
        "immediately\nspin": [
            1122085995
        ],
        "again:\nlisting": [
            1122085995
        ],
        "simulating": [
            1122085995
        ],
        "shutting": [
            1122085995
        ],
        "interface\nnode": [
            1122085995
        ],
        "\nbecause": [
            1122085995
        ],
        "\ndisconnected": [
            1122085995
        ],
        "\n\n98chapter": [
            1122085995
        ],
        "pods\n$": [
            1122085995
        ],
        "age\nkubia-oini2": [
            1122085995
        ],
        "10m\nkubia-k0xz6": [
            1122085995
        ],
        "10m\nkubia-q3vkg": [
            1122085995
        ],
        "unknown": [
            1122085995
        ],
        "10m": [
            1122085995
        ],
        "\nkubia-dmdck": [
            1122085995
        ],
        "5s": [
            1122085995
        ],
        "\nlooking": [
            1122085995
        ],
        "kubia-dmdck": [
            1122085995
        ],
        "again\nhave": [
            1122085995
        ],
        "again\ndone": [
            1122085995
        ],
        "(either": [
            1122085995
        ],
        "unreach-\nable)": [
            1122085995
        ],
        "immediate": [
            1122085995
        ],
        "heals": [
            1122085995
        ],
        "itself\nautomatically": [
            1122085995
        ],
        "reset": [
            1122085995
        ],
        "gke-kubia-default-pool-b46381f1-zwko\nwhen": [
            1122085995
        ],
        "boots": [
            1122085995
        ],
        "whose\nstatus": [
            1122085995
        ],
        "deleted\n4.2.4moving": [
            1122085995
        ],
        "replicationcontroller\npods": [
            1122085995
        ],
        "in\nany": [
            1122085995
        ],
        "scope\nof": [
            1122085995
        ],
        "to\nanother\ntipalthough": [
            1122085995
        ],
        "refer-\nence": [
            1122085995
        ],
        "\nmetadataownerreferences": [
            1122085995
        ],
        "easily\nfind": [
            1122085995
        ],
        "to\nif": [
            1122085995
        ],
        "man-\naged": [
            1122085995
        ],
        "resched-\nuled": [
            1122085995
        ],
        "replication\ncontroller": [
            1122085995
        ],
        "spun": [
            1122085995
        ],
        "pods\nthat": [
            1122085995
        ],
        "value\nto": [
            1122085995
        ],
        "will\nhave": [
            1122085995
        ],
        "cares": [
            1122085995
        ],
        "referenced": [
            1122085995
        ],
        "unreachable\nthis": [
            1122085995
        ],
        "\nfive": [
            1122085995
        ],
        "ago\n": [
            1122085995
        ],
        "\n\n99introducing": [
            1122085995
        ],
        "replicationcontrollers\nadding": [
            1122085995
        ],
        "to\nits": [
            1122085995
        ],
        "type=special\npod": [
            1122085995
        ],
        "kubia-dmdck\"": [
            1122085995
        ],
        "labeled\n$": [
            1122085995
        ],
        "labels\nkubia-oini2": [
            1122085995
        ],
        "11m": [
            1122085995
        ],
        "app=kubia\nkubia-k0xz6": [
            1122085995
        ],
        "app=kubia\nkubia-dmdck": [
            1122085995
        ],
        "app=kubiatype=special\nyou’ve": [
            1122085995
        ],
        "type=special": [
            1122085995
        ],
        "occurred": [
            1122085995
        ],
        "concerned\nchanging": [
            1122085995
        ],
        "app=foo": [
            1122085995
        ],
        "labeled\nthe": [
            1122085995
        ],
        "necessary;": [
            1122085995
        ],
        "warn-\ning": [
            1122085995
        ],
        "exist-\ning": [
            1122085995
        ],
        "label’s": [
            1122085995
        ],
        "intent": [
            1122085995
        ],
        "pods:": [
            1122085995
        ],
        "app\nname": [
            1122085995
        ],
        "app\nkubia-2qneh": [
            1122085995
        ],
        "2s": [
            1122085995
        ],
        "\nkubia-oini2": [
            1122085995
        ],
        "20m": [
            1122085995
        ],
        "kubia\nkubia-k0xz6": [
            1122085995
        ],
        "kubia\nkubia-dmdck": [
            1122085995
        ],
        "\nnoteyou’re": [
            1122085995
        ],
        "column\nthere": [
            1122085995
        ],
        "altogether:": [
            1122085995
        ],
        "illustrates": [
            1122085995
        ],
        "pods\nand": [
            1122085995
        ],
        "to\napp=foo": [
            1122085995
        ],
        "con-\ntroller’s": [
            1122085995
        ],
        "the\nnewly": [
            1122085995
        ],
        "replaces\nthe": [
            1122085995
        ],
        "the\nscope": [
            1122085995
        ],
        "replicationcontroller\npod": [
            1122085995
        ],
        "\nreplicationcontroller\n": [
            1122085995
        ],
        "\n\n100chapter": [
            1122085995
        ],
        "pods\nreplicationcontroller": [
            1122085995
        ],
        "spins": [
            1122085995
        ],
        "kubia-2qneh": [
            1122085995
        ],
        "to\nthree": [
            1122085995
        ],
        "until\nyou": [
            1122085995
        ],
        "anymore)\nremoving": [
            1122085995
        ],
        "practice\nremoving": [
            1122085995
        ],
        "when\nyou": [
            1122085995
        ],
        "bug\nthat": [
            1122085995
        ],
        "behaving": [
            1122085995
        ],
        "badly": [
            1122085995
        ],
        "malfunctioning": [
            1122085995
        ],
        "or\nplay": [
            1122085995
        ],
        "\nchanging": [
            1122085995
        ],
        "selector\nas": [
            1122085995
        ],
        "you\nthink": [
            1122085995
        ],
        "selector?": [
            1122085995
        ],
        "result": [
            1122085995
        ],
        "abso-\nlutely": [
            1122085995
        ],
        "but\nthat’s": [
            1122085995
        ],
        "this\ninitial": [
            1122085995
        ],
        "stateafter": [
            1122085995
        ],
        "re-labelling\nre-labelkubia-dmdck\napp:": [
            1122085995
        ],
        "kubia\npod:\nkubia-oini2\napp:": [
            1122085995
        ],
        "kubia\npod:\nkubia-2qneh\n[containercreating]\npod:\nkubia-dmdck\napp:": [
            1122085995
        ],
        "kubia\ntype:": [
            1122085995
        ],
        "special\ntype:": [
            1122085995
        ],
        "special\napp:": [
            1122085995
        ],
        "fooapp:": [
            1122085995
        ],
        "kubia\npod:\nkubia-dmdck\napp:": [
            1122085995
        ],
        "kubia\npod:\nkubia-k0xz6\nreplicationcontroller:": [
            1122085995
        ],
        "3\nselector:app=kubia\nreplicationcontroller:": [
            1122085995
        ],
        "3\nselector:app=kubia\npod:\nkubia-oini2\nfigure": [
            1122085995
        ],
        "\n\n101introducing": [
            1122085995
        ],
        "replicationcontrollers\nchapter": [
            1122085995
        ],
        "controller’s\nlabel": [
            1122085995
        ],
        "that\n4.2.5changing": [
            1122085995
        ],
        "template\na": [
            1122085995
        ],
        "pod\ntemplate": [
            1122085995
        ],
        "cookies\nyou": [
            1122085995
        ],
        "cut": [
            1122085995
        ],
        "afterward": [
            1122085995
        ],
        "figure\n46).": [
            1122085995
        ],
        "template\nas": [
            1122085995
        ],
        "editing": [
            1122085995
        ],
        "edit": [
            1122085995
        ],
        "editor\nfind": [
            1122085995
        ],
        "you\nsave": [
            1122085995
        ],
        "replicationcontroller\nand": [
            1122085995
        ],
        "message:\nreplicationcontroller": [
            1122085995
        ],
        "edited\nyou": [
            1122085995
        ],
        "changed\nbut": [
            1122085995
        ],
        "upgrading": [
            1122085995
        ],
        "of\ndoing": [
            1122085995
        ],
        "\nreplication\ncontroller\nreplicas:": [
            1122085995
        ],
        "3\ntemplate:\nabc\nreplication\ncontroller\nreplicas:": [
            1122085995
        ],
        "3\ntemplate:\na\nreplication\ncontroller\nreplicas:": [
            1122085995
        ],
        "3\ntemplate:\nd\nabcabcab\nchange\ntemplate\ndelete\na": [
            1122085995
        ],
        "pod\nrc": [
            1122085995
        ],
        "creates\nnew": [
            1122085995
        ],
        "pod\nfigure": [
            1122085995
        ],
        "\neffect": [
            1122085995
        ],
        "\n\n102chapter": [
            1122085995
        ],
        "pods\n42.6horizontally": [
            1122085995
        ],
        "instances\nis": [
            1122085995
        ],
        "rep-\nlicas": [
            1122085995
        ],
        "(when": [
            1122085995
        ],
        "down)": [
            1122085995
        ],
        "replicationcontroller\nyour": [
            1122085995
        ],
        "running\nyou’re": [
            1122085995
        ],
        "you’ve\nalready": [
            1122085995
        ],
        "before:\n$": [
            1122085995
        ],
        "--replicas=10\nbut": [
            1122085995
        ],
        "differently": [
            1122085995
        ],
        "definition\ninstead": [
            1122085995
        ],
        "declarative\nway": [
            1122085995
        ],
        "definition:\n$": [
            1122085995
        ],
        "kubia\nwhen": [
            1122085995
        ],
        "opens": [
            1122085995
        ],
        "specreplicas": [
            1122085995
        ],
        "10\nas": [
            1122085995
        ],
        "listing\n#": [
            1122085995
        ],
        "beginning": [
            1122085995
        ],
        "#'": [
            1122085995
        ],
        "ignored\n#": [
            1122085995
        ],
        "empty": [
            1122085995
        ],
        "abort": [
            1122085995
        ],
        "saving": [
            1122085995
        ],
        "\n#": [
            1122085995
        ],
        "reopened": [
            1122085995
        ],
        "failures\napiversion:": [
            1122085995
        ],
        "replicationcontroller\nconfiguring": [
            1122085995
        ],
        "editor\nyou": [
            1122085995
        ],
        "kube_editor\nenvironment": [
            1122085995
        ],
        "variable": [
            1122085995
        ],
        "nano": [
            1122085995
        ],
        "kubernetes\nresources": [
            1122085995
        ],
        "an\nequivalent": [
            1122085995
        ],
        "file):\nexport": [
            1122085995
        ],
        "kube_editor=/usr/bin/nano\"\nif": [
            1122085995
        ],
        "kube_editor": [
            1122085995
        ],
        "\neditor": [
            1122085995
        ],
        "variable\nlisting": [
            1122085995
        ],
        "edit\n": [
            1122085995
        ],
        "\n\n103introducing": [
            1122085995
        ],
        "replicationcontrollers\nmetadata:\n": [
            1122085995
        ],
        "..\nspec:\n": [
            1122085995
        ],
        "selector:\n": [
            1122085995
        ],
        "..\nwhen": [
            1122085995
        ],
        "and\nit": [
            1122085995
        ],
        "10:\n$": [
            1122085995
        ],
        "21m\nthere": [
            1122085995
        ],
        "clearer": [
            1122085995
        ],
        "declarative\nchange": [
            1122085995
        ],
        "something\nscaling": [
            1122085995
        ],
        "command\nnow": [
            1122085995
        ],
        "--replicas=3\nall": [
            1122085995
        ],
        "replicationcontroller’s\ndefinition—like": [
            1122085995
        ],
        "declarative": [
            1122085995
        ],
        "approach": [
            1122085995
        ],
        "scaling\nhorizontally": [
            1122085995
        ],
        "stating": [
            1122085995
        ],
        "desire:": [
            1122085995
        ],
        "“i": [
            1122085995
        ],
        "to\nhave": [
            1122085995
        ],
        "running”": [
            1122085995
        ],
        "do\nit": [
            1122085995
        ],
        "imag-\nine": [
            1122085995
        ],
        "and\nthen": [
            1122085995
        ],
        "more\nwork": [
            1122085995
        ],
        "and\nin": [
            1122085995
        ],
        "you\nenable": [
            1122085995
        ],
        "\n42.7deleting": [
            1122085995
        ],
        "replicationcontroller\nwhen": [
            1122085995
        ],
        "also\ndeleted": [
            1122085995
        ],
        "integral": [
            1122085995
        ],
        "47.\n": [
            1122085995
        ],
        "replicaset\nfor": [
            1122085995
        ],
        "next).": [
            1122085995
        ],
        "the\nchange": [
            1122085995
        ],
        "line\n": [
            1122085995
        ],
        "\n\n104chapter": [
            1122085995
        ],
        "pods\npods": [
            1122085995
        ],
        "interruption": [
            1122085995
        ],
        "its\npods": [
            1122085995
        ],
        "passing": [
            1122085995
        ],
        "\n--cascade=false": [
            1122085995
        ],
        "--cascade=false\nreplicationcontroller": [
            1122085995
        ],
        "deleted\nyou’ve": [
            1122085995
        ],
        "the\nproper": [
            1122085995
        ],
        "again\n4.3using": [
            1122085995
        ],
        "replicationcontrollers\ninitially": [
            1122085995
        ],
        "replicating\npods": [
            1122085995
        ],
        "a\nreplicaset": [
            1122085995
        ],
        "generation": [
            1122085995
        ],
        "and\nreplaces": [
            1122085995
        ],
        "(replicationcontrollers": [
            1122085995
        ],
        "deprecated)": [
            1122085995
        ],
        "felt": [
            1122085995
        ],
        "in\nkubernetes": [
            1122085995
        ],
        "wild": [
            1122085995
        ],
        "good\nfor": [
            1122085995
        ],
        "shouldn’t\nhave": [
            1122085995
        ],
        "trouble": [
            1122085995
        ],
        "deletion\ndelete": [
            1122085995
        ],
        "rc\npod:\nkubia-q3vkg\npod:\nkubia-53thy\npod:\nkubia-k0xz6\npod:\nkubia-q3vkg\npod:\nkubia-53thy\npod:\nkubia-k0xz6\nreplicationcontroller:": [
            1122085995
        ],
        "kubiaapp:": [
            1122085995
        ],
        "kubia\napp:": [
            1122085995
        ],
        "kubia\nfigure": [
            1122085995
        ],
        "replication": [
            1122085995
        ],
        "--cascade=false": [
            1122085995
        ],
        "unmanaged\n": [
            1122085995
        ],
        "\n\n105using": [
            1122085995
        ],
        "replicationcontrollers\n": [
            1122085995
        ],
        "about\nin": [
            1122085995
        ],
        "dif-\nfer": [
            1122085995
        ],
        "replicationcontrollers\n4.3.1comparing": [
            1122085995
        ],
        "expressive\npod": [
            1122085995
        ],
        "matching\npods": [
            1122085995
        ],
        "of\nits": [
            1122085995
        ],
        "value\n": [
            1122085995
        ],
        "label\nenv=production": [
            1122085995
        ],
        "env=devel": [
            1122085995
        ],
        "match\neither": [
            1122085995
        ],
        "\nenv=production": [
            1122085995
        ],
        "merely": [
            1122085995
        ],
        "presence\nof": [
            1122085995
        ],
        "replica-\nset": [
            1122085995
        ],
        "\nenv": [
            1122085995
        ],
        "is\n(you": [
            1122085995
        ],
        "\nenv=*)\n4.3.2defining": [
            1122085995
        ],
        "replicaset\nyou’re": [
            1122085995
        ],
        "orphaned": [
            1122085995
        ],
        "abandoned": [
            1122085995
        ],
        "adopted\nby": [
            1122085995
        ],
        "rewrite": [
            1122085995
        ],
        "kubia-replicasetyaml": [
            1122085995
        ],
        "following\nlisting\napiversion:": [
            1122085995
        ],
        "apps/v1beta2": [
            1122085995
        ],
        "kubia\nspec:\n": [
            1122085995
        ],
        "3\n": [
            1122085995
        ],
        "matchlabels:": [
            1122085995
        ],
        "replicaset:": [
            1122085995
        ],
        "kubia-replicasetyaml\nreplicasets": [
            1122085995
        ],
        "\napi": [
            1122085995
        ],
        "\ngroup": [
            1122085995
        ],
        "v1beta2\nyou’re": [
            1122085995
        ],
        "matchlabels": [
            1122085995
        ],
        "\nselector": [
            1122085995
        ],
        "\nreplicationcontroller’s": [
            1122085995
        ],
        "selector\nthe": [
            1122085995
        ],
        "\n\n106chapter": [
            1122085995
        ],
        "pods\nthe": [
            1122085995
        ],
        "note": [
            1122085995
        ],
        "to\nensure": [
            1122085995
        ],
        "\napiversion": [
            1122085995
        ],
        "a\nresource": [
            1122085995
        ],
        "selector\nmatchlabels\n.": [
            1122085995
        ],
        "expressive)": [
            1122085995
        ],
        "selectors\nin": [
            1122085995
        ],
        "well\nbecause": [
            1122085995
        ],
        "ear-\nlier": [
            1122085995
        ],
        "replicaset\nwill": [
            1122085995
        ],
        "wing": [
            1122085995
        ],
        "\n43.3creating": [
            1122085995
        ],
        "replicaset\ncreate": [
            1122085995
        ],
        "after\nthat": [
            1122085995
        ],
        "describe:\n$": [
            1122085995
        ],
        "rs\nname": [
            1122085995
        ],
        "3s\ntipuse": [
            1122085995
        ],
        "rs": [
            1122085995
        ],
        "stands": [
            1122085995
        ],
        "replicaset\n$": [
            1122085995
        ],
        "rs\nname:": [
            1122085995
        ],
        "desired\npods": [
            1122085995
        ],
        "failed\npod": [
            1122085995
        ],
        "app=kubia\nabout": [
            1122085995
        ],
        "attribute\nthis": [
            1122085995
        ],
        "opportunity": [
            1122085995
        ],
        "apiversion": [
            1122085995
        ],
        "things:\nthe": [
            1122085995
        ],
        "(which": [
            1122085995
        ],
        "case)\nthe": [
            1122085995
        ],
        "(v1beta2)\nyou’ll": [
            1122085995
        ],
        "called\nthe": [
            1122085995
        ],
        "(you\njust": [
            1122085995
        ],
        "version—for": [
            1122085995
        ],
        "\napiversion:": [
            1122085995
        ],
        "when\ndefining": [
            1122085995
        ],
        "kubernetes\nversions": [
            1122085995
        ],
        "categorized": [
            1122085995
        ],
        "book’s\ncovers": [
            1122085995
        ],
        "respective": [
            1122085995
        ],
        "groups\n": [
            1122085995
        ],
        "\n\n107using": [
            1122085995
        ],
        "it’s\nshowing": [
            1122085995
        ],
        "see\nthey’re": [
            1122085995
        ],
        "new\nones": [
            1122085995
        ],
        "\n43.4using": [
            1122085995
        ],
        "selectors\nthe": [
            1122085995
        ],
        "more\nexpressive": [
            1122085995
        ],
        "intentionally": [
            1122085995
        ],
        "\nmatchlabels": [
            1122085995
        ],
        "\nmatchexpressions\nproperty": [
            1122085995
        ],
        "matchexpressions:": [
            1122085995
        ],
        "key:": [
            1122085995
        ],
        "operator:": [
            1122085995
        ],
        "values:": [
            1122085995
        ],
        "\nnoteonly": [
            1122085995
        ],
        "definition\nin": [
            1122085995
        ],
        "archive\nyou": [
            1122085995
        ],
        "expressions": [
            1122085995
        ],
        "expression\nmust": [
            1122085995
        ],
        "\nkey": [
            1122085995
        ],
        "operator": [
            1122085995
        ],
        "(depending": [
            1122085995
        ],
        "operator)": [
            1122085995
        ],
        "of\nvalues": [
            1122085995
        ],
        "valid": [
            1122085995
        ],
        "operators:\nin—label’s": [
            1122085995
        ],
        "values\nnotin—label’s": [
            1122085995
        ],
        "values\nexists—pod": [
            1122085995
        ],
        "import-\nant)": [
            1122085995
        ],
        "\nvalues": [
            1122085995
        ],
        "field\ndoesnotexist—pod": [
            1122085995
        ],
        "values\nproperty": [
            1122085995
        ],
        "specified\nif": [
            1122085995
        ],
        "evaluate": [
            1122085995
        ],
        "the\nselector": [
            1122085995
        ],
        "matchexpressions": [
            1122085995
        ],
        "selector\nlisting": [
            1122085995
        ],
        "kubia-replicaset-matchexpressionsyaml\nthis": [
            1122085995
        ],
        "\ncontain": [
            1122085995
        ],
        "key\nthe": [
            1122085995
        ],
        "\nmust": [
            1122085995
        ],
        "“kubia”\n": [
            1122085995
        ],
        "\n\n108chapter": [
            1122085995
        ],
        "pods\n43.5wrapping": [
            1122085995
        ],
        "replicasets\nthis": [
            1122085995
        ],
        "replicationcontrollers\nremember": [
            1122085995
        ],
        "find\nreplicationcontrollers": [
            1122085995
        ],
        "people’s": [
            1122085995
        ],
        "deployments\n": [
            1122085995
        ],
        "clean": [
            1122085995
        ],
        "the\nreplicaset": [
            1122085995
        ],
        "replicationcontroller:\n$": [
            1122085995
        ],
        "kubia\nreplicaset": [
            1122085995
        ],
        "deleted\ndeleting": [
            1122085995
        ],
        "that’s\nthe": [
            1122085995
        ],
        "\n44running": [
            1122085995
        ],
        "\ndaemonsets\nboth": [
            1122085995
        ],
        "number\nof": [
            1122085995
        ],
        "run\nexactly": [
            1122085995
        ],
        "48).\n": [
            1122085995
        ],
        "opera-\ntions": [
            1122085995
        ],
        "collector": [
            1122085995
        ],
        "every\nnode": [
            1122085995
        ],
        "work\nnode": [
            1122085995
        ],
        "1\npod\npod\npod\nreplicaset\nreplicas:": [
            1122085995
        ],
        "5\nnode": [
            1122085995
        ],
        "2\npod\npod\nnode": [
            1122085995
        ],
        "3\npod\ndaemonset\nexactly": [
            1122085995
        ],
        "replica\non": [
            1122085995
        ],
        "node\nnode": [
            1122085995
        ],
        "4\npod\npod\npod\nfigure": [
            1122085995
        ],
        "daemonsets": [
            1122085995
        ],
        "\nscatter": [
            1122085995
        ],
        "\n\n109running": [
            1122085995
        ],
        "daemonsets\noutside": [
            1122085995
        ],
        "init\nscripts": [
            1122085995
        ],
        "systemd": [
            1122085995
        ],
        "can\nstill": [
            1122085995
        ],
        "\n44.1using": [
            1122085995
        ],
        "node\nto": [
            1122085995
        ],
        "much\nlike": [
            1122085995
        ],
        "daemon-\nset": [
            1122085995
        ],
        "target": [
            1122085995
        ],
        "skip": [
            1122085995
        ],
        "they\naren’t": [
            1122085995
        ],
        "scattered": [
            1122085995
        ],
        "deploys\neach": [
            1122085995
        ],
        "48.\n": [
            1122085995
        ],
        "replicationcontroller)": [
            1122085995
        ],
        "notion": [
            1122085995
        ],
        "a\ndesired": [
            1122085995
        ],
        "match-\ning": [
            1122085995
        ],
        "else-\nwhere": [
            1122085995
        ],
        "immediately\ndeploys": [
            1122085995
        ],
        "inadvertently\ndeletes": [
            1122085995
        ],
        "daemonset’s": [
            1122085995
        ],
        "it\n4.4.2using": [
            1122085995
        ],
        "nodes\na": [
            1122085995
        ],
        "pods\nshould": [
            1122085995
        ],
        "\nnode-\nselector\n": [
            1122085995
        ],
        "definition\n(similar": [
            1122085995
        ],
        "3\na": [
            1122085995
        ],
        "similar—it": [
            1122085995
        ],
        "must\ndeploy": [
            1122085995
        ],
        "\nnotelater": [
            1122085995
        ],
        "unschedulable\npreventing": [
            1122085995
        ],
        "pods\neven": [
            1122085995
        ],
        "the\nscheduler": [
            1122085995
        ],
        "bypass": [
            1122085995
        ],
        "scheduler\ncompletely": [
            1122085995
        ],
        "desirable": [
            1122085995
        ],
        "run\nsystem": [
            1122085995
        ],
        "nodes\nexplaining": [
            1122085995
        ],
        "example\nlet’s": [
            1122085995
        ],
        "ssd-monitor": [
            1122085995
        ],
        "nodes\nthat": [
            1122085995
        ],
        "solid-state": [
            1122085995
        ],
        "(ssd)": [
            1122085995
        ],
        "dae-\nmon": [
            1122085995
        ],
        "have\nadded": [
            1122085995
        ],
        "\ndisk=ssd": [
            1122085995
        ],
        "selects": [
            1122085995
        ],
        "49.\n": [
            1122085995
        ],
        "\n\n110chapter": [
            1122085995
        ],
        "pods\ncreating": [
            1122085995
        ],
        "definition\nyou’ll": [
            1122085995
        ],
        "mock": [
            1122085995
        ],
        "prints\n“ssd": [
            1122085995
        ],
        "ok”": [
            1122085995
        ],
        "prepared": [
            1122085995
        ],
        "mock\ncontainer": [
            1122085995
        ],
        "building\nyour": [
            1122085995
        ],
        "ssd-monitor\nspec:": [
            1122085995
        ],
        "matchlabels:\n": [
            1122085995
        ],
        "ssd-monitor\n": [
            1122085995
        ],
        "metadata:\n": [
            1122085995
        ],
        "labels:\n": [
            1122085995
        ],
        "spec:\n": [
            1122085995
        ],
        "disk:": [
            1122085995
        ],
        "main\n": [
            1122085995
        ],
        "luksa/ssd-monitor\nyou’re": [
            1122085995
        ],
        "the\nluksa/ssd-monitor": [
            1122085995
        ],
        "each\nnode": [
            1122085995
        ],
        "label\nlisting": [
            1122085995
        ],
        "daemonset:": [
            1122085995
        ],
        "ssd-monitor-daemonsetyaml\nnode": [
            1122085995
        ],
        "1\npod:\nssd-monitor\nnode": [
            1122085995
        ],
        "2node": [
            1122085995
        ],
        "3\ndaemonset:\nsssd-monitor\nnode": [
            1122085995
        ],
        "selector:\ndisk=ssd\nnode": [
            1122085995
        ],
        "4\ndisk:": [
            1122085995
        ],
        "ssddisk:": [
            1122085995
        ],
        "ssd\nunschedulable\npod:\nssd-monitor\npod:\nssd-monitor\nfigure": [
            1122085995
        ],
        "\nnodes\ndaemonsets": [
            1122085995
        ],
        "\napps": [
            1122085995
        ],
        "\nversion": [
            1122085995
        ],
        "v1beta2\nthe": [
            1122085995
        ],
        "disk=ssd": [
            1122085995
        ],
        "\n\n111running": [
            1122085995
        ],
        "daemonsets\ncreating": [
            1122085995
        ],
        "daemonset\nyou’ll": [
            1122085995
        ],
        "file:\n$": [
            1122085995
        ],
        "ssd-monitor-daemonsetyaml\ndaemonset": [
            1122085995
        ],
        "ssd-monitor\"": [
            1122085995
        ],
        "created\nlet’s": [
            1122085995
        ],
        "daemonset:\n$": [
            1122085995
        ],
        "ds\nname": [
            1122085995
        ],
        "node-selector": [
            1122085995
        ],
        "\nssd-monitor": [
            1122085995
        ],
        "disk=ssd\nthose": [
            1122085995
        ],
        "zeroes": [
            1122085995
        ],
        "strange": [
            1122085995
        ],
        "po\nno": [
            1122085995
        ],
        "found\nwhere": [
            1122085995
        ],
        "on?": [
            1122085995
        ],
        "forgot": [
            1122085995
        ],
        "nodes\nwith": [
            1122085995
        ],
        "problem—you": [
            1122085995
        ],
        "should\ndetect": [
            1122085995
        ],
        "nodes’": [
            1122085995
        ],
        "a\nmatching": [
            1122085995
        ],
        "\nadding": [
            1122085995
        ],
        "node(s)\nregardless": [
            1122085995
        ],
        "labeling": [
            1122085995
        ],
        "it:\n$": [
            1122085995
        ],
        "version\nminikube": [
            1122085995
        ],
        "4d": [
            1122085995
        ],
        "v16.0\nnow": [
            1122085995
        ],
        "disk=ssd\nnode": [
            1122085995
        ],
        "minikube\"": [
            1122085995
        ],
        "labeled\nnotereplace": [
            1122085995
        ],
        "not\nusing": [
            1122085995
        ],
        "see:\n$": [
            1122085995
        ],
        "po\nname": [
            1122085995
        ],
        "age\nssd-monitor-hgxwq": [
            1122085995
        ],
        "35s\nokay;": [
            1122085995
        ],
        "further\nnodes": [
            1122085995
        ],
        "\nremoving": [
            1122085995
        ],
        "node\nnow": [
            1122085995
        ],
        "mislabeled": [
            1122085995
        ],
        "a\nspinning": [
            1122085995
        ],
        "label?\n$": [
            1122085995
        ],
        "disk=hdd": [
            1122085995
        ],
        "--overwrite\nnode": [
            1122085995
        ],
        "labeled\n": [
            1122085995
        ],
        "\n\n112chapter": [
            1122085995
        ],
        "pods\nlet’s": [
            1122085995
        ],
        "node:\n$": [
            1122085995
        ],
        "4m\nthe": [
            1122085995
        ],
        "knew": [
            1122085995
        ],
        "right?": [
            1122085995
        ],
        "this\nwraps": [
            1122085995
        ],
        "exploration": [
            1122085995
        ],
        "\nssd-monitor\ndaemonset": [
            1122085995
        ],
        "deleting\nthe": [
            1122085995
        ],
        "\n45running": [
            1122085995
        ],
        "\nup": [
            1122085995
        ],
        "have\ncases": [
            1122085995
        ],
        "terminates": [
            1122085995
        ],
        "work\nreplicationcontrollers": [
            1122085995
        ],
        "tasks": [
            1122085995
        ],
        "are\nnever": [
            1122085995
        ],
        "completed": [
            1122085995
        ],
        "but\nin": [
            1122085995
        ],
        "\n45.1introducing": [
            1122085995
        ],
        "resource\nkubernetes": [
            1122085995
        ],
        "discussed": [
            1122085995
        ],
        "whose\ncontainer": [
            1122085995
        ],
        "finishes": [
            1122085995
        ],
        "it\ndoes": [
            1122085995
        ],
        "will\nbe": [
            1122085995
        ],
        "rescheduled": [
            1122085995
        ],
        "code)": [
            1122085995
        ],
        "config-\nured": [
            1122085995
        ],
        "the\nnode": [
            1122085995
        ],
        "pod\nwhich": [
            1122085995
        ],
        "ad": [
            1122085995
        ],
        "hoc": [
            1122085995
        ],
        "crucial": [
            1122085995
        ],
        "fin-\nishes": [
            1122085995
        ],
        "finish\nbut": [
            1122085995
        ],
        "is\nperforming": [
            1122085995
        ],
        "doesn’t\nmake": [
            1122085995
        ],
        "sense—especially": [
            1122085995
        ],
        "hours": [
            1122085995,
            1478412827
        ],
        "you\nneeded": [
            1122085995
        ],
        "export": [
            1122085995
        ],
        "emulate": [
            1122085995
        ],
        "invokes": [
            1122085995
        ],
        "sleep\ncommand": [
            1122085995
        ],
        "hub\nbut": [
            1122085995
        ],
        "peek": [
            1122085995
        ],
        "archive\n": [
            1122085995
        ],
        "\n\n113running": [
            1122085995
        ],
        "task\n45.2defining": [
            1122085995
        ],
        "resource\ncreate": [
            1122085995
        ],
        "batch/v1": [
            1122085995
        ],
        "batch-job\nspec:": [
            1122085995
        ],
        "batch-job": [
            1122085995
        ],
        "onfailure": [
            1122085995
        ],
        "luksa/batch-job\njobs": [
            1122085995
        ],
        "\nluksa/batch-job": [
            1122085995
        ],
        "exits": [
            1122085995
        ],
        "the\nprocesses": [
            1122085995
        ],
        "finish": [
            1122085995
        ],
        "\nrestartpolicy\nlisting": [
            1122085995
        ],
        "job:": [
            1122085995
        ],
        "exporteryaml\nnode": [
            1122085995
        ],
        "(unmanaged)\npod": [
            1122085995
        ],
        "(managed": [
            1122085995
        ],
        "replicaset)\npod": [
            1122085995
        ],
        "c": [
            1122085995
        ],
        "job)\nnode": [
            1122085995
        ],
        "failsjob": [
            1122085995
        ],
        "c2": [
            1122085995
        ],
        "finishes\ntime\npod": [
            1122085995
        ],
        "b2": [
            1122085995
        ],
        "job)\npod": [
            1122085995
        ],
        "rescheduled\nbecause": [
            1122085995
        ],
        "nothing\nmanaging": [
            1122085995
        ],
        "it\nfigure": [
            1122085995
        ],
        "successfully\njobs": [
            1122085995
        ],
        "v1\nyou’re": [
            1122085995
        ],
        "\nbased": [
            1122085995
        ],
        "template)\njobs": [
            1122085995
        ],
        "\nwhich": [
            1122085995
        ],
        "always\n": [
            1122085995
        ],
        "\n\n114chapter": [
            1122085995
        ],
        "pods\npod": [
            1122085995
        ],
        "defaults": [
            1122085995
        ],
        "policy\nbecause": [
            1122085995
        ],
        "indefinitely": [
            1122085995
        ],
        "set\nthe": [
            1122085995
        ],
        "\nonfailure": [
            1122085995
        ],
        "prevents": [
            1122085995
        ],
        "resource)\n4.5.3seeing": [
            1122085995
        ],
        "immediately:\n$": [
            1122085995
        ],
        "jobs\nname": [
            1122085995
        ],
        "age\nbatch-job": [
            1122085995
        ],
        "2s\n$": [
            1122085995
        ],
        "age\nbatch-job-28qf4": [
            1122085995
        ],
        "4s\nafter": [
            1122085995
        ],
        "passed": [
            1122085995
        ],
        "\n--show-all": [
            1122085995
        ],
        "-a)": [
            1122085995
        ],
        "-a\nname": [
            1122085995
        ],
        "logs;\nfor": [
            1122085995
        ],
        "example:\n$": [
            1122085995
        ],
        "batch-job-28qf4\nfri": [
            1122085995
        ],
        "09:58:22": [
            1122085995
        ],
        "utc": [
            1122085995
        ],
        "starting\nfri": [
            1122085995
        ],
        "10:00:22": [
            1122085995
        ],
        "finished": [
            1122085995
        ],
        "succesfully\nthe": [
            1122085995
        ],
        "do\nthat": [
            1122085995
        ],
        "again:\n$": [
            1122085995
        ],
        "job\nname": [
            1122085995
        ],
        "9m\nthe": [
            1122085995
        ],
        "informa-\ntion": [
            1122085995
        ],
        "\nyes": [
            1122085995
        ],
        "true?": [
            1122085995
        ],
        "column\nindicate?": [
            1122085995
        ],
        "\n45.4running": [
            1122085995
        ],
        "job\njobs": [
            1122085995
        ],
        "paral-\nlel": [
            1122085995
        ],
        "sequentially": [
            1122085995
        ],
        "\ncompletions": [
            1122085995
        ],
        "parallelism": [
            1122085995
        ],
        "prop-\nerties": [
            1122085995
        ],
        "spec\n": [
            1122085995
        ],
        "\n\n115running": [
            1122085995
        ],
        "task\nrunning": [
            1122085995
        ],
        "sequentially\nif": [
            1122085995
        ],
        "completions": [
            1122085995
        ],
        "job’s": [
            1122085995
        ],
        "example\napiversion:": [
            1122085995
        ],
        "batch/v1\nkind:": [
            1122085995
        ],
        "job\nmetadata:\n": [
            1122085995
        ],
        "multi-completion-batch-job\nspec:\n": [
            1122085995
        ],
        "completions:": [
            1122085995
        ],
        "<template": [
            1122085995
        ],
        "411>\nthis": [
            1122085995
        ],
        "com-\nplete": [
            1122085995
        ],
        "may\ncreate": [
            1122085995
        ],
        "overall\nrunning": [
            1122085995
        ],
        "parallel\ninstead": [
            1122085995
        ],
        "run\nmultiple": [
            1122085995
        ],
        "parallel\nwith": [
            1122085995
        ],
        "\nparallelism": [
            1122085995
        ],
        "parallelism:": [
            1122085995
        ],
        "<same": [
            1122085995
        ],
        "411>\nby": [
            1122085995
        ],
        "parallel:\n$": [
            1122085995
        ],
        "age\nmulti-completion-batch-job-lmmnk": [
            1122085995
        ],
        "21s\nmulti-completion-batch-job-qx4nq": [
            1122085995
        ],
        "21s\nas": [
            1122085995
        ],
        "finish\nsuccessfully\nlisting": [
            1122085995
        ],
        "multi-completion-batch-jobyaml\nlisting": [
            1122085995
        ],
        "parallel:": [
            1122085995
        ],
        "multi-completion-parallel-batch-jobyaml\nsetting": [
            1122085995
        ],
        "\n5": [
            1122085995
        ],
        "sequentially\nthis": [
            1122085995
        ],
        "\nsuccessfully\nup": [
            1122085995
        ],
        "parallel\n": [
            1122085995
        ],
        "\n\n116chapter": [
            1122085995
        ],
        "pods\nscaling": [
            1122085995
        ],
        "job\nyou": [
            1122085995
        ],
        "is\nsimilar": [
            1122085995
        ],
        "multi-completion-batch-job": [
            1122085995
        ],
        "--replicas": [
            1122085995
        ],
        "3\njob": [
            1122085995
        ],
        "multi-completion-batch-job\"": [
            1122085995
        ],
        "scaled\nbecause": [
            1122085995
        ],
        "spun\nup": [
            1122085995
        ],
        "running\n4.5.5limiting": [
            1122085995
        ],
        "complete\nwe": [
            1122085995
        ],
        "finish?": [
            1122085995
        ],
        "fast\nenough)?\n": [
            1122085995
        ],
        "\nactivedeadlineseconds": [
            1122085995
        ],
        "will\nmark": [
            1122085995
        ],
        "\nnoteyou": [
            1122085995
        ],
        "retried": [
            1122085995
        ],
        "is\nmarked": [
            1122085995
        ],
        "\nspecbackofflimit": [
            1122085995
        ],
        "dont": [
            1122085995
        ],
        "6\n4.6scheduling": [
            1122085995
        ],
        "future\njob": [
            1122085995
        ],
        "many\nbatch": [
            1122085995
        ],
        "repeatedly": [
            1122085995
        ],
        "specified\ninterval": [
            1122085995
        ],
        "linux-": [
            1122085995
        ],
        "unix-like": [
            1122085995
        ],
        "as\ncron": [
            1122085995
        ],
        "too\n": [
            1122085995
        ],
        "cron": [
            1122085995
        ],
        "the\nschedule": [
            1122085995
        ],
        "well-known": [
            1122085995
        ],
        "you’re\nfamiliar": [
            1122085995
        ],
        "cronjobs": [
            1122085995
        ],
        "matter\nof": [
            1122085995
        ],
        "seconds\n": [
            1122085995
        ],
        "job\ntemplate": [
            1122085995
        ],
        "or\nmore": [
            1122085995
        ],
        "as\nyou": [
            1122085995
        ],
        "\n46.1creating": [
            1122085995
        ],
        "cronjob\nimagine": [
            1122085995
        ],
        "minutes\nto": [
            1122085995
        ],
        "specification\n": [
            1122085995
        ],
        "\n\n117scheduling": [
            1122085995
        ],
        "future\napiversion:": [
            1122085995
        ],
        "batch/v1beta1": [
            1122085995
        ],
        "cronjob\nmetadata:\n": [
            1122085995
        ],
        "batch-job-every-fifteen-minutes\nspec:\n": [
            1122085995
        ],
        "schedule:": [
            1122085995
        ],
        "015,30,45": [
            1122085995
        ],
        "*": [
            1122085995
        ],
        "jobtemplate:\n": [
            1122085995
        ],
        "periodic-batch-job": [
            1122085995
        ],
        "luksa/batch-job": [
            1122085995
        ],
        "\nas": [
            1122085995
        ],
        "template\nfrom": [
            1122085995
        ],
        "\nconfiguring": [
            1122085995
        ],
        "schedule\nif": [
            1122085995
        ],
        "tutorials": [
            1122085995
        ],
        "and\nexplanations": [
            1122085995
        ],
        "con-\ntains": [
            1122085995
        ],
        "entries:\nminute\nhour\nday": [
            1122085995
        ],
        "month\nmonth\nday": [
            1122085995
        ],
        "week\nin": [
            1122085995
        ],
        "be\n015,30,45": [
            1122085995
        ],
        "mark": [
            1122085995
        ],
        "hour\n(first": [
            1122085995
        ],
        "asterisk)": [
            1122085995
        ],
        "(second": [
            1122085995
        ],
        "(third\nasterisk)": [
            1122085995
        ],
        "week": [
            1122085995
        ],
        "(fourth": [
            1122085995
        ],
        "the\nmonth": [
            1122085995
        ],
        "\n030": [
            1122085995
        ],
        "3am": [
            1122085995
        ],
        "every\nsunday": [
            1122085995
        ],
        "\n0": [
            1122085995
        ],
        "sunday)\nconfiguring": [
            1122085995
        ],
        "jobtemplate": [
            1122085995
        ],
        "the\ncronjob": [
            1122085995
        ],
        "it\n4.6.2understanding": [
            1122085995
        ],
        "run\njob": [
            1122085995
        ],
        "approximately": [
            1122085995
        ],
        "sched-\nuled": [
            1122085995
        ],
        "resource:": [
            1122085995
        ],
        "cronjobyaml\napi": [
            1122085995
        ],
        "v1beta1\nthis": [
            1122085995
        ],
        "\nevery": [
            1122085995
        ],
        "day\nthe": [
            1122085995
        ],
        "\njob": [
            1122085995
        ],
        "\nwill": [
            1122085995
        ],
        "cronjob\n": [
            1122085995
        ],
        "\n\n118chapter": [
            1122085995
        ],
        "have\na": [
            1122085995
        ],
        "requirement": [
            1122085995
        ],
        "deadline": [
            1122085995
        ],
        "\nstartingdeadlineseconds": [
            1122085995
        ],
        "field\nin": [
            1122085995
        ],
        "batch/v1beta1\nkind:": [
            1122085995
        ],
        "cronjob\nspec:\n": [
            1122085995
        ],
        "*\n": [
            1122085995
        ],
        "startingdeadlineseconds:": [
            1122085995
        ],
        "..\nin": [
            1122085995
        ],
        "10:30:00\nif": [
            1122085995
        ],
        "10:30:15": [
            1122085995
        ],
        "be\nshown": [
            1122085995
        ],
        "circumstances": [
            1122085995
        ],
        "exe-\ncution": [
            1122085995
        ],
        "combat": [
            1122085995
        ],
        "idempo-\ntent": [
            1122085995
        ],
        "(running": [
            1122085995
        ],
        "unwanted\nresults)": [
            1122085995
        ],
        "work\nthat": [
            1122085995
        ],
        "(missed)": [
            1122085995
        ],
        "run\n4.7summary\nyou’ve": [
            1122085995
        ],
        "the\nevent": [
            1122085995
        ],
        "that\nyou": [
            1122085995
        ],
        "as\nsoon": [
            1122085995
        ],
        "(where": [
            1122085995
        ],
        "considered\nhealthy)\npods": [
            1122085995
        ],
        "they’re\ndeleted": [
            1122085995
        ],
        "evicted\nfrom": [
            1122085995
        ],
        "node\nreplicationcontrollers": [
            1122085995
        ],
        "replicas\nrunning\nscaling": [
            1122085995
        ],
        "a\nreplicationcontroller\npods": [
            1122085995
        ],
        "owned": [
            1122085995
        ],
        "between\nthem": [
            1122085995
        ],
        "necessary\na": [
            1122085995
        ],
        "the\ntemplate": [
            1122085995
        ],
        "startingdeadlineseconds": [
            1122085995
        ],
        "cronjob\nat": [
            1122085995
        ],
        "\npast": [
            1122085995
        ],
        "time\n": [
            1122085995
        ],
        "\n\n119summary\nreplicationcontrollers": [
            1122085995
        ],
        "deployments\nwhich": [
            1122085995
        ],
        "functionality": [
            1122085995
        ],
        "features\nreplicationcontrollers": [
            1122085995
        ],
        "random": [
            1122085995
        ],
        "nodes\nwhereas": [
            1122085995
        ],
        "pod\ndefined": [
            1122085995
        ],
        "daemonset\npods": [
            1122085995
        ],
        "job\nresource": [
            1122085995
        ],
        "object\njobs": [
            1122085995
        ],
        "sometime": [
            1122085995
        ],
        "cronjob\nresources": [
            1122085995
        ],
        "\n\n120\nservices:": [
            1122085995
        ],
        "enabling\nclients": [
            1122085995
        ],
        "discover\nand": [
            1122085995
        ],
        "similar\nresources": [
            1122085995
        ],
        "work\nindependently": [
            1122085995
        ],
        "stimulus": [
            1122085995
        ],
        "to\nrespond": [
            1122085995
        ],
        "will\nusually": [
            1122085995
        ],
        "cluster\nor": [
            1122085995
        ],
        "they\nprovide": [
            1122085995
        ],
        "configure\nthis": [
            1122085995
        ],
        "address\ndiscovering": [
            1122085995
        ],
        "cluster\nexposing": [
            1122085995
        ],
        "clients\nconnecting": [
            1122085995
        ],
        "\ncluster\ncontrolling": [
            1122085995
        ],
        "not\ntroubleshooting": [
            1122085995
        ],
        "services\n": [
            1122085995
        ],
        "\n\n121introducing": [
            1122085995
        ],
        "services\neach": [
            1122085995
        ],
        "providing\nthe": [
            1122085995
        ],
        "wouldn’t\nwork": [
            1122085995
        ],
        "because\npods": [
            1122085995
        ],
        "ephemeral—they": [
            1122085995
        ],
        "someone\nscaled": [
            1122085995
        ],
        "failed\nkubernetes": [
            1122085995
        ],
        "assigns": [
            1122085995
        ],
        "started—clients": [
            1122085995
        ],
        "pod\nup": [
            1122085995
        ],
        "front\nhorizontal": [
            1122085995
        ],
        "service—each": [
            1122085995
        ],
        "those\npods": [
            1122085995
        ],
        "backing\nthe": [
            1122085995
        ],
        "ips": [
            1122085995
        ],
        "the\nindividual": [
            1122085995
        ],
        "address\nto": [
            1122085995
        ],
        "type—services—\nthat": [
            1122085995
        ],
        "chapter\n5.1introducing": [
            1122085995
        ],
        "services\na": [
            1122085995
        ],
        "of\nentry": [
            1122085995
        ],
        "address\nand": [
            1122085995
        ],
        "to\nthat": [
            1122085995
        ],
        "routed": [
            1122085995
        ],
        "backing\nthat": [
            1122085995
        ],
        "cluster\nat": [
            1122085995
        ],
        "\nexplaining": [
            1122085995
        ],
        "revisit": [
            1122085995
        ],
        "data-\nbase": [
            1122085995
        ],
        "may\nonly": [
            1122085995
        ],
        "the\nsystem": [
            1122085995
        ],
        "function:\nexternal": [
            1122085995
        ],
        "there’s\nonly": [
            1122085995
        ],
        "hundreds\nthe": [
            1122085995
        ],
        "causing\nits": [
            1122085995
        ],
        "reconfigure": [
            1122085995
        ],
        "every\ntime": [
            1122085995
        ],
        "moved\nby": [
            1122085995
        ],
        "external\nclients": [
            1122085995
        ],
        "backend\npod": [
            1122085995
        ],
        "doesn’t\n": [
            1122085995
        ],
        "\n\n122chapter": [
            1122085995
        ],
        "5services:": [
            1122085995
        ],
        "pods\nchange": [
            1122085995
        ],
        "you\nalso": [
            1122085995
        ],
        "through\neither": [
            1122085995
        ],
        "ser-\nvices": [
            1122085995
        ],
        "backing": [
            1122085995
        ],
        "between\nthem)": [
            1122085995
        ],
        "51.\nyou": [
            1122085995
        ],
        "see-\ning": [
            1122085995
        ],
        "created\n5.1.1creating": [
            1122085995
        ],
        "services\nas": [
            1122085995
        ],
        "ser-\nvice": [
            1122085995
        ],
        "load-balanced": [
            1122085995
        ],
        "define\nwhich": [
            1122085995
        ],
        "aren’t?": [
            1122085995
        ],
        "52.\n": [
            1122085995
        ],
        "three\ninstances": [
            1122085995
        ],
        "replicationcontroller\nagain": [
            1122085995
        ],
        "\nfrontend": [
            1122085995
        ],
        "21.1.1\nexternal": [
            1122085995
        ],
        "client\nfrontend": [
            1122085995
        ],
        "21.1.2\nfrontend": [
            1122085995
        ],
        "21.1.3\nbackend": [
            1122085995
        ],
        "pod\nip:": [
            1122085995
        ],
        "21.1.4\nfrontend": [
            1122085995
        ],
        "service\nip:": [
            1122085995
        ],
        "11.1.1\nbackend": [
            1122085995
        ],
        "11.1.2\nfrontend": [
            1122085995
        ],
        "components\nbackend": [
            1122085995
        ],
        "components\nfigure": [
            1122085995
        ],
        "\n\n123introducing": [
            1122085995
        ],
        "services\ncreating": [
            1122085995
        ],
        "expose\nthe": [
            1122085995
        ],
        "easiest": [
            1122085995
        ],
        "already\nused": [
            1122085995
        ],
        "the\nexpose": [
            1122085995
        ],
        "one\nused": [
            1122085995
        ],
        "by\nposting": [
            1122085995
        ],
        "descriptor\ncreate": [
            1122085995
        ],
        "kubia-svcyaml": [
            1122085995
        ],
        "contents\napiversion:": [
            1122085995
        ],
        "ports:\n": [
            1122085995
        ],
        "targetport:": [
            1122085995
        ],
        "\nyou’re": [
            1122085995
        ],
        "and\nroute": [
            1122085995
        ],
        "\napp=kubia\nlabel": [
            1122085995
        ],
        "create\nlisting": [
            1122085995
        ],
        "service:": [
            1122085995
        ],
        "kubia-svcyaml\napp:": [
            1122085995
        ],
        "kubia\npod:": [
            1122085995
        ],
        "kubia-q3vkg\npod:": [
            1122085995
        ],
        "kubia-k0xz6\npod:": [
            1122085995
        ],
        "kubia-53thy\nclient\nservice:": [
            1122085995
        ],
        "kubia\nselector:app=kubia\napp:": [
            1122085995
        ],
        "\ndetermine": [
            1122085995
        ],
        "to\nall": [
            1122085995
        ],
        "service\n": [
            1122085995
        ],
        "\n\n124chapter": [
            1122085995
        ],
        "pods\nexamining": [
            1122085995
        ],
        "service\nafter": [
            1122085995
        ],
        "see\nthat": [
            1122085995
        ],
        "service:\n$": [
            1122085995
        ],
        "10111.240.1": [
            1122085995
        ],
        "30d\nkubia": [
            1122085995
        ],
        "10111.249.153": [
            1122085995
        ],
        "80/tcp": [
            1122085995
        ],
        "6m": [
            1122085995
        ],
        "10111.249.153.": [
            1122085995
        ],
        "purpose\nof": [
            1122085995
        ],
        "usually\nalso": [
            1122085995
        ],
        "let’s\nuse": [
            1122085995
        ],
        "does\ntesting": [
            1122085995
        ],
        "cluster\nyou": [
            1122085995
        ],
        "ways:\nthe": [
            1122085995
        ],
        "service’s\ncluster": [
            1122085995
        ],
        "see\nwhat": [
            1122085995
        ],
        "was\nyou": [
            1122085995
        ],
        "command\nyou": [
            1122085995
        ],
        "through\nthe": [
            1122085995
        ],
        "command\nlet’s": [
            1122085995
        ],
        "\nremotely": [
            1122085995
        ],
        "containers\nthe": [
            1122085995
        ],
        "remotely": [
            1122085995
        ],
        "inside\nan": [
            1122085995
        ],
        "the\ncontents": [
            1122085995
        ],
        "\nkubectl\nget\n": [
            1122085995
        ],
        "chosen": [
            1122085995
        ],
        "\nkubia-7nog1": [
            1122085995
        ],
        "target)": [
            1122085995
        ],
        "to\nobtain": [
            1122085995
        ],
        "(using": [
            1122085995
        ],
        "own:": [
            1122085995
        ],
        "kubia-7nog1": [
            1122085995
        ],
        "--": [
            1122085995
        ],
        "http://10111.249.153\nyou’ve": [
            1122085995
        ],
        "kubia-gzwli\nif": [
            1122085995
        ],
        "recognize\nthat": [
            1122085995
        ],
        "\nhere’s": [
            1122085995
        ],
        "\nservice\n": [
            1122085995
        ],
        "\n\n125introducing": [
            1122085995
        ],
        "services\nlet’s": [
            1122085995
        ],
        "the\nsequence": [
            1122085995
        ],
        "\ncurl": [
            1122085995
        ],
        "is\nbacked": [
            1122085995
        ],
        "intercepted": [
            1122085995
        ],
        "connection\nselected": [
            1122085995
        ],
        "forwarded": [
            1122085995
        ],
        "nodejs\nrunning": [
            1122085995
        ],
        "con-\ntaining": [
            1122085995
        ],
        "which\nwas": [
            1122085995
        ],
        "\nkubectl\nwhy": [
            1122085995
        ],
        "dash?\nthe": [
            1122085995
        ],
        "dash": [
            1122085995
        ],
        "(--)": [
            1122085995
        ],
        "for\nkubectl": [
            1122085995
        ],
        "executed\ninside": [
            1122085995
        ],
        "no\narguments": [
            1122085995
        ],
        "dash\nthere": [
            1122085995
        ],
        "\n-s": [
            1122085995
        ],
        "interpreted": [
            1122085995
        ],
        "would\nresult": [
            1122085995
        ],
        "misleading": [
            1122085995
        ],
        "error:\n$": [
            1122085995
        ],
        "http://10111.249.153\nthe": [
            1122085995
        ],
        "refused": [
            1122085995
        ],
        "\nspecify": [
            1122085995
        ],
        "port?\nthis": [
            1122085995
        ],
        "refusing": [
            1122085995
        ],
        "because\nkubectl": [
            1122085995
        ],
        "option\nis": [
            1122085995
        ],
        "default)\n3.": [
            1122085995
        ],
        "request\n4": [
            1122085995
        ],
        "http\nconnection": [
            1122085995
        ],
        "randomly\nselected": [
            1122085995
        ],
        "pod\n2": [
            1122085995
        ],
        "container\nrunning": [
            1122085995
        ],
        "nodejs\n6.": [
            1122085995
        ],
        "sentcurl\nback": [
            1122085995
        ],
        "and\nprinted": [
            1122085995
        ],
        "it\n5": [
            1122085995
        ],
        "is\nsent": [
            1122085995
        ],
        "curl\npod:": [
            1122085995
        ],
        "kubia-7nog1\ncontainer\nnodejs\ncurl": [
            1122085995
        ],
        "http://\n10111.249.153\npod:": [
            1122085995
        ],
        "kubia-gzwli\ncontainer\nnodejs\npod:": [
            1122085995
        ],
        "kubia-5fje3\ncontainer\nnodejs\n1.kubectl": [
            1122085995
        ],
        "exec\nservice:": [
            1122085995
        ],
        "kubia\n10111.249.153:80\nfigure": [
            1122085995
        ],
        "\n\n126chapter": [
            1122085995
        ],
        "but\ninside": [
            1122085995
        ],
        "service\nconfiguring": [
            1122085995
        ],
        "service\nif": [
            1122085995
        ],
        "pod\nwith": [
            1122085995
        ],
        "invocation": [
            1122085995
        ],
        "forwards": [
            1122085995
        ],
        "connection\nto": [
            1122085995
        ],
        "redi-\nrected": [
            1122085995
        ],
        "\nsessionaffinity": [
            1122085995
        ],
        "property\nto": [
            1122085995
        ],
        "\nclientip": [
            1122085995
        ],
        "(instead": [
            1122085995
        ],
        "sessionaffinity:": [
            1122085995
        ],
        "clientip\n": [
            1122085995
        ],
        "redirect": [
            1122085995
        ],
        "originating": [
            1122085995
        ],
        "ip\nto": [
            1122085995
        ],
        "affin-\nity": [
            1122085995
        ],
        "affinity:": [
            1122085995
        ],
        "\nnone": [
            1122085995
        ],
        "clientip\nyou": [
            1122085995
        ],
        "cookie-based": [
            1122085995
        ],
        "services\ndeal": [
            1122085995
        ],
        "udp": [
            1122085995
        ],
        "payload": [
            1122085995
        ],
        "carry": [
            1122085995
        ],
        "because\ncookies": [
            1122085995
        ],
        "protocol": [
            1122085995
        ],
        "which\nexplains": [
            1122085995
        ],
        "cookies": [
            1122085995
        ],
        "\nexposing": [
            1122085995
        ],
        "service\nyour": [
            1122085995
        ],
        "listened": [
            1122085995
        ],
        "ports—let’s": [
            1122085995
        ],
        "for\nhttps—you": [
            1122085995
        ],
        "pod’s\nports": [
            1122085995
        ],
        "using\na": [
            1122085995
        ],
        "multi-port": [
            1122085995
        ],
        "ip\nnotewhen": [
            1122085995
        ],
        "name\nfor": [
            1122085995
        ],
        "clientip": [
            1122085995
        ],
        "configured\nlisting": [
            1122085995
        ],
        "definition\n": [
            1122085995
        ],
        "\n\n127introducing": [
            1122085995
        ],
        "services\nspec:\n": [
            1122085995
        ],
        "https": [
            1122085995
        ],
        "applies": [
            1122085995
        ],
        "whole—it": [
            1122085995
        ],
        "different\nsubsets": [
            1122085995
        ],
        "services\nbecause": [
            1122085995
        ],
        "listen": [
            1122085995
        ],
        "service\nand": [
            1122085995
        ],
        "you\nusing": [
            1122085995
        ],
        "ports\nin": [
            1122085995
        ],
        "also\ngive": [
            1122085995
        ],
        "makes\nthe": [
            1122085995
        ],
        "well-known\n": [
            1122085995
        ],
        "suppose": [
            1122085995
        ],
        "listing\nkind:": [
            1122085995
        ],
        "pod\nspec:\n": [
            1122085995
        ],
        "definition\nlisting": [
            1122085995
        ],
        "service\nport": [
            1122085995
        ],
        "8080\nport": [
            1122085995
        ],
        "\npods’": [
            1122085995
        ],
        "8443\nthe": [
            1122085995
        ],
        "\napplies": [
            1122085995
        ],
        "service\ncontainer’s": [
            1122085995
        ],
        "http\nport": [
            1122085995
        ],
        "https\nport": [
            1122085995
        ],
        "\ncontainer’s": [
            1122085995
        ],
        "\nport": [
            1122085995
        ],
        "https\n": [
            1122085995
        ],
        "\n\n128chapter": [
            1122085995
        ],
        "ports?": [
            1122085995
        ],
        "is\nthat": [
            1122085995
        ],
        "service\nspec": [
            1122085995
        ],
        "you’d\nlike": [
            1122085995
        ],
        "80?": [
            1122085995
        ],
        "(while": [
            1122085995
        ],
        "port’s": [
            1122085995
        ],
        "unchanged)": [
            1122085995
        ],
        "numbers\ndepending": [
            1122085995
        ],
        "receiving": [
            1122085995
        ],
        "(port": [
            1122085995
        ],
        "80\non": [
            1122085995
        ],
        "ones)\n5.1.2discovering": [
            1122085995
        ],
        "services\nby": [
            1122085995
        ],
        "unchanged": [
            1122085995
        ],
        "may\nchange": [
            1122085995
        ],
        "the\nservice’s": [
            1122085995
        ],
        "service?": [
            1122085995
        ],
        "config-\nuration": [
            1122085995
        ],
        "pod?": [
            1122085995
        ],
        "client\npods": [
            1122085995
        ],
        "port\ndiscovering": [
            1122085995
        ],
        "variables\nwhen": [
            1122085995
        ],
        "initializes": [
            1122085995
        ],
        "pointing\nto": [
            1122085995
        ],
        "the\nclient": [
            1122085995
        ],
        "by\ninspecting": [
            1122085995
        ],
        "environment\nof": [
            1122085995
        ],
        "exec\ncommand": [
            1122085995
        ],
        "only\nafter": [
            1122085995
        ],
        "couldn’t\nhave": [
            1122085995
        ],
        "first\n": [
            1122085995
        ],
        "delete\nall": [
            1122085995
        ],
        "remember\nyou": [
            1122085995
        ],
        "kubia-7nog1\"": [
            1122085995
        ],
        "kubia-bf50t\"": [
            1122085995
        ],
        "kubia-gzwli\"": [
            1122085995
        ],
        "(i’m": [
            1122085995
        ],
        "that)": [
            1122085995
        ],
        "as\nyour": [
            1122085995
        ],
        "container\nas": [
            1122085995
        ],
        "\n\n129introducing": [
            1122085995
        ],
        "services\n$": [
            1122085995
        ],
        "kubia-3inly": [
            1122085995
        ],
        "env\npath=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nhostname=kubia-3inly\nkubernetes_service_host=10111.240.1\nkubernetes_service_port=443\n...\nkubia_service_host=10.111.249.153": [
            1122085995
        ],
        "\nkubia_service_port=80": [
            1122085995
        ],
        "\n..\ntwo": [
            1122085995
        ],
        "defined": [
            1122085995
        ],
        "cluster:": [
            1122085995
        ],
        "(you\nsaw": [
            1122085995
        ],
        "command);": [
            1122085995
        ],
        "service-\nrelated": [
            1122085995
        ],
        "pertain": [
            1122085995
        ],
        "the\nkubia": [
            1122085995
        ],
        "kubia_service\n_host\n": [
            1122085995
        ],
        "kubia_service_port": [
            1122085995
        ],
        "frontend-backend": [
            1122085995
        ],
        "\nbackend-database": [
            1122085995
        ],
        "then\nhave": [
            1122085995
        ],
        "vari-\nables": [
            1122085995
        ],
        "\nbackend_database_service_host": [
            1122085995
        ],
        "backend_database_service_port\nnotedashes": [
            1122085995
        ],
        "converted": [
            1122085995
        ],
        "underscores": [
            1122085995
        ],
        "let-\nters": [
            1122085995
        ],
        "uppercased": [
            1122085995
        ],
        "prefix": [
            1122085995
        ],
        "\nenvironment": [
            1122085995
        ],
        "isn’t\nthis": [
            1122085995
        ],
        "dns?": [
            1122085995
        ],
        "and\nallow": [
            1122085995
        ],
        "turns": [
            1122085995
        ],
        "does!\ndiscovering": [
            1122085995
        ],
        "dns\nremember": [
            1122085995
        ],
        "namespace?": [
            1122085995
        ],
        "\nkube-dns": [
            1122085995
        ],
        "corre-\nsponding": [
            1122085995
        ],
        "name\n": [
            1122085995
        ],
        "suggests": [
            1122085995
        ],
        "(kubernetes": [
            1122085995
        ],
        "modifying\neach": [
            1122085995
        ],
        "\n/etc/resolvconf": [
            1122085995
        ],
        "file)": [
            1122085995
        ],
        "\nnotewhether": [
            1122085995
        ],
        "configurable\nthrough": [
            1122085995
        ],
        "\ndnspolicy": [
            1122085995
        ],
        "spec\neach": [
            1122085995
        ],
        "know\nthe": [
            1122085995
        ],
        "qualified": [
            1122085995
        ],
        "(fqdn)\ninstead": [
            1122085995
        ],
        "resorting": [
            1122085995
        ],
        "service-related": [
            1122085995
        ],
        "container\nhere’s": [
            1122085995
        ],
        "\n\n130chapter": [
            1122085995
        ],
        "pods\nconnecting": [
            1122085995
        ],
        "fqdn\nto": [
            1122085995
        ],
        "backend-\ndatabase": [
            1122085995
        ],
        "opening": [
            1122085995
        ],
        "fqdn:\nbackend-databasedefault.svc.cluster.local\nbackend-database": [
            1122085995
        ],
        "\nsvccluster.local": [
            1122085995
        ],
        "cluster\ndomain": [
            1122085995
        ],
        "suffix": [
            1122085995
        ],
        "is\nusing": [
            1122085995
        ],
        "postgres)": [
            1122085995
        ],
        "that\nshouldn’t": [
            1122085995
        ],
        "the\nenvironment": [
            1122085995
        ],
        "variable\nconnecting": [
            1122085995
        ],
        "omit": [
            1122085995
        ],
        "\nsvccluster\n.local\n": [
            1122085995
        ],
        "\nbackend-\ndatabase\n": [
            1122085995
        ],
        "right?\n": [
            1122085995
        ],
        "fqdn": [
            1122085995
        ],
        "its\nip": [
            1122085995
        ],
        "use\nkubectl": [
            1122085995
        ],
        "of\nrunning": [
            1122085995
        ],
        "then\nrun": [
            1122085995
        ],
        "2\nwhen": [
            1122085995
        ],
        "entered": [
            1122085995
        ],
        "-it\nbash\n": [
            1122085995
        ],
        "container\nyou": [
            1122085995
        ],
        "shell)": [
            1122085995
        ],
        "a\npod’s": [
            1122085995
        ],
        "want\nwithout": [
            1122085995
        ],
        "run\nnotethe": [
            1122085995
        ],
        "shell’s": [
            1122085995
        ],
        "image\nfor": [
            1122085995
        ],
        "exec:\n$": [
            1122085995
        ],
        "bash\nroot@kubia-3inly:/#": [
            1122085995
        ],
        "kubia\nservice": [
            1122085995
        ],
        "ways:\nroot@kubia-3inly:/#": [
            1122085995
        ],
        "http://kubiadefault.svc.cluster.local\nyou’ve": [
            1122085995
        ],
        "kubia-5asi2\nroot@kubia-3inly:/#": [
            1122085995
        ],
        "http://kubiadefault\nyou’ve": [
            1122085995
        ],
        "kubia-3inly\n": [
            1122085995
        ],
        "\n\n131connecting": [
            1122085995
        ],
        "cluster\nroot@kubia-3inly:/#": [
            1122085995
        ],
        "http://kubia\nyou’ve": [
            1122085995
        ],
        "kubia-8awf3\nyou": [
            1122085995
        ],
        "requested\nurl": [
            1122085995
        ],
        "resolver": [
            1122085995
        ],
        "/etc/resolvconf\nfile": [
            1122085995
        ],
        "understand:\nroot@kubia-3inly:/#": [
            1122085995
        ],
        "cat": [
            1122085995
        ],
        "/etc/resolvconf\nsearch": [
            1122085995
        ],
        "defaultsvc.cluster.local": [
            1122085995
        ],
        "svccluster.local": [
            1122085995
        ],
        "clusterlocal": [
            1122085995
        ],
        "..\nunderstanding": [
            1122085995
        ],
        "ping": [
            1122085995
        ],
        "ip\none": [
            1122085995
        ],
        "soon\ncreate": [
            1122085995
        ],
        "service?\n": [
            1122085995
        ],
        "try-\ning": [
            1122085995
        ],
        "access\nthe": [
            1122085995
        ],
        "see\nif": [
            1122085995
        ],
        "now:\nroot@kubia-3inly:/#": [
            1122085995
        ],
        "kubia\nping": [
            1122085995
        ],
        "kubiadefault.svc.cluster.local": [
            1122085995
        ],
        "(10111.249.153):": [
            1122085995
        ],
        "bytes\n^c---": [
            1122085995
        ],
        "---\n54": [
            1122085995
        ],
        "100%": [
            1122085995
        ],
        "packet": [
            1122085995
        ],
        "loss\nhmm": [
            1122085995
        ],
        "curl-ing": [
            1122085995
        ],
        "pinging": [
            1122085995
        ],
        "meaning": [
            1122085995
        ],
        "combined": [
            1122085995
        ],
        "port\nwe’ll": [
            1122085995
        ],
        "men-\ntion": [
            1122085995
        ],
        "broken\nservice": [
            1122085995
        ],
        "catches": [
            1122085995
        ],
        "guard\n5.2connecting": [
            1122085995
        ],
        "cluster\nup": [
            1122085995
        ],
        "the\nkubernetes": [
            1122085995
        ],
        "to\npods": [
            1122085995
        ],
        "ip(s)": [
            1122085995
        ],
        "discov-\nery": [
            1122085995
        ],
        "they\nconnect": [
            1122085995
        ],
        "services\n5.2.1introducing": [
            1122085995
        ],
        "endpoints\nbefore": [
            1122085995
        ],
        "shed": [
            1122085995
        ],
        "light": [
            1122085995
        ],
        "services\ndon’t": [
            1122085995
        ],
        "link": [
            1122085995
        ],
        "sits": [
            1122085995
        ],
        "between—the": [
            1122085995
        ],
        "endpoints\nresource": [
            1122085995
        ],
        "\n\n132chapter": [
            1122085995
        ],
        "default\nlabels:": [
            1122085995
        ],
        "<none>\nselector:": [
            1122085995
        ],
        "\ntype:": [
            1122085995
        ],
        "clusterip\nip:": [
            1122085995
        ],
        "10111.249.153\nport:": [
            1122085995
        ],
        "<unset>": [
            1122085995
        ],
        "80/tcp\nendpoints:": [
            1122085995
        ],
        "10108.1.4:808010.108.2.5:8080,10.108.2.6:8080": [
            1122085995
        ],
        "\nsession": [
            1122085995
        ],
        "none\nno": [
            1122085995
        ],
        "events\nan": [
            1122085995
        ],
        "(yes": [
            1122085995
        ],
        "plural)": [
            1122085995
        ],
        "display\nits": [
            1122085995
        ],
        "1h\nalthough": [
            1122085995
        ],
        "when\nredirecting": [
            1122085995
        ],
        "incoming": [
            1122085995
        ],
        "ips\nand": [
            1122085995
        ],
        "connects": [
            1122085995
        ],
        "the\nincoming": [
            1122085995
        ],
        "location\n5.2.2manually": [
            1122085995
        ],
        "endpoints\nyou": [
            1122085995
        ],
        "decou-\npled": [
            1122085995
        ],
        "the\nendpoints": [
            1122085995
        ],
        "(after": [
            1122085995
        ],
        "include\nin": [
            1122085995
        ],
        "of\nendpoints": [
            1122085995
        ],
        "selector\nyou’ll": [
            1122085995
        ],
        "service\nmetadata:\n": [
            1122085995
        ],
        "external-service": [
            1122085995
        ],
        "describe\nlisting": [
            1122085995
        ],
        "external-serviceyaml\nthe": [
            1122085995
        ],
        "\nendpoints\nthe": [
            1122085995
        ],
        "pod\nips": [
            1122085995
        ],
        "ports\nthat": [
            1122085995
        ],
        "represent\nthe": [
            1122085995
        ],
        "of\nthis": [
            1122085995
        ],
        "\nmatch": [
            1122085995
        ],
        "\nobject": [
            1122085995
        ],
        "listing)\nthis": [
            1122085995
        ],
        "defined\n": [
            1122085995
        ],
        "\n\n133connecting": [
            1122085995
        ],
        "service\ncreating": [
            1122085995
        ],
        "selector\nendpoints": [
            1122085995
        ],
        "hasn’t": [
            1122085995
        ],
        "been\ncreated": [
            1122085995
        ],
        "its\nyaml": [
            1122085995
        ],
        "manifest\napiversion:": [
            1122085995
        ],
        "endpoints\nmetadata:\n": [
            1122085995
        ],
        "\nsubsets:\n": [
            1122085995
        ],
        "addresses:\n": [
            1122085995
        ],
        "1111.11.11": [
            1122085995
        ],
        "2222.22.22": [
            1122085995
        ],
        "list\nof": [
            1122085995
        ],
        "end-\npoints": [
            1122085995
        ],
        "posted": [
            1122085995
        ],
        "regular\nservice": [
            1122085995
        ],
        "include\nthe": [
            1122085995
        ],
        "ip:port": [
            1122085995
        ],
        "be\nload": [
            1122085995
        ],
        "endpoints\nif": [
            1122085995
        ],
        "reverse—by": [
            1122085995
        ],
        "service\nlisting": [
            1122085995
        ],
        "external-service-endpointsyaml\nthe": [
            1122085995
        ],
        "listing)\nthe": [
            1122085995
        ],
        "endpoints\npodpodpod\nexternal": [
            1122085995
        ],
        "1111.11.11:80\nexternal": [
            1122085995
        ],
        "2222.22.22:80\nservice\n10.111.249.214:80\nkubernetes": [
            1122085995
        ],
        "cluster\ninternet\nfigure": [
            1122085995
        ],
        "endpoints\n": [
            1122085995
        ],
        "\n\n134chapter": [
            1122085995
        ],
        "pods\nkubernetes": [
            1122085995
        ],
        "remain\nconstant": [
            1122085995
        ],
        "\n52.3creating": [
            1122085995
        ],
        "service\ninstead": [
            1122085995
        ],
        "method": [
            1122085995
        ],
        "qualified\ndomain": [
            1122085995
        ],
        "(fqdn)\ncreating": [
            1122085995
        ],
        "externalname": [
            1122085995
        ],
        "service\nto": [
            1122085995
        ],
        "service\nresource": [
            1122085995
        ],
        "\ntype": [
            1122085995
        ],
        "a\npublic": [
            1122085995
        ],
        "apisomecompany.com.": [
            1122085995
        ],
        "points": [
            1122085995
        ],
        "to\nit": [
            1122085995
        ],
        "external-service\nspec:\n": [
            1122085995
        ],
        "externalname:": [
            1122085995
        ],
        "someapisomecompany.com": [
            1122085995
        ],
        "80\nafter": [
            1122085995
        ],
        "the\nexternal-servicedefault.svc.cluster.local": [
            1122085995
        ],
        "external-\nservice\n)": [
            1122085995
        ],
        "hides": [
            1122085995
        ],
        "service\nname": [
            1122085995
        ],
        "\nexternalname": [
            1122085995
        ],
        "clusterip": [
            1122085995
        ],
        "creating\nan": [
            1122085995
        ],
        "service—either": [
            1122085995
        ],
        "selector\non": [
            1122085995
        ],
        "automatically\n": [
            1122085995
        ],
        "solely": [
            1122085995
        ],
        "level—a": [
            1122085995
        ],
        "cname\ndns": [
            1122085995
        ],
        "record": [
            1122085995
        ],
        "will\nconnect": [
            1122085995
        ],
        "bypassing": [
            1122085995
        ],
        "for\nthis": [
            1122085995
        ],
        "\nnotea": [
            1122085995
        ],
        "cname": [
            1122085995
        ],
        "a\nnumeric": [
            1122085995
        ],
        "address\n5.3exposing": [
            1122085995
        ],
        "clients\nup": [
            1122085995
        ],
        "webserv-\ners": [
            1122085995
        ],
        "depicted": [
            1122085995
        ],
        "55.\nlisting": [
            1122085995
        ],
        "externalname-type": [
            1122085995
        ],
        "external-service-externalnameyaml\nservice": [
            1122085995
        ],
        "externalname\nthe": [
            1122085995
        ],
        "\nname": [
            1122085995
        ],
        "\n\n135exposing": [
            1122085995
        ],
        "clients\nyou": [
            1122085995
        ],
        "externally:\nsetting": [
            1122085995
        ],
        "nodeport—for": [
            1122085995
        ],
        "node\nopens": [
            1122085995
        ],
        "(hence": [
            1122085995
        ],
        "name)": [
            1122085995
        ],
        "received\non": [
            1122085995
        ],
        "the\ninternal": [
            1122085995
        ],
        "\nsetting": [
            1122085995
        ],
        "extension": [
            1122085995
        ],
        "type—this\nmakes": [
            1122085995
        ],
        "provisioned\nfrom": [
            1122085995
        ],
        "redi-\nrects": [
            1122085995
        ],
        "balancer’s": [
            1122085995
        ],
        "ip\ncreating": [
            1122085995
        ],
        "radically": [
            1122085995
        ],
        "address—it": [
            1122085995
        ],
        "operates": [
            1122085995
        ],
        "7)\nand": [
            1122085995
        ],
        "ingress\nresources": [
            1122085995
        ],
        "54.": [
            1122085995
        ],
        "\n53.1using": [
            1122085995
        ],
        "\nnodeport": [
            1122085995
        ],
        "kubernetes\nreserve": [
            1122085995
        ],
        "them)": [
            1122085995
        ],
        "and\nforward": [
            1122085995
        ],
        "(their": [
            1122085995
        ],
        "\nclusterip)": [
            1122085995
        ],
        "nodeport\nservice": [
            1122085995
        ],
        "accessed": [
            1122085995
        ],
        "also\nthrough": [
            1122085995
        ],
        "service\nyou’ll": [
            1122085995
        ],
        "listing\nshows": [
            1122085995
        ],
        "cluster\nexternal": [
            1122085995
        ],
        "clientservice\npodpodpod\nfigure": [
            1122085995
        ],
        "clients\n": [
            1122085995
        ],
        "\n\n136chapter": [
            1122085995
        ],
        "pods\napiversion:": [
            1122085995
        ],
        "kubia-nodeport\nspec:\n": [
            1122085995
        ],
        "nodeport:": [
            1122085995
        ],
        "kubia\nyou": [
            1122085995
        ],
        "to\nacross": [
            1122085995
        ],
        "mandatory;": [
            1122085995
        ],
        "a\nrandom": [
            1122085995
        ],
        "warning\nabout": [
            1122085995
        ],
        "firewall": [
            1122085995
        ],
        "\nexamining": [
            1122085995
        ],
        "service\nlet’s": [
            1122085995
        ],
        "kubia-nodeport\nname": [
            1122085995
        ],
        "age\nkubia-nodeport": [
            1122085995
        ],
        "10111.254.223": [
            1122085995
        ],
        "<nodes>": [
            1122085995
        ],
        "80:30123/tcp": [
            1122085995
        ],
        "2m\nlook": [
            1122085995
        ],
        "indicating": [
            1122085995
        ],
        "accessible\nthrough": [
            1122085995
        ],
        "\nport(s)": [
            1122085995
        ],
        "(\n80)": [
            1122085995
        ],
        "(30123)": [
            1122085995
        ],
        "addresses:\n1011.254.223:80\n<1st": [
            1122085995
        ],
        "ip>:30123\n<2nd": [
            1122085995
        ],
        "ip>:30123": [
            1122085995
        ],
        "on\nfigure": [
            1122085995
        ],
        "nodes\n(this": [
            1122085995
        ],
        "gke;": [
            1122085995
        ],
        "the\nprinciple": [
            1122085995
        ],
        "same)": [
            1122085995
        ],
        "definition:": [
            1122085995
        ],
        "kubia-svc-nodeportyaml\nset": [
            1122085995
        ],
        "nodeport\nthis": [
            1122085995
        ],
        "\nservice’s": [
            1122085995
        ],
        "ip\nthis": [
            1122085995
        ],
        "\nyour": [
            1122085995
        ],
        "\n\n137exposing": [
            1122085995
        ],
        "clients\na": [
            1122085995
        ],
        "node\nchanging": [
            1122085995
        ],
        "service\nas": [
            1122085995
        ],
        "node\nport": [
            1122085995
        ],
        "platform’s": [
            1122085995
        ],
        "firewalls": [
            1122085995
        ],
        "external\nconnections": [
            1122085995
        ],
        "firewall-rules": [
            1122085995
        ],
        "kubia-svc-rule": [
            1122085995
        ],
        "--allow=tcp:30123\ncreated": [
            1122085995
        ],
        "[https://wwwgoogleapis.com/compute/v1/projects/kubia-\n1295/global/firewalls/kubia-svc-rule].\nname": [
            1122085995
        ],
        "src_ranges": [
            1122085995
        ],
        "src_tags": [
            1122085995
        ],
        "target_tags\nkubia-svc-rule": [
            1122085995
        ],
        "00.0.0/0": [
            1122085995
        ],
        "tcp:30123\nyou": [
            1122085995
        ],
        "sidebar": [
            1122085995
        ],
        "that\n": [
            1122085995
        ],
        "client\npod\nnode": [
            1122085995
        ],
        "130211.99.206\nnode": [
            1122085995
        ],
        "130211.97.55\nport": [
            1122085995
        ],
        "30123\nport": [
            1122085995
        ],
        "8080\npod\nport": [
            1122085995
        ],
        "8080\nservice\nfigure": [
            1122085995
        ],
        "\n\n138chapter": [
            1122085995
        ],
        "pods\nonce": [
            1122085995
        ],
        "accessing": [
            1122085995
        ],
        "them:\n$": [
            1122085995
        ],
        "http://130211.97.55:30123\nyouve": [
            1122085995
        ],
        "kubia-ym8or\n$": [
            1122085995
        ],
        "http://130211.99.206:30123\nyouve": [
            1122085995
        ],
        "kubia-xueq1\ntipwhen": [
            1122085995
        ],
        "services\nthrough": [
            1122085995
        ],
        "<service-name>": [
            1122085995
        ],
        "[-n\n<namespace>]\n\nas": [
            1122085995
        ],
        "internet": [
            1122085995
        ],
        "30123\non": [
            1122085995
        ],
        "can’t\naccess": [
            1122085995
        ],
        "front\nof": [
            1122085995
        ],
        "spreading": [
            1122085995
        ],
        "and\nnever": [
            1122085995
        ],
        "is\ndeployed": [
            1122085995
        ],
        "infrastructure)": [
            1122085995
        ],
        "provisioned": [
            1122085995
        ],
        "next\n5.3.2exposing": [
            1122085995
        ],
        "balancer\nkubernetes": [
            1122085995
        ],
        "provi-\nsion": [
            1122085995
        ],
        "the\nusing": [
            1122085995
        ],
        "jsonpath": [
            1122085995
        ],
        "of\nsifting": [
            1122085995
        ],
        "jsonpath={items[*].status.\n➥": [
            1122085995
        ],
        "addresses[?(@type==externalip\")].address}\n130.211.97.55": [
            1122085995
        ],
        "130211.99.206\nyou’re": [
            1122085995
        ],
        "a\njsonpath": [
            1122085995
        ],
        "familiar": [
            1122085995
        ],
        "xpath": [
            1122085995
        ],
        "xml": [
            1122085995
        ],
        "jsonpath\nis": [
            1122085995
        ],
        "basically": [
            1122085995
        ],
        "\nkubectl\nto": [
            1122085995
        ],
        "following:\ngo": [
            1122085995
        ],
        "items": [
            1122085995
        ],
        "attribute\nfor": [
            1122085995
        ],
        "element": [
            1122085995
        ],
        "enter": [
            1122085995
        ],
        "attribute\nfilter": [
            1122085995
        ],
        "externalip\nfinally": [
            1122085995
        ],
        "elements\nto": [
            1122085995
        ],
        "documentation\nat": [
            1122085995
        ],
        "http://kubernetesio/docs/user-guide/jsonpath.": [
            1122085995
        ],
        "\n\n139exposing": [
            1122085995
        ],
        "clients\nservice’s": [
            1122085995
        ],
        "your\nservice": [
            1122085995
        ],
        "\nloadbalancer\nservices": [
            1122085995
        ],
        "behave": [
            1122085995
        ],
        "like\na": [
            1122085995
        ],
        "node-\nport\n": [
            1122085995
        ],
        "supports\nloadbalancer": [
            1122085995
        ],
        "front": [
            1122085995
        ],
        "following\nyaml": [
            1122085995
        ],
        "kubia-loadbalancer\nspec:\n": [
            1122085995
        ],
        "80\n": [
            1122085995
        ],
        "kubia\nthe": [
            1122085995
        ],
        "(you’re": [
            1122085995
        ],
        "instead)": [
            1122085995
        ],
        "balancer\nafter": [
            1122085995
        ],
        "the\nload": [
            1122085995
        ],
        "kubia-loadbalancer\nname": [
            1122085995
        ],
        "age\nkubia-loadbalancer": [
            1122085995
        ],
        "10111.241.153": [
            1122085995
        ],
        "130211.53.173": [
            1122085995
        ],
        "80:32143/tcp": [
            1122085995
        ],
        "1m\nin": [
            1122085995
        ],
        "address:\n$": [
            1122085995
        ],
        "http://130211.53.173\nyouve": [
            1122085995
        ],
        "kubia-xueq1\nsuccess!": [
            1122085995
        ],
        "the\nway": [
            1122085995
        ],
        "loadbalancer-type": [
            1122085995
        ],
        "kubia-svc-loadbalanceryaml\nthis": [
            1122085995
        ],
        "obtains": [
            1122085995
        ],
        "\ninfrastructure": [
            1122085995
        ],
        "\n\n140chapter": [
            1122085995
        ],
        "pods\nsee": [
            1122085995
        ],
        "delivered": [
            1122085995
        ],
        "clients\n(\ncurl": [
            1122085995
        ],
        "the\nsession": [
            1122085995
        ],
        "browsers\nbecause": [
            1122085995
        ],
        "your\nweb": [
            1122085995
        ],
        "strike": [
            1122085995
        ],
        "odd—the": [
            1122085995
        ],
        "hit\nthe": [
            1122085995
        ],
        "the\nmeantime?": [
            1122085995
        ],
        "double-check": [
            1122085995
        ],
        "session\naffinity": [
            1122085995
        ],
        "different\npods": [
            1122085995
        ],
        "\ncurl?\nlet": [
            1122085995
        ],
        "keep-alive": [
            1122085995
        ],
        "and\nsends": [
            1122085995
        ],
        "new\nconnection": [
            1122085995
        ],
        "opened": [
            1122085995
        ],
        "belonging\nto": [
            1122085995
        ],
        "\nnone\nusers": [
            1122085995
        ],
        "(until": [
            1122085995
        ],
        "closed)\nkubernetes": [
            1122085995
        ],
        "client\nload": [
            1122085995
        ],
        "balancer\nip:": [
            1122085995
        ],
        "130211.53.173:80\npod\nnode": [
            1122085995
        ],
        "32143\nport": [
            1122085995
        ],
        "\n\n141exposing": [
            1122085995
        ],
        "clients\nimplicitly": [
            1122085995
        ],
        "for-\nwarded": [
            1122085995
        ],
        "instances\n": [
            1122085995
        ],
        "an\nadditional": [
            1122085995
        ],
        "infrastructure-provided": [
            1122085995
        ],
        "dis-\nplay": [
            1122085995
        ],
        "node\nips": [
            1122085995
        ],
        "well\ntipif": [
            1122085995
        ],
        "be\nprovisioned": [
            1122085995
        ],
        "(at": [
            1122085995
        ],
        "vm’s": [
            1122085995
        ],
        "address)\n5.3.3understanding": [
            1122085995
        ],
        "connections\nyou": [
            1122085995
        ],
        "to\nservices": [
            1122085995
        ],
        "hops\nwhen": [
            1122085995
        ],
        "also\nincludes": [
            1122085995
        ],
        "first)": [
            1122085995
        ],
        "chosen\npod": [
            1122085995
        ],
        "hop": [
            1122085995
        ],
        "reach": [
            1122085995
        ],
        "be\ndesirable": [
            1122085995
        ],
        "external\ntraffic": [
            1122085995
        ],
        "by\nsetting": [
            1122085995
        ],
        "\nexternaltrafficpolicy": [
            1122085995
        ],
        "section:\nspec:\n": [
            1122085995
        ],
        "externaltrafficpolicy:": [
            1122085995
        ],
        "local\n": [
            1122085995
        ],
        "..\nif": [
            1122085995
        ],
        "opened\nthrough": [
            1122085995
        ],
        "if\nno": [
            1122085995
        ],
        "hang": [
            1122085995
        ],
        "random\nglobal": [
            1122085995
        ],
        "annotation)": [
            1122085995
        ],
        "therefore\nneed": [
            1122085995
        ],
        "at\nleast": [
            1122085995
        ],
        "spread\nevenly": [
            1122085995
        ],
        "case\n": [
            1122085995
        ],
        "evenly": [
            1122085995
        ],
        "50%": [
            1122085995
        ],
        "25%": [
            1122085995
        ],
        "58.\n": [
            1122085995
        ],
        "\n\n142chapter": [
            1122085995
        ],
        "pods\nbeing": [
            1122085995
        ],
        "non-preservation": [
            1122085995
        ],
        "ip\nusually": [
            1122085995
        ],
        "obtain": [
            1122085995
        ],
        "packets’": [
            1122085995
        ],
        "trans-\nlation": [
            1122085995
        ],
        "(snat)": [
            1122085995
        ],
        "some\napplications": [
            1122085995
        ],
        "example\nthis": [
            1122085995
        ],
        "browser’s": [
            1122085995
        ],
        "pres-\nervation": [
            1122085995
        ],
        "receiv-\ning": [
            1122085995
        ],
        "(snat": [
            1122085995
        ],
        "performed)\n5.4exposing": [
            1122085995
        ],
        "\nresource\nyou’ve": [
            1122085995
        ],
        "but\nanother": [
            1122085995
        ],
        "exists—creating": [
            1122085995
        ],
        "resource\ndefinitioningress": [
            1122085995
        ],
        "(noun)—the": [
            1122085995
        ],
        "entering;": [
            1122085995
        ],
        "to\nenter;": [
            1122085995
        ],
        "entryway": [
            1122085995
        ],
        "\nlet": [
            1122085995
        ],
        "the\noutside": [
            1122085995
        ],
        "ingresses": [
            1122085995
        ],
        "needed\none": [
            1122085995
        ],
        "bal-\nancer": [
            1122085995
        ],
        "when\nproviding": [
            1122085995
        ],
        "dozens": [
            1122085995
        ],
        "the\ningress": [
            1122085995
        ],
        "59.\n": [
            1122085995
        ],
        "\n50%\n50%50%\n25%25%\nnode": [
            1122085995
        ],
        "a\npod\nnode": [
            1122085995
        ],
        "b\npod\npod\nload": [
            1122085995
        ],
        "balancer\nfigure": [
            1122085995
        ],
        "\npolicy": [
            1122085995
        ],
        "uneven": [
            1122085995
        ],
        "\nload": [
            1122085995
        ],
        "\n\n143exposing": [
            1122085995
        ],
        "resource\ningresses": [
            1122085995
        ],
        "stack": [
            1122085995
        ],
        "(http)": [
            1122085995
        ],
        "can’t\nunderstanding": [
            1122085995
        ],
        "required\nbefore": [
            1122085995
        ],
        "emphasize": [
            1122085995
        ],
        "cluster\ndifferent": [
            1122085995
        ],
        "implementations": [
            1122085995
        ],
        "controller\nbut": [
            1122085995
        ],
        "http\nload-balancing": [
            1122085995
        ],
        "didn’t\nprovide": [
            1122085995
        ],
        "add-on": [
            1122085995
        ],
        "enabled\nto": [
            1122085995
        ],
        "following\nsidebar": [
            1122085995
        ],
        "enabled\nenabling": [
            1122085995
        ],
        "minikube\nif": [
            1122085995
        ],
        "add-ons:\n$": [
            1122085995
        ],
        "addons": [
            1122085995
        ],
        "list\n-": [
            1122085995
        ],
        "default-storageclass:": [
            1122085995
        ],
        "enabled\n-": [
            1122085995
        ],
        "kube-dns:": [
            1122085995
        ],
        "heapster:": [
            1122085995
        ],
        "disabled\n-": [
            1122085995
        ],
        "ingress:": [
            1122085995
        ],
        "disabled": [
            1122085995
        ],
        "\n-": [
            1122085995
        ],
        "registry-creds:": [
            1122085995
        ],
        "addon-manager:": [
            1122085995
        ],
        "dashboard:": [
            1122085995
        ],
        "enabled\nyou’ll": [
            1122085995
        ],
        "be\npretty": [
            1122085995
        ],
        "\ndashboard": [
            1122085995
        ],
        "kube-dns": [
            1122085995
        ],
        "ingress\nadd-on": [
            1122085995
        ],
        "action:\n$": [
            1122085995
        ],
        "ingress\ningress": [
            1122085995
        ],
        "enabled\npodpodpod\npodpodpod\npodpodpod\npodpodpod\ningress\nclient\nservice\nkubiaexample.com/kubia\nfoo.example.com\nkubia.example.com/foo\nservice\nbar.example.com\nservice\nservice\nfigure": [
            1122085995
        ],
        "ingress\nthe": [
            1122085995
        ],
        "\nisn’t": [
            1122085995
        ],
        "enabled\n": [
            1122085995
        ],
        "\n\n144chapter": [
            1122085995
        ],
        "pods\ntipthe": [
            1122085995
        ],
        "--all-namespaces": [
            1122085995
        ],
        "resource)": [
            1122085995
        ],
        "or\nif": [
            1122085995
        ],
        "namespaces\n5.4.1creating": [
            1122085995
        ],
        "resource\nyou’ve": [
            1122085995
        ],
        "confirmed": [
            1122085995
        ],
        "can\nnow": [
            1122085995
        ],
        "manifest\nfor": [
            1122085995
        ],
        "like\napiversion:": [
            1122085995
        ],
        "extensions/v1beta1\nkind:": [
            1122085995
        ],
        "ingress\nmetadata:\n": [
            1122085995
        ],
        "rules:\n": [
            1122085995
        ],
        "host:": [
            1122085995
        ],
        "kubiaexample.com": [
            1122085995
        ],
        "http:\n": [
            1122085995
        ],
        "paths:\n": [
            1122085995
        ],
        "backend:\n": [
            1122085995
        ],
        "servicename:": [
            1122085995
        ],
        "kubia-nodeport": [
            1122085995
        ],
        "serviceport:": [
            1122085995
        ],
        "rule": [
            1122085995
        ],
        "received\nby": [
            1122085995
        ],
        "\nkubiaexample.com": [
            1122085995
        ],
        "be\nsent": [
            1122085995
        ],
        "\nkubia-nodeport": [
            1122085995
        ],
        "\n(continued)\nthis": [
            1122085995
        ],
        "likely": [
            1122085995
        ],
        "the\ncontroller": [
            1122085995
        ],
        "\nkube-system": [
            1122085995
        ],
        "\n--all-namespaces": [
            1122085995
        ],
        "--all-namespaces\nnamespace": [
            1122085995
        ],
        "kubia-rsv5m": [
            1122085995
        ],
        "13h\ndefault": [
            1122085995
        ],
        "kubia-fe4ad": [
            1122085995
        ],
        "kubia-ke823": [
            1122085995
        ],
        "13h\nkube-system": [
            1122085995
        ],
        "default-http-backend-5wb0h": [
            1122085995
        ],
        "18m\nkube-system": [
            1122085995
        ],
        "kube-addon-manager-minikube": [
            1122085995
        ],
        "6d\nkube-system": [
            1122085995
        ],
        "kube-dns-v20-101vq": [
            1122085995
        ],
        "3/3": [
            1122085995
        ],
        "kubernetes-dashboard-jxd9l": [
            1122085995
        ],
        "nginx-ingress-controller-gdts0": [
            1122085995
        ],
        "18m\nat": [
            1122085995
        ],
        "suggests\nthat": [
            1122085995
        ],
        "nginx": [
            1122085995
        ],
        "(an": [
            1122085995
        ],
        "reverse": [
            1122085995
        ],
        "proxy)": [
            1122085995
        ],
        "functionality\nlisting": [
            1122085995
        ],
        "kubia-ingressyaml\nthis": [
            1122085995
        ],
        "maps": [
            1122085995
        ],
        "service\nall": [
            1122085995
        ],
        "kubia-\nnodeport": [
            1122085995
        ],
        "\n\n145exposing": [
            1122085995
        ],
        "resource\nnoteingress": [
            1122085995
        ],
        "require\nthe": [
            1122085995
        ],
        "itself\n5.4.2accessing": [
            1122085995
        ],
        "ingress\nto": [
            1122085995
        ],
        "http://kubiaexample.com": [
            1122085995
        ],
        "resolves": [
            1122085995
        ],
        "ingresses:\n$": [
            1122085995
        ],
        "ingresses\nname": [
            1122085995
        ],
        "192168.99.100": [
            1122085995
        ],
        "29m\nnotewhen": [
            1122085995
        ],
        "appear\nbecause": [
            1122085995
        ],
        "provisions": [
            1122085995
        ],
        "scenes\nthe": [
            1122085995
        ],
        "\naddress": [
            1122085995
        ],
        "\nensuring": [
            1122085995
        ],
        "ingress’": [
            1122085995
        ],
        "address\nonce": [
            1122085995
        ],
        "resolve\nkubiaexample.com": [
            1122085995
        ],
        "\n/etc/hosts": [
            1122085995
        ],
        "(or\nc:\\windows\\system32\\drivers\\etc\\hosts": [
            1122085995
        ],
        "windows):\n192168.99.100": [
            1122085995
        ],
        "kubiaexample.com\naccessing": [
            1122085995
        ],
        "ingress\neverything": [
            1122085995
        ],
        "http://kubiaexample.com\n(using": [
            1122085995
        ],
        "\ncurl):\n$": [
            1122085995
        ],
        "http://kubiaexample.com\nyouve": [
            1122085995
        ],
        "kubia-ke823\nyou’ve": [
            1122085995
        ],
        "at\nhow": [
            1122085995
        ],
        "unfolded\nunderstanding": [
            1122085995
        ],
        "work\nfigure": [
            1122085995
        ],
        "connected": [
            1122085995
        ],
        "ingress\ncontroller": [
            1122085995
        ],
        "the\ndns": [
            1122085995
        ],
        "system)": [
            1122085995
        ],
        "controller\nthe": [
            1122085995
        ],
        "specified\nkubiaexample.com": [
            1122085995
        ],
        "header": [
            1122085995
        ],
        "determined\nwhich": [
            1122085995
        ],
        "trying": [
            1122085995,
            2119133144
        ],
        "looked": [
            1122085995
        ],
        "associated": [
            1122085995
        ],
        "it\nonly": [
            1122085995
        ],
        "\n\n146chapter": [
            1122085995
        ],
        "pods\n54.3exposing": [
            1122085995
        ],
        "ingress\nif": [
            1122085995
        ],
        "paths": [
            1122085995
        ],
        "arrays\nso": [
            1122085995
        ],
        "to\nmultiple": [
            1122085995
        ],
        "\npaths": [
            1122085995
        ],
        "\nmapping": [
            1122085995
        ],
        "host\nyou": [
            1122085995
        ],
        "listing\n...\n": [
            1122085995
        ],
        "kubiaexample.com\n": [
            1122085995
        ],
        "/kubia": [
            1122085995
        ],
        "backend:": [
            1122085995
        ],
        "/foo": [
            1122085995
        ],
        "depending": [
            1122085995
        ],
        "single\nip": [
            1122085995
        ],
        "controller)\nlisting": [
            1122085995
        ],
        "paths\nnode": [
            1122085995
        ],
        "b\npodpod\ningress\ncontroller\nendpointsserviceingress\nclient\n2": [
            1122085995
        ],
        "get\nrequest": [
            1122085995
        ],
        "header\nhost:": [
            1122085995
        ],
        "kubiaexample.com\n3.": [
            1122085995
        ],
        "sends\nrequest": [
            1122085995
        ],
        "pods\n1.": [
            1122085995
        ],
        "up\nkubiaexample.com\ndns\nfigure": [
            1122085995
        ],
        "ingress\nrequests": [
            1122085995
        ],
        "kubiaexample.com/kubia": [
            1122085995
        ],
        "service\nrequests": [
            1122085995
        ],
        "kubiaexample.com/bar": [
            1122085995
        ],
        "\n\n147exposing": [
            1122085995
        ],
        "resource\nmapping": [
            1122085995
        ],
        "hosts\nsimilarly": [
            1122085995
        ],
        "the\nhttp": [
            1122085995
        ],
        "(only)": [
            1122085995
        ],
        "listing\nspec:\n": [
            1122085995
        ],
        "fooexample.com": [
            1122085995
        ],
        "barexample.com": [
            1122085995
        ],
        "/\n": [
            1122085995
        ],
        "80\nrequests": [
            1122085995
        ],
        "bar\ndepending": [
            1122085995
        ],
        "in\nweb": [
            1122085995
        ],
        "barexam-\nple.com": [
            1122085995
        ],
        "\n54.4configuring": [
            1122085995
        ],
        "traffic\nyou’ve": [
            1122085995
        ],
        "https?": [
            1122085995
        ],
        "take\na": [
            1122085995
        ],
        "certificate": [
            1122085995
        ],
        "ingress\nwhen": [
            1122085995
        ],
        "termi-\nnates": [
            1122085995
        ],
        "controller\nis": [
            1122085995
        ],
        "encrypted": [
            1122085995
        ],
        "exam-\nple": [
            1122085995
        ],
        "that\nyou": [
            1122085995
        ],
        "be\nstored": [
            1122085995
        ],
        "the\nsecret": [
            1122085995
        ],
        "certificate:\n$": [
            1122085995
        ],
        "openssl": [
            1122085995
        ],
        "genrsa": [
            1122085995
        ],
        "-out": [
            1122085995
        ],
        "tlskey": [
            1122085995
        ],
        "2048\n$": [
            1122085995
        ],
        "req": [
            1122085995
        ],
        "-new": [
            1122085995
        ],
        "-x509": [
            1122085995
        ],
        "-key": [
            1122085995
        ],
        "tlscert": [
            1122085995
        ],
        "-days": [
            1122085995
        ],
        "-subj": [
            1122085995
        ],
        "/cn=kubiaexample.com\nlisting": [
            1122085995
        ],
        "hosts\nrequests": [
            1122085995
        ],
        "\nfooexample.com": [
            1122085995
        ],
        "\nrouted": [
            1122085995
        ],
        "foo\nrequests": [
            1122085995
        ],
        "\nbarexample.com": [
            1122085995
        ],
        "bar\n": [
            1122085995
        ],
        "\n\n148chapter": [
            1122085995
        ],
        "pods\nthen": [
            1122085995
        ],
        "tls-secret": [
            1122085995
        ],
        "--cert=tlscert": [
            1122085995
        ],
        "--key=tlskey\nsecret": [
            1122085995
        ],
        "tls-secret\"": [
            1122085995
        ],
        "tls-secret\nnow": [
            1122085995
        ],
        "for\nkubiaexample.com.": [
            1122085995
        ],
        "tls:": [
            1122085995
        ],
        "hosts:": [
            1122085995
        ],
        "kubia-nodeport\n": [
            1122085995
        ],
        "80\ntipinstead": [
            1122085995
        ],
        "re-creating": [
            1122085995
        ],
        "invoke": [
            1122085995
        ],
        "apply": [
            1122085995
        ],
        "kubia-ingress-tlsyaml": [
            1122085995
        ],
        "file\nsigning": [
            1122085995
        ],
        "certificates": [
            1122085995
        ],
        "certificatesigningrequest": [
            1122085995
        ],
        "resource\ninstead": [
            1122085995
        ],
        "signing": [
            1122085995
        ],
        "signed": [
            1122085995
        ],
        "\ncertificatesigningrequest": [
            1122085995
        ],
        "(csr)": [
            1122085995
        ],
        "applications\ncan": [
            1122085995
        ],
        "csr": [
            1122085995
        ],
        "human\noperator": [
            1122085995
        ],
        "automated": [
            1122085995
        ],
        "approve": [
            1122085995
        ],
        "<name": [
            1122085995
        ],
        "csr>": [
            1122085995
        ],
        "retrieved": [
            1122085995
        ],
        "csr’s": [
            1122085995
        ],
        "statuscertificate\nfield.": [
            1122085995
        ],
        "\nnote": [
            1122085995
        ],
        "signer": [
            1122085995
        ],
        "cluster;": [
            1122085995
        ],
        "otherwise\ncreating": [
            1122085995
        ],
        "approving": [
            1122085995
        ],
        "denying": [
            1122085995
        ],
        "have\nany": [
            1122085995
        ],
        "effect\nlisting": [
            1122085995
        ],
        "traffic:": [
            1122085995
        ],
        "kubia-ingress-tlsyaml\nthe": [
            1122085995
        ],
        "attribute\ntls": [
            1122085995
        ],
        "accepted": [
            1122085995
        ],
        "hostname\nthe": [
            1122085995
        ],
        "\nshould": [
            1122085995
        ],
        "obtained": [
            1122085995
        ],
        "tls-\nsecret": [
            1122085995
        ],
        "previously\n": [
            1122085995
        ],
        "\n\n149signaling": [
            1122085995
        ],
        "ingress:\n$": [
            1122085995
        ],
        "-k": [
            1122085995
        ],
        "-v": [
            1122085995
        ],
        "https://kubiaexample.com/kubia\n*": [
            1122085995
        ],
        "connect()": [
            1122085995
        ],
        "(#0)\n..\n*": [
            1122085995
        ],
        "certificate:\n*": [
            1122085995
        ],
        "subject:": [
            1122085995
        ],
        "cn=kubiaexample.com\n...\n>": [
            1122085995
        ],
        "http/11\n>": [
            1122085995
        ],
        "..\nyouve": [
            1122085995
        ],
        "kubia-xueq1\nthe": [
            1122085995
        ],
        "certifi-\ncate": [
            1122085995
        ],
        "ingress\n": [
            1122085995
        ],
        "with\nnotesupport": [
            1122085995
        ],
        "varies": [
            1122085995
        ],
        "con-\ntroller": [
            1122085995
        ],
        "implementation-specific": [
            1122085995
        ],
        "documenta-\ntion": [
            1122085995
        ],
        "\ningresses": [
            1122085995
        ],
        "many\nimprovements": [
            1122085995
        ],
        "only\nl7": [
            1122085995
        ],
        "(http/https)": [
            1122085995
        ],
        "l4": [
            1122085995
        ],
        "planned\n5.5signaling": [
            1122085995
        ],
        "connections\nthere’s": [
            1122085995
        ],
        "regarding": [
            1122085995
        ],
        "ingresses\nyou’ve": [
            1122085995
        ],
        "labels\nmatch": [
            1122085995
        ],
        "it\nbecomes": [
            1122085995
        ],
        "redirected": [
            1122085995
        ],
        "if\nthe": [
            1122085995
        ],
        "serving": [
            1122085995
        ],
        "immediately?": [
            1122085995
        ],
        "warm-up": [
            1122085995
        ],
        "and\naffecting": [
            1122085995
        ],
        "receiving\nrequests": [
            1122085995
        ],
        "already-running": [
            1122085995
        ],
        "process\nrequests": [
            1122085995
        ],
        "ready\n5.5.1introducing": [
            1122085995
        ],
        "probes\nin": [
            1122085995
        ],
        "keep\nyour": [
            1122085995
        ],
        "automatically\nsimilar": [
            1122085995
        ],
        "probe\nfor": [
            1122085995
        ],
        "invoked": [
            1122085995
        ],
        "specific\npod": [
            1122085995
        ],
        "returns\nsuccess": [
            1122085995
        ],
        "signaling": [
            1122085995
        ],
        "container\nkubernetes": [
            1122085995
        ],
        "simple\n": [
            1122085995
        ],
        "\n\n150chapter": [
            1122085995
        ],
        "pods\nget": [
            1122085995
        ],
        "a\nwhole": [
            1122085995
        ],
        "which\ntakes": [
            1122085995
        ],
        "developer’s": [
            1122085995
        ],
        "probes\nlike": [
            1122085995
        ],
        "exist:\nan": [
            1122085995
        ],
        "deter-\nmined": [
            1122085995
        ],
        "process’": [
            1122085995
        ],
        "code\nan": [
            1122085995
        ],
        "is\nready": [
            1122085995
        ],
        "not\na": [
            1122085995
        ],
        "ready\nunderstanding": [
            1122085995
        ],
        "probes\nwhen": [
            1122085995
        ],
        "configurable\namount": [
            1122085995
        ],
        "it\ninvokes": [
            1122085995
        ],
        "reports": [
            1122085995
        ],
        "becomes\nready": [
            1122085995
        ],
        "re-added": [
            1122085995
        ],
        "or\nrestarted": [
            1122085995
        ],
        "distinction": [
            1122085995
        ],
        "probes\nliveness": [
            1122085995
        ],
        "replacing\nthem": [
            1122085995
        ],
        "serve": [
            1122085995
        ],
        "container\nstart": [
            1122085995
        ],
        "removed\nfrom": [
            1122085995
        ],
        "all\nendpoints\nservice\nselector:app=kubia\napp:": [
            1122085995
        ],
        "kubia-q3vkg\napp:": [
            1122085995
        ],
        "kubia-k0xz6\napp:": [
            1122085995
        ],
        "kubia-53thy\nnot": [
            1122085995
        ],
        "ready\nthis": [
            1122085995
        ],
        "longer\nan": [
            1122085995
        ],
        "its\nreadiness": [
            1122085995
        ],
        "failed\nfigure": [
            1122085995
        ],
        "\n\n151signaling": [
            1122085995
        ],
        "connections\nunderstanding": [
            1122085995
        ],
        "important\nimagine": [
            1122085995
        ],
        "servers)\ndepends": [
            1122085995
        ],
        "if\nat": [
            1122085995
        ],
        "experiences": [
            1122085995
        ],
        "connectivity": [
            1122085995
        ],
        "can’t\nreach": [
            1122085995
        ],
        "wise": [
            1122085995
        ],
        "instances\naren’t": [
            1122085995
        ],
        "experiencing": [
            1122085995
        ],
        "nor-\nmally": [
            1122085995
        ],
        "never\nnotice": [
            1122085995
        ],
        "system\n5.5.2adding": [
            1122085995
        ],
        "pod\nnext": [
            1122085995
        ],
        "template\nyou’ll": [
            1122085995
        ],
        "your\nexisting": [
            1122085995
        ],
        "container\nspecification": [
            1122085995
        ],
        "\nspectemplate.spec.containers.": [
            1122085995
        ],
        "look\nlike": [
            1122085995
        ],
        "replicationcontroller\n..\nspec:\n": [
            1122085995
        ],
        "readinessprobe:": [
            1122085995
        ],
        "exec:": [
            1122085995
        ],
        "command:": [
            1122085995
        ],
        "/var/ready": [
            1122085995
        ],
        "..\nthe": [
            1122085995
        ],
        "\nls": [
            1122085995
        ],
        "non-zero": [
            1122085995
        ],
        "exit\ncode": [
            1122085995
        ],
        "succeed;": [
            1122085995
        ],
        "probe:": [
            1122085995
        ],
        "kubia-rc-readinessprobeyaml\na": [
            1122085995
        ],
        "readinessprobe": [
            1122085995
        ],
        "\n\n152chapter": [
            1122085995
        ],
        "toggle": [
            1122085995
        ],
        "its\nresult": [
            1122085995
        ],
        "question": [
            1122085995
        ],
        "the\npods": [
            1122085995
        ],
        "report": [
            1122085995
        ],
        "has\nno": [
            1122085995
        ],
        "col-\numn": [
            1122085995
        ],
        "as\nendpoints": [
            1122085995
        ],
        "\nobserving": [
            1122085995
        ],
        "status\nlist": [
            1122085995
        ],
        "not:\n$": [
            1122085995
        ],
        "age\nkubia-2r1qb": [
            1122085995
        ],
        "1m\nkubia-3rax1": [
            1122085995
        ],
        "1m\nkubia-3yw4s": [
            1122085995
        ],
        "1m\nthe": [
            1122085995
        ],
        "readi-\nness": [
            1122085995
        ],
        "\n/var/ready": [
            1122085995
        ],
        "file\nwhose": [
            1122085995
        ],
        "succeed:\n$": [
            1122085995
        ],
        "kubia-2r1qb": [
            1122085995
        ],
        "touch": [
            1122085995
        ],
        "/var/ready\nyou’ve": [
            1122085995
        ],
        "\nkubia-2r1qb": [
            1122085995
        ],
        "doesn’t\nyet": [
            1122085995
        ],
        "0\nwhich": [
            1122085995
        ],
        "let’s\nsee": [
            1122085995
        ],
        "kubia-2r1qb\nname": [
            1122085995
        ],
        "result?": [
            1122085995
        ],
        "contain\nthe": [
            1122085995
        ],
        "line:\nreadiness:": [
            1122085995
        ],
        "[ls": [
            1122085995
        ],
        "/var/ready]": [
            1122085995
        ],
        "#success=1\n➥": [
            1122085995
        ],
        "#failure=3\nthe": [
            1122085995
        ],
        "periodically—every": [
            1122085995
        ],
        "pod\nisn’t": [
            1122085995
        ],
        "at\nthe": [
            1122085995
        ],
        "end-\npoint": [
            1122085995
        ],
        "(run": [
            1122085995
        ],
        "kubia-loadbalancer": [
            1122085995
        ],
        "confirm)": [
            1122085995
        ],
        "\n\n153signaling": [
            1122085995
        ],
        "connections\nhitting": [
            1122085995
        ],
        "http://130211.53.173\nyou’ve": [
            1122085995
        ],
        "kubia-2r1qb\n$": [
            1122085995
        ],
        "kubia-2r1qb\n..\n$": [
            1122085995
        ],
        "kubia-2r1qb\neven": [
            1122085995
        ],
        "being\nready": [
            1122085995
        ],
        "\n55.3understanding": [
            1122085995
        ],
        "do\nthis": [
            1122085995
        ],
        "demonstrating": [
            1122085995
        ],
        "do\nin": [
            1122085995
        ],
        "on\nwhether": [
            1122085995
        ],
        "wants": [
            1122085995
        ],
        "to)": [
            1122085995
        ],
        "flipping": [
            1122085995
        ],
        "\ntipif": [
            1122085995
        ],
        "add\nenabled=true": [
            1122085995
        ],
        "service\nremove": [
            1122085995
        ],
        "service\nalways": [
            1122085995
        ],
        "probe\nbefore": [
            1122085995
        ],
        "conclude": [
            1122085995
        ],
        "notes": [
            1122085995
        ],
        "that\ni": [
            1122085995
        ],
        "they’ll\nbecome": [
            1122085995
        ],
        "to\nstart": [
            1122085995
        ],
        "“connection": [
            1122085995
        ],
        "refused”": [
            1122085995
        ],
        "send-\ning": [
            1122085995
        ],
        "\ndon’t": [
            1122085995
        ],
        "logic": [
            1122085995
        ],
        "probes\nthe": [
            1122085995
        ],
        "(pod\nshutdown)": [
            1122085995
        ],
        "con-\nnections": [
            1122085995
        ],
        "termination": [
            1122085995
        ],
        "think\nyou": [
            1122085995
        ],
        "proce-\ndure": [
            1122085995
        ],
        "initiated": [
            1122085995
        ],
        "you\ndelete": [
            1122085995
        ],
        "\n\n154chapter": [
            1122085995
        ],
        "pods\n56using": [
            1122085995
        ],
        "to\nconnect": [
            1122085995
        ],
        "endpoints)": [
            1122085995
        ],
        "client\nneeds": [
            1122085995
        ],
        "clearly\nisn’t": [
            1122085995
        ],
        "is?\n": [
            1122085995
        ],
        "always\nstrive": [
            1122085995
        ],
        "kubernetes-agnostic": [
            1122085995
        ],
        "ideal": [
            1122085995
        ],
        "lookups": [
            1122085995
        ],
        "usually\nwhen": [
            1122085995
        ],
        "ip—the\nservice’s": [
            1122085995
        ],
        "service\n(you": [
            1122085995
        ],
        "specification)": [
            1122085995
        ],
        "dns\nserver": [
            1122085995
        ],
        "a\nrecords": [
            1122085995
        ],
        "pointing": [
            1122085995
        ],
        "them\n5.6.1creating": [
            1122085995
        ],
        "service\nsetting": [
            1122085995
        ],
        "as\nkubernetes": [
            1122085995
        ],
        "assign": [
            1122085995
        ],
        "\nkubia-headless": [
            1122085995
        ],
        "definition\napiversion:": [
            1122085995
        ],
        "kubia-headless\nspec:\n": [
            1122085995
        ],
        "clusterip:": [
            1122085995
        ],
        "kubia\nafter": [
            1122085995
        ],
        "get\nand": [
            1122085995
        ],
        "(part": [
            1122085995
        ],
        "of)\nlisting": [
            1122085995
        ],
        "kubia-svc-headlessyaml\nthis": [
            1122085995
        ],
        "headless\n": [
            1122085995
        ],
        "\n\n155using": [
            1122085995
        ],
        "“part": [
            1122085995
        ],
        "of”": [
            1122085995
        ],
        "service\nbefore": [
            1122085995
        ],
        "continuing": [
            1122085995
        ],
        "creating\nthe": [
            1122085995
        ],
        "<pod": [
            1122085995
        ],
        "/var/ready\n56.2discovering": [
            1122085995
        ],
        "dns\nwith": [
            1122085995
        ],
        "pods\nunfortunately": [
            1122085995
        ],
        "nslookup": [
            1122085995
        ],
        "dig)\nbinary": [
            1122085995
        ],
        "lookup\n": [
            1122085995
        ],
        "you\nneed?": [
            1122085995
        ],
        "dns-related": [
            1122085995
        ],
        "\ntutum/dnsutils": [
            1122085995
        ],
        "\nnslookup": [
            1122085995
        ],
        "dig\nbinaries": [
            1122085995
        ],
        "right?\nluckily": [
            1122085995
        ],
        "way\nrunning": [
            1122085995
        ],
        "manifest\nin": [
            1122085995
        ],
        "pod—you": [
            1122085995
        ],
        "dnsutils": [
            1122085995
        ],
        "--image=tutum/dnsutils": [
            1122085995
        ],
        "--generator=run-pod/v1\n➥": [
            1122085995
        ],
        "--command": [
            1122085995
        ],
        "infinity\npod": [
            1122085995
        ],
        "dnsutils\"": [
            1122085995
        ],
        "trick": [
            1122085995
        ],
        "--generator=run-pod/v1": [
            1122085995
        ],
        "records": [
            1122085995
        ],
        "lookup:\n$": [
            1122085995
        ],
        "kubia-headless\n..\nname:": [
            1122085995
        ],
        "kubia-headlessdefault.svc.cluster.local\naddress:": [
            1122085995
        ],
        "10108.1.4": [
            1122085995
        ],
        "\nname:": [
            1122085995
        ],
        "10108.2.5": [
            1122085995
        ],
        "kubia-headlessdefault.svc\n.cluster.local\n": [
            1122085995
        ],
        "which\nshows": [
            1122085995
        ],
        "\n\n156chapter": [
            1122085995
        ],
        "(non-headless)": [
            1122085995
        ],
        "such\nas": [
            1122085995
        ],
        "ip:\n$": [
            1122085995
        ],
        "kubia\n..\nname:": [
            1122085995
        ],
        "kubiadefault.svc.cluster.local\naddress:": [
            1122085995
        ],
        "10111.249.153\nalthough": [
            1122085995
        ],
        "seem": [
            1122085995
        ],
        "that\ndifferent": [
            1122085995
        ],
        "clients’": [
            1122085995
        ],
        "con-\nnect": [
            1122085995
        ],
        "connect\ndirectly": [
            1122085995
        ],
        "round-robin": [
            1122085995
        ],
        "proxy\n5.6.3discovering": [
            1122085995
        ],
        "ready\nyou’ve": [
            1122085995
        ],
        "some-\ntimes": [
            1122085995
        ],
        "resort": [
            1122085995
        ],
        "unready": [
            1122085995
        ],
        "must\nadd": [
            1122085995
        ],
        "service:\nkind:": [
            1122085995
        ],
        "servicealpha.kubernetes.io/tolerate-unready-endpoints:": [
            1122085995
        ],
        "true\"\nwarningas": [
            1122085995
        ],
        "alpha\nfeature": [
            1122085995
        ],
        "field\ncalled": [
            1122085995
        ],
        "\npublishnotreadyaddresses": [
            1122085995
        ],
        "tolerate-unready-\nendpoints\n": [
            1122085995
        ],
        "19.0": [
            1122085995
        ],
        "honored": [
            1122085995
        ],
        "yet\n(the": [
            1122085995
        ],
        "not)": [
            1122085995
        ],
        "changed\n5.7troubleshooting": [
            1122085995
        ],
        "services\nservices": [
            1122085995
        ],
        "frustration": [
            1122085995
        ],
        "many\ndevelopers": [
            1122085995
        ],
        "lose": [
            1122085995
        ],
        "heaps": [
            1122085995
        ],
        "can’t\nconnect": [
            1122085995
        ],
        "troubleshoot": [
            1122085995
        ],
        "order\n": [
            1122085995
        ],
        "unable": [
            1122085995
        ],
        "by\ngoing": [
            1122085995
        ],
        "list:\n": [
            1122085995
        ],
        "\n\n157summary\nfirst": [
            1122085995
        ],
        "outside\ndon’t": [
            1122085995
        ],
        "accessible\n(remember": [
            1122085995
        ],
        "work)\nif": [
            1122085995
        ],
        "succeeding;": [
            1122085995
        ],
        "service\nto": [
            1122085995
        ],
        "endpoints\nif": [
            1122085995
        ],
        "myservicemynamespace.svc.cluster.local": [
            1122085995
        ],
        "myservicemynamespace)": [
            1122085995
        ],
        "fqdn\ncheck": [
            1122085995
        ],
        "port\ntry": [
            1122085995
        ],
        "correct": [
            1122085995
        ],
        "port\nif": [
            1122085995
        ],
        "isn’t\nonly": [
            1122085995
        ],
        "binding": [
            1122085995
        ],
        "localhost\nthis": [
            1122085995
        ],
        "resolve": [
            1122085995,
            2119133144
        ],
        "they’re\nimplemented": [
            1122085995
        ],
        "them\n5.8summary\nin": [
            1122085995
        ],
        "are\nproviding": [
            1122085995
        ],
        "kubernetes\nexposes": [
            1122085995
        ],
        "stable\nip": [
            1122085995
        ],
        "port\nmakes": [
            1122085995
        ],
        "either\nnodeport": [
            1122085995
        ],
        "loadbalancer\nenables": [
            1122085995
        ],
        "by\nlooking": [
            1122085995
        ],
        "variables\nallows": [
            1122085995
        ],
        "residing": [
            1122085995
        ],
        "instead\nprovides": [
            1122085995
        ],
        "type\nexposes": [
            1122085995
        ],
        "(consuming": [
            1122085995
        ],
        "ip)\n": [
            1122085995
        ],
        "\n\n158chapter": [
            1122085995
        ],
        "pods\nuses": [
            1122085995
        ],
        "or\nshouldn’t": [
            1122085995
        ],
        "endpoint\nenables": [
            1122085995
        ],
        "service\nalong": [
            1122085995
        ],
        "to\ntroubleshoot": [
            1122085995
        ],
        "them\nmodify": [
            1122085995
        ],
        "kubernetes/compute": [
            1122085995
        ],
        "engine\nexecute": [
            1122085995
        ],
        "\nrun": [
            1122085995
        ],
        "container\nmodify": [
            1122085995
        ],
        "command\nrun": [
            1122085995
        ],
        "--generator=run-pod/v1\n": [
            1122085995
        ],
        "\n\n159\nvolumes:": [
            1122085995
        ],
        "attaching\ndisk": [
            1122085995
        ],
        "containers\nin": [
            1122085995
        ],
        "namely": [
            1122085995
        ],
        "daemonsets\njobs,": [
            1122085995
        ],
        "containers\ncan": [
            1122085995
        ],
        "inside\nthem": [
            1122085995
        ],
        "one\nwould": [
            1122085995
        ],
        "disks": [
            1122085995
        ],
        "file-\nsystem": [
            1122085995
        ],
        "pods\ncreating": [
            1122085995
        ],
        "\ncontainers\nusing": [
            1122085995
        ],
        "pod\nattaching": [
            1122085995
        ],
        "\npersistent": [
            1122085995
        ],
        "pods\nusing": [
            1122085995
        ],
        "pre-provisioned": [
            1122085995
        ],
        "storage\ndynamic": [
            1122085995
        ],
        "storage\n": [
            1122085995
        ],
        "\n\n160chapter": [
            1122085995
        ],
        "6volumes:": [
            1122085995
        ],
        "image\nat": [
            1122085995
        ],
        "combine": [
            1122085995
        ],
        "(either\nbecause": [
            1122085995
        ],
        "signaled": [
            1122085995
        ],
        "anymore)": [
            1122085995
        ],
        "will\nnot": [
            1122085995
        ],
        "even\nthough": [
            1122085995
        ],
        "fin-\nished": [
            1122085995
        ],
        "(or\nwant)": [
            1122085995
        ],
        "persisted": [
            1122085995
        ],
        "preserve": [
            1122085995
        ],
        "directories\nthat": [
            1122085995
        ],
        "data\n": [
            1122085995
        ],
        "top-level": [
            1122085995
        ],
        "resources\nlike": [
            1122085995
        ],
        "destroyed": [
            1122085995
        ],
        "volume’s": [
            1122085995
        ],
        "persist": [
            1122085995
        ],
        "container\nrestarts": [
            1122085995
        ],
        "were\nwritten": [
            1122085995
        ],
        "\n61introducing": [
            1122085995
        ],
        "volumes\nkubernetes": [
            1122085995
        ],
        "spec-\nification—much": [
            1122085995
        ],
        "can-\nnot": [
            1122085995
        ],
        "mount": [
            1122085995
        ],
        "filesystem\n6.1.1explaining": [
            1122085995
        ],
        "example\nimagine": [
            1122085995
        ],
        "61).": [
            1122085995
        ],
        "container\nruns": [
            1122085995
        ],
        "html": [
            1122085995
        ],
        "pages": [
            1122085995
        ],
        "/var/htdocs": [
            1122085995
        ],
        "stores\nthe": [
            1122085995
        ],
        "/var/logs": [
            1122085995
        ],
        "agent": [
            1122085995
        ],
        "html\nfiles": [
            1122085995
        ],
        "/var/html": [
            1122085995
        ],
        "finds": [
            1122085995
        ],
        "(rotates": [
            1122085995
        ],
        "compresses": [
            1122085995
        ],
        "analyzes": [
            1122085995
        ],
        "whatever)\n": [
            1122085995
        ],
        "without\nthem": [
            1122085995
        ],
        "generator\nwould": [
            1122085995
        ],
        "server\ncouldn’t": [
            1122085995
        ],
        "would\nserve": [
            1122085995
        ],
        "rotator": [
            1122085995
        ],
        "its\n/var/logs": [
            1122085995
        ],
        "nothing\n": [
            1122085995
        ],
        "\n\n161introducing": [
            1122085995
        ],
        "volumes\npod\ncontainer:": [
            1122085995
        ],
        "webserver\nfilesystem\nwebserver\nprocess\nwrites\nreads\n/\nvar/\nhtdocs/\nlogs/\ncontainer:": [
            1122085995
        ],
        "contentagent\nfilesystem\ncontentagent\nprocess\nwrites\n/\nvar/\nhtml/\ncontainer:": [
            1122085995
        ],
        "logrotator\nfilesystem\nlogrotator\nprocess\nreads\n/\nvar/\nlogs/\nfigure": [
            1122085995
        ],
        "\nsame": [
            1122085995
        ],
        "storage\npod\ncontainer:": [
            1122085995
        ],
        "webserver\nfilesystem\n/\nvar/\nhtdocs/\nlogs/\ncontainer:": [
            1122085995
        ],
        "contentagent\nfilesystem\n/\nvar/\nhtml/\ncontainer:": [
            1122085995
        ],
        "logrotator\nfilesystem\n/\nvar/\nlogs/\nvolume:\npublichtml\nvolume:\nlogvol\nfigure": [
            1122085995
        ],
        "paths\n": [
            1122085995
        ],
        "\n\n162chapter": [
            1122085995
        ],
        "containers\nthan": [
            1122085995
        ],
        "locations\nin": [
            1122085995
        ],
        "mounting": [
            1122085995
        ],
        "in\nthree": [
            1122085995
        ],
        "how\n": [
            1122085995
        ],
        "\npublichtml": [
            1122085995
        ],
        "web-\nserver\n": [
            1122085995
        ],
        "serves\nfiles": [
            1122085995
        ],
        "\ncontentagent": [
            1122085995
        ],
        "at\n/var/html": [
            1122085995
        ],
        "vol-\nume": [
            1122085995
        ],
        "agent\n": [
            1122085995
        ],
        "\nlogvol": [
            1122085995
        ],
        "is\nmounted": [
            1122085995
        ],
        "\nwebserver": [
            1122085995
        ],
        "logrotator": [
            1122085995
        ],
        "note\nthat": [
            1122085995
        ],
        "its\nfiles": [
            1122085995
        ],
        "not\nenough": [
            1122085995
        ],
        "pod;": [
            1122085995
        ],
        "\nvolumemount": [
            1122085995
        ],
        "type\nof": [
            1122085995
        ],
        "\nemptydir": [
            1122085995
        ],
        "are\neither": [
            1122085995
        ],
        "populated": [
            1122085995
        ],
        "initialization": [
            1122085995
        ],
        "an\nexisting": [
            1122085995
        ],
        "populating": [
            1122085995
        ],
        "mount-\ning": [
            1122085995
        ],
        "intact\neven": [
            1122085995
        ],
        "exist\n6.1.2introducing": [
            1122085995
        ],
        "types\na": [
            1122085995
        ],
        "variety": [
            1122085995
        ],
        "generic": [
            1122085995
        ],
        "never\nheard": [
            1122085995
        ],
        "technologies—i": [
            1122085995
        ],
        "probably\nonly": [
            1122085995
        ],
        "of\nseveral": [
            1122085995
        ],
        "types:\nemptydir—a": [
            1122085995
        ],
        "transient": [
            1122085995
        ],
        "data\nhostpath—used": [
            1122085995
        ],
        "filesystem\ninto": [
            1122085995
        ],
        "pod\ngitrepo—a": [
            1122085995
        ],
        "initialized": [
            1122085995
        ],
        "repository\nnfs—an": [
            1122085995
        ],
        "nfs": [
            1122085995
        ],
        "pod\ngcepersistentdisk": [
            1122085995
        ],
        "disk)": [
            1122085995
        ],
        "awselastic-\nblockstore\n": [
            1122085995
        ],
        "elastic": [
            1122085995
        ],
        "volume)": [
            1122085995
        ],
        "azuredisk\n(microsoft": [
            1122085995
        ],
        "volume)—used": [
            1122085995
        ],
        "provider-specific\nstorage\n": [
            1122085995
        ],
        "\n\n163using": [
            1122085995
        ],
        "containers\ncinder": [
            1122085995
        ],
        "cephfs": [
            1122085995
        ],
        "iscsi": [
            1122085995
        ],
        "flocker": [
            1122085995
        ],
        "glusterfs": [
            1122085995
        ],
        "quobyte": [
            1122085995
        ],
        "rbd": [
            1122085995
        ],
        "flexvolume": [
            1122085995
        ],
        "vsphere-\nvolume\n": [
            1122085995
        ],
        "photonpersistentdisk": [
            1122085995
        ],
        "scaleio—used": [
            1122085995
        ],
        "of\nnetwork": [
            1122085995
        ],
        "storage\nconfigmap": [
            1122085995
        ],
        "downwardapi—special": [
            1122085995
        ],
        "pod\npersistentvolumeclaim—a": [
            1122085995
        ],
        "pre-": [
            1122085995
        ],
        "per-\nsistent": [
            1122085995
        ],
        "(we’ll": [
            1122085995
        ],
        "chapter)\nthese": [
            1122085995
        ],
        "(\nsecret": [
            1122085995
        ],
        "configmap)": [
            1122085995
        ],
        "for\nexposing": [
            1122085995
        ],
        "as\nwe’ve": [
            1122085995
        ],
        "volume\nmounted": [
            1122085995
        ],
        "not\n6.2using": [
            1122085995
        ],
        "containers\nalthough": [
            1122085995
        ],
        "prove": [
            1122085995
        ],
        "first\nfocus": [
            1122085995
        ],
        "pod\n6.2.1using": [
            1122085995
        ],
        "volume\nthe": [
            1122085995
        ],
        "as\nan": [
            1122085995
        ],
        "needs\nto": [
            1122085995
        ],
        "are\nlost": [
            1122085995
        ],
        "deleted\n": [
            1122085995
        ],
        "temporarily": [
            1122085995
        ],
        "sort\noperation": [
            1122085995
        ],
        "dataset": [
            1122085995
        ],
        "(remember": [
            1122085995
        ],
        "read-write": [
            1122085995
        ],
        "layer\nin": [
            1122085995
        ],
        "container?)": [
            1122085995
        ],
        "subtle": [
            1122085995
        ],
        "container’s\nfilesystem": [
            1122085995
        ],
        "book)\nso": [
            1122085995
        ],
        "\nusing": [
            1122085995
        ],
        "pod\nlet’s": [
            1122085995
        ],
        "rota-\ntor": [
            1122085995
        ],
        "simplify": [
            1122085995
        ],
        "bit": [
            1122085995
        ],
        "web\nserver": [
            1122085995
        ],
        "\nfortune": [
            1122085995
        ],
        "generate\nthe": [
            1122085995
        ],
        "quote": [
            1122085995
        ],
        "you\nrun": [
            1122085995
        ],
        "script": [
            1122085995
        ],
        "and\nstores": [
            1122085995
        ],
        "indexhtml.": [
            1122085995
        ],
        "\n\n164chapter": [
            1122085995
        ],
        "containers\ndocker": [
            1122085995
        ],
        "fortune": [
            1122085995
        ],
        "the\none": [
            1122085995
        ],
        "\nluksa/fortune": [
            1122085995
        ],
        "a\nrefresher": [
            1122085995
        ],
        "sidebar\ncreating": [
            1122085995
        ],
        "pod\nmanifest": [
            1122085995
        ],
        "fortune-podyaml": [
            1122085995
        ],
        "fortune\nspec:\n": [
            1122085995
        ],
        "containers:\nbuilding": [
            1122085995
        ],
        "image\nhere’s": [
            1122085995
        ],
        "inside\nit": [
            1122085995
        ],
        "\nfortuneloopsh": [
            1122085995
        ],
        "contents:\n#!/bin/bash\ntrap": [
            1122085995
        ],
        "exit\"": [
            1122085995
        ],
        "sigint\nmkdir": [
            1122085995
        ],
        "/var/htdocs\nwhile": [
            1122085995
        ],
        ":\ndo\n": [
            1122085995
        ],
        "$(date)": [
            1122085995
        ],
        "/var/htdocs/indexhtml\n": [
            1122085995
        ],
        "/usr/games/fortune": [
            1122085995
        ],
        ">": [
            1122085995
        ],
        "10\ndone\nthen": [
            1122085995
        ],
        "following:\nfrom": [
            1122085995
        ],
        "ubuntu:latest\nrun": [
            1122085995
        ],
        ";": [
            1122085995
        ],
        "-y": [
            1122085995
        ],
        "fortune\nadd": [
            1122085995
        ],
        "fortuneloopsh": [
            1122085995
        ],
        "/bin/fortuneloopsh\nentrypoint": [
            1122085995
        ],
        "/bin/fortuneloopsh\nthe": [
            1122085995
        ],
        "ubuntu:latest": [
            1122085995
        ],
        "fortune\nbinary": [
            1122085995
        ],
        "with\napt-get": [
            1122085995
        ],
        "/bin": [
            1122085995
        ],
        "folder\nin": [
            1122085995
        ],
        "should\nbe": [
            1122085995
        ],
        "run\nafter": [
            1122085995
        ],
        "following\ntwo": [
            1122085995
        ],
        "\nluksa": [
            1122085995
        ],
        "luksa/fortune": [
            1122085995
        ],
        "luksa/fortune\nlisting": [
            1122085995
        ],
        "volume:": [
            1122085995
        ],
        "fortune-podyaml\n": [
            1122085995
        ],
        "\n\n165using": [
            1122085995
        ],
        "html-generator": [
            1122085995
        ],
        "nginx:alpine": [
            1122085995
        ],
        "web-server": [
            1122085995
        ],
        "/usr/share/nginx/html": [
            1122085995
        ],
        "tcp\n": [
            1122085995
        ],
        "emptydir:": [
            1122085995
        ],
        "\nhtml-generator": [
            1122085995
        ],
        "writ-\ning": [
            1122085995
        ],
        "/var/htdocs/indexhtml": [
            1122085995
        ],
        "10\nseconds": [
            1122085995
        ],
        "indexhtml": [
            1122085995
        ],
        "writ-\nten": [
            1122085995
        ],
        "\nweb-server": [
            1122085995
        ],
        "/usr/share/nginx/html\ndirectory": [
            1122085995
        ],
        "from)": [
            1122085995
        ],
        "mounted\nthe": [
            1122085995
        ],
        "there\nby": [
            1122085995
        ],
        "an\nhttp": [
            1122085995
        ],
        "as\nthe": [
            1122085995
        ],
        "by\nforwarding": [
            1122085995
        ],
        "8080:80\nforwarding": [
            1122085995
        ],
        "1270.0.1:8080": [
            1122085995
        ],
        "80\nforwarding": [
            1122085995
        ],
        "[::1]:8080": [
            1122085995
        ],
        "80\nnoteas": [
            1122085995
        ],
        "forwarding\nnow": [
            1122085995
        ],
        "use\ncurl": [
            1122085995
        ],
        "that:\n$": [
            1122085995
        ],
        "http://localhost:8080\nbeware": [
            1122085995
        ],
        "tall": [
            1122085995
        ],
        "blond": [
            1122085995
        ],
        "man": [
            1122085995
        ],
        "black": [
            1122085995
        ],
        "shoe\nif": [
            1122085995
        ],
        "different\nmessage": [
            1122085995
        ],
        "combining": [
            1122085995
        ],
        "volume\ncan": [
            1122085995
        ],
        "glue": [
            1122085995
        ],
        "enhance": [
            1122085995
        ],
        "does\nthe": [
            1122085995
        ],
        "image\nthe": [
            1122085995
        ],
        "\nat": [
            1122085995
        ],
        "\nmounted": [
            1122085995
        ],
        "read-only\na": [
            1122085995
        ],
        "\ncalled": [
            1122085995
        ],
        "above\n": [
            1122085995
        ],
        "\n\n166chapter": [
            1122085995
        ],
        "containers\nspecifying": [
            1122085995
        ],
        "medium": [
            1122085995
        ],
        "emptydir\nthe": [
            1122085995
        ],
        "worker\nnode": [
            1122085995
        ],
        "performance": [
            1122085995
        ],
        "disks\nbut": [
            1122085995
        ],
        "tmpfs": [
            1122085995
        ],
        "memory\ninstead": [
            1122085995
        ],
        "\nemptydir’s": [
            1122085995
        ],
        "this:\nvolumes:\n": [
            1122085995
        ],
        "html\n": [
            1122085995
        ],
        "emptydir:\n": [
            1122085995
        ],
        "medium:": [
            1122085995
        ],
        "it\nafter": [
            1122085995
        ],
        "populate": [
            1122085995
        ],
        "type\nis": [
            1122085995
        ],
        "\ngitrepo": [
            1122085995
        ],
        "next\n6.2.2using": [
            1122085995
        ],
        "gitrepo": [
            1122085995
        ],
        "cloning": [
            1122085995
        ],
        "a\ngit": [
            1122085995
        ],
        "revision": [
            1122085995
        ],
        "(but\nbefore": [
            1122085995
        ],
        "created)": [
            1122085995
        ],
        "unfolds\nnoteafter": [
            1122085995
        ],
        "sync": [
            1122085995
        ],
        "repo\nit’s": [
            1122085995
        ],
        "referencing": [
            1122085995
        ],
        "push\nadditional": [
            1122085995
        ],
        "commits": [
            1122085995
        ],
        "by\na": [
            1122085995
        ],
        "website\nand": [
            1122085995
        ],
        "time\nthe": [
            1122085995
        ],
        "the\nthis": [
            1122085995
        ],
        "emptydir’s": [
            1122085995
        ],
        "\nstored": [
            1122085995
        ],
        "memory\npod\ncontainer\nuser\ngitrepo\nvolume\n1.": [
            1122085995
        ],
        "replication\ncontroller)": [
            1122085995
        ],
        "volume\n2": [
            1122085995
        ],
        "creates\nan": [
            1122085995
        ],
        "and\nclones": [
            1122085995
        ],
        "git\nrepository": [
            1122085995
        ],
        "it\n3": [
            1122085995
        ],
        "started\n(with": [
            1122085995
        ],
        "path)\nrepository\nfigure": [
            1122085995
        ],
        "\ngit": [
            1122085995
        ],
        "repository\n": [
            1122085995
        ],
        "\n\n167using": [
            1122085995
        ],
        "containers\nonly": [
            1122085995
        ],
        "changes\nto": [
            1122085995
        ],
        "cloned": [
            1122085995
        ],
        "repository\nbefore": [
            1122085995
        ],
        "repo": [
            1122085995
        ],
        "https://githubcom/luksa/kubia-website-example.git.\nyou’ll": [
            1122085995
        ],
        "fork": [
            1122085995
        ],
        "(create": [
            1122085995
        ],
        "github)": [
            1122085995
        ],
        "push\nchanges": [
            1122085995
        ],
        "time\nyou’ll": [
            1122085995
        ],
        "(be\nsure": [
            1122085995
        ],
        "repository)": [
            1122085995
        ],
        "gitrepo-volume-pod\nspec:\n": [
            1122085995
        ],
        "nginx:alpine\n": [
            1122085995
        ],
        "web-server\n": [
            1122085995
        ],
        "volumemounts:\n": [
            1122085995
        ],
        "/usr/share/nginx/html\n": [
            1122085995
        ],
        "volumes:\n": [
            1122085995
        ],
        "gitrepo:": [
            1122085995
        ],
        "repository:": [
            1122085995
        ],
        "https://githubcom/luksa/kubia-website-example.git": [
            1122085995
        ],
        "revision:": [
            1122085995
        ],
        "directory:": [
            1122085995
        ],
        "then\nthe": [
            1122085995
        ],
        "(dot)\nthe": [
            1122085995
        ],
        "kubia-website-example": [
            1122085995
        ],
        "subdirectory\nwhich": [
            1122085995
        ],
        "to\ncheck": [
            1122085995
        ],
        "branch": [
            1122085995
        ],
        "volume\nis": [
            1122085995
        ],
        "by\nexecuting": [
            1122085995
        ],
        "cluster)": [
            1122085995
        ],
        "gitrepo-volume-podyaml\nyou’re": [
            1122085995
        ],
        "clone\nthis": [
            1122085995
        ],
        "repository\nthe": [
            1122085995
        ],
        "out\nyou": [
            1122085995
        ],
        "\ndir": [
            1122085995
        ],
        "volume\n": [
            1122085995
        ],
        "\n\n168chapter": [
            1122085995
        ],
        "containers\nconfirming": [
            1122085995
        ],
        "repo\nnow": [
            1122085995
        ],
        "directly—click": [
            1122085995
        ],
        "your\ngithub": [
            1122085995
        ],
        "click": [
            1122085995
        ],
        "pencil": [
            1122085995
        ],
        "icon": [
            1122085995
        ],
        "it\nchange": [
            1122085995
        ],
        "commit": [
            1122085995
        ],
        "clicking": [
            1122085995
        ],
        "button": [
            1122085995
        ],
        "bottom\n": [
            1122085995
        ],
        "the\nhtml": [
            1122085995
        ],
        "visible": [
            1122085995
        ],
        "because\nthe": [
            1122085995
        ],
        "by\nhitting": [
            1122085995
        ],
        "create\nit": [
            1122085995
        ],
        "could\nrun": [
            1122085995
        ],
        "repository\ni": [
            1122085995
        ],
        "exer-\ncise": [
            1122085995
        ],
        "pointers\nintroducing": [
            1122085995
        ],
        "container:": [
            1122085995
        ],
        "aug-\nments": [
            1122085995
        ],
        "cramming": [
            1122085995
        ],
        "the\nmain": [
            1122085995
        ],
        "reusable": [
            1122085995
        ],
        "synchronized\nwith": [
            1122085995
        ],
        "search": [
            1122085995
        ],
        "“git": [
            1122085995
        ],
        "sync”": [
            1122085995
        ],
        "many\nimages": [
            1122085995
        ],
        "and\nconfigure": [
            1122085995
        ],
        "set\neverything": [
            1122085995
        ],
        "correctly": [
            1122085995
        ],
        "kept\nin": [
            1122085995
        ],
        "\nnotean": [
            1122085995
        ],
        "the\nstep-by-step": [
            1122085995
        ],
        "repositories\nthere’s": [
            1122085995
        ],
        "we\nhaven’t": [
            1122085995
        ],
        "it\nturns": [
            1122085995
        ],
        "consensus": [
            1122085995
        ],
        "keep\nthe": [
            1122085995
        ],
        "repositories\nthrough": [
            1122085995
        ],
        "config\noptions": [
            1122085995
        ],
        "clone": [
            1122085995
        ],
        "git-\nsync": [
            1122085995
        ],
        "\n\n169accessing": [
            1122085995
        ],
        "filesystem\nwrapping": [
            1122085995
        ],
        "volume\na": [
            1122085995
        ],
        "exclusively": [
            1122085995
        ],
        "volumes\nhowever,": [
            1122085995
        ],
        "survive\nmultiple": [
            1122085995
        ],
        "instantiations": [
            1122085995
        ],
        "next\n6.3accessing": [
            1122085995
        ],
        "filesystem\nmost": [
            1122085995
        ],
        "oblivious": [
            1122085995
        ],
        "be\nmanaged": [
            1122085995
        ],
        "daemonset)": [
            1122085995
        ],
        "node’s\nfilesystem": [
            1122085995
        ],
        "devices": [
            1122085995
        ],
        "this\npossible": [
            1122085995
        ],
        "\nhostpath": [
            1122085995
        ],
        "\n63.1introducing": [
            1122085995
        ],
        "(see\nfigure": [
            1122085995
        ],
        "64).": [
            1122085995
        ],
        "\nhost-\npath\n": [
            1122085995
        ],
        "files\nhostpath": [
            1122085995
        ],
        "because\nboth": [
            1122085995
        ],
        "volumes’": [
            1122085995
        ],
        "torn\ndown": [
            1122085995
        ],
        "next\npod": [
            1122085995
        ],
        "will\nsee": [
            1122085995
        ],
        "same\nnode": [
            1122085995
        ],
        "pod\nnode": [
            1122085995
        ],
        "1\npod\nhostpath\nvolume\npod\nhostpath\nvolume\nnode": [
            1122085995
        ],
        "2\npod\nhostpath\nvolume\n/some/path/on/host/some/path/on/host\nfigure": [
            1122085995
        ],
        "mounts": [
            1122085995
        ],
        "\n\n170chapter": [
            1122085995
        ],
        "thinking": [
            1122085995
        ],
        "database’s\ndata": [
            1122085995
        ],
        "specific\nnode’s": [
            1122085995
        ],
        "volume\nfor": [
            1122085995
        ],
        "to\n6.3.2examining": [
            1122085995
        ],
        "volumes\nlet’s": [
            1122085995
        ],
        "system-wide": [
            1122085995
        ],
        "age\nfluentd-kubia-4ebc2f1e-9a3e": [
            1122085995
        ],
        "4d\nfluentd-kubia-4ebc2f1e-e2vz": [
            1122085995
        ],
        "31d\n..\npick": [
            1122085995
        ],
        "listing)\n$": [
            1122085995
        ],
        "fluentd-kubia-4ebc2f1e-9a3e": [
            1122085995
        ],
        "kube-system\nname:": [
            1122085995
        ],
        "fluentd-cloud-logging-gke-kubia-default-pool-4ebc2f1e-9a3e\nnamespace:": [
            1122085995
        ],
        "kube-system\n..\nvolumes:\n": [
            1122085995
        ],
        "varlog:\n": [
            1122085995
        ],
        "(bare": [
            1122085995
        ],
        "volume)\n": [
            1122085995
        ],
        "/var/log\n": [
            1122085995
        ],
        "varlibdockercontainers:\n": [
            1122085995
        ],
        "/var/lib/docker/containers\ntipif": [
            1122085995
        ],
        "pod\naha!": [
            1122085995
        ],
        "/var/log": [
            1122085995
        ],
        "/var/lib/docker/containers": [
            1122085995
        ],
        "lucky": [
            1122085995
        ],
        "pod\nusing": [
            1122085995
        ],
        "check\nthe": [
            1122085995
        ],
        "node’s\nlog": [
            1122085995
        ],
        "kubeconfig": [
            1122085995
        ],
        "ca": [
            1122085995
        ],
        "certificates\n": [
            1122085995
        ],
        "we’ll\nsee": [
            1122085995
        ],
        "learn\nabout": [
            1122085995
        ],
        "logs\n": [
            1122085995
        ],
        "\n\n171using": [
            1122085995
        ],
        "storage\ntipremember": [
            1122085995
        ],
        "\n64using": [
            1122085995
        ],
        "storage\nwhen": [
            1122085995
        ],
        "that\nsame": [
            1122085995
        ],
        "use\nany": [
            1122085995
        ],
        "network-attached": [
            1122085995
        ],
        "(nas)\n": [
            1122085995
        ],
        "persisting": [
            1122085995
        ],
        "mongodb": [
            1122085995
        ],
        "document-oriented": [
            1122085995
        ],
        "nosql": [
            1122085995
        ],
        "without\na": [
            1122085995
        ],
        "non-persistent": [
            1122085995
        ],
        "testing\npurposes": [
            1122085995
        ],
        "the\nmongodb": [
            1122085995
        ],
        "\n64.1using": [
            1122085995
        ],
        "volume\nif": [
            1122085995
        ],
        "runs\nyour": [
            1122085995
        ],
        "(gce)": [
            1122085995
        ],
        "persistent\ndisk": [
            1122085995
        ],
        "automati-\ncally—you": [
            1122085995
        ],
        "storage\nmanually": [
            1122085995
        ],
        "chance": [
            1122085995
        ],
        "disk\nyou’ll": [
            1122085995
        ],
        "same\nzone": [
            1122085995
        ],
        "\ngcloud": [
            1122085995
        ],
        "command\nlike": [
            1122085995
        ],
        "list\nname": [
            1122085995
        ],
        "master_version": [
            1122085995
        ],
        "..\nkubia": [
            1122085995
        ],
        "europe-west1-b": [
            1122085995
        ],
        "12.5": [
            1122085995
        ],
        "104155.84.137": [
            1122085995
        ],
        "create\nthe": [
            1122085995
        ],
        "--size=1gib": [
            1122085995
        ],
        "--zone=europe-west1-b": [
            1122085995
        ],
        "mongodb\nwarning:": [
            1122085995
        ],
        "[200gb]": [
            1122085995
        ],
        "\npoor": [
            1122085995
        ],
        "i/o": [
            1122085995
        ],
        "see:": [
            1122085995
        ],
        "\nhttps://developersgoogle.com/compute/docs/disks#pdperformance.\ncreated": [
            1122085995
        ],
        "[https://wwwgoogleapis.com/compute/v1/projects/rapid-pivot-\n136513/zones/europe-west1-b/disks/mongodb].\nname": [
            1122085995
        ],
        "size_gb": [
            1122085995
        ],
        "status\nmongodb": [
            1122085995
        ],
        "pd-standard": [
            1122085995
        ],
        "ready\n": [
            1122085995
        ],
        "\n\n172chapter": [
            1122085995
        ],
        "containers\nthis": [
            1122085995
        ],
        "gib": [
            1122085995
        ],
        "can\nignore": [
            1122085995
        ],
        "warning": [
            1122085995
        ],
        "disk’s": [
            1122085995
        ],
        "perfor-\nmance": [
            1122085995
        ],
        "tests": [
            1122085995
        ],
        "run\ncreating": [
            1122085995
        ],
        "gcepersistentdisk": [
            1122085995
        ],
        "volume\nnow": [
            1122085995
        ],
        "volume\ninside": [
            1122085995
        ],
        "mongodb-data": [
            1122085995
        ],
        "gcepersistentdisk:": [
            1122085995
        ],
        "pdname:": [
            1122085995
        ],
        "fstype:": [
            1122085995
        ],
        "ext4": [
            1122085995
        ],
        "mongo\n": [
            1122085995
        ],
        "mongodb\n": [
            1122085995
        ],
        "/data/db": [
            1122085995
        ],
        "27017\n": [
            1122085995
        ],
        "tcp\nnoteif": [
            1122085995
        ],
        "\nmongodb-pod-hostpathyaml": [
            1122085995
        ],
        "volume\ninstead": [
            1122085995
        ],
        "pd\nthe": [
            1122085995
        ],
        "65).": [
            1122085995
        ],
        "data\nlisting": [
            1122085995
        ],
        "mongodb-pod-gcepdyaml\nthe": [
            1122085995
        ],
        "the\nvolume\n(also\nreferenced\nwhen\nmounting\nthe": [
            1122085995
        ],
        "volume)\nthe": [
            1122085995
        ],
        "disk\nthe": [
            1122085995
        ],
        "\ndisk": [
            1122085995
        ],
        "\npd": [
            1122085995
        ],
        "earlier\nthe": [
            1122085995
        ],
        "\n(a": [
            1122085995
        ],
        "filesystem)\nthe": [
            1122085995
        ],
        "\nstores": [
            1122085995
        ],
        "data\npod:": [
            1122085995
        ],
        "mongodb\ncontainer:": [
            1122085995
        ],
        "mongodb\nvolumemounts:\nname:": [
            1122085995
        ],
        "mongodb-data\nmountpath:": [
            1122085995
        ],
        "/data/db\ngcepersistentdisk:\npdname:": [
            1122085995
        ],
        "mongodb\ngce\npersistent": [
            1122085995
        ],
        "disk:\nmongodb\nvolume:\nmongodb\nfigure": [
            1122085995
        ],
        "\nexternal": [
            1122085995
        ],
        "disk\n": [
            1122085995
        ],
        "\n\n173using": [
            1122085995
        ],
        "storage\nwriting": [
            1122085995
        ],
        "documents": [
            1122085995
        ],
        "database\nnow": [
            1122085995
        ],
        "store\n": [
            1122085995
        ],
        "mongo\nmongodb": [
            1122085995
        ],
        "version:": [
            1122085995
        ],
        "32.8\nconnecting": [
            1122085995
        ],
        "to:": [
            1122085995
        ],
        "mongodb://1270.0.1:27017\nwelcome": [
            1122085995
        ],
        "shell\nfor": [
            1122085995
        ],
        "interactive": [
            1122085995
        ],
        "help\"\nfor": [
            1122085995
        ],
        "comprehensive": [
            1122085995
        ],
        "see\n": [
            1122085995
        ],
        "http://docsmongodb.org/\nquestions?": [
            1122085995
        ],
        "group\n": [
            1122085995
        ],
        "http://groupsgoogle.com/group/mongodb-user\n...\n>": [
            1122085995
        ],
        "\nmongodb": [
            1122085995
        ],
        "per-\nsistently": [
            1122085995
        ],
        "insert": [
            1122085995
        ],
        "document\nwith": [
            1122085995
        ],
        "commands:": [
            1122085995
        ],
        "\n>": [
            1122085995
        ],
        "mystore\nswitched": [
            1122085995
        ],
        "db": [
            1122085995
        ],
        "mystore\n>": [
            1122085995
        ],
        "dbfoo.insert({name:foo'})\nwriteresult({": [
            1122085995
        ],
        "ninserted\"": [
            1122085995
        ],
        ":": [
            1122085995
        ],
        "})\nyou’ve": [
            1122085995
        ],
        "inserted": [
            1122085995
        ],
        "document": [
            1122085995
        ],
        "(name:": [
            1122085995
        ],
        "’foo’)": [
            1122085995
        ],
        "now\nuse": [
            1122085995
        ],
        "\nfind()": [
            1122085995
        ],
        "inserted:\n>": [
            1122085995
        ],
        "dbfoo.find()\n{": [
            1122085995
        ],
        "_id\"": [
            1122085995
        ],
        "objectid(57a61eb9de0cfd512374cc75\")": [
            1122085995
        ],
        "name\"": [
            1122085995
        ],
        "foo\"": [
            1122085995
        ],
        "}\nthere": [
            1122085995
        ],
        "\nre-creating": [
            1122085995
        ],
        "verifying": [
            1122085995
        ],
        "(type": [
            1122085995
        ],
        "press": [
            1122085995
        ],
        "enter)": [
            1122085995
        ],
        "mongodb\npod": [
            1122085995
        ],
        "mongodb\"": [
            1122085995
        ],
        "deleted\n$": [
            1122085995
        ],
        "mongodb-pod-gcepdyaml\npod": [
            1122085995
        ],
        "node\ntipyou": [
            1122085995
        ],
        "po\n-o\n": [
            1122085995
        ],
        "wide\nlisting": [
            1122085995
        ],
        "\n\n174chapter": [
            1122085995
        ],
        "containers\nonce": [
            1122085995
        ],
        "the\ndocument": [
            1122085995
        ],
        "shell\n...\n>": [
            1122085995
        ],
        "}\nas": [
            1122085995
        ],
        "it\nthis": [
            1122085995
        ],
        "confirms": [
            1122085995
        ],
        "pod\ninstances": [
            1122085995
        ],
        "playing": [
            1122085995
        ],
        "but\nhold": [
            1122085995
        ],
        "chapter\n6.4.2using": [
            1122085995
        ],
        "storage\nthe": [
            1122085995
        ],
        "elsewhere": [
            1122085995
        ],
        "you\nshould": [
            1122085995
        ],
        "infrastructure\n": [
            1122085995
        ],
        "ec2": [
            1122085995
        ],
        "\nawselasticblockstore": [
            1122085995
        ],
        "if\nyour": [
            1122085995
        ],
        "microsoft": [
            1122085995
        ],
        "\nazurefile": [
            1122085995
        ],
        "azuredisk\nvolume": [
            1122085995
        ],
        "virtually": [
            1122085995
        ],
        "definition\nusing": [
            1122085995
        ],
        "disk\nyou’d": [
            1122085995
        ],
        "(see\nthose": [
            1122085995
        ],
        "bold)\napiversion:": [
            1122085995
        ],
        "awselasticblockstore:": [
            1122085995
        ],
        "mongodb’s": [
            1122085995
        ],
        "pod\nlisting": [
            1122085995
        ],
        "awselasticblockstore": [
            1122085995
        ],
        "mongodb-pod-awsyaml\nusing": [
            1122085995
        ],
        "\ninstead": [
            1122085995
        ],
        "gcepersistentdisk\n": [
            1122085995
        ],
        "\n\n175using": [
            1122085995
        ],
        "volumeid:": [
            1122085995
        ],
        "my-volume": [
            1122085995
        ],
        "..\nusing": [
            1122085995
        ],
        "vast": [
            1122085995
        ],
        "array": [
            1122085995
        ],
        "sup-\nported": [
            1122085995
        ],
        "to\nmount": [
            1122085995
        ],
        "path\nexported": [
            1122085995
        ],
        "nfs:": [
            1122085995
        ],
        "server:": [
            1122085995
        ],
        "12.3.4": [
            1122085995
        ],
        "/some/path": [
            1122085995
        ],
        "technologies\nother": [
            1122085995
        ],
        "glusterfs\nfor": [
            1122085995
        ],
        "rados": [
            1122085995
        ],
        "device": [
            1122085995
        ],
        "cinder": [
            1122085995
        ],
        "cephfs\nflocker,": [
            1122085995
        ],
        "fc": [
            1122085995
        ],
        "(fibre": [
            1122085995
        ],
        "channel)": [
            1122085995
        ],
        "you’re\nnot": [
            1122085995
        ],
        "a\nbroad": [
            1122085995
        ],
        "whichever": [
            1122085995
        ],
        "are\nused": [
            1122085995
        ],
        "types\nyou": [
            1122085995
        ],
        "\nexplain": [
            1122085995
        ],
        "type\nand": [
            1122085995
        ],
        "stuff?": [
            1122085995
        ],
        "creat-\ning": [
            1122085995
        ],
        "be\nleft": [
            1122085995
        ],
        "administrator?": [
            1122085995
        ],
        "what\nkubernetes": [
            1122085995
        ],
        "feels": [
            1122085995
        ],
        "worst": [
            1122085995
        ],
        "definition\nmeans": [
            1122085995
        ],
        "you\ncan’t": [
            1122085995
        ],
        "this\nisn’t": [
            1122085995
        ],
        "improve\non": [
            1122085995
        ],
        "section\nlisting": [
            1122085995
        ],
        "mongodb-pod-nfsyaml\nspecify": [
            1122085995
        ],
        "ebs": [
            1122085995
        ],
        "before\nthis": [
            1122085995
        ],
        "share\nthe": [
            1122085995
        ],
        "\nnfs": [
            1122085995
        ],
        "exported": [
            1122085995
        ],
        "\n\n176chapter": [
            1122085995
        ],
        "containers\n65decoupling": [
            1122085995
        ],
        "technology\nall": [
            1122085995
        ],
        "explored": [
            1122085995
        ],
        "nfs-backed": [
            1122085995
        ],
        "actual\nserver": [
            1122085995
        ],
        "which\naims": [
            1122085995
        ],
        "hide": [
            1122085995
        ],
        "leav-\ning": [
            1122085995
        ],
        "worrying": [
            1122085995
        ],
        "apps\nportable": [
            1122085995
        ],
        "datacenters\n": [
            1122085995
        ],
        "to\nknow": [
            1122085995
        ],
        "don’t\nhave": [
            1122085995
        ],
        "infrastruc-\nture-related": [
            1122085995
        ],
        "dealings": [
            1122085995
        ],
        "sole": [
            1122085995
        ],
        "administrator\n": [
            1122085995
        ],
        "mem-\nory": [
            1122085995
        ],
        "configure\nthe": [
            1122085995
        ],
        "request\n6.5.1introducing": [
            1122085995
        ],
        "persistentvolumeclaims\nto": [
            1122085995
        ],
        "with\ninfrastructure": [
            1122085995
        ],
        "persistent-\nvolumes": [
            1122085995
        ],
        "as\nyou’ve": [
            1122085995
        ],
        "be\nused": [
            1122085995
        ],
        "regular\npod": [
            1122085995
        ],
        "illustrate": [
            1122085995
        ],
        "persistentvolumes\nand": [
            1122085995
        ],
        "66.\npod\nadmin\nvolume\n1.": [
            1122085995
        ],
        "(nfs": [
            1122085995
        ],
        "similar)\n2": [
            1122085995
        ],
        "(pv)\nby": [
            1122085995
        ],
        "pv": [
            1122085995
        ],
        "api\nnfs\nexport\npersistent\nvolume\nuser\npersistent\nvolumeclaim\n3": [
            1122085995
        ],
        "a\npersistentvolumeclaim": [
            1122085995
        ],
        "(pvc)\n4": [
            1122085995
        ],
        "of\nadequate": [
            1122085995
        ],
        "access\nmode": [
            1122085995
        ],
        "binds": [
            1122085995
        ],
        "pvc\nto": [
            1122085995
        ],
        "pv\n5": [
            1122085995
        ],
        "volume\nreferencing": [
            1122085995
        ],
        "pvc\nfigure": [
            1122085995
        ],
        "admins": [
            1122085995
        ],
        "persistentvolumeclaims\n": [
            1122085995
        ],
        "\n\n177decoupling": [
            1122085995
        ],
        "technology\ninstead": [
            1122085995
        ],
        "technology-specific": [
            1122085995
        ],
        "registers": [
            1122085995
        ],
        "api\nserver": [
            1122085995
        ],
        "access\nmodes": [
            1122085995
        ],
        "first\ncreate": [
            1122085995
        ],
        "minimum": [
            1122085995,
            1118639836
        ],
        "submits": [
            1122085995
        ],
        "and\nbinds": [
            1122085995
        ],
        "pod\nother": [
            1122085995
        ],
        "delet-\ning": [
            1122085995
        ],
        "persistentvolumeclaim\n6.5.2creating": [
            1122085995
        ],
        "persistentvolume\nlet’s": [
            1122085995
        ],
        "gce\npersistent": [
            1122085995
        ],
        "cluster\nadministrator": [
            1122085995
        ],
        "then\nyou’ll": [
            1122085995
        ],
        "persistentvol-\nume": [
            1122085995
        ],
        "64.1": [
            1122085995
        ],
        "persistent-\nvolume": [
            1122085995
        ],
        "server\napiversion:": [
            1122085995
        ],
        "persistentvolume\nmetadata:\n": [
            1122085995
        ],
        "mongodb-pv\nspec:\n": [
            1122085995
        ],
        "capacity:": [
            1122085995
        ],
        "storage:": [
            1122085995
        ],
        "1gi": [
            1122085995
        ],
        "accessmodes:": [
            1122085995
        ],
        "readwriteonce": [
            1122085995
        ],
        "readonlymany": [
            1122085995
        ],
        "persistentvolumereclaimpolicy:": [
            1122085995
        ],
        "retain": [
            1122085995
        ],
        "persistentvolume:": [
            1122085995
        ],
        "mongodb-pv-gcepdyaml\ndefining": [
            1122085995
        ],
        "\npersistentvolume’s": [
            1122085995
        ],
        "size\nit": [
            1122085995
        ],
        "\nclient": [
            1122085995
        ],
        "retained": [
            1122085995
        ],
        "\nerased": [
            1122085995
        ],
        "deleted)\nthe": [
            1122085995
        ],
        "\nbacked": [
            1122085995
        ],
        "\n\n178chapter": [
            1122085995
        ],
        "containers\nnoteif": [
            1122085995
        ],
        "mongodb-pv-host-\npathyaml": [
            1122085995
        ],
        "file\nwhen": [
            1122085995
        ],
        "its\ncapacity": [
            1122085995
        ],
        "by\nmultiple": [
            1122085995
        ],
        "the\npersistentvolume": [
            1122085995
        ],
        "is\ndeleted)": [
            1122085995
        ],
        "look\nclosely": [
            1122085995
        ],
        "listing)\nspec:\n": [
            1122085995
        ],
        "..\nafter": [
            1122085995
        ],
        "claimed": [
            1122085995
        ],
        "persistentvolumes:\n$": [
            1122085995
        ],
        "pv\nname": [
            1122085995
        ],
        "capacity": [
            1122085995
        ],
        "reclaimpolicy": [
            1122085995
        ],
        "accessmodes": [
            1122085995
        ],
        "claim\nmongodb-pv": [
            1122085995
        ],
        "rworox": [
            1122085995
        ],
        "\nnoteseveral": [
            1122085995
        ],
        "omitted": [
            1122085995
        ],
        "for\npersistentvolume\nas": [
            1122085995
        ],
        "\nnotepersistentvolumes": [
            1122085995
        ],
        "67).\nthey’re": [
            1122085995
        ],
        "pd": [
            1122085995
        ],
        "\n\n179decoupling": [
            1122085995
        ],
        "technology\n65.3claiming": [
            1122085995
        ],
        "\npersistentvolumeclaim\nnow": [
            1122085995
        ],
        "lay": [
            1122085995
        ],
        "hats": [
            1122085995
        ],
        "claiming": [
            1122085995
        ],
        "pod\nbecause": [
            1122085995
        ],
        "is\nrescheduled": [
            1122085995
        ],
        "new\none": [
            1122085995
        ],
        "persistentvolumeclaim\nyou’ll": [
            1122085995
        ],
        "manifest\nlike": [
            1122085995
        ],
        "through\nkubectl": [
            1122085995
        ],
        "create\napiversion:": [
            1122085995
        ],
        "persistentvolumeclaim\nmetadata:\n": [
            1122085995
        ],
        "mongodb-pvc": [
            1122085995
        ],
        "persistentvolumeclaim:": [
            1122085995
        ],
        "mongodb-pvcyaml\npod(s)pod(s)\npersistent\nvolume\npersistent\nvolume\npersistent\nvolume\npersistent\nvolume\n...\nuser": [
            1122085995
        ],
        "a\npersistent\nvolume\nclaim(s)\npersistent\nvolume\nclaim(s)\nnamespace": [
            1122085995
        ],
        "a\nuser": [
            1122085995
        ],
        "b\nnamespace": [
            1122085995
        ],
        "b\nnodenodenodenodenodenode\npersistent\nvolume\nfigure": [
            1122085995
        ],
        "\npersistentvolumeclaims\nthe": [
            1122085995
        ],
        "claim—you’ll": [
            1122085995
        ],
        "\nneed": [
            1122085995
        ],
        "\nclaim": [
            1122085995
        ],
        "\n\n180chapter": [
            1122085995
        ],
        "containers\nspec:\n": [
            1122085995
        ],
        "resources:\n": [
            1122085995
        ],
        "storageclassname:": [
            1122085995
        ],
        "\"": [
            1122085995
        ],
        "persistentvolume\nand": [
            1122085995
        ],
        "persistentvolume’s": [
            1122085995
        ],
        "to\naccommodate": [
            1122085995
        ],
        "modes": [
            1122085995
        ],
        "must\ninclude": [
            1122085995
        ],
        "gib\nof": [
            1122085995
        ],
        "\nreadwriteonce": [
            1122085995
        ],
        "claim\nlisting": [
            1122085995
        ],
        "persistentvolumeclaims\nlist": [
            1122085995
        ],
        "pvc:\n$": [
            1122085995
        ],
        "pvc\nname": [
            1122085995
        ],
        "age\nmongodb-pvc": [
            1122085995
        ],
        "mongodb-pv": [
            1122085995
        ],
        "3s\nnotewe’re": [
            1122085995
        ],
        "pvc": [
            1122085995
        ],
        "persistentvolumeclaim\nthe": [
            1122085995
        ],
        "\nbound": [
            1122085995
        ],
        "abbreviations\nused": [
            1122085995
        ],
        "modes:\nrwo—readwriteonce—only": [
            1122085995
        ],
        "reading\nand": [
            1122085995
        ],
        "writing\nrox—readonlymany—multiple": [
            1122085995
        ],
        "reading\nrwx—readwritemany—multiple": [
            1122085995
        ],
        "writing\nnoterwo": [
            1122085995
        ],
        "rox": [
            1122085995
        ],
        "rwx": [
            1122085995
        ],
        "pods!\nlisting": [
            1122085995
        ],
        "persistentvolumes\nyou": [
            1122085995
        ],
        "age\nmongodb-pv": [
            1122085995
        ],
        "default/mongodb-pvc": [
            1122085995
        ],
        "default\npart": [
            1122085995
        ],
        "resides": [
            1122085995
        ],
        "default\nrequesting": [
            1122085995
        ],
        "storage\nyou": [
            1122085995
        ],
        "(performing": [
            1122085995
        ],
        "reads": [
            1122085995
        ],
        "writes)\nyou’ll": [
            1122085995
        ],
        "\nabout": [
            1122085995
        ],
        "provisioning\n": [
            1122085995
        ],
        "\n\n181decoupling": [
            1122085995
        ],
        "technology\nnamespace)": [
            1122085995
        ],
        "cluster-scoped\nand": [
            1122085995
        ],
        "can\nonly": [
            1122085995
        ],
        "namespace\n6.5.4using": [
            1122085995
        ],
        "nobody": [
            1122085995
        ],
        "volume\nuntil": [
            1122085995
        ],
        "persistent-\nvolumeclaim": [
            1122085995
        ],
        "directly!)": [
            1122085995
        ],
        "mongodb-data\n": [
            1122085995
        ],
        "/data/db\n": [
            1122085995
        ],
        "claimname:": [
            1122085995
        ],
        "\ngo": [
            1122085995
        ],
        "indeed": [
            1122085995
        ],
        "same\npersistentvolume": [
            1122085995
        ],
        "}\nand": [
            1122085995
        ],
        "you‘re": [
            1122085995
        ],
        "mongodb\npreviously\nlisting": [
            1122085995
        ],
        "mongodb-pod-pvcyaml\nlisting": [
            1122085995
        ],
        "pv\nreferencing": [
            1122085995
        ],
        "\n\n182chapter": [
            1122085995
        ],
        "containers\n65.5understanding": [
            1122085995
        ],
        "claims\nexamine": [
            1122085995
        ],
        "disk—\ndirectly": [
            1122085995
        ],
        "claim\nconsider": [
            1122085995
        ],
        "indirect": [
            1122085995
        ],
        "obtaining": [
            1122085995
        ],
        "infrastructure\nis": [
            1122085995
        ],
        "user)": [
            1122085995
        ],
        "persistentvolumeclaim\nbut": [
            1122085995
        ],
        "technology\nused": [
            1122085995
        ],
        "different\nkubernetes": [
            1122085995
        ],
        "infrastructure-specific": [
            1122085995
        ],
        "the\nclaim": [
            1122085995
        ],
        "once”": [
            1122085995
        ],
        "references": [
            1122085995
        ],
        "volumes\npod:": [
            1122085995
        ],
        "disk:\nmongodb\nvolume:\nmongodb\npod:": [
            1122085995
        ],
        "/data/db\npersistentvolumeclaim:\nclaimname:": [
            1122085995
        ],
        "mongodb-pvc\ngcepersistentdisk:\npdname:": [
            1122085995
        ],
        "disk:\nmongodb\npersistentvolume:\nmongodb-pv\n(1": [
            1122085995
        ],
        "gi": [
            1122085995
        ],
        "rwo": [
            1122085995
        ],
        "rwx)\nvolume:\nmongodb\nclaim": [
            1122085995
        ],
        "lists\n1gi": [
            1122085995
        ],
        "and\nreadwriteonce\naccess\npersistentvolumeclaim:\nmongodb-pvc\nfigure": [
            1122085995
        ],
        "pv\n": [
            1122085995
        ],
        "\n\n183decoupling": [
            1122085995
        ],
        "technology\n65.6recycling": [
            1122085995
        ],
        "persistentvolumes\nbefore": [
            1122085995
        ],
        "experi-\nment": [
            1122085995
        ],
        "persistentvolumeclaim:\n$": [
            1122085995
        ],
        "mongodb-pvc\npersistentvolumeclaim": [
            1122085995
        ],
        "mongodb-pvc\"": [
            1122085995
        ],
        "deleted\nwhat": [
            1122085995
        ],
        "again?": [
            1122085995
        ],
        "not?": [
            1122085995
        ],
        "show?\n$": [
            1122085995
        ],
        "13s\nthe": [
            1122085995
        ],
        "claim’s": [
            1122085995
        ],
        "now?\nmaybe": [
            1122085995
        ],
        "5m\nthe": [
            1122085995
        ],
        "like\nbefore": [
            1122085995
        ],
        "be\nbound": [
            1122085995
        ],
        "it\nup": [
            1122085995
        ],
        "data\nstored": [
            1122085995
        ],
        "different\nnamespace": [
            1122085995
        ],
        "tenant)\nreclaiming": [
            1122085995
        ],
        "manually\nyou": [
            1122085995
        ],
        "it—by": [
            1122085995
        ],
        "\npersistentvolumereclaimpolicy": [
            1122085995
        ],
        "wanted\nkubernetes": [
            1122085995
        ],
        "as\nfar": [
            1122085995
        ],
        "recycle": [
            1122085995
        ],
        "it\navailable": [
            1122085995
        ],
        "decision": [
            1122085995
        ],
        "can\neither": [
            1122085995
        ],
        "alone": [
            1122085995
        ],
        "pod\nreclaiming": [
            1122085995
        ],
        "automatically\ntwo": [
            1122085995
        ],
        "reclaim": [
            1122085995
        ],
        "exist:": [
            1122085995
        ],
        "deletes\nthe": [
            1122085995
        ],
        "way\nthe": [
            1122085995
        ],
        "persistentvolume-\nclaims": [
            1122085995
        ],
        "69.\n": [
            1122085995
        ],
        "\ndelete": [
            1122085995
        ],
        "\nrecycle": [
            1122085995
        ],
        "\n\n184chapter": [
            1122085995
        ],
        "containers\na": [
            1122085995
        ],
        "your\nown": [
            1122085995
        ],
        "the\nspecific": [
            1122085995
        ],
        "volume\ntipyou": [
            1122085995
        ],
        "existing\npersistentvolume": [
            1122085995
        ],
        "easily\nchange": [
            1122085995
        ],
        "\nretain": [
            1122085995
        ],
        "losing": [
            1122085995
        ],
        "valuable": [
            1122085995
        ],
        "data\n6.6dynamic": [
            1122085995
        ],
        "persistentvolumes\nyou’ve": [
            1122085995
        ],
        "easy\nto": [
            1122085995
        ],
        "pro-\nvision": [
            1122085995
        ],
        "job\nautomatically": [
            1122085995
        ],
        "persistentvolumes\n": [
            1122085995
        ],
        "provisioner": [
            1122085995
        ],
        "choose\nwhat": [
            1122085995
        ],
        "\nstorageclass": [
            1122085995
        ],
        "in\ntheir": [
            1122085995
        ],
        "when\nprovisioning": [
            1122085995
        ],
        "\nnotesimilar": [
            1122085995
        ],
        "namespaced\nkubernetes": [
            1122085995
        ],
        "provisioners": [
            1122085995
        ],
        "popular": [
            1122085995
        ],
        "admin-\nistrator": [
            1122085995
        ],
        "deployed\non-premises": [
            1122085995
        ],
        "deployed\npersistentvolume\npersistentvolumeclaim": [
            1122085995
        ],
        "1pod": [
            1122085995
        ],
        "2\npersistentvolumeclaim": [
            1122085995
        ],
        "3\npvc": [
            1122085995
        ],
        "deleted;\npv": [
            1122085995
        ],
        "automatically\nrecycled": [
            1122085995
        ],
        "ready\nto": [
            1122085995
        ],
        "and\nre-used": [
            1122085995
        ],
        "again\nuser": [
            1122085995
        ],
        "creates\npersistentvolumeclaim\npod": [
            1122085995
        ],
        "2\nunmounts\npvc\npod": [
            1122085995
        ],
        "2\nmounts\npvc\npod": [
            1122085995
        ],
        "1\nmounts\npvc\npod": [
            1122085995
        ],
        "1\nunmounts\npvc\nadmin": [
            1122085995
        ],
        "deletes\npersistentvolume\nadmin": [
            1122085995
        ],
        "creates\npersistentvolume\ntime\nfigure": [
            1122085995
        ],
        "lifespan": [
            1122085995
        ],
        "\n\n185dynamic": [
            1122085995
        ],
        "pre-provisioning": [
            1122085995
        ],
        "they\nneed": [
            1122085995
        ],
        "more)": [
            1122085995
        ],
        "storageclasses": [
            1122085995
        ],
        "new\npersistentvolume": [
            1122085995
        ],
        "the\ngreat": [
            1122085995
        ],
        "(obviously\nyou": [
            1122085995
        ],
        "space)": [
            1122085995
        ],
        "\n66.1defining": [
            1122085995
        ],
        "\nresources\nbefore": [
            1122085995
        ],
        "storageclass\nresources": [
            1122085995
        ],
        "storagek8s.io/v1\nkind:": [
            1122085995
        ],
        "storageclass\nmetadata:\n": [
            1122085995
        ],
        "fast\nprovisioner:": [
            1122085995
        ],
        "kubernetesio/gce-pd": [
            1122085995
        ],
        "\nparameters:\n": [
            1122085995
        ],
        "pd-ssd": [
            1122085995
        ],
        "zone:": [
            1122085995
        ],
        "storageclass-fast-hostpathyaml.\nthe": [
            1122085995
        ],
        "provision-\ning": [
            1122085995
        ],
        "storageclass\nthe": [
            1122085995
        ],
        "provisioner\nand": [
            1122085995
        ],
        "(pd)\nprovisioner": [
            1122085995
        ],
        "for\nother": [
            1122085995
        ],
        "used\n6.6.2requesting": [
            1122085995
        ],
        "persistentvolumeclaim\nafter": [
            1122085995
        ],
        "name\nin": [
            1122085995
        ],
        "class\nyou": [
            1122085995
        ],
        "pvc\napiversion:": [
            1122085995
        ],
        "storageclass-fast-gcepdyaml\nlisting": [
            1122085995
        ],
        "provisioning:": [
            1122085995
        ],
        "mongodb-pvc-dpyaml\nthe": [
            1122085995
        ],
        "\nuse": [
            1122085995
        ],
        "persistentvolume\nthe": [
            1122085995
        ],
        "provisioner\n": [
            1122085995
        ],
        "\n\n186chapter": [
            1122085995
        ],
        "fast": [
            1122085995
        ],
        "requests:\n": [
            1122085995
        ],
        "100mi\n": [
            1122085995
        ],
        "accessmodes:\n": [
            1122085995
        ],
        "readwriteonce\napart": [
            1122085995
        ],
        "now\nalso": [
            1122085995
        ],
        "\nfast": [
            1122085995
        ],
        "storageclass\nresource": [
            1122085995
        ],
        "non-existing": [
            1122085995
        ],
        "provisioning\nof": [
            1122085995
        ],
        "\nprovisioningfailed": [
            1122085995
        ],
        "pvc)\nexamining": [
            1122085995
        ],
        "pv\nnext": [
            1122085995
        ],
        "mongodb-pvc\nname": [
            1122085995
        ],
        "storageclass\nmongodb-pvc": [
            1122085995
        ],
        "pvc-1e6bc048": [
            1122085995
        ],
        "actual\nname": [
            1122085995
        ],
        "above)": [
            1122085995
        ],
        "automatically:\n$": [
            1122085995
        ],
        "\nmongodb-pv": [
            1122085995
        ],
        "\npvc-1e6bc048": [
            1122085995
        ],
        "fast\nnoteonly": [
            1122085995
        ],
        "pertinent": [
            1122085995
        ],
        "shown\nyou": [
            1122085995
        ],
        "means\nthe": [
            1122085995
        ],
        "beside": [
            1122085995
        ],
        "pro-\nvisioner": [
            1122085995
        ],
        "\nkubernetesio/gce-pd": [
            1122085995
        ],
        "disks\nyou": [
            1122085995
        ],
        "status\ngke-kubia-dyn-pvc-1e6bc048": [
            1122085995
        ],
        "europe-west1-d": [
            1122085995
        ],
        "ready\ngke-kubia-default-pool-71df": [
            1122085995
        ],
        "ready\ngke-kubia-default-pool-79cd": [
            1122085995
        ],
        "ready\ngke-kubia-default-pool-blc4": [
            1122085995
        ],
        "ready\nmongodb": [
            1122085995
        ],
        "class\n": [
            1122085995
        ],
        "\n\n187dynamic": [
            1122085995
        ],
        "persistentvolumes\nas": [
            1122085995
        ],
        "dynamically\nand": [
            1122085995
        ],
        "classes\nthe": [
            1122085995
        ],
        "or\nother": [
            1122085995
        ],
        "characteristics": [
            1122085995
        ],
        "decides": [
            1122085995
        ],
        "for\neach": [
            1122085995
        ],
        "by\nname": [
            1122085995
        ],
        "portability\nyourself": [
            1122085995
        ],
        "using\ngke": [
            1122085995
        ],
        "storage\nclass": [
            1122085995
        ],
        "storageclass-fast-\nhostpathyaml": [
            1122085995
        ],
        "tailor-made": [
            1122085995
        ],
        "exact\nsame": [
            1122085995
        ],
        "pvcs": [
            1122085995
        ],
        "across\ndifferent": [
            1122085995
        ],
        "clusters\n6.6.3dynamic": [
            1122085995
        ],
        "class\nas": [
            1122085995
        ],
        "progressed": [
            1122085995
        ],
        "has\nbecome": [
            1122085995
        ],
        "reflect": [
            1122085995
        ],
        "storage\nhas": [
            1122085995
        ],
        "classes\nwhen": [
            1122085995
        ],
        "now?\nhere": [
            1122085995
        ],
        "gke:\n$": [
            1122085995
        ],
        "sc\nname": [
            1122085995
        ],
        "type\nfast": [
            1122085995
        ],
        "kubernetesio/gce-pd\nstandard": [
            1122085995
        ],
        "(default)": [
            1122085995
        ],
        "kubernetesio/gce-pd\nnotewe’re": [
            1122085995
        ],
        "sc": [
            1122085995
        ],
        "storageclass\nbeside": [
            1122085995
        ],
        "class\nexists": [
            1122085995
        ],
        "compare:\n$": [
            1122085995
        ],
        "k8sio/minikube-hostpath\nstandard": [
            1122085995
        ],
        "k8sio/minikube-hostpath\nagain": [
            1122085995
        ],
        "comparing": [
            1122085995
        ],
        "\n\n188chapter": [
            1122085995
        ],
        "containers\nusing": [
            1122085995
        ],
        "k8sio/\nminikube-hostpath\n.": [
            1122085995
        ],
        "class\nyou’re": [
            1122085995
        ],
        "a\ngke": [
            1122085995
        ],
        "storageclassbeta.kubernetes.io/is-default-class:": [
            1122085995
        ],
        "2017-05-16t15:24:11z\n": [
            1122085995
        ],
        "addonmanagerkubernetes.io/mode:": [
            1122085995
        ],
        "ensureexists\n": [
            1122085995
        ],
        "kubernetesio/cluster-service:": [
            1122085995
        ],
        "true\"\n": [
            1122085995
        ],
        "standard\n": [
            1122085995
        ],
        "180\"\n": [
            1122085995
        ],
        "/apis/storagek8s.io/v1/storageclassesstandard\n": [
            1122085995
        ],
        "b6498511-3a4b-11e7-ba2c-42010a840014\nparameters:": [
            1122085995
        ],
        "\nprovisioner:": [
            1122085995
        ],
        "is\nwhat’s": [
            1122085995
        ],
        "persistentvolumeclaim\ndoesn’t": [
            1122085995
        ],
        "storageclassname": [
            1122085995
        ],
        "(on\ngoogle": [
            1122085995
        ],
        "\npd-standard": [
            1122085995
        ],
        "provi-\nsioned": [
            1122085995
        ],
        "mongodb-pvc2\nspec:": [
            1122085995
        ],
        "100mi": [
            1122085995
        ],
        "gke\nlisting": [
            1122085995
        ],
        "defined:": [
            1122085995
        ],
        "mongodb-pvc-dp-nostorageclassyaml\nthis": [
            1122085995
        ],
        "\nmarks": [
            1122085995
        ],
        "\nclass": [
            1122085995
        ],
        "default\nthe": [
            1122085995
        ],
        "parameter": [
            1122085995
        ],
        "pvs": [
            1122085995
        ],
        "\nattribute": [
            1122085995
        ],
        "\nexamples)\n": [
            1122085995
        ],
        "\n\n189dynamic": [
            1122085995
        ],
        "persistentvolumes\nthis": [
            1122085995
        ],
        "case:\n$": [
            1122085995
        ],
        "mongodb-pvc2\nname": [
            1122085995
        ],
        "storageclass\nmongodb-pvc2": [
            1122085995
        ],
        "pvc-95a5ec12": [
            1122085995
        ],
        "standard\n$": [
            1122085995
        ],
        "pvc-95a5ec12\nname": [
            1122085995
        ],
        "\npvc-95a5ec12": [
            1122085995
        ],
        "status\ngke-kubia-dyn-pvc-95a5ec12": [
            1122085995
        ],
        "ready\n..\nforcing": [
            1122085995
        ],
        "\np\nersistentvolumes\nthis": [
            1122085995
        ],
        "611\n(when": [
            1122085995
        ],
        "manually)": [
            1122085995
        ],
        "me\nrepeat": [
            1122085995
        ],
        "here:\nkind:": [
            1122085995
        ],
        "persistentvolumeclaim\nspec:\n": [
            1122085995
        ],
        "despite": [
            1122085995
        ],
        "being\nan": [
            1122085995
        ],
        "demon-\nstrate": [
            1122085995
        ],
        "didn’t\nwant": [
            1122085995
        ],
        "interfere": [
            1122085995
        ],
        "\ntipexplicitly": [
            1122085995
        ],
        "pre-\nprovisioned": [
            1122085995
        ],
        "persistentvolume\nunderstanding": [
            1122085995
        ],
        "provisioning\nthis": [
            1122085995
        ],
        "summarize": [
            1122085995
        ],
        "(with": [
            1122085995
        ],
        "\nstorage-\nclassname\n": [
            1122085995
        ],
        "necessary)": [
            1122085995
        ],
        "refers": [
            1122085995
        ],
        "610.\n": [
            1122085995
        ],
        "\npv": [
            1122085995
        ],
        "\n\n190chapter": [
            1122085995
        ],
        "containers\n67summary\nthis": [
            1122085995
        ],
        "to\ncreate": [
            1122085995
        ],
        "container\nuse": [
            1122085995
        ],
        "data\nuse": [
            1122085995
        ],
        "startup\nuse": [
            1122085995
        ],
        "node\nmount": [
            1122085995
        ],
        "restarts\ndecouple": [
            1122085995
        ],
        "persistentvolumeclaims\nhave": [
            1122085995
        ],
        "dynami-\ncally": [
            1122085995
        ],
        "persistentvolumeclaim\nprevent": [
            1122085995
        ],
        "interfering": [
            1122085995
        ],
        "persistentvolume\nin": [
            1122085995
        ],
        "deliver": [
            1122085995
        ],
        "we’ve\nmentioned": [
            1122085995
        ],
        "explored\npod\nadmin\nvolume\n1.": [
            1122085995
        ],
        "persistentvolume\nprovisioner": [
            1122085995
        ],
        "one’s": [
            1122085995
        ],
        "deployed)\n2": [
            1122085995
        ],
        "storageclasses\nand": [
            1122085995
        ],
        "marks": [
            1122085995
        ],
        "the\ndefault": [
            1122085995
        ],
        "already\nexist)\nactual\nstorage\npersistent\nvolume\nuser\npersistent\nvolume\nprovisioner\npersistent\nvolumeclaim\nstorage\nclass\n3": [
            1122085995
        ],
        "the\nstorageclasses": [
            1122085995
        ],
        "default)\n6": [
            1122085995
        ],
        "with\na": [
            1122085995
        ],
        "the\npvc": [
            1122085995
        ],
        "name\n4": [
            1122085995
        ],
        "the\nstorageclass": [
            1122085995
        ],
        "provisioner\nreferenced": [
            1122085995
        ],
        "asks": [
            1122085995
        ],
        "provisioner\nto": [
            1122085995
        ],
        "the\npvc’s": [
            1122085995
        ],
        "and\nstorage": [
            1122085995
        ],
        "parameters\nin": [
            1122085995
        ],
        "storageclass\n5": [
            1122085995
        ],
        "\n\n191\nconfigmaps": [
            1122085995
        ],
        "secrets:\nconfiguring": [
            1122085995
        ],
        "applications\nup": [
            1122085995
        ],
        "you’ve\nrun": [
            1122085995
        ],
        "exercises": [
            1122085995
        ],
        "(set-\ntings": [
            1122085995
        ],
        "differ": [
            1122085995
        ],
        "to\npass": [
            1122085995
        ],
        "kubernetes\n7.1configuring": [
            1122085995
        ],
        "applications\nbefore": [
            1122085995
        ],
        "kubernetes\nlet’s": [
            1122085995
        ],
        "configured\n": [
            1122085995
        ],
        "bake": [
            1122085995
        ],
        "application\nitself": [
            1122085995
        ],
        "covers\nchanging": [
            1122085995
        ],
        "container\npassing": [
            1122085995
        ],
        "app\nsetting": [
            1122085995
        ],
        "app\nconfiguring": [
            1122085995
        ],
        "configmaps\npassing": [
            1122085995
        ],
        "secrets\n": [
            1122085995
        ],
        "\n\n192chapter": [
            1122085995
        ],
        "7configmaps": [
            1122085995
        ],
        "applications\napp": [
            1122085995
        ],
        "configuration\noptions": [
            1122085995
        ],
        "popu-\nlar": [
            1122085995
        ],
        "having\nthe": [
            1122085995
        ],
        "a\ncertain": [
            1122085995
        ],
        "official": [
            1122085995
        ],
        "mysql": [
            1122085995
        ],
        "uses\nan": [
            1122085995
        ],
        "\nmysql_root_password": [
            1122085995
        ],
        "the\nroot": [
            1122085995
        ],
        "super-user": [
            1122085995
        ],
        "configuration\nfiles": [
            1122085995
        ],
        "tricky": [
            1122085995
        ],
        "file\ninto": [
            1122085995
        ],
        "baking": [
            1122085995
        ],
        "configuration\ninto": [
            1122085995
        ],
        "image\nevery": [
            1122085995
        ],
        "can\nsee": [
            1122085995
        ],
        "creden-\ntials": [
            1122085995
        ],
        "encryption": [
            1122085995
        ],
        "config\nnicely": [
            1122085995
        ],
        "versioned": [
            1122085995
        ],
        "rollback": [
            1122085995
        ],
        "a\nsimpler": [
            1122085995
        ],
        "kubernetes\nresource": [
            1122085995
        ],
        "repository\nor": [
            1122085995
        ],
        "file-based": [
            1122085995
        ],
        "configuration\ndata": [
            1122085995
        ],
        "chapter\n": [
            1122085995
        ],
        "can\nconfigure": [
            1122085995
        ],
        "by\npassing": [
            1122085995
        ],
        "containers\nsetting": [
            1122085995
        ],
        "container\nmounting": [
            1122085995
        ],
        "volume\nwe’ll": [
            1122085995
        ],
        "options\ndon’t": [
            1122085995
        ],
        "pri-\nvate": [
            1122085995
        ],
        "offers\nanother": [
            1122085995
        ],
        "first-class": [
            1122085995
        ],
        "chapter\n7.2passing": [
            1122085995
        ],
        "command\ndefined": [
            1122085995
        ],
        "overriding": [
            1122085995
        ],
        "as\npart": [
            1122085995
        ],
        "\n\n193passing": [
            1122085995
        ],
        "containers\ninstead": [
            1122085995
        ],
        "com-\nmand-line": [
            1122085995
        ],
        "now\n7.2.1defining": [
            1122085995
        ],
        "docker\nthe": [
            1122085995
        ],
        "parts:": [
            1122085995
        ],
        "entrypoint": [
            1122085995
        ],
        "cmd\nin": [
            1122085995
        ],
        "parts:\nentrypoint": [
            1122085995
        ],
        "started\ncmd": [
            1122085995
        ],
        "entrypoint\nalthough": [
            1122085995
        ],
        "\ncmd": [
            1122085995
        ],
        "instruction": [
            1122085995
        ],
        "execute\nwhen": [
            1122085995
        ],
        "\nentrypoint": [
            1122085995
        ],
        "instruction\nand": [
            1122085995
        ],
        "arguments\n$": [
            1122085995
        ],
        "<image>\nor": [
            1122085995
        ],
        "override": [
            1122085995
        ],
        "whatever’s": [
            1122085995
        ],
        "cmd": [
            1122085995
        ],
        "dockerfile:\n$": [
            1122085995
        ],
        "<image>": [
            1122085995
        ],
        "<arguments>\nunderstanding": [
            1122085995
        ],
        "forms\nbut": [
            1122085995
        ],
        "forms:\nshell": [
            1122085995
        ],
        "form—for": [
            1122085995
        ],
        "appjs.\nexec": [
            1122085995
        ],
        "appjs\"].\nthe": [
            1122085995
        ],
        "entry-\npoint\n": [
            1122085995
        ],
        "instruction:": [
            1122085995
        ],
        "appjs\"]\nthis": [
            1122085995
        ],
        "4675d": [
            1122085995
        ],
        "x\n": [
            1122085995
        ],
        "command\n": [
            1122085995
        ],
        "ssl": [
            1122085995
        ],
        "x\nif": [
            1122085995
        ],
        "(entrypoint": [
            1122085995
        ],
        "appjs)": [
            1122085995
        ],
        "processes:\n$": [
            1122085995
        ],
        "e4bad": [
            1122085995
        ],
        "\n\n194chapter": [
            1122085995
        ],
        "rs+": [
            1122085995
        ],
        "x\nas": [
            1122085995
        ],
        "(pid": [
            1122085995
        ],
        "1)": [
            1122085995
        ],
        "process\ninstead": [
            1122085995
        ],
        "(\npid": [
            1122085995
        ],
        "7)": [
            1122085995
        ],
        "that\nshell": [
            1122085995
        ],
        "\nshell": [
            1122085995
        ],
        "exec\nform": [
            1122085995
        ],
        "instruction\nmaking": [
            1122085995
        ],
        "interval": [
            1122085995
        ],
        "image\nlet’s": [
            1122085995
        ],
        "configu-\nrable": [
            1122085995
        ],
        "\ninterval": [
            1122085995
        ],
        "initialize": [
            1122085995
        ],
        "listing\n#!/bin/bash\ntrap": [
            1122085995
        ],
        "sigint\ninterval=$1\necho": [
            1122085995
        ],
        "generate": [
            1122085995
        ],
        "$interval": [
            1122085995
        ],
        "seconds\nmkdir": [
            1122085995
        ],
        "$interval\ndone\nyou’ve": [
            1122085995
        ],
        "so\nit": [
            1122085995
        ],
        "\nexec": [
            1122085995
        ],
        "to\n10": [
            1122085995
        ],
        "[/bin/fortuneloopsh\"]": [
            1122085995
        ],
        "[10\"]": [
            1122085995
        ],
        "\nargs": [
            1122085995
        ],
        "latest:\n$": [
            1122085995
        ],
        "dockerio/luksa/fortune:args": [
            1122085995
        ],
        "dockerio/luksa/fortune:args\nyou": [
            1122085995
        ],
        "docker:\n$": [
            1122085995
        ],
        "dockerio/luksa/fortune:args\nconfigured": [
            1122085995
        ],
        "seconds\nfri": [
            1122085995
        ],
        "10:39:44": [
            1122085995
        ],
        "/var/htdocs/indexhtml\nlisting": [
            1122085995
        ],
        "argument:": [
            1122085995
        ],
        "fortune-args/\nfortuneloopsh\nlisting": [
            1122085995
        ],
        "fortune-args/dockerfile\nthe": [
            1122085995
        ],
        "instruction\nthe": [
            1122085995
        ],
        "\n\n195passing": [
            1122085995
        ],
        "containers\nnoteyou": [
            1122085995
        ],
        "control+c\nand": [
            1122085995
        ],
        "argument:\n$": [
            1122085995
        ],
        "15\nconfigured": [
            1122085995
        ],
        "seconds\nnow": [
            1122085995
        ],
        "honors": [
            1122085995
        ],
        "use\nit": [
            1122085995
        ],
        "pod\n7.2.2overriding": [
            1122085995
        ],
        "kubernetes\nin": [
            1122085995
        ],
        "args": [
            1122085995
        ],
        "some/image\n": [
            1122085995
        ],
        "[/bin/command\"]\n": [
            1122085995
        ],
        "args:": [
            1122085995
        ],
        "[arg1\"": [
            1122085995
        ],
        "arg2\"": [
            1122085995
        ],
        "arg3\"]\nin": [
            1122085995
        ],
        "rarely": [
            1122085995
        ],
        "command\n(except": [
            1122085995
        ],
        "all)": [
            1122085995
        ],
        "71.\nrunning": [
            1122085995
        ],
        "interval\nto": [
            1122085995
        ],
        "fortune-\npodyaml": [
            1122085995
        ],
        "fortune-pod-argsyaml": [
            1122085995
        ],
        "fortune2s": [
            1122085995
        ],
        "arguments\ntable": [
            1122085995
        ],
        "kubernetes\ndockerkubernetesdescription\nentrypointcommandthe": [
            1122085995
        ],
        "container\ncmdargsthe": [
            1122085995
        ],
        "executable\nlisting": [
            1122085995
        ],
        "fortune-pod-argsyaml\nyou": [
            1122085995
        ],
        "\n\n196chapter": [
            1122085995
        ],
        "applications\nspec:\n": [
            1122085995
        ],
        "luksa/fortune:args": [
            1122085995
        ],
        "[2\"]": [
            1122085995
        ],
        "html-generator\n": [
            1122085995
        ],
        "/var/htdocs\n..\nyou": [
            1122085995
        ],
        "the\nvalues": [
            1122085995
        ],
        "it\nis": [
            1122085995
        ],
        "notation:\n": [
            1122085995
        ],
        "args:\n": [
            1122085995
        ],
        "foo\n": [
            1122085995
        ],
        "15\"\ntipyou": [
            1122085995
        ],
        "enclose": [
            1122085995
        ],
        "quotations": [
            1122085995
        ],
        "you\nmust": [
            1122085995
        ],
        "numbers)": [
            1122085995
        ],
        "command-\nline": [
            1122085995
        ],
        "it\nthrough": [
            1122085995
        ],
        "variables\n7.3setting": [
            1122085995
        ],
        "a\nsource": [
            1122085995
        ],
        "kubernetes\nallows": [
            1122085995
        ],
        "71.": [
            1122085995
        ],
        "use-\nful": [
            1122085995
        ],
        "inherited": [
            1122085995
        ],
        "its\ncontainers": [
            1122085995
        ],
        "exists\nnotelike": [
            1122085995
        ],
        "and\narguments": [
            1122085995
        ],
        "variables\nalso": [
            1122085995
        ],
        "created\nusing": [
            1122085995
        ],
        "fortune:args": [
            1122085995
        ],
        "fortune:latest\nthis": [
            1122085995
        ],
        "\nscript": [
            1122085995
        ],
        "seconds\npod\ncontainer": [
            1122085995
        ],
        "a\nenvironment": [
            1122085995
        ],
        "variables\nfoo=bar\nabc=123\ncontainer": [
            1122085995
        ],
        "b\nenvironment": [
            1122085995
        ],
        "variables\nfoo=foobar\nbar=567\nfigure": [
            1122085995
        ],
        "\n\n197setting": [
            1122085995
        ],
        "container\nmaking": [
            1122085995
        ],
        "variable\nlet’s": [
            1122085995
        ],
        "sigint\necho": [
            1122085995
        ],
        "$interval\ndone\nall": [
            1122085995
        ],
        "row": [
            1122085995
        ],
        "because\nyour": [
            1122085995
        ],
        "was\nwritten": [
            1122085995
        ],
        "\nsystemgetenv(interval\")": [
            1122085995
        ],
        "use\nprocessenv.interval": [
            1122085995
        ],
        "python": [
            1122085995
        ],
        "osenviron[interval'].\n7.3.1specifying": [
            1122085995
        ],
        "definition\nafter": [
            1122085995
        ],
        "(i’ve": [
            1122085995
        ],
        "luksa/fortune:env": [
            1122085995
        ],
        "time)": [
            1122085995
        ],
        "and\npushing": [
            1122085995
        ],
        "luksa/fortune:env\n": [
            1122085995
        ],
        "value:": [
            1122085995
        ],
        "30\"": [
            1122085995
        ],
        "html-generator\n..\nas": [
            1122085995
        ],
        "defini-\ntion": [
            1122085995
        ],
        "\nnotedon’t": [
            1122085995
        ],
        "forget": [
            1122085995
        ],
        "automatically\nexposes": [
            1122085995
        ],
        "these\nenvironment": [
            1122085995
        ],
        "auto-injected": [
            1122085995
        ],
        "configuration\nlisting": [
            1122085995
        ],
        "var:": [
            1122085995
        ],
        "fortune-env/\nfortuneloopsh\nlisting": [
            1122085995
        ],
        "fortune-pod-envyaml\nadding": [
            1122085995
        ],
        "list\n": [
            1122085995
        ],
        "\n\n198chapter": [
            1122085995
        ],
        "applications\n73.2referring": [
            1122085995
        ],
        "value\nin": [
            1122085995
        ],
        "fixed": [
            1122085995
        ],
        "\n$(var)": [
            1122085995
        ],
        "syntax": [
            1122085995
        ],
        "listing\nenv:\n-": [
            1122085995
        ],
        "first_var\n": [
            1122085995
        ],
        "foo\"\n-": [
            1122085995
        ],
        "second_var\n": [
            1122085995
        ],
        "$(first_var)bar\"\nin": [
            1122085995
        ],
        "second_var’s": [
            1122085995
        ],
        "foobar\"": [
            1122085995
        ],
        "and\nargs": [
            1122085995
        ],
        "74.5.\n7.3.3understanding": [
            1122085995
        ],
        "\nvariables\nhaving": [
            1122085995
        ],
        "hardcoded": [
            1122085995
        ],
        "have\nseparate": [
            1122085995
        ],
        "reuse\nthe": [
            1122085995
        ],
        "decouple": [
            1122085995
        ],
        "the\nconfiguration": [
            1122085995
        ],
        "configmap\nresource": [
            1122085995
        ],
        "\nvaluefrom\ninstead": [
            1122085995
        ],
        "\n74decoupling": [
            1122085995
        ],
        "configmap\nthe": [
            1122085995
        ],
        "vary\nbetween": [
            1122085995
        ],
        "source\ncode": [
            1122085995
        ],
        "microservices\narchitectures": [
            1122085995
        ],
        "compose": [
            1122085995
        ],
        "individual\ncomponents": [
            1122085995
        ],
        "configuration\nout": [
            1122085995
        ],
        "description\n7.4.1introducing": [
            1122085995
        ],
        "configmaps\nkubernetes": [
            1122085995
        ],
        "a\nconfigmap": [
            1122085995
        ],
        "key/value": [
            1122085995
        ],
        "ranging": [
            1122085995
        ],
        "from\nshort": [
            1122085995
        ],
        "literals": [
            1122085995
        ],
        "it\nexists": [
            1122085995
        ],
        "72).": [
            1122085995
        ],
        "environment\nlisting": [
            1122085995
        ],
        "\n\n199decoupling": [
            1122085995
        ],
        "configmap\nvariables": [
            1122085995
        ],
        "$(env_var)": [
            1122085995
        ],
        "syn-\ntax": [
            1122085995
        ],
        "arguments\nsure": [
            1122085995
        ],
        "this\nyou": [
            1122085995
        ],
        "separate\nstandalone": [
            1122085995
        ],
        "(development": [
            1122085995
        ],
        "produc-\ntion": [
            1122085995
        ],
        "dif-\nferent": [
            1122085995
        ],
        "73).\npod\nenvironment": [
            1122085995
        ],
        "variables\nconfigmap\nkey1=value1\nkey2=value2\n..\nconfigmap\nvolume\nfigure": [
            1122085995
        ],
        "\nconfigmap": [
            1122085995
        ],
        "volumes\nconfigmap:\napp-config\nnamespace:": [
            1122085995
        ],
        "development\n(contains\ndevelopment\nvalues)\npod(s)\nconfigmaps": [
            1122085995
        ],
        "manifests\npods": [
            1122085995
        ],
        "manifests\nnamespace:": [
            1122085995
        ],
        "production\nconfigmap:\napp-config\n(contains\nproduction\nvalues)\npod(s)\nfigure": [
            1122085995
        ],
        "\nenvironments\n": [
            1122085995
        ],
        "\n\n200chapter": [
            1122085995
        ],
        "applications\n74.2creating": [
            1122085995
        ],
        "configmap\nlet’s": [
            1122085995
        ],
        "fill": [
            1122085995
        ],
        "environment\nvariable": [
            1122085995
        ],
        "special\nkubectl": [
            1122085995
        ],
        "generic\nkubectl": [
            1122085995
        ],
        "command\nyou": [
            1122085995
        ],
        "map’s": [
            1122085995
        ],
        "literal": [
            1122085995
        ],
        "first:\n$": [
            1122085995
        ],
        "fortune-config": [
            1122085995
        ],
        "--from-literal=sleep-interval=25\nconfigmap": [
            1122085995
        ],
        "fortune-config\"": [
            1122085995
        ],
        "created\nnoteconfigmap": [
            1122085995
        ],
        "subdomain": [
            1122085995
        ],
        "(they": [
            1122085995
        ],
        "alphanumeric": [
            1122085995
        ],
        "characters": [
            1122085995
        ],
        "dots)": [
            1122085995
        ],
        "may\noptionally": [
            1122085995
        ],
        "leading": [
            1122085995
        ],
        "dot\nthis": [
            1122085995
        ],
        "\nfortune-config": [
            1122085995
        ],
        "single-entry": [
            1122085995
        ],
        "sleep-interval\n=25\n": [
            1122085995
        ],
        "(figure": [
            1122085995
        ],
        "74).\nconfigmaps": [
            1122085995
        ],
        "multi-\nple": [
            1122085995
        ],
        "\n--from-literal": [
            1122085995
        ],
        "arguments:\n$": [
            1122085995
        ],
        "myconfigmap\n➥": [
            1122085995
        ],
        "--from-literal=foo=bar": [
            1122085995
        ],
        "--from-literal=bar=baz": [
            1122085995
        ],
        "--from-literal=one=two\nlet’s": [
            1122085995
        ],
        "kubectl\nget\n": [
            1122085995
        ],
        "v1\ndata:\n": [
            1122085995
        ],
        "sleep-interval:": [
            1122085995
        ],
        "25\"": [
            1122085995
        ],
        "2016-08-11t20:31:08z\n": [
            1122085995
        ],
        "default\n": [
            1122085995
        ],
        "910025\"\n": [
            1122085995
        ],
        "/api/v1/namespaces/default/configmaps/fortune-config\n": [
            1122085995
        ],
        "88c4167e-6002-11e6-a50d-42010af00237\nlisting": [
            1122085995
        ],
        "definition\nsleep-interval25\nconfigmap:": [
            1122085995
        ],
        "fortune-config\nfigure": [
            1122085995
        ],
        "entry\nthe": [
            1122085995
        ],
        "map\nthis": [
            1122085995
        ],
        "\ndescribes": [
            1122085995
        ],
        "\n(you’re": [
            1122085995
        ],
        "name)\n": [
            1122085995
        ],
        "\n\n201decoupling": [
            1122085995
        ],
        "configmap\nnothing": [
            1122085995
        ],
        "extraordinary": [
            1122085995
        ],
        "wouldn’t\nneed": [
            1122085995
        ],
        "\nmetadata": [
            1122085995
        ],
        "course)": [
            1122085995
        ],
        "posted\nit": [
            1122085995
        ],
        "well-known\n$": [
            1122085995
        ],
        "fortune-configyaml\ncreating": [
            1122085995
        ],
        "file\nconfigmaps": [
            1122085995
        ],
        "coarse-grained": [
            1122085995
        ],
        "files\nto": [
            1122085995
        ],
        "from\ndisk": [
            1122085995
        ],
        "configmap:\n$": [
            1122085995
        ],
        "my-config": [
            1122085995
        ],
        "--from-file=config-fileconf\nwhen": [
            1122085995
        ],
        "config-fileconf": [
            1122085995
        ],
        "the\nkey": [
            1122085995
        ],
        "\nconfig-fileconf": [
            1122085995
        ],
        "filename": [
            1122085995
        ],
        "key)": [
            1122085995
        ],
        "but\nyou": [
            1122085995
        ],
        "--from-file=customkey=config-fileconf\nthis": [
            1122085995
        ],
        "file’s": [
            1122085995
        ],
        "customkey": [
            1122085995
        ],
        "literals\nyou": [
            1122085995
        ],
        "\n--from-file": [
            1122085995
        ],
        "directory\ninstead": [
            1122085995
        ],
        "importing": [
            1122085995
        ],
        "import": [
            1122085995
        ],
        "file\ndirectory:\n$": [
            1122085995
        ],
        "--from-file=/path/to/dir\nin": [
            1122085995
        ],
        "specified\ndirectory": [
            1122085995
        ],
        "\ncombining": [
            1122085995
        ],
        "options\nwhen": [
            1122085995
        ],
        "combination": [
            1122085995
        ],
        "mentioned\nhere": [
            1122085995
        ],
        "archive—you": [
            1122085995
        ],
        "command):\n$": [
            1122085995
        ],
        "--from-file=foojson": [
            1122085995
        ],
        "--from-file=bar=foobarconf": [
            1122085995
        ],
        "--from-file=config-opts/": [
            1122085995
        ],
        "--from-literal=some=thing": [
            1122085995
        ],
        "\nhere": [
            1122085995
        ],
        "sources:": [
            1122085995
        ],
        "file\nanother": [
            1122085995
        ],
        "key)\nand": [
            1122085995
        ],
        "sources": [
            1122085995,
            1118639836
        ],
        "resulting": [
            1122085995
        ],
        "configmap\na": [
            1122085995
        ],
        "file\na": [
            1122085995
        ],
        "key\na": [
            1122085995
        ],
        "directory\na": [
            1122085995
        ],
        "\n\n202chapter": [
            1122085995
        ],
        "applications\n74.3passing": [
            1122085995
        ],
        "\nvariable\nhow": [
            1122085995
        ],
        "three\noptions": [
            1122085995
        ],
        "simplest—setting": [
            1122085995
        ],
        "the\nvaluefrom": [
            1122085995
        ],
        "73.3.": [
            1122085995
        ],
        "like\nthe": [
            1122085995
        ],
        "var": [
            1122085995
        ],
        "map:": [
            1122085995
        ],
        "fortune-pod-env-configmapyaml\nconfigmap:": [
            1122085995
        ],
        "my-config\nkey\nfoojson\nfoo.json\nvalue\nbarabc\ndebugtrue\nrepeat100\nsomething\n{\nfoo:": [
            1122085995
        ],
        "bar\nbaz:": [
            1122085995
        ],
        "5\n}\nconfig-opts": [
            1122085995
        ],
        "directory\nliteral\nsome=thing\n{\nfoo:": [
            1122085995
        ],
        "5\n}\n--from-file=foojson\n--from-file=config-opts/\n--from-literal=some=thing\nfoobar.conf\nabc\ndebug\ntrue\nrepeat\n100\n--from-file=bar=foobar.conf\nfigure": [
            1122085995
        ],
        "\n\n203decoupling": [
            1122085995
        ],
        "configmap\nmetadata:\n": [
            1122085995
        ],
        "fortune-env-from-configmap\nspec:\n": [
            1122085995
        ],
        "valuefrom:": [
            1122085995
        ],
        "configmapkeyref:": [
            1122085995
        ],
        "sleep-interval": [
            1122085995
        ],
        "\n..\nyou": [
            1122085995
        ],
        "\n25": [
            1122085995
        ],
        "76).\nreferencing": [
            1122085995
        ],
        "you\ncreate": [
            1122085995
        ],
        "other\ncontainer": [
            1122085995
        ],
        "pod\nnoteyou": [
            1122085995
        ],
        "optional": [
            1122085995
        ],
        "(by": [
            1122085995
        ],
        "setting\nconfigmapkeyrefoptional:": [
            1122085995
        ],
        "true)": [
            1122085995
        ],
        "exist\nthis": [
            1122085995
        ],
        "specifica-\ntion": [
            1122085995
        ],
        "(even": [
            1122085995
        ],
        "for\nmultiple": [
            1122085995
        ],
        "pods)": [
            1122085995
        ],
        "splattered": [
            1122085995
        ],
        "dupli-\ncated": [
            1122085995
        ],
        "manifests)": [
            1122085995
        ],
        "interval\ninstead": [
            1122085995
        ],
        "youre": [
            1122085995
        ],
        "\ninitializing": [
            1122085995
        ],
        "\nyoure": [
            1122085995
        ],
        "referencing\nyoure": [
            1122085995
        ],
        "configmap\nconfigmap:": [
            1122085995
        ],
        "fortune-config\nsleep-interval\n25\npod\ncontainer:": [
            1122085995
        ],
        "web-server\ncontainer:": [
            1122085995
        ],
        "html-generator\nenvironment": [
            1122085995
        ],
        "variables\ninterval=25\nfortuneloopsh\nprocess\nfigure": [
            1122085995
        ],
        "\n\n204chapter": [
            1122085995
        ],
        "applications\n74.4passing": [
            1122085995
        ],
        "once\nwhen": [
            1122085995
        ],
        "and\nerror-prone": [
            1122085995
        ],
        "luckily\nkubernetes": [
            1122085995
        ],
        "foo-bar": [
            1122085995
        ],
        "can\nexpose": [
            1122085995
        ],
        "\nenvfrom": [
            1122085995
        ],
        "of\nenv": [
            1122085995
        ],
        "example\nspec:\n": [
            1122085995
        ],
        "some-image\n": [
            1122085995
        ],
        "envfrom:": [
            1122085995
        ],
        "prefix:": [
            1122085995
        ],
        "config_": [
            1122085995
        ],
        "configmapref:": [
            1122085995
        ],
        "my-config-map": [
            1122085995
        ],
        "\n..\nas": [
            1122085995
        ],
        "(config_": [
            1122085995
        ],
        "in\nthis": [
            1122085995
        ],
        "present": [
            1122085995
        ],
        "\nconfig_foo": [
            1122085995
        ],
        "config_bar": [
            1122085995
        ],
        "\ndid": [
            1122085995
        ],
        "entries\n(\nfoo": [
            1122085995
        ],
        "foo-bar)?": [
            1122085995
        ],
        "foo-bar\nconfigmap": [
            1122085995
        ],
        "entry?\n": [
            1122085995
        ],
        "\nconfig_foo-bar": [
            1122085995
        ],
        "name\nbecause": [
            1122085995
        ],
        "convert": [
            1122085995
        ],
        "doesn’t\nconvert": [
            1122085995
        ],
        "proper\nformat": [
            1122085995
        ],
        "skips": [
            1122085995
        ],
        "informing": [
            1122085995
        ],
        "skipped": [
            1122085995
        ],
        "it)\n7.4.5passing": [
            1122085995
        ],
        "argument\nnow": [
            1122085995
        ],
        "main\nprocess": [
            1122085995
        ],
        "\npodspec.containers.args": [
            1122085995
        ],
        "vari-\nable": [
            1122085995
        ],
        "77.\n": [
            1122085995
        ],
        "yaml\n": [
            1122085995
        ],
        "vars": [
            1122085995
        ],
        "configmap\nusing": [
            1122085995
        ],
        "envfrom": [
            1122085995
        ],
        "env\nall": [
            1122085995
        ],
        "prefixed": [
            1122085995
        ],
        "config_\nreferencing": [
            1122085995
        ],
        "my-config-map\n": [
            1122085995
        ],
        "\n\n205decoupling": [
            1122085995
        ],
        "configmap\napiversion:": [
            1122085995
        ],
        "fortune-args-from-configmap\nspec:\n": [
            1122085995
        ],
        "[$(interval)\"]": [
            1122085995
        ],
        "the\n$(env_variable_name)": [
            1122085995
        ],
        "inject": [
            1122085995
        ],
        "\n74.6using": [
            1122085995
        ],
        "files\npassing": [
            1122085995
        ],
        "arguments\nis": [
            1122085995
        ],
        "use\none": [
            1122085995
        ],
        "entry’s": [
            1122085995
        ],
        "arguments:": [
            1122085995
        ],
        "fortune-pod-args-configmapyaml\nconfigmap:": [
            1122085995
        ],
        "variables\ninterval=25\nfortuneloopsh": [
            1122085995
        ],
        "$(interval)\nfigure": [
            1122085995
        ],
        "argument\nusing": [
            1122085995
        ],
        "\nnot": [
            1122085995
        ],
        "variable\ndefining": [
            1122085995
        ],
        "\nexactly": [
            1122085995
        ],
        "before\nreferencing": [
            1122085995
        ],
        "argument\n": [
            1122085995
        ],
        "\n\n206chapter": [
            1122085995
        ],
        "configmap\ninstead": [
            1122085995
        ],
        "different\nexample": [
            1122085995
        ],
        "the\nfortune": [
            1122085995
        ],
        "compress\nresponses": [
            1122085995
        ],
        "compression": [
            1122085995
        ],
        "nginx\nneeds": [
            1122085995
        ],
        "listing\nserver": [
            1122085995
        ],
        "80;\n": [
            1122085995
        ],
        "server_name": [
            1122085995
        ],
        "wwwkubia-example.com;\n": [
            1122085995
        ],
        "on;": [
            1122085995
        ],
        "gzip_types": [
            1122085995
        ],
        "text/plain": [
            1122085995
        ],
        "application/xml;": [
            1122085995
        ],
        "/usr/share/nginx/html;\n": [
            1122085995
        ],
        "index": [
            1122085995
        ],
        "indexhtm;\n": [
            1122085995
        ],
        "}\n}\nnow": [
            1122085995
        ],
        "config-\nmap\n": [
            1122085995
        ],
        "the\nnginx": [
            1122085995
        ],
        "configmap-files": [
            1122085995
        ],
        "the\nprevious": [
            1122085995
        ],
        "configmap-files/my-nginx-configconf.": [
            1122085995
        ],
        "configmap\nalso": [
            1122085995
        ],
        "\nsleep-interval": [
            1122085995
        ],
        "plain": [
            1122085995
        ],
        "78).\nnow": [
            1122085995
        ],
        "--from-file=configmap-files\nconfigmap": [
            1122085995
        ],
        "created\nlisting": [
            1122085995
        ],
        "compression:": [
            1122085995
        ],
        "my-nginx-configconf\nthis": [
            1122085995
        ],
        "files\nconfigmap-files/\nmy-nginx-config.conf\nserver": [
            1122085995
        ],
        "{\nlisten": [
            1122085995
        ],
        "80;\nserver_name": [
            1122085995
        ],
        "wwwkubia...\n...\n}\nsleep-interval\n25\nfigure": [
            1122085995
        ],
        "\nconfigmap-files": [
            1122085995
        ],
        "files\n": [
            1122085995
        ],
        "\n\n207decoupling": [
            1122085995
        ],
        "like\n$": [
            1122085995
        ],
        "my-nginx-configconf:": [
            1122085995
        ],
        "{": [
            1122085995
        ],
        "80;": [
            1122085995
        ],
        "wwwkubia-example.com;": [
            1122085995
        ],
        "/usr/share/nginx/html;": [
            1122085995
        ],
        "indexhtm;": [
            1122085995
        ],
        "}": [
            1122085995
        ],
        "configmap\n..\nnotethe": [
            1122085995
        ],
        "pipeline": [
            1122085995
        ],
        "character": [
            1122085995
        ],
        "colon": [
            1122085995
        ],
        "entries\nsignals": [
            1122085995
        ],
        "multi-line": [
            1122085995
        ],
        "follows\nthe": [
            1122085995
        ],
        "names\nof": [
            1122085995
        ],
        "your\npod’s": [
            1122085995
        ],
        "volume\ncreating": [
            1122085995
        ],
        "creating\na": [
            1122085995
        ],
        "only\nthing": [
            1122085995
        ],
        "config-\nmap’s": [
            1122085995
        ],
        "entries\n": [
            1122085995
        ],
        "/etc/nginx/nginxconf.": [
            1122085995
        ],
        "image\nalready": [
            1122085995
        ],
        "default\nconfig": [
            1122085995
        ],
        "conf": [
            1122085995
        ],
        "/etc/nginx/confd/": [
            1122085995
        ],
        "subdirec-\ntory": [
            1122085995
        ],
        "achieve\n": [
            1122085995
        ],
        "irrelevant": [
            1122085995
        ],
        "but\nyou’ll": [
            1122085995
        ],
        "archive)\n": [
            1122085995
        ],
        "file\nthe": [
            1122085995
        ],
        "\nnginx": [
            1122085995
        ],
        "\ncontents\nthe": [
            1122085995
        ],
        "entry\n": [
            1122085995
        ],
        "\n\n208chapter": [
            1122085995
        ],
        "applications\napiversion:": [
            1122085995
        ],
        "fortune-configmap-volume\nspec:\n": [
            1122085995
        ],
        "config\n": [
            1122085995
        ],
        "/etc/nginx/confd": [
            1122085995
        ],
        "configmap:": [
            1122085995
        ],
        "fortune-config\nconfigmap": [
            1122085995
        ],
        "make\nnginx": [
            1122085995
        ],
        "\nverifying": [
            1122085995
        ],
        "compress": [
            1122085995
        ],
        "responses": [
            1122085995
        ],
        "can\nverify": [
            1122085995
        ],
        "port-forwarding": [
            1122085995
        ],
        "localhost:8080": [
            1122085995
        ],
        "and\nchecking": [
            1122085995
        ],
        "files:": [
            1122085995
        ],
        "fortune-pod-configmap-\nvolumeyaml\npod\ncontainer:": [
            1122085995
        ],
        "html-generator\ncontainer:": [
            1122085995
        ],
        "web-server\nfilesystem\n/\netc/\nnginx/\nconfd/\nconfigmap:": [
            1122085995
        ],
        "fortune-config\nmy-nginx-configconf\nserver": [
            1122085995
        ],
        "{\n..\n}\nvolume:\nconfig\nfigure": [
            1122085995
        ],
        "volume\nyou’re": [
            1122085995
        ],
        "location\nthe": [
            1122085995
        ],
        "configmap\n": [
            1122085995
        ],
        "\n\n209decoupling": [
            1122085995
        ],
        "configmap\n$": [
            1122085995
        ],
        "fortune-configmap-volume": [
            1122085995
        ],
        "8080:80": [
            1122085995
        ],
        "&\nforwarding": [
            1122085995
        ],
        "80\n$": [
            1122085995
        ],
        "-h": [
            1122085995
        ],
        "accept-encoding:": [
            1122085995
        ],
        "-i": [
            1122085995
        ],
        "localhost:8080\nhttp/11": [
            1122085995
        ],
        "ok\nserver:": [
            1122085995
        ],
        "nginx/111.1\ndate:": [
            1122085995
        ],
        "thu": [
            1122085995
        ],
        "aug": [
            1122085995
        ],
        "11:52:57": [
            1122085995
        ],
        "gmt\ncontent-type:": [
            1122085995
        ],
        "text/html\nlast-modified:": [
            1122085995
        ],
        "11:52:55": [
            1122085995
        ],
        "gmt\nconnection:": [
            1122085995
        ],
        "keep-alive\netag:": [
            1122085995
        ],
        "w/57b5a197-37\"\ncontent-encoding:": [
            1122085995
        ],
        "contents\nthe": [
            1122085995
        ],
        "the\n/etc/nginx/confd": [
            1122085995
        ],
        "/etc/nginx/confd\nmy-nginx-config.conf\nsleep-interval\nboth": [
            1122085995
        ],
        "the\nsleep-interval": [
            1122085995
        ],
        "there\nbecause": [
            1122085995
        ],
        "\nfortuneloop": [
            1122085995
        ],
        "create\ntwo": [
            1122085995
        ],
        "somehow": [
            1122085995
        ],
        "having\ncontainers": [
            1122085995
        ],
        "implies": [
            1122085995
        ],
        "should\nprobably": [
            1122085995
        ],
        "volume\nluckily": [
            1122085995
        ],
        "configmap’s\nentries—in": [
            1122085995
        ],
        "\nmy-nginx-configconf": [
            1122085995
        ],
        "the\nfortuneloop": [
            1122085995
        ],
        "through\nan": [
            1122085995
        ],
        "the\nvolume’s": [
            1122085995
        ],
        "\nitems": [
            1122085995
        ],
        "items:": [
            1122085995
        ],
        "my-nginx-configconf": [
            1122085995
        ],
        "gzipconf": [
            1122085995
        ],
        "enabled\nlisting": [
            1122085995
        ],
        "\nfortune-pod-configmap-volume-with-itemsyaml\nthis": [
            1122085995
        ],
        "compressed\nselecting": [
            1122085995
        ],
        "them\nyou": [
            1122085995
        ],
        "\nunder": [
            1122085995
        ],
        "included\nthe": [
            1122085995
        ],
        "file\n": [
            1122085995
        ],
        "\n\n210chapter": [
            1122085995
        ],
        "applications\nwhen": [
            1122085995
        ],
        "individual\nentry": [
            1122085995
        ],
        "the\ngzipconf": [
            1122085995
        ],
        "directory\nthere’s": [
            1122085995
        ],
        "previous\nexample": [
            1122085995
        ],
        "non-\nempty": [
            1122085995
        ],
        "filesys-\ntem": [
            1122085995
        ],
        "the\nfilesystem": [
            1122085995
        ],
        "terrible": [
            1122085995
        ],
        "effects": [
            1122085995
        ],
        "/etc": [
            1122085995
        ],
        "likely\nbreak": [
            1122085995
        ],
        "/etc\ndirectory": [
            1122085995
        ],
        "/etc\nyou": [
            1122085995
        ],
        "all\nmounting": [
            1122085995
        ],
        "directory\nnaturally": [
            1122085995
        ],
        "into\nan": [
            1122085995
        ],
        "\nsubpath\nproperty": [
            1122085995
        ],
        "volumemount": [
            1122085995
        ],
        "perhaps": [
            1122085995
        ],
        "to\nexplain": [
            1122085995
        ],
        "visually": [
            1122085995
        ],
        "710).\n": [
            1122085995
        ],
        "myconfigconf": [
            1122085995
        ],
        "someconfigconf.": [
            1122085995
        ],
        "\nsubpath": [
            1122085995
        ],
        "listing\npod\ncontainer\nfilesystem\n/\netc/\nsomeconfig.conf\nexistingfile1\nexistingfile2\nconfigmap:": [
            1122085995
        ],
        "app-config\nmyconfigconf\ncontents\nof": [
            1122085995
        ],
        "file\nanother-filecontents\nof": [
            1122085995
        ],
        "file\nconfigmap\nvolume\nmyconfigconf\nanother-file\nexistingfile1\nand": [
            1122085995
        ],
        "existingfile2\naren’t": [
            1122085995
        ],
        "hidden\nonly": [
            1122085995
        ],
        "mounted\ninto": [
            1122085995
        ],
        "(yet": [
            1122085995
        ],
        "a\ndifferent": [
            1122085995
        ],
        "filename)\nanother-file": [
            1122085995
        ],
        "isn’t\nmounted": [
            1122085995
        ],
        "the\ncontainer\nfigure": [
            1122085995
        ],
        "\n\n211decoupling": [
            1122085995
        ],
        "configmap\nspec:\n": [
            1122085995
        ],
        "myvolume\n": [
            1122085995
        ],
        "/etc/someconfigconf": [
            1122085995
        ],
        "subpath:": [
            1122085995
        ],
        "subpath": [
            1122085995
        ],
        "of\nmounting": [
            1122085995
        ],
        "mounting\nindividual": [
            1122085995
        ],
        "deficiency": [
            1122085995
        ],
        "learn\nmore": [
            1122085995
        ],
        "initial\nstate": [
            1122085995
        ],
        "saying": [
            1122085995
        ],
        "permissions\nsetting": [
            1122085995
        ],
        "volume\nby": [
            1122085995
        ],
        "(-rw-r—r--)\nyou": [
            1122085995
        ],
        "\ndefaultmode": [
            1122085995
        ],
        "configmap:\n": [
            1122085995
        ],
        "fortune-config\n": [
            1122085995
        ],
        "defaultmode:": [
            1122085995
        ],
        "6600\"": [
            1122085995
        ],
        "\nalthough": [
            1122085995
        ],
        "may\nwant": [
            1122085995
        ],
        "readable": [
            1122085995
        ],
        "is\nowned": [
            1122085995
        ],
        "\n74.7updating": [
            1122085995
        ],
        "app\nwe’ve": [
            1122085995
        ],
        "command-line\narguments": [
            1122085995
        ],
        "inability": [
            1122085995
        ],
        "restart\nthe": [
            1122085995
        ],
        "reload\nthem": [
            1122085995
        ],
        "files\nwarningbe": [
            1122085995
        ],
        "surprisingly": [
            1122085995
        ],
        "time\nfor": [
            1122085995
        ],
        "minute)\nlisting": [
            1122085995
        ],
        "permissions:": [
            1122085995
        ],
        "fortune-pod-configmap-volume-defaultmodeyaml": [
            1122085995
        ],
        "entry\nthis": [
            1122085995
        ],
        "-rw-rw------\n": [
            1122085995
        ],
        "\n\n212chapter": [
            1122085995
        ],
        "applications\nediting": [
            1122085995
        ],
        "pod\nreload": [
            1122085995
        ],
        "file\nfrom": [
            1122085995
        ],
        "restarting\nthe": [
            1122085995
        ],
        "switching": [
            1122085995
        ],
        "config-\nmap": [
            1122085995
        ],
        "edit:\n$": [
            1122085995
        ],
        "fortune-config\nonce": [
            1122085995
        ],
        "then\nclose": [
            1122085995
        ],
        "file\nin": [
            1122085995
        ],
        "printing": [
            1122085995
        ],
        "web-server\n➥": [
            1122085995
        ],
        "/etc/nginx/confd/my-nginx-config.conf\nif": [
            1122085995
        ],
        "the\nfiles": [
            1122085995
        ],
        "you’ll\nfind": [
            1122085995
        ],
        "reload": [
            1122085995
        ],
        "them\nautomatically": [
            1122085995
        ],
        "\nsignaling": [
            1122085995
        ],
        "config\nnginx": [
            1122085995
        ],
        "files\nwhich": [
            1122085995
        ],
        "reload\nnow": [
            1122085995
        ],
        "compressed": [
            1122085995
        ],
        "\ncontent-encoding:": [
            1122085995
        ],
        "header)\nyou’ve": [
            1122085995
        ],
        "or\nrecreate": [
            1122085995
        ],
        "atomically\nyou": [
            1122085995
        ],
        "and\nreloads": [
            1122085995
        ],
        "\nconfigmap\nvolume": [
            1122085995
        ],
        "atomically": [
            1122085995
        ],
        "which\nmeans": [
            1122085995
        ],
        "symbolic": [
            1122085995
        ],
        "links": [
            1122085995
        ],
        "-la": [
            1122085995
        ],
        "/etc/nginx/confd\ntotal": [
            1122085995
        ],
        "4\ndrwxr-xr-x": [
            1122085995
        ],
        "12:15": [
            1122085995
        ],
        ".4984_09_04_12_15_06.865837643\nlisting": [
            1122085995
        ],
        "\n\n213using": [
            1122085995
        ],
        "containers\nlrwxrwxrwx": [
            1122085995
        ],
        ".data": [
            1122085995
        ],
        ".4984_09_04_12_15_06.865837643\nlrwxrwxrwx": [
            1122085995
        ],
        ".data/my-nginx-config.conf\nlrwxrwxrwx": [
            1122085995
        ],
        ".data/sleep-interval\nas": [
            1122085995
        ],
        "\n.data": [
            1122085995
        ],
        "dir": [
            1122085995
        ],
        "\n.4984_09_04_something.": [
            1122085995
        ],
        "kubernetes\ncreates": [
            1122085995
        ],
        "re-links": [
            1122085995
        ],
        "\n.data\nsymbolic": [
            1122085995
        ],
        "once\nunderstanding": [
            1122085995
        ],
        "updated\none": [
            1122085995
        ],
        "relates": [
            1122085995
        ],
        "configmap-backed": [
            1122085995
        ],
        "updated!\nat": [
            1122085995
        ],
        "update\nits": [
            1122085995
        ],
        "workaround": [
            1122085995
        ],
        "different\ndirectory": [
            1122085995
        ],
        "sym-\nlink": [
            1122085995
        ],
        "the\nsymlink": [
            1122085995
        ],
        "starts\nunderstanding": [
            1122085995
        ],
        "consequences": [
            1122085995
        ],
        "configmap\none": [
            1122085995
        ],
        "immutability": [
            1122085995
        ],
        "allows\nus": [
            1122085995
        ],
        "reloading": [
            1122085995
        ],
        "configura-\ntion": [
            1122085995
        ],
        "differently—those\npods": [
            1122085995
        ],
        "whereas\nthe": [
            1122085995
        ],
        "reason)": [
            1122085995
        ],
        "config\ntherefore": [
            1122085995
        ],
        "existing\nconfigmap": [
            1122085995
        ],
        "it)": [
            1122085995
        ],
        "a\nbig": [
            1122085995
        ],
        "volumes\naren’t": [
            1122085995
        ],
        "synchronously": [
            1122085995
        ],
        "pods\nmay": [
            1122085995
        ],
        "minute\n7.5using": [
            1122085995
        ],
        "containers\nall": [
            1122085995
        ],
        "non-sensitive\nconfiguration": [
            1122085995
        ],
        "the\nstart": [
            1122085995
        ],
        "cre-\ndentials": [
            1122085995
        ],
        "secure\n": [
            1122085995
        ],
        "\n\n214chapter": [
            1122085995
        ],
        "applications\n75.1introducing": [
            1122085995
        ],
        "secrets\nto": [
            1122085995
        ],
        "distribute": [
            1122085995
        ],
        "configmaps—they’re": [
            1122085995
        ],
        "key-value\npairs": [
            1122085995
        ],
        "can\npass": [
            1122085995
        ],
        "variables\nexpose": [
            1122085995
        ],
        "volume\nkubernetes": [
            1122085995
        ],
        "safe": [
            1122085995
        ],
        "distributed\nto": [
            1122085995
        ],
        "nodes\nthemselves": [
            1122085995
        ],
        "storage\nwhich": [
            1122085995
        ],
        "wiping": [
            1122085995
        ],
        "etcd)": [
            1122085995
        ],
        "in\nunencrypted": [
            1122085995
        ],
        "secured": [
            1122085995
        ],
        "sensi-\ntive": [
            1122085995
        ],
        "storage\nsecure": [
            1122085995
        ],
        "unauthorized": [
            1122085995
        ],
        "any-\none": [
            1122085995
        ],
        "sen-\nsitive": [
            1122085995
        ],
        "encrypted\nform": [
            1122085995
        ],
        "imperative": [
            1122085995
        ],
        "prop-\nerly": [
            1122085995
        ],
        "choosing": [
            1122085995
        ],
        "simple:\nuse": [
            1122085995
        ],
        "kept\nunder": [
            1122085995
        ],
        "not-sensitive": [
            1122085995
        ],
        "secret\nyou": [
            1122085995
        ],
        "detail\n7.5.2introducing": [
            1122085995
        ],
        "secret\nyou’ll": [
            1122085995
        ],
        "every\ncontainer": [
            1122085995
        ],
        "contained": [
            1122085995
        ],
        "default-token-cfee9:\n": [
            1122085995
        ],
        "secret)\n": [
            1122085995
        ],
        "default-token-cfee9\nevery": [
            1122085995
        ],
        "default-token-cfee9": [
            1122085995
        ],
        "because\nsecrets": [
            1122085995
        ],
        "the\ndefault-token": [
            1122085995
        ],
        "secrets\nname": [
            1122085995
        ],
        "age\ndefault-token-cfee9": [
            1122085995
        ],
        "kubernetesio/service-account-token": [
            1122085995
        ],
        "39d\n": [
            1122085995
        ],
        "\n\n215using": [
            1122085995
        ],
        "containers\nyou": [
            1122085995
        ],
        "secrets\nname:": [
            1122085995
        ],
        "default-token-cfee9\nnamespace:": [
            1122085995
        ],
        "<none>\nannotations:": [
            1122085995
        ],
        "kubernetesio/service-account.name=default\n": [
            1122085995
        ],
        "kubernetesio/service-account.uid=cc04bb39-b53f-42010af00237\ntype:": [
            1122085995
        ],
        "kubernetesio/service-account-token\ndata\n====\nca.crt:": [
            1122085995
        ],
        "bytes": [
            1122085995
        ],
        "\nnamespace:": [
            1122085995
        ],
        "\ntoken:": [
            1122085995
        ],
        "eyjhbgcioijsuzi1niisinr5cci6ikpxvcj9..": [
            1122085995
        ],
        "entries—cacrt": [
            1122085995
        ],
        "token—\nwhich": [
            1122085995
        ],
        "securely": [
            1122085995
        ],
        "server\nfrom": [
            1122085995
        ],
        "\nsecret\nvolume": [
            1122085995
        ],
        "mounted:\nmounts:\n": [
            1122085995
        ],
        "/var/run/secrets/kubernetesio/serviceaccount": [
            1122085995
        ],
        "default-token-cfee9\nnoteby": [
            1122085995
        ],
        "default-token": [
            1122085995
        ],
        "disable": [
            1122085995
        ],
        "\nautomountservice-\naccounttoken\n": [
            1122085995
        ],
        "false": [
            1122085995
        ],
        "accounts": [
            1122085995
        ],
        "book)\nto": [
            1122085995
        ],
        "fig-\nure": [
            1122085995
        ],
        "711.\n": [
            1122085995
        ],
        "three\nentries": [
            1122085995
        ],
        "/var/run/secrets/kubernetesio/serviceaccount/\nca.crt\nnamespace\ntoken\nyou’ll": [
            1122085995
        ],
        "chapter\nlisting": [
            1122085995
        ],
        "secret\nthis": [
            1122085995
        ],
        "\ncontains": [
            1122085995
        ],
        "\nentries\n": [
            1122085995
        ],
        "\n\n216chapter": [
            1122085995
        ],
        "applications\n75.3creating": [
            1122085995
        ],
        "secret\nnow": [
            1122085995
        ],
        "fortune-serving": [
            1122085995
        ],
        "nginx\ncontainer": [
            1122085995
        ],
        "a\ncertificate": [
            1122085995
        ],
        "it\nand": [
            1122085995
        ],
        "secret\n": [
            1122085995
        ],
        "(do": [
            1122085995
        ],
        "machine)\nyou": [
            1122085995
        ],
        "cert": [
            1122085995
        ],
        "the\nfortune-https": [
            1122085995
        ],
        "directory):\n$": [
            1122085995
        ],
        "httpskey": [
            1122085995
        ],
        "httpscert": [
            1122085995
        ],
        "\n/cn=wwwkubia-example.com\nnow": [
            1122085995
        ],
        "additional\ndummy": [
            1122085995
        ],
        "\nbar": [
            1122085995
        ],
        "two:\n$": [
            1122085995
        ],
        "foo\nnow": [
            1122085995
        ],
        "files:\n$": [
            1122085995
        ],
        "fortune-https": [
            1122085995
        ],
        "--from-file=httpskey\n➥": [
            1122085995
        ],
        "--from-file=httpscert": [
            1122085995
        ],
        "--from-file=foo\nsecret": [
            1122085995
        ],
        "fortune-https\"": [
            1122085995
        ],
        "created\nthis": [
            1122085995
        ],
        "a\ngeneric": [
            1122085995
        ],
        "(httpskey": [
            1122085995
        ],
        "key/file)": [
            1122085995
        ],
        "you\nlearned": [
            1122085995
        ],
        "\n--from-file=fortune-\nhttps\n": [
            1122085995
        ],
        "individually\npod\ncontainer\nfilesystem\n/\nvar/\nrun/\nsecrets/\nkubernetes.io/\nserviceaccount/\ndefault": [
            1122085995
        ],
        "secret\ndefault": [
            1122085995
        ],
        "token\nsecret\nvolume\ncacrt...\n...\n...\nnamespace\ntoken\nfigure": [
            1122085995
        ],
        "\n\n217using": [
            1122085995
        ],
        "containers\nnoteyou’re": [
            1122085995
        ],
        "tls\nsecret": [
            1122085995
        ],
        "5\nthis": [
            1122085995
        ],
        "though\n7.5.4comparing": [
            1122085995
        ],
        "secrets\nsecrets": [
            1122085995
        ],
        "drove": [
            1122085995
        ],
        "kubernetes\ndevelopers": [
            1122085995
        ],
        "a\nwhile": [
            1122085995
        ],
        "created\n$": [
            1122085995
        ],
        "foo:": [
            1122085995
        ],
        "ymfycg==\n": [
            1122085995
        ],
        "httpscert:": [
            1122085995
        ],
        "ls0tls1crudjtibdrvjusuzjq0furs0tls0tck1jsurcekndq..\n": [
            1122085995
        ],
        "httpskey:": [
            1122085995
        ],
        "ls0tls1crudjtibsu0egufjjvkfursblrvktls0tlqpnsulfce..\nkind:": [
            1122085995
        ],
        "secret\n..\nnow": [
            1122085995
        ],
        "}\n": [
            1122085995
        ],
        "25\nkind:": [
            1122085995
        ],
        "configmap\n..\nnotice": [
            1122085995
        ],
        "difference?": [
            1122085995
        ],
        "secret’s": [
            1122085995
        ],
        "base64-encoded\nstrings": [
            1122085995
        ],
        "made\nworking": [
            1122085995
        ],
        "painful": [
            1122085995
        ],
        "you\nhad": [
            1122085995
        ],
        "encode": [
            1122085995
        ],
        "decode": [
            1122085995
        ],
        "data\nthe": [
            1122085995
        ],
        "base64": [
            1122085995
        ],
        "encoding": [
            1122085995
        ],
        "binary\nvalues": [
            1122085995
        ],
        "plain-text": [
            1122085995
        ],
        "in\nyaml": [
            1122085995
        ],
        "maximum": [
            1122085995
        ],
        "1mb\nlisting": [
            1122085995
        ],
        "configmap’s": [
            1122085995
        ],
        "\n\n218chapter": [
            1122085995
        ],
        "applications\nintroducing": [
            1122085995
        ],
        "stringdata": [
            1122085995
        ],
        "field\nbecause": [
            1122085995
        ],
        "secret’s\nvalues": [
            1122085995
        ],
        "\nstringdata": [
            1122085995
        ],
        "used\nkind:": [
            1122085995
        ],
        "secret\napiversion:": [
            1122085995
        ],
        "v1\nstringdata:": [
            1122085995
        ],
        "\ndata:\n": [
            1122085995
        ],
        "ls0tls1crudjtibsu0egufjjvkfursblrvktls0tlqpnsulfce..\nthe": [
            1122085995
        ],
        "write-only": [
            1122085995
        ],
        "(note:": [
            1122085995
        ],
        "read-only)": [
            1122085995
        ],
        "the\nstringdata": [
            1122085995
        ],
        "string-\ndata\n": [
            1122085995
        ],
        "data\nand": [
            1122085995
        ],
        "base64-encoded": [
            1122085995
        ],
        "\nreading": [
            1122085995
        ],
        "decoded": [
            1122085995
        ],
        "(regardless": [
            1122085995
        ],
        "plain\ntext": [
            1122085995
        ],
        "binary)": [
            1122085995
        ],
        "the\nfile’s": [
            1122085995
        ],
        "directly\n7.5.5using": [
            1122085995
        ],
        "\nmodifying": [
            1122085995
        ],
        "https\nfor": [
            1122085995
        ],
        "fortune-config\nafter": [
            1122085995
        ],
        "my-nginx-\nconfigconf\n": [
            1122085995
        ],
        "listing\n...\ndata:\n": [
            1122085995
        ],
        "ssl;\n": [
            1122085995
        ],
        "wwwkubia-example.com;\nlisting": [
            1122085995
        ],
        "field\nlisting": [
            1122085995
        ],
        "non-binary": [
            1122085995
        ],
        "data\nsee": [
            1122085995
        ],
        "“plain": [
            1122085995
        ],
        "text”": [
            1122085995
        ],
        "base64-encoded\n": [
            1122085995
        ],
        "\n\n219using": [
            1122085995
        ],
        "ssl_certificate": [
            1122085995
        ],
        "certs/httpscert;": [
            1122085995
        ],
        "ssl_certificate_key": [
            1122085995
        ],
        "certs/httpskey;": [
            1122085995
        ],
        "ssl_protocols": [
            1122085995
        ],
        "tlsv1": [
            1122085995
        ],
        "tlsv11": [
            1122085995
        ],
        "tlsv12;\n": [
            1122085995
        ],
        "ssl_ciphers": [
            1122085995
        ],
        "high:!anull:!md5;\n": [
            1122085995
        ],
        "|\n..\nthis": [
            1122085995
        ],
        "configures": [
            1122085995
        ],
        "/etc/nginx/certs\nso": [
            1122085995
        ],
        "\nmounting": [
            1122085995
        ],
        "holding\nthe": [
            1122085995
        ],
        "fortune-https\nspec:\n": [
            1122085995
        ],
        "env:\n": [
            1122085995
        ],
        "interval\n": [
            1122085995
        ],
        "configmapkeyref:\n": [
            1122085995
        ],
        "sleep-interval\n": [
            1122085995
        ],
        "/var/htdocs\n": [
            1122085995
        ],
        "/etc/nginx/confd\n": [
            1122085995
        ],
        "certs": [
            1122085995
        ],
        "/etc/nginx/certs/": [
            1122085995
        ],
        "80\nlisting": [
            1122085995
        ],
        "fortune-pod-httpsyaml\nthe": [
            1122085995
        ],
        "\nrelative": [
            1122085995
        ],
        "/etc/nginx\nyou": [
            1122085995
        ],
        "/etc/nginx/certs": [
            1122085995
        ],
        "\n\n220chapter": [
            1122085995
        ],
        "443\n": [
            1122085995
        ],
        "{}\n": [
            1122085995
        ],
        "items:\n": [
            1122085995
        ],
        "my-nginx-configconf\n": [
            1122085995
        ],
        "httpsconf\n": [
            1122085995
        ],
        "\nmuch": [
            1122085995
        ],
        "712\nshows": [
            1122085995
        ],
        "\ndefault-token": [
            1122085995
        ],
        "and\nvolume": [
            1122085995
        ],
        "figure\nnotelike": [
            1122085995
        ],
        "file\npermissions": [
            1122085995
        ],
        "\ndefaultmode\nproperty\nyou": [
            1122085995
        ],
        "secret\ncontainer:": [
            1122085995
        ],
        "html-generator\nsecret:": [
            1122085995
        ],
        "fortune-https\ndefault": [
            1122085995
        ],
        "shown\nsecret\nvolume:\ncerts\nemptydir\nvolume:\nhtml\nconfigmap\nvolume:\nconfig\nhttpscert...\n...\n...\nhttps.key\nfoo\n/etc/nginx/conf.d/\n/etc/nginx/certs/\n/usr/share/nginx/html/\n/var/htdocs\nconfigmap:": [
            1122085995
        ],
        "{\n..\n}\npod\nenvironment": [
            1122085995
        ],
        "variables:\ninterval=25\nsleep-interval25\nfigure": [
            1122085995
        ],
        "\n\n221using": [
            1122085995
        ],
        "containers\ntesting": [
            1122085995
        ],
        "secret\nonce": [
            1122085995
        ],
        "port-\nforward": [
            1122085995
        ],
        "tunnel": [
            1122085995
        ],
        "server\nwith": [
            1122085995
        ],
        "\ncurl:": [
            1122085995
        ],
        "8443:443": [
            1122085995
        ],
        "1270.0.1:8443": [
            1122085995
        ],
        "443\nforwarding": [
            1122085995
        ],
        "[::1]:8443": [
            1122085995
        ],
        "443\n$": [
            1122085995
        ],
        "https://localhost:8443": [
            1122085995
        ],
        "-k\nif": [
            1122085995
        ],
        "the\nserver’s": [
            1122085995
        ],
        "be\ndone": [
            1122085995
        ],
        "verbose": [
            1122085995
        ],
        "-v\n*": [
            1122085995
        ],
        "(#0)\n*": [
            1122085995
        ],
        "::1..\n*": [
            1122085995
        ],
        "(::1)": [
            1122085995
        ],
        "initializing": [
            1122085995
        ],
        "nss": [
            1122085995
        ],
        "certpath:": [
            1122085995
        ],
        "sql:/etc/pki/nssdb\n*": [
            1122085995
        ],
        "skipping": [
            1122085995
        ],
        "verification\n*": [
            1122085995
        ],
        "tls_ecdhe_rsa_with_aes_256_gcm_sha384\n*": [
            1122085995
        ],
        "cn=wwwkubia-example.com": [
            1122085995
        ],
        "\n*": [
            1122085995
        ],
        "date:": [
            1122085995
        ],
        "18:43:13": [
            1122085995
        ],
        "gmt": [
            1122085995
        ],
        "expire": [
            1122085995
        ],
        "wwwkubia-example.com": [
            1122085995
        ],
        "issuer:": [
            1122085995
        ],
        "memory\nyou": [
            1122085995
        ],
        "in-memory": [
            1122085995
        ],
        "(tmpfs)": [
            1122085995
        ],
        "mounts\nin": [
            1122085995
        ],
        "certs\ntmpfs": [
            1122085995
        ],
        "(rorelatime)": [
            1122085995
        ],
        "disk\nwhere": [
            1122085995
        ],
        "compromised": [
            1122085995
        ],
        "variables\ninstead": [
            1122085995
        ],
        "as\nenvironment": [
            1122085995
        ],
        "\nfoo_secret": [
            1122085995
        ],
        "snippet": [
            1122085995
        ],
        "nginx\nthe": [
            1122085995
        ],
        "\nmatches": [
            1122085995
        ],
        "\ncreated": [
            1122085995
        ],
        "\n\n222chapter": [
            1122085995
        ],
        "foo_secret\n": [
            1122085995
        ],
        "secretkeyref:": [
            1122085995
        ],
        "\nsecretkeyref": [
            1122085995
        ],
        "config-\nmapkeyref\n": [
            1122085995
        ],
        "dump": [
            1122085995
        ],
        "startup\nwhich": [
            1122085995
        ],
        "unintentionally": [
            1122085995
        ],
        "inherit": [
            1122085995
        ],
        "third-party": [
            1122085995
        ],
        "binary\nyou": [
            1122085995
        ],
        "\ntipthink": [
            1122085995
        ],
        "twice": [
            1122085995
        ],
        "to\nyour": [
            1122085995
        ],
        "always\nuse": [
            1122085995
        ],
        "secrets\n7.5.6understanding": [
            1122085995
        ],
        "secrets\nyou’ve": [
            1122085995
        ],
        "contain\nbut": [
            1122085995
        ],
        "it—for": [
            1122085995
        ],
        "example\nwhen": [
            1122085995
        ],
        "also\ndone": [
            1122085995
        ],
        "registries\nwhich": [
            1122085995
        ],
        "private\nimage": [
            1122085995
        ],
        "reg-\nistry": [
            1122085995
        ],
        "see\nhow": [
            1122085995
        ],
        "that\nusing": [
            1122085995
        ],
        "hub\ndocker": [
            1122085995
        ],
        "private\nrepositories": [
            1122085995
        ],
        "http://hubdocker\n.com": [
            1122085995
        ],
        "checkbox": [
            1122085995
        ],
        "do\ntwo": [
            1122085995
        ],
        "things:\ncreate": [
            1122085995
        ],
        "registry\nreference": [
            1122085995
        ],
        "imagepullsecrets": [
            1122085995
        ],
        "manifest\nlisting": [
            1122085995
        ],
        "variable\nthe": [
            1122085995
        ],
        "secret\nthe": [
            1122085995
        ],
        "\nholding": [
            1122085995
        ],
        "expose\n": [
            1122085995
        ],
        "\n\n223using": [
            1122085995
        ],
        "containers\ncreating": [
            1122085995
        ],
        "authenticating": [
            1122085995
        ],
        "registry\ncreating": [
            1122085995
        ],
        "registry\nisn’t": [
            1122085995
        ],
        "75.3.": [
            1122085995
        ],
        "you\nuse": [
            1122085995
        ],
        "and\noptions:\n$": [
            1122085995
        ],
        "docker-registry": [
            1122085995
        ],
        "mydockerhubsecret": [
            1122085995
        ],
        "\\\n": [
            1122085995
        ],
        "--docker-username=myusername": [
            1122085995
        ],
        "--docker-password=mypassword": [
            1122085995
        ],
        "\\": [
            1122085995
        ],
        "--docker-email=myemail@provider.com\nrather": [
            1122085995
        ],
        "called\nmydockerhubsecret": [
            1122085995
        ],
        "and\nemail": [
            1122085995
        ],
        "describe\nyou’ll": [
            1122085995
        ],
        "\ndockercfg.": [
            1122085995
        ],
        "the\ndockercfg": [
            1122085995
        ],
        "the\ndocker": [
            1122085995
        ],
        "command\nusing": [
            1122085995
        ],
        "definition\nto": [
            1122085995
        ],
        "docker\nhub": [
            1122085995
        ],
        "private-pod\nspec:\n": [
            1122085995
        ],
        "imagepullsecrets:": [
            1122085995
        ],
        "username/private:tag\n": [
            1122085995
        ],
        "main\nin": [
            1122085995
        ],
        "mydockerhubsecret\nsecret": [
            1122085995
        ],
        "it’s\nlikely": [
            1122085995
        ],
        "soon\nnot": [
            1122085995
        ],
        "pod\ngiven": [
            1122085995
        ],
        "won-\nder": [
            1122085995
        ],
        "the\ncase": [
            1122085995
        ],
        "pods\nautomatically": [
            1122085995
        ],
        "serviceaccount\nlisting": [
            1122085995
        ],
        "pod-with-private-imageyaml\nthis": [
            1122085995
        ],
        "\n\n224chapter": [
            1122085995
        ],
        "applications\n76summary\nthis": [
            1122085995
        ],
        "to\noverride": [
            1122085995
        ],
        "definition\npass": [
            1122085995
        ],
        "process\nset": [
            1122085995
        ],
        "container\ndecouple": [
            1122085995
        ],
        "configmap\nstore": [
            1122085995
        ],
        "containers\ncreate": [
            1122085995
        ],
        "image\nregistry\nin": [
            1122085995
        ],
        "we\nlearned": [
            1122085995
        ],
        "\n\n225\naccessing": [
            1122085995
        ],
        "metadata\nand": [
            1122085995
        ],
        "resources\nfrom": [
            1122085995
        ],
        "applications\napplications": [
            1122085995
        ],
        "in\nincluding": [
            1122085995
        ],
        "information?": [
            1122085995
        ],
        "and\nhow": [
            1122085995
        ],
        "how\nto": [
            1122085995
        ],
        "resources\nthis": [
            1122085995
        ],
        "covers\nusing": [
            1122085995
        ],
        "\ncontainers\nexploring": [
            1122085995
        ],
        "api\nleaving": [
            1122085995
        ],
        "verification": [
            1122085995
        ],
        "proxy\naccessing": [
            1122085995
        ],
        "container\nunderstanding": [
            1122085995
        ],
        "pattern\nusing": [
            1122085995
        ],
        "\n\n226chapter": [
            1122085995
        ],
        "8accessing": [
            1122085995
        ],
        "applications\n81passing": [
            1122085995
        ],
        "api\nin": [
            1122085995
        ],
        "volumes\nthis": [
            1122085995
        ],
        "is\nscheduled": [
            1122085995
        ],
        "until\nthat": [
            1122085995
        ],
        "point—such": [
            1122085995
        ],
        "name\n(when": [
            1122085995
        ],
        "generated;": [
            1122085995
        ],
        "replicaset\nor": [
            1122085995
        ],
        "controller)?": [
            1122085995
        ],
        "annotations?": [
            1122085995
        ],
        "repeat": [
            1122085995
        ],
        "in\nmultiple": [
            1122085995
        ],
        "places\n": [
            1122085995
        ],
        "or\nfiles": [
            1122085995
        ],
        "\ndownwardapi": [
            1122085995
        ],
        "api\nisn’t": [
            1122085995
        ],
        "of\nhaving": [
            1122085995
        ],
        "81.\n8.1.1understanding": [
            1122085995
        ],
        "metadata\nthe": [
            1122085995
        ],
        "processes\nrunning": [
            1122085995
        ],
        "containers:\nthe": [
            1122085995
        ],
        "name\nthe": [
            1122085995
        ],
        "address\ncontainer:": [
            1122085995
        ],
        "main\nenvironment\nvariables\napi": [
            1122085995
        ],
        "server\nused": [
            1122085995
        ],
        "environment\nvariables": [
            1122085995
        ],
        "the\ndownwardapi": [
            1122085995
        ],
        "volume\npod": [
            1122085995
        ],
        "manifest\n-": [
            1122085995
        ],
        "metadata\n-": [
            1122085995
        ],
        "status\npod\ndownwardapi\nvolume\napp": [
            1122085995
        ],
        "process\nfigure": [
            1122085995
        ],
        "\n\n227passing": [
            1122085995
        ],
        "api\nthe": [
            1122085995
        ],
        "to\nthe": [
            1122085995
        ],
        "on\nthe": [
            1122085995
        ],
        "under\nthe": [
            1122085995
        ],
        "container\nthe": [
            1122085995
        ],
        "labels\nthe": [
            1122085995
        ],
        "annotations\nmost": [
            1122085995
        ],
        "cpu/memory": [
            1122085995
        ],
        "introduced\nyet": [
            1122085995
        ],
        "know\nis": [
            1122085995
        ],
        "authenticates": [
            1122085995
        ],
        "14\nthey’re": [
            1122085995
        ],
        "guaranteed": [
            1122085995
        ],
        "maxi-\nmum": [
            1122085995
        ],
        "get\n": [
            1122085995
        ],
        "be\nexposed": [
            1122085995
        ],
        "acquired": [
            1122085995
        ],
        "(for\nexample": [
            1122085995
        ],
        "directly)": [
            1122085995
        ],
        "sim-\npler": [
            1122085995
        ],
        "alternative\n": [
            1122085995
        ],
        "process\n8.1.2exposing": [
            1122085995
        ],
        "variables\nfirst": [
            1122085995
        ],
        "pod\nfrom": [
            1122085995
        ],
        "downward\nspec:\n": [
            1122085995
        ],
        "busybox\n": [
            1122085995
        ],
        "[sleep\"": [
            1122085995
        ],
        "9999999\"]\n": [
            1122085995
        ],
        "15m\n": [
            1122085995
        ],
        "memory:": [
            1122085995
        ],
        "100ki\n": [
            1122085995
        ],
        "limits:\n": [
            1122085995
        ],
        "100m\n": [
            1122085995
        ],
        "4mi\n": [
            1122085995
        ],
        "pod_name\nlisting": [
            1122085995
        ],
        "variables:": [
            1122085995
        ],
        "downward-api-envyaml\n": [
            1122085995
        ],
        "\n\n228chapter": [
            1122085995
        ],
        "fieldref:": [
            1122085995
        ],
        "fieldpath:": [
            1122085995
        ],
        "metadataname": [
            1122085995
        ],
        "pod_namespace\n": [
            1122085995
        ],
        "valuefrom:\n": [
            1122085995
        ],
        "fieldref:\n": [
            1122085995
        ],
        "metadatanamespace\n": [
            1122085995
        ],
        "pod_ip\n": [
            1122085995
        ],
        "statuspodip\n": [
            1122085995
        ],
        "node_name\n": [
            1122085995
        ],
        "specnodename\n": [
            1122085995
        ],
        "service_account\n": [
            1122085995
        ],
        "specserviceaccountname\n": [
            1122085995
        ],
        "container_cpu_request_millicores\n": [
            1122085995
        ],
        "resourcefieldref:": [
            1122085995
        ],
        "requestscpu": [
            1122085995
        ],
        "divisor:": [
            1122085995
        ],
        "container_memory_limit_kibibytes\n": [
            1122085995
        ],
        "resourcefieldref:\n": [
            1122085995
        ],
        "limitsmemory\n": [
            1122085995
        ],
        "1ki\nwhen": [
            1122085995
        ],
        "val-\nues": [
            1122085995
        ],
        "\npod_name\npod_ip,": [
            1122085995
        ],
        "pod_namespace": [
            1122085995
        ],
        "\nnode_name": [
            1122085995
        ],
        "\nservice_account\nenvironment": [
            1122085995
        ],
        "hold\nthe": [
            1122085995
        ],
        "consume\n": [
            1122085995
        ],
        "divi-\nsor": [
            1122085995
        ],
        "divided": [
            1122085995
        ],
        "divisor": [
            1122085995
        ],
        "the\nresult": [
            1122085995
        ],
        "set-\nting": [
            1122085995
        ],
        "\n1m": [
            1122085995
        ],
        "(one": [
            1122085995
        ],
        "milli-core": [
            1122085995
        ],
        "one-thousandth": [
            1122085995
        ],
        "a\ncpu": [
            1122085995
        ],
        "core)": [
            1122085995
        ],
        "\n15m": [
            1122085995
        ],
        "variable\ncontainer_cpu_request_millicores": [
            1122085995
        ],
        "memory\nlimit": [
            1122085995
        ],
        "\n4mi": [
            1122085995
        ],
        "(4": [
            1122085995
        ],
        "mebibytes)": [
            1122085995
        ],
        "1ki": [
            1122085995
        ],
        "(1": [
            1122085995
        ],
        "kibibyte)": [
            1122085995
        ],
        "container_memory\n_limit_kibibytes\n": [
            1122085995
        ],
        "absolute": [
            1122085995
        ],
        "\nfield": [
            1122085995
        ],
        "manifest\na": [
            1122085995
        ],
        "resourcefieldref": [
            1122085995
        ],
        "fieldref\nfor": [
            1122085995
        ],
        "\ndefine": [
            1122085995
        ],
        "\nvalue": [
            1122085995
        ],
        "need\n": [
            1122085995
        ],
        "\n\n229passing": [
            1122085995
        ],
        "api\nthe": [
            1122085995
        ],
        "core\nor": [
            1122085995
        ],
        "millicore": [
            1122085995
        ],
        "limits/requests": [
            1122085995
        ],
        "(byte)\n1k": [
            1122085995
        ],
        "(kilobyte)": [
            1122085995
        ],
        "(kibibyte)": [
            1122085995
        ],
        "(megabyte)": [
            1122085995
        ],
        "1mi": [
            1122085995
        ],
        "(mebibyte)": [
            1122085995
        ],
        "env\npath=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nhostname=downward\ncontainer_memory_limit_kibibytes=4096\npod_name=downward\npod_namespace=default\npod_ip=100.0.10\nnode_name=gke-kubia-default-pool-32a2cac8-sgl7\nservice_account=default\ncontainer_cpu_request_millicores=15\nkubernetes_service_host=10.3.240.1\nkubernetes_service_port=443\n...\nlisting": [
            1122085995
        ],
        "pod\npod": [
            1122085995
        ],
        "manifest\nmetadata:\nname:": [
            1122085995
        ],
        "downward\nnamespace:": [
            1122085995
        ],
        "default\nspec:\nnodename:": [
            1122085995
        ],
        "minikube\nserviceaccountname:": [
            1122085995
        ],
        "default\ncontainers:\n-": [
            1122085995
        ],
        "main\nimage:": [
            1122085995
        ],
        "busybox\ncommand:": [
            1122085995
        ],
        "9999999\"]\nresources:\nrequests:\ncpu:": [
            1122085995
        ],
        "15m\nmemory:": [
            1122085995
        ],
        "100ki\nlimits:\ncpu:": [
            1122085995
        ],
        "100m\nmemory:": [
            1122085995
        ],
        "4mi\n..\nstatus:\npodip:": [
            1122085995
        ],
        "17217.0.4\n...\npod:": [
            1122085995
        ],
        "downward\ncontainer:": [
            1122085995
        ],
        "main\nenvironment": [
            1122085995
        ],
        "variables\npod_name=downward\npod_namespace=default\npod_ip=17217.0.4\nnode_name=minikube\nservice_account=default\ncontainer_cpu_request_millicores=15\ncontainer_memory_limit_kibibytes=4096\ndivisor:": [
            1122085995
        ],
        "1m\ndivisor:": [
            1122085995
        ],
        "1ki\nfigure": [
            1122085995
        ],
        "variables\n": [
            1122085995
        ],
        "\n\n230chapter": [
            1122085995
        ],
        "applications\nall": [
            1122085995
        ],
        "how-\never": [
            1122085995
        ],
        "\n81.3passing": [
            1122085995
        ],
        "variables\nyou": [
            1122085995
        ],
        "use\na": [
            1122085995
        ],
        "nei-\nther": [
            1122085995
        ],
        "downward\n": [
            1122085995
        ],
        "key1:": [
            1122085995
        ],
        "value1": [
            1122085995
        ],
        "key2:": [
            1122085995
        ],
        "multi": [
            1122085995
        ],
        "/etc/downward": [
            1122085995
        ],
        "downwardapi:": [
            1122085995
        ],
        "podname\"": [
            1122085995
        ],
        "podnamespace\"\n": [
            1122085995
        ],
        "metadatanamespace\nlisting": [
            1122085995
        ],
        "downward-api-volumeyaml\nthese": [
            1122085995
        ],
        "\ndownward": [
            1122085995
        ],
        "/etc/downward\nyou’re": [
            1122085995
        ],
        "downward\nthe": [
            1122085995
        ],
        "(from": [
            1122085995
        ],
        "manifest)": [
            1122085995
        ],
        "podname": [
            1122085995
        ],
        "\n\n231passing": [
            1122085995
        ],
        "api\n": [
            1122085995
        ],
        "labels\"": [
            1122085995
        ],
        "metadatalabels": [
            1122085995
        ],
        "annotations\"": [
            1122085995
        ],
        "metadataannotations": [
            1122085995
        ],
        "containercpurequestmillicores\"\n": [
            1122085995
        ],
        "containername:": [
            1122085995
        ],
        "requestscpu\n": [
            1122085995
        ],
        "1m\n": [
            1122085995
        ],
        "containermemorylimitbytes\"\n": [
            1122085995
        ],
        "1\ninstead": [
            1122085995
        ],
        "\ndownwardapiitems": [
            1122085995
        ],
        "attribute\nin": [
            1122085995
        ],
        "item": [
            1122085995
        ],
        "\npath": [
            1122085995
        ],
        "filename)": [
            1122085995
        ],
        "written\nto": [
            1122085995
        ],
        "pod-level": [
            1122085995
        ],
        "83).\nthe": [
            1122085995
        ],
        "/etc/downward/labels": [
            1122085995
        ],
        "\nwritten": [
            1122085995
        ],
        "/etc/downward/\nannotations": [
            1122085995
        ],
        "file\ndownwardapi": [
            1122085995
        ],
        "default\nlabels:\nfoo:": [
            1122085995
        ],
        "bar\nannotations:\nkey1:": [
            1122085995
        ],
        "value1\n..\nspec:\ncontainers:\n-": [
            1122085995
        ],
        "4mi\n..\n/podname\n/podnamespace\n/labels\n/annotations\n/containercpurequestmillicores\n/containermemorylimitbytes\ndivisor:": [
            1122085995
        ],
        "1\ndivisor:": [
            1122085995
        ],
        "1m\ncontainer:": [
            1122085995
        ],
        "main\npod:": [
            1122085995
        ],
        "downward\nfilesystem\n/\netc/\ndownward/\nfigure": [
            1122085995
        ],
        "\n\n232chapter": [
            1122085995
        ],
        "applications\ndelete": [
            1122085995
        ],
        "you\nmounted": [
            1122085995
        ],
        "/etc/downward/": [
            1122085995
        ],
        "-ll": [
            1122085995
        ],
        "/etc/downward\n-rw-r--r--": [
            1122085995
        ],
        "10:23": [
            1122085995
        ],
        "annotations\n-rw-r--r--": [
            1122085995
        ],
        "containercpurequestmillicores\n-rw-r--r--": [
            1122085995
        ],
        "containermemorylimitbytes\n-rw-r--r--": [
            1122085995
        ],
        "labels\n-rw-r--r--": [
            1122085995
        ],
        "podname\n-rw-r--r--": [
            1122085995
        ],
        "podnamespace\nnoteas": [
            1122085995
        ],
        "defaultmode": [
            1122085995
        ],
        "correspond": [
            1122085995
        ],
        "them\nhere": [
            1122085995
        ],
        "you\nexposed": [
            1122085995
        ],
        "in\n$": [
            1122085995
        ],
        "/etc/downward/labels\nfoo=bar\"\n$": [
            1122085995
        ],
        "/etc/downward/annotations\nkey1=value1\"\nkey2=\"multi\\nline\\nvalue\\n\"\nkubernetesio/config.seen=\"2016-11-28t14:27:45.664924282z\"\nkubernetes.io/config.source=\"api\"\nas": [
            1122085995
        ],
        "label/annotation": [
            1122085995
        ],
        "key=value": [
            1122085995
        ],
        "sepa-\nrate": [
            1122085995
        ],
        "newline": [
            1122085995
        ],
        "denoted\nwith": [
            1122085995
        ],
        "\n\\n\nupdating": [
            1122085995
        ],
        "annotations\nyou": [
            1122085995
        ],
        "holding\nthem": [
            1122085995
        ],
        "and\nannotations": [
            1122085995
        ],
        "were\nexposed": [
            1122085995
        ],
        "after\nthey’re": [
            1122085995
        ],
        "modified\nlisting": [
            1122085995
        ],
        "volume\nlisting": [
            1122085995
        ],
        "\n\n233talking": [
            1122085995
        ],
        "server\nreferring": [
            1122085995
        ],
        "container-level": [
            1122085995
        ],
        "specification\nbefore": [
            1122085995
        ],
        "con-\ntainer-level": [
            1122085995
        ],
        "(done": [
            1122085995
        ],
        "using\nresourcefieldref)": [
            1122085995
        ],
        "resource\nfield": [
            1122085995
        ],
        "field\ninside": [
            1122085995
        ],
        "container\nyou’re": [
            1122085995
        ],
        "slightly\nmore": [
            1122085995
        ],
        "allows\nyou": [
            1122085995
        ],
        "(but\nboth": [
            1122085995
        ],
        "container\ncan": [
            1122085995
        ],
        "api\nas": [
            1122085995
        ],
        "expects": [
            1122085995
        ],
        "down-\nward": [
            1122085995
        ],
        "rewrite\nthe": [
            1122085995
        ],
        "collects": [
            1122085995
        ],
        "need\nmore": [
            1122085995
        ],
        "next\n8.2talking": [
            1122085995
        ],
        "server\nwe’ve": [
            1122085995
        ],
        "own\nmetadata": [
            1122085995
        ],
        "the\ndownward": [
            1122085995
        ],
        "cases\nlisting": [
            1122085995
        ],
        "volume\ncontainer": [
            1122085995
        ],
        "specified\n": [
            1122085995
        ],
        "\n\n234chapter": [
            1122085995
        ],
        "be\nobtained": [
            1122085995
        ],
        "but\nwhen": [
            1122085995
        ],
        "most\nup-to-date": [
            1122085995
        ],
        "84).\nbefore": [
            1122085995
        ],
        "first\nexplore": [
            1122085995
        ],
        "what\ntalking": [
            1122085995
        ],
        "like\n8.2.1exploring": [
            1122085995
        ],
        "api\nyou’ve": [
            1122085995
        ],
        "planning": [
            1122085995
        ],
        "on\ndeveloping": [
            1122085995
        ],
        "cluster-info:\n$": [
            1122085995
        ],
        "https://192168.99.100:8443\nbecause": [
            1122085995
        ],
        "curl’s": [
            1122085995
        ],
        "--insecure": [
            1122085995
        ],
        "-k)\noption": [
            1122085995
        ],
        "far:\n$": [
            1122085995
        ],
        "https://192168.99.100:8443": [
            1122085995
        ],
        "-k\nunauthorized\nluckily": [
            1122085995
        ],
        "rather": [
            1122085995
        ],
        "server\nthrough": [
            1122085995
        ],
        "accepts": [
            1122085995
        ],
        "proxies": [
            1122085995
        ],
        "authenti-\ncation": [
            1122085995
        ],
        "also\nmakes": [
            1122085995
        ],
        "middle": [
            1122085995
        ],
        "(by\nverifying": [
            1122085995
        ],
        "request)\ncontainer\napi": [
            1122085995
        ],
        "server\npod\napp": [
            1122085995
        ],
        "process\napi": [
            1122085995
        ],
        "objects\nfigure": [
            1122085995
        ],
        "objects\n": [
            1122085995
        ],
        "\n\n235talking": [
            1122085995
        ],
        "proxy\nstarting": [
            1122085995
        ],
        "1270.0.1:8001\nyou": [
            1122085995
        ],
        "starts\nup": [
            1122085995
        ],
        "works:\n$": [
            1122085995
        ],
        "localhost:8001\n{\n": [
            1122085995
        ],
        "paths\":": [
            1122085995
        ],
        "[\n": [
            1122085995
        ],
        "/api\"\n": [
            1122085995
        ],
        "/api/v1\"\n": [
            1122085995
        ],
        "..\nvoila!": [
            1122085995
        ],
        "exploring\nexploring": [
            1122085995
        ],
        "proxy\nyou": [
            1122085995
        ],
        "to\nhttp://localhost:8001": [
            1122085995
        ],
        "base\nurl": [
            1122085995
        ],
        "http://localhost:8001\n{\n": [
            1122085995
        ],
        "/api/v1\"": [
            1122085995
        ],
        "/apis\"\n": [
            1122085995
        ],
        "/apis/apps\"\n": [
            1122085995
        ],
        "/apis/apps/v1beta1\"\n": [
            1122085995
        ],
        "/apis/batch\"": [
            1122085995
        ],
        "/apis/batch/v1\"": [
            1122085995
        ],
        "/apis/batch/v2alpha1\"": [
            1122085995
        ],
        "..\nthese": [
            1122085995
        ],
        "resource\ndefinitions": [
            1122085995
        ],
        "recognize": [
            1122085995
        ],
        "\nbatch/v1": [
            1122085995
        ],
        "/apis/batch/v1": [
            1122085995
        ],
        "and\nversion": [
            1122085995
        ],
        "\n/api/v1\ncorresponds": [
            1122085995
        ],
        "apiversion:": [
            1122085995
        ],
        "created\n(pods": [
            1122085995
        ],
        "resource\ntypes": [
            1122085995
        ],
        "earliest": [
            1122085995
        ],
        "endpoints:": [
            1122085995
        ],
        "http://localhost:8001\nmost": [
            1122085995
        ],
        "here\nthe": [
            1122085995
        ],
        "\ntwo": [
            1122085995
        ],
        "versions\n": [
            1122085995
        ],
        "\n\n236chapter": [
            1122085995
        ],
        "applications\nany": [
            1122085995
        ],
        "api\ngroups;": [
            1122085995
        ],
        "\nnotethese": [
            1122085995
        ],
        "considered\nto": [
            1122085995
        ],
        "group\nexploring": [
            1122085995
        ],
        "group’s": [
            1122085995
        ],
        "endpoint\nlet’s": [
            1122085995
        ],
        "the\n/apis/batch": [
            1122085995
        ],
        "now)": [
            1122085995
        ],
        "http://localhost:8001/apis/batch\n{\n": [
            1122085995
        ],
        "kind\":": [
            1122085995
        ],
        "apigroup\"\n": [
            1122085995
        ],
        "apiversion\":": [
            1122085995
        ],
        "v1\"\n": [
            1122085995
        ],
        "name\":": [
            1122085995
        ],
        "batch\"\n": [
            1122085995
        ],
        "versions\":": [
            1122085995
        ],
        "groupversion\":": [
            1122085995
        ],
        "batch/v1\"": [
            1122085995
        ],
        "version\":": [
            1122085995
        ],
        "v1\"": [
            1122085995
        ],
        "batch/v2alpha1\"": [
            1122085995
        ],
        "v2alpha1\"": [
            1122085995
        ],
        "]\n": [
            1122085995
        ],
        "preferredversion\":": [
            1122085995
        ],
        "serveraddressbyclientcidrs\":": [
            1122085995
        ],
        "null\n}\nthe": [
            1122085995
        ],
        "preferred": [
            1122085995
        ],
        "what’s\nbehind": [
            1122085995
        ],
        "\n/apis/batch/v1": [
            1122085995
        ],
        "http://localhost:8001/apis/batch/v1\n{\n": [
            1122085995
        ],
        "apiresourcelist\"": [
            1122085995
        ],
        "resources\":": [
            1122085995
        ],
        "[": [
            1122085995
        ],
        "jobs\"": [
            1122085995
        ],
        "namespaced\":": [
            1122085995
        ],
        "job\"": [
            1122085995
        ],
        "/apis/batch:": [
            1122085995
        ],
        "http://localhost:8001/apis/batch\nlisting": [
            1122085995
        ],
        "batch/v1:": [
            1122085995
        ],
        "http://localhost:8001/apis/batch/v1\nthe": [
            1122085995
        ],
        "versions\nclients": [
            1122085995
        ],
        "\nv2alpha1\nthis": [
            1122085995
        ],
        "group\nhere’s": [
            1122085995
        ],
        "group\nthis": [
            1122085995
        ],
        "namespaced\n": [
            1122085995
        ],
        "\n\n237talking": [
            1122085995
        ],
        "verbs\":": [
            1122085995
        ],
        "create\"": [
            1122085995
        ],
        "delete\"": [
            1122085995
        ],
        "deletecollection\"": [
            1122085995
        ],
        "get\"": [
            1122085995
        ],
        "list\"": [
            1122085995
        ],
        "patch\"": [
            1122085995
        ],
        "update\"": [
            1122085995
        ],
        "watch\"": [
            1122085995
        ],
        "jobs/status\"": [
            1122085995
        ],
        "job\"\n": [
            1122085995
        ],
        "]\n}\nas": [
            1122085995
        ],
        "\nkind": [
            1122085995
        ],
        "\nnamespaced": [
            1122085995
        ],
        "one;": [
            1122085995
        ],
        "don’t)\nand": [
            1122085995
        ],
        "\nverbs": [
            1122085995
        ],
        "the\nname\":": [
            1122085995
        ],
        "/apis/batch/v1/jobs": [
            1122085995
        ],
        "\nverbs\"": [
            1122085995
        ],
        "resources\nthrough": [
            1122085995
        ],
        "also\nexposed": [
            1122085995
        ],
        "\njobs/status": [
            1122085995
        ],
        "of\najob)\nlisting": [
            1122085995
        ],
        "/apis/batch/\nv1/jobs\n": [
            1122085995
        ],
        "http://localhost:8001/apis/batch/v1/jobs\n{\n": [
            1122085995
        ],
        "joblist\"\n": [
            1122085995
        ],
        "batch/v1\"\n": [
            1122085995
        ],
        "metadata\":": [
            1122085995
        ],
        "selflink\":": [
            1122085995
        ],
        "/apis/batch/v1/jobs\"\n": [
            1122085995
        ],
        "resourceversion\":": [
            1122085995
        ],
        "225162\"\n": [
            1122085995
        ],
        "}\nlisting": [
            1122085995
        ],
        "jobs:": [
            1122085995
        ],
        "http://localhost:8001/apis/batch/v1/jobs\nhere": [
            1122085995
        ],
        "verbs": [
            1122085995
        ],
        "\njobs;": [
            1122085995
        ],
        "\ncollection": [
            1122085995
        ],
        "them;": [
            1122085995
        ],
        "\nwatch": [
            1122085995
        ],
        "them)\nresources": [
            1122085995
        ],
        "\nspecial": [
            1122085995
        ],
        "status\nthe": [
            1122085995
        ],
        "\nretrieved": [
            1122085995
        ],
        "patched": [
            1122085995
        ],
        "updated\n": [
            1122085995
        ],
        "\n\n238chapter": [
            1122085995
        ],
        "items\":": [
            1122085995
        ],
        "my-job\"\n": [
            1122085995
        ],
        "namespace\":": [
            1122085995
        ],
        "default\"\n": [
            1122085995
        ],
        "..\nyou": [
            1122085995
        ],
        "be\nempty": [
            1122085995
        ],
        "chapter08/my-jobyaml": [
            1122085995
        ],
        "rest\nendpoint": [
            1122085995
        ],
        "810.\nretrieving": [
            1122085995
        ],
        "back\nonly": [
            1122085995
        ],
        "to\nretrieve": [
            1122085995
        ],
        "(\nname:": [
            1122085995
        ],
        "my-job;": [
            1122085995
        ],
        "default)\nyou": [
            1122085995
        ],
        "\n/apis/batch/v1/namespaces/default/jobs/\nmy-job\n": [
            1122085995
        ],
        "http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job\n{\n": [
            1122085995
        ],
        "..\nas": [
            1122085995
        ],
        "my-job": [
            1122085995
        ],
        "resource\nexactly": [
            1122085995
        ],
        "run:\n$": [
            1122085995
        ],
        "json\nyou’ve": [
            1122085995
        ],
        "browse": [
            1122085995
        ],
        "any\nspecial": [
            1122085995
        ],
        "is\ndescribed": [
            1122085995
        ],
        "enough\nto": [
            1122085995
        ],
        "\n82.2talking": [
            1122085995
        ],
        "pod\nyou’ve": [
            1122085995
        ],
        "(usually)\ndon’t": [
            1122085995
        ],
        "things:\nfind": [
            1122085995
        ],
        "server\nmake": [
            1122085995
        ],
        "impersonating": [
            1122085995
        ],
        "it\nauthenticate": [
            1122085995
        ],
        "server;": [
            1122085995
        ],
        "anything\nlisting": [
            1122085995
        ],
        "\n\n239talking": [
            1122085995
        ],
        "server\nyou’ll": [
            1122085995
        ],
        "pod\nthat": [
            1122085995
        ],
        "\nsleep": [
            1122085995
        ],
        "a\nshell": [
            1122085995
        ],
        "from\nwithin": [
            1122085995
        ],
        "\ncurl\n": [
            1122085995
        ],
        "you\nsearch": [
            1122085995
        ],
        "\ntutum/curl": [
            1122085995
        ],
        "so\nuse": [
            1122085995
        ],
        "own)": [
            1122085995
        ],
        "curl\nspec:\n": [
            1122085995
        ],
        "tutum/curl": [
            1122085995
        ],
        "9999999\"]": [
            1122085995
        ],
        "\nafter": [
            1122085995
        ],
        "bash\nroot@curl:/#\nyou’re": [
            1122085995
        ],
        "server\nfinding": [
            1122085995
        ],
        "address\nfirst": [
            1122085995
        ],
        "easy\nbecause": [
            1122085995
        ],
        "svc:\n$": [
            1122085995
        ],
        "100.0.1": [
            1122085995
        ],
        "46d\nand": [
            1122085995
        ],
        "looking\nup": [
            1122085995
        ],
        "\nkubernetes_service_host": [
            1122085995
        ],
        "kubernetes_service_port": [
            1122085995
        ],
        "(inside\nthe": [
            1122085995
        ],
        "container):\nroot@curl:/#": [
            1122085995
        ],
        "kubernetes_service\nkubernetes_service_port=443\nkubernetes_service_host=100.0.1\nkubernetes_service_port_https=443\nlisting": [
            1122085995
        ],
        "curlyaml\nusing": [
            1122085995
        ],
        "\navailable": [
            1122085995
        ],
        "\ncommand": [
            1122085995
        ],
        "\nkeep": [
            1122085995
        ],
        "\n\n240chapter": [
            1122085995
        ],
        "applications\nyou": [
            1122085995
        ],
        "even\nneed": [
            1122085995
        ],
        "to\nhttps://kubernetes": [
            1122085995
        ],
        "fair": [
            1122085995
        ],
        "at\nyou": [
            1122085995
        ],
        "srv\nrecord": [
            1122085995
        ],
        "on\nport": [
            1122085995
        ],
        "through\nhttps:\nroot@curl:/#": [
            1122085995
        ],
        "https://kubernetes\ncurl:": [
            1122085995
        ],
        "(60)": [
            1122085995
        ],
        "problem:": [
            1122085995
        ],
        "issuer": [
            1122085995
        ],
        "certificate\n..\nif": [
            1122085995
        ],
        "youd": [
            1122085995
        ],
        "curls": [
            1122085995
        ],
        "use\n": [
            1122085995
        ],
        "--insecure)": [
            1122085995
        ],
        "option\nalthough": [
            1122085995
        ],
        "(and\nthis": [
            1122085995
        ],
        "correct)": [
            1122085995
        ],
        "route": [
            1122085995
        ],
        "blindly": [
            1122085995
        ],
        "trusting": [
            1122085995
        ],
        "you’re\nconnecting": [
            1122085995
        ],
        "authentic": [
            1122085995
        ],
        "check\nits": [
            1122085995
        ],
        "\ntipnever": [
            1122085995
        ],
        "application\ndoing": [
            1122085995
        ],
        "attacker\nusing": [
            1122085995
        ],
        "man-in-the-middle": [
            1122085995
        ],
        "attack\nverifying": [
            1122085995
        ],
        "identity\nin": [
            1122085995
        ],
        "discussing": [
            1122085995
        ],
        "\ndefault-token-xyz": [
            1122085995
        ],
        "at\n/var/run/secrets/kubernetesio/serviceaccount/.": [
            1122085995
        ],
        "secret\nagain": [
            1122085995
        ],
        "directory:\nroot@curl:/#": [
            1122085995
        ],
        "/var/run/secrets/kubernetesio/serviceaccount/": [
            1122085995
        ],
        "\ncacrt": [
            1122085995
        ],
        "token\nthe": [
            1122085995
        ],
        "right\nnow": [
            1122085995
        ],
        "cacrt": [
            1122085995
        ],
        "author-\nity": [
            1122085995
        ],
        "(ca)": [
            1122085995
        ],
        "sign": [
            1122085995
        ],
        "\ncurl\nallows": [
            1122085995
        ],
        "--cacert": [
            1122085995
        ],
        "again:\nroot@curl:/#": [
            1122085995
        ],
        "/var/run/secrets/kubernetesio/serviceaccount\n": [
            1122085995
        ],
        "/cacrt": [
            1122085995
        ],
        "https://kubernetes\nunauthorized\nnoteyou": [
            1122085995
        ],
        "“unauthorized”\n": [
            1122085995
        ],
        "\n\n241talking": [
            1122085995
        ],
        "server\nokay": [
            1122085995
        ],
        "verified": [
            1122085995
        ],
        "certificate\nwas": [
            1122085995
        ],
        "\nunauthorized": [
            1122085995
        ],
        "\ncurl_ca_bundle": [
            1122085995
        ],
        "don’t\nneed": [
            1122085995
        ],
        "\n--cacert": [
            1122085995
        ],
        "curl:\nroot@curl:/#": [
            1122085995
        ],
        "curl_ca_bundle=/var/run/secrets/kubernetesio/\n": [
            1122085995
        ],
        "serviceaccount/cacrt\nyou": [
            1122085995
        ],
        "--cacert:\nroot@curl:/#": [
            1122085995
        ],
        "https://kubernetes\nunauthorized\nthis": [
            1122085995
        ],
        "nicer": [
            1122085995
        ],
        "(curl)": [
            1122085995
        ],
        "trusts": [
            1122085995
        ],
        "authorized": [
            1122085995
        ],
        "who\nyou": [
            1122085995
        ],
        "are\nauthenticating": [
            1122085995
        ],
        "server\nyou": [
            1122085995
        ],
        "authenticate": [
            1122085995
        ],
        "update\nand/or": [
            1122085995
        ],
        "an\nauthentication": [
            1122085995
        ],
        "secret\nmentioned": [
            1122085995
        ],
        "\ntoken": [
            1122085995
        ],
        "the\nsecret’s": [
            1122085995
        ],
        "purpose": [
            1122085995
        ],
        "an\nenvironment": [
            1122085995
        ],
        "variable:\nroot@curl:/#": [
            1122085995
        ],
        "token=$(cat": [
            1122085995
        ],
        "/var/run/secrets/kubernetesio/\n": [
            1122085995
        ],
        "serviceaccount/token)\nthe": [
            1122085995
        ],
        "listing\nroot@curl:/#": [
            1122085995
        ],
        "authorization:": [
            1122085995
        ],
        "bearer": [
            1122085995
        ],
        "$token": [
            1122085995
        ],
        "https://kubernetes\n{\n": [
            1122085995
        ],
        "/apis/authorizationk8s.io\"": [
            1122085995
        ],
        "/ui/\"\n": [
            1122085995
        ],
        "/version\"\n": [
            1122085995
        ],
        "]\n}\nlisting": [
            1122085995
        ],
        "\n\n242chapter": [
            1122085995
        ],
        "applications\nas": [
            1122085995
        ],
        "the\nrequest": [
            1122085995
        ],
        "recognized": [
            1122085995
        ],
        "proper\nresponse": [
            1122085995
        ],
        "few\nsections": [
            1122085995
        ],
        "in\ngetting": [
            1122085995
        ],
        "in\nin": [
            1122085995
        ],
        "noticed\nyour": [
            1122085995
        ],
        "explicitly\npass": [
            1122085995
        ],
        "con-\ntents": [
            1122085995
        ],
        "ns=$(cat": [
            1122085995
        ],
        "serviceaccount/namespace)": [
            1122085995
        ],
        "\nroot@curl:/#": [
            1122085995
        ],
        "$token\n": [
            1122085995
        ],
        "https://kubernetes/api/v1/namespaces/$ns/pods\n{\n": [
            1122085995
        ],
        "podlist\"\n": [
            1122085995
        ],
        "..\nand": [
            1122085995
        ],
        "directory\nyou": [
            1122085995
        ],
        "man-\nner": [
            1122085995
        ],
        "\nput": [
            1122085995
        ],
        "or\npatch": [
            1122085995
        ],
        "\ndisabling": [
            1122085995
        ],
        "(rbac)\nif": [
            1122085995
        ],
        "not\nbe": [
            1122085995
        ],
        "(parts": [
            1122085995
        ],
        "accounts\nand": [
            1122085995
        ],
        "clusterrolebinding": [
            1122085995
        ],
        "permissive-binding": [
            1122085995
        ],
        "--clusterrole=cluster-admin": [
            1122085995
        ],
        "--group=system:serviceaccounts\nthis": [
            1122085995
        ],
        "(we": [
            1122085995
        ],
        "cluster-admin": [
            1122085995
        ],
        "privileges\nallowing": [
            1122085995
        ],
        "dangerous": [
            1122085995
        ],
        "and\nshould": [
            1122085995
        ],
        "fine\nlisting": [
            1122085995
        ],
        "\n\n243talking": [
            1122085995
        ],
        "server\nrecapping": [
            1122085995
        ],
        "properly:\nthe": [
            1122085995
        ],
        "certif-\nicate": [
            1122085995
        ],
        "authority": [
            1122085995
        ],
        "when\nperforming": [
            1122085995
        ],
        "crud": [
            1122085995
        ],
        "namespace\ndefinitioncrud": [
            1122085995
        ],
        "\npost": [
            1122085995
        ],
        "patch/put": [
            1122085995
        ],
        "respectively\nall": [
            1122085995
        ],
        "85.\n8.2.3simplifying": [
            1122085995
        ],
        "\ncontainers\ndealing": [
            1122085995
        ],
        "tokens": [
            1122085995
        ],
        "seems": [
            1122085995
        ],
        "too\ncomplicated": [
            1122085995
        ],
        "validation": [
            1122085995
        ],
        "certifi-\ncates": [
            1122085995
        ],
        "admit": [
            1122085995
        ],
        "times)": [
            1122085995
        ],
        "server\nget": [
            1122085995
        ],
        "/api/v1/namespaces/<namespace>/pods\nauthorization:": [
            1122085995
        ],
        "<token>\npod\ncontainer\nfilesystemapp\n/\nvar/\nrun/\nsecrets/\nkubernetesio/\nserviceaccount/\ndefault": [
            1122085995
        ],
        "volume\ncacrttokennamespace\nserver\ncertificate\nvalidate\ncertificate\nfigure": [
            1122085995
        ],
        "\n\n244chapter": [
            1122085995
        ],
        "82.1?": [
            1122085995
        ],
        "ran\nthe": [
            1122085995
        ],
        "it\ntake": [
            1122085995
        ],
        "can\nbe": [
            1122085995
        ],
        "well\nintroducing": [
            1122085995
        ],
        "pattern\nimagine": [
            1122085995
        ],
        "(among": [
            1122085995
        ],
        "sec-\ntion": [
            1122085995
        ],
        "alongside": [
            1122085995
        ],
        "https)": [
            1122085995
        ],
        "ambassador\nproxy": [
            1122085995
        ],
        "trans-\nparently": [
            1122085995
        ],
        "86).": [
            1122085995
        ],
        "token’s": [
            1122085995
        ],
        "\nsecret\nvolume\nbecause": [
            1122085995
        ],
        "loopback": [
            1122085995
        ],
        "app\ncan": [
            1122085995
        ],
        "localhost\nrunning": [
            1122085995
        ],
        "pattern": [
            1122085995
        ],
        "the\ncurl": [
            1122085995
        ],
        "general-purpose\nkubectl-proxy": [
            1122085995
        ],
        "find\nthe": [
            1122085995
        ],
        "/chapter08/kubectl-proxy/)": [
            1122085995
        ],
        "yourself\n": [
            1122085995
        ],
        "curl-with-ambassador\nspec:\n": [
            1122085995
        ],
        "main\nlisting": [
            1122085995
        ],
        "curl-with-ambassadoryaml\ncontainer:\nmain\ncontainer:\nambassador\nhttphttps\napi": [
            1122085995
        ],
        "server\npod\nfigure": [
            1122085995
        ],
        "\n\n245talking": [
            1122085995
        ],
        "tutum/curl\n": [
            1122085995
        ],
        "luksa/kubectl-proxy:16.2": [
            1122085995
        ],
        "with\n$": [
            1122085995
        ],
        "curl-with-ambassador": [
            1122085995
        ],
        "bash\nroot@curl-with-ambassador:/#\nyour": [
            1122085995
        ],
        "container\nhence": [
            1122085995
        ],
        "option\ntalking": [
            1122085995
        ],
        "ambassador\nnext": [
            1122085995
        ],
        "by\ndefault": [
            1122085995
        ],
        "pod\nshare": [
            1122085995
        ],
        "local-\nhost:8001\n": [
            1122085995
        ],
        "listing\nroot@curl-with-ambassador:/#": [
            1122085995
        ],
        "]\n}\nsuccess!": [
            1122085995
        ],
        "time\nyou": [
            1122085995
        ],
        "87.": [
            1122085995
        ],
        "the\nplain": [
            1122085995
        ],
        "(without": [
            1122085995
        ],
        "headers)": [
            1122085995
        ],
        "validating": [
            1122085995
        ],
        "certificate\n": [
            1122085995
        ],
        "the\ncomplexities": [
            1122085995
        ],
        "apps\nregardless": [
            1122085995
        ],
        "downside": [
            1122085995
        ],
        "resources\nlisting": [
            1122085995
        ],
        "kubectl-proxy": [
            1122085995
        ],
        "\n\n246chapter": [
            1122085995
        ],
        "applications\n82.4using": [
            1122085995
        ],
        "server\nif": [
            1122085995
        ],
        "can\noften": [
            1122085995
        ],
        "library": [
            1122085995
        ],
        "especially\nif": [
            1122085995
        ],
        "\nkubectl-proxy": [
            1122085995
        ],
        "plan": [
            1122085995
        ],
        "it’s\nbetter": [
            1122085995
        ],
        "libraries\nusing": [
            1122085995
        ],
        "libraries\ncurrently": [
            1122085995
        ],
        "api\nmachinery": [
            1122085995
        ],
        "(sig):\ngolang": [
            1122085995
        ],
        "client—https://githubcom/kubernetes/client-go\npython—https://github.com/kubernetes-incubator/client-python\nnotethe": [
            1122085995
        ],
        "community": [
            1122085995
        ],
        "groups\n(sigs)": [
            1122085995
        ],
        "kubernetes\necosystem": [
            1122085995
        ],
        "https://githubcom/kubernetes/com-\nmunity/blob/master/sig-list.md.\nin": [
            1122085995
        ],
        "officially": [
            1122085995
        ],
        "user-contributed": [
            1122085995
        ],
        "cli-\nent": [
            1122085995
        ],
        "languages:\njava": [
            1122085995
        ],
        "fabric8—https://githubcom/fabric8io/kubernetes-client\njava": [
            1122085995
        ],
        "amdatu—https://bitbucketorg/amdatulabs/amdatu-kubernetes\nnode.js": [
            1122085995
        ],
        "tenxcloud—https://githubcom/tenxcloud/node-kubernetes-client\nnode.js": [
            1122085995
        ],
        "godaddy—https://githubcom/godaddy/kubernetes-client\nphp—https://github.com/devstub/kubernetes-api-php-client\nanother": [
            1122085995
        ],
        "php": [
            1122085995
        ],
        "client—https://githubcom/maclof/kubernetes-client\ncontainer:": [
            1122085995
        ],
        "main\napi": [
            1122085995
        ],
        "server\nsleepcurl\ncontainer:": [
            1122085995
        ],
        "ambassador\nkubectl": [
            1122085995
        ],
        "proxy\nport": [
            1122085995
        ],
        "8001\nget": [
            1122085995
        ],
        "http://localhost:8001\nget": [
            1122085995
        ],
        "https://kubernetes:443\nauthorization:": [
            1122085995
        ],
        "<token>\npod\nfigure": [
            1122085995
        ],
        "offloading": [
            1122085995
        ],
        "\nambassador": [
            1122085995
        ],
        "\n\n247talking": [
            1122085995
        ],
        "server\nruby—https://githubcom/ch00k/kubr\nanother": [
            1122085995
        ],
        "ruby": [
            1122085995
        ],
        "client—https://githubcom/abonas/kubeclient\nclojure—https://github.com/yanatan16/clj-kubernetes-api\nscala—https://github.com/doriordan/skuber\nperl—https://metacpan.org/pod/net::kubernetes\nthese": [
            1122085995
        ],
        "won’t\nneed": [
            1122085995
        ],
        "fabric8": [
            1122085995
        ],
        "client\nto": [
            1122085995
        ],
        "fabric8\nkubernetes": [
            1122085995
        ],
        "client\nimport": [
            1122085995
        ],
        "javautil.arrays;\nimport": [
            1122085995
        ],
        "iofabric8.kubernetes.api.model.pod;\nimport": [
            1122085995
        ],
        "iofabric8.kubernetes.api.model.podlist;\nimport": [
            1122085995
        ],
        "iofabric8.kubernetes.client.defaultkubernetesclient;\nimport": [
            1122085995
        ],
        "iofabric8.kubernetes.client.kubernetesclient;\npublic": [
            1122085995
        ],
        "void": [
            1122085995
        ],
        "main(string[]": [
            1122085995
        ],
        "args)": [
            1122085995
        ],
        "throws": [
            1122085995
        ],
        "exception": [
            1122085995
        ],
        "kubernetesclient": [
            1122085995
        ],
        "defaultkubernetesclient();\n": [
            1122085995
        ],
        "//": [
            1122085995
        ],
        "podlist": [
            1122085995
        ],
        "clientpods().innamespace(default\").list();\n": [
            1122085995
        ],
        "podsgetitems().stream()\n": [
            1122085995
        ],
        "foreach(s": [
            1122085995
        ],
        "systemout.println(found": [
            1122085995
        ],
        "+\n": [
            1122085995
        ],
        "sgetmetadata().getname()));\n": [
            1122085995
        ],
        "systemout.println(creating": [
            1122085995
        ],
        "pod);\n": [
            1122085995
        ],
        "clientpods().innamespace(default\")\n": [
            1122085995
        ],
        "createnew()\n": [
            1122085995
        ],
        "withnewmetadata()\n": [
            1122085995
        ],
        "withname(programmatically-created-pod\")\n": [
            1122085995
        ],
        "endmetadata()\n": [
            1122085995
        ],
        "withnewspec()\n": [
            1122085995
        ],
        "addnewcontainer()\n": [
            1122085995
        ],
        "withname(main\")\n": [
            1122085995
        ],
        "withimage(busybox\")\n": [
            1122085995
        ],
        "withcommand(arrays.aslist(sleep\"": [
            1122085995
        ],
        "99999\"))\n": [
            1122085995
        ],
        "endcontainer()\n": [
            1122085995
        ],
        "endspec()\n": [
            1122085995
        ],
        "done();\n": [
            1122085995
        ],
        "systemout.println(created": [
            1122085995
        ],
        "(add": [
            1122085995
        ],
        "it)\n": [
            1122085995
        ],
        "edit()\n": [
            1122085995
        ],
        "editmetadata()\nlisting": [
            1122085995
        ],
        "client\n": [
            1122085995
        ],
        "\n\n248chapter": [
            1122085995
        ],
        "addtolabels(foo\"": [
            1122085995
        ],
        "bar\")\n": [
            1122085995
        ],
        "systemout.println(added": [
            1122085995
        ],
        "foo=bar": [
            1122085995
        ],
        "systemout.println(waiting": [
            1122085995
        ],
        "pod..);\n": [
            1122085995
        ],
        "threadsleep(60000);\n": [
            1122085995
        ],
        "delete();\n": [
            1122085995
        ],
        "systemout.println(deleted": [
            1122085995
        ],
        "}\n}\nthe": [
            1122085995
        ],
        "self-explanatory": [
            1122085995
        ],
        "exposes\na": [
            1122085995
        ],
        "fluent": [
            1122085995
        ],
        "domain-specific-language": [
            1122085995
        ],
        "(dsl)": [
            1122085995
        ],
        "and\nunderstand\nbuilding": [
            1122085995
        ],
        "swagger": [
            1122085995
        ],
        "openapi\nif": [
            1122085995
        ],
        "the\nswagger": [
            1122085995
        ],
        "framework": [
            1122085995
        ],
        "/swaggerapi": [
            1122085995
        ],
        "openapi": [
            1122085995
        ],
        "at\n/swaggerjson.": [
            1122085995
        ],
        "visit": [
            1122085995
        ],
        "http://swaggerio.\nexploring": [
            1122085995
        ],
        "ui\nearlier": [
            1122085995
        ],
        "api\ninstead": [
            1122085995
        ],
        "ui": [
            1122085995
        ],
        "for\nexploring": [
            1122085995
        ],
        "of\nexploring": [
            1122085995
        ],
        "ui\n": [
            1122085995
        ],
        "inte-\ngrated": [
            1122085995
        ],
        "by\nrunning": [
            1122085995
        ],
        "\n--enable-swagger-ui=true": [
            1122085995
        ],
        "option\ntipif": [
            1122085995
        ],
        "the\ncluster:": [
            1122085995
        ],
        "--extra-config=apiserverfeatures.enable-\nswaggerui=true\nafter": [
            1122085995
        ],
        "to:\nhttp(s)://<api": [
            1122085995
        ],
        "server>:<port>/swagger-ui\ni": [
            1122085995
        ],
        "urge": [
            1122085995
        ],
        "patch": [
            1122085995
        ],
        "resources\nor": [
            1122085995
        ],
        "\n\n249summary\n83summary\nafter": [
            1122085995
        ],
        "get\ndata": [
            1122085995
        ],
        "you’ve\nlearned\nhow": [
            1122085995
        ],
        "volume\nhow": [
            1122085995
        ],
        "unit\nthe": [
            1122085995
        ],
        "requires\nhow": [
            1122085995
        ],
        "which\nmay": [
            1122085995
        ],
        "annotations)": [
            1122085995
        ],
        "\nhow": [
            1122085995
        ],
        "proxy\nhow": [
            1122085995
        ],
        "or\ndns": [
            1122085995
        ],
        "kubernetes\nhow": [
            1122085995
        ],
        "itself\nhow": [
            1122085995
        ],
        "simpler\nhow": [
            1122085995
        ],
        "minutes\nin": [
            1122085995
        ],
        "learning\nmore": [
            1122085995
        ],
        "such\ndetails": [
            1122085995
        ],
        "resources—deployments\nand": [
            1122085995
        ],
        "\n\n250\ndeployments:": [
            1122085995
        ],
        "updating\napplications": [
            1122085995
        ],
        "declaratively\nyou": [
            1122085995
        ],
        "them\ninto": [
            1122085995
        ],
        "permanent": [
            1122085995
        ],
        "secret\nand": [
            1122085995
        ],
        "non-secret": [
            1122085995
        ],
        "other\nyou": [
            1122085995
        ],
        "running\nsmaller": [
            1122085995
        ],
        "components—microservices": [
            1122085995
        ],
        "else?": [
            1122085995
        ],
        "to\nupdate": [
            1122085995
        ],
        "move\ntoward": [
            1122085995
        ],
        "zero-downtime": [
            1122085995
        ],
        "using\nonly": [
            1122085995
        ],
        "deployment\nthis": [
            1122085995
        ],
        "covers\nreplacing": [
            1122085995
        ],
        "versions\nupdating": [
            1122085995
        ],
        "pods\nupdating": [
            1122085995
        ],
        "\nresources\nperforming": [
            1122085995
        ],
        "updates\nautomatically": [
            1122085995
        ],
        "blocking": [
            1122085995
        ],
        "rollouts": [
            1122085995
        ],
        "versions\ncontrolling": [
            1122085995
        ],
        "rollout\nreverting": [
            1122085995
        ],
        "version\n": [
            1122085995
        ],
        "\n\n251updating": [
            1122085995
        ],
        "pods\nresource": [
            1122085995
        ],
        "reading—it’s": [
            1122085995
        ],
        "sounds\n9.1updating": [
            1122085995
        ],
        "point\nyou": [
            1122085995
        ],
        "(apps": [
            1122085995
        ],
        "or\nexternal": [
            1122085995
        ],
        "clients)": [
            1122085995
        ],
        "kubernetes\n(shown": [
            1122085995
        ],
        "91).\ninitially": [
            1122085995
        ],
        "application—let’s": [
            1122085995
        ],
        "is\ntagged": [
            1122085995
        ],
        "image\nrepository": [
            1122085995
        ],
        "\nv2": [
            1122085995
        ],
        "with\nthis": [
            1122085995
        ],
        "running\nthe": [
            1122085995
        ],
        "following:\ndelete": [
            1122085995
        ],
        "ones\nstart": [
            1122085995
        ],
        "either\nby": [
            1122085995
        ],
        "or\nsequentially": [
            1122085995
        ],
        "gradually\nboth": [
            1122085995
        ],
        "strategies": [
            1122085995
        ],
        "unavailable": [
            1122085995
        ],
        "option\nrequires": [
            1122085995
        ],
        "schema": [
            1122085995
        ],
        "or\nthe": [
            1122085995
        ],
        "version\nreplicationcontroller\nor": [
            1122085995
        ],
        "replicaset\nclientsservice\npodpodpod\nfigure": [
            1122085995
        ],
        "\n\n252chapter": [
            1122085995
        ],
        "9deployments:": [
            1122085995
        ],
        "declaratively\n": [
            1122085995
        ],
        "kubernetes?": [
            1122085995
        ],
        "manually;": [
            1122085995
        ],
        "automatically\n9.1.1deleting": [
            1122085995
        ],
        "ones\nyou": [
            1122085995
        ],
        "instances\nwith": [
            1122085995
        ],
        "replicationcontroller\ncreates": [
            1122085995
        ],
        "easily\nreplace": [
            1122085995
        ],
        "no\npods": [
            1122085995
        ],
        "92.\nthis": [
            1122085995
        ],
        "downtime\nbetween": [
            1122085995
        ],
        "started\n9.1.2spinning": [
            1122085995
        ],
        "ones\nif": [
            1122085995
        ],
        "downtime": [
            1122085995
        ],
        "and\npod": [
            1122085995
        ],
        "template\nchanged\nv": [
            1122085995
        ],
        "deleted1\nmanually\nreplicationcontroller\nservice\npod:": [
            1122085995
        ],
        "v1pod:": [
            1122085995
        ],
        "v1\npod\ntemplate:": [
            1122085995
        ],
        "v2\nreplicationcontroller\npod\ntemplate:": [
            1122085995
        ],
        "v2\npod:": [
            1122085995
        ],
        "v1\nservice\npod:": [
            1122085995
        ],
        "v2pod:": [
            1122085995
        ],
        "v2\nreplicationcontroller\nservice\npod:": [
            1122085995
        ],
        "v1\npod:": [
            1122085995
        ],
        "v1\nreplicationcontroller\nservice\npod:": [
            1122085995
        ],
        "v2\nshort": [
            1122085995
        ],
        "of\ndowntime": [
            1122085995
        ],
        "here\nv2": [
            1122085995
        ],
        "by\nreplicationcontroller\nfigure": [
            1122085995
        ],
        "\n\n253updating": [
            1122085995
        ],
        "pods\nonly": [
            1122085995
        ],
        "because\nyou’ll": [
            1122085995
        ],
        "replicationcontrol-\nlers": [
            1122085995
        ],
        "far\nswitching": [
            1122085995
        ],
        "once\npods": [
            1122085995
        ],
        "fronted": [
            1122085995
        ],
        "the\ninitial": [
            1122085995
        ],
        "then\nonce": [
            1122085995
        ],
        "93.": [
            1122085995
        ],
        "blue-green\ndeployment": [
            1122085995
        ],
        "cor-\nrectly": [
            1122085995
        ],
        "replicationcontroller\nnoteyou": [
            1122085995
        ],
        "selec-\ntor\n": [
            1122085995
        ],
        "command\nperforming": [
            1122085995
        ],
        "update\ninstead": [
            1122085995
        ],
        "can\nalso": [
            1122085995
        ],
        "replaces": [
            1122085995
        ],
        "slowly\nscaling": [
            1122085995
        ],
        "this\ncase": [
            1122085995
        ],
        "pods\nso": [
            1122085995
        ],
        "directs": [
            1122085995
        ],
        "94.\n": [
            1122085995
        ],
        "laborious": [
            1122085995
        ],
        "dozen": [
            1122085995
        ],
        "proper\norder": [
            1122085995
        ],
        "the\nrolling": [
            1122085995
        ],
        "section\nserviceservice\nreplicationcontroller:\nv1\npod:": [
            1122085995
        ],
        "v1\nreplicationcontroller:\nv2\npod\ntemplate:": [
            1122085995
        ],
        "v2\nreplicationcontroller:\nv1\npod:": [
            1122085995
        ],
        "v2\nfigure": [
            1122085995
        ],
        "ones\n": [
            1122085995
        ],
        "\n\n254chapter": [
            1122085995
        ],
        "declaratively\n92performing": [
            1122085995
        ],
        "\nreplicationcontroller\ninstead": [
            1122085995
        ],
        "can\nhave": [
            1122085995
        ],
        "process\nmuch": [
            1122085995
        ],
        "outdated": [
            1122085995
        ],
        "nev-\nertheless": [
            1122085995
        ],
        "walk": [
            1122085995
        ],
        "historically": [
            1122085995
        ],
        "without\nintroducing": [
            1122085995
        ],
        "\n92.1running": [
            1122085995
        ],
        "app\nobviously": [
            1122085995
        ],
        "web-\napp": [
            1122085995
        ],
        "app\nyou’ll": [
            1122085995
        ],
        "will\nallow": [
            1122085995
        ],
        "i’ve\nalready": [
            1122085995
        ],
        "\nluksa/kubia:v1": [
            1122085995
        ],
        "code\nconst": [
            1122085995
        ],
        "starting..);\nlisting": [
            1122085995
        ],
        "v1/appjs\nservice\npod:": [
            1122085995
        ],
        "v1\nreplication\ncontroller:\nv1\nv1\nreplication\ncontroller:\nv2\npod:": [
            1122085995
        ],
        "v2\nservice\npod:": [
            1122085995
        ],
        "v2\nv2\nreplication\ncontroller:\nv1\nv1\nreplication\ncontroller:\nv2\nv2\nreplication\ncontroller:\nv1\nreplication\ncontroller:\nv2\nv2\nreplication\ncontroller:\nv1\nv1\nv1\nreplication\ncontroller:\nv2\nv2\nfigure": [
            1122085995
        ],
        "\n\n255performing": [
            1122085995
        ],
        "replicationcontroller\nvar": [
            1122085995
        ],
        "responseend(this": [
            1122085995
        ],
        "httpcreateserver(handler);\nwww.listen(8080);\nrunning": [
            1122085995
        ],
        "file\nto": [
            1122085995
        ],
        "to\nenable": [
            1122085995
        ],
        "two\nresources": [
            1122085995
        ],
        "delimited": [
            1122085995
        ],
        "replicationcontroller\nmetadata:\n": [
            1122085995
        ],
        "kubia-v1\nspec:\n": [
            1122085995
        ],
        "luksa/kubia:v1": [
            1122085995
        ],
        "nodejs\n---": [
            1122085995
        ],
        "loadbalancer\n": [
            1122085995
        ],
        "kubia-v1": [
            1122085995
        ],
        "called\nkubia": [
            1122085995
        ],
        "external\nip": [
            1122085995
        ],
        "listing\nlisting": [
            1122085995
        ],
        "kubia-rc-and-service-v1yaml\nthe": [
            1122085995
        ],
        "fronts": [
            1122085995
        ],
        "\nreplicationcontroller\nyou’re": [
            1122085995
        ],
        "image\nyaml": [
            1122085995
        ],
        "\ndefinitions": [
            1122085995
        ],
        "separated": [
            1122085995
        ],
        "dashes\n": [
            1122085995
        ],
        "\n\n256chapter": [
            1122085995
        ],
        "declaratively\n$": [
            1122085995
        ],
        "103.246.195": [
            1122085995
        ],
        "130211.109.222": [
            1122085995
        ],
        "5m\n$": [
            1122085995
        ],
        "true;": [
            1122085995
        ],
        "http://130211.109.222;": [
            1122085995
        ],
        "done\nthis": [
            1122085995
        ],
        "kubia-v1-qr192\nthis": [
            1122085995
        ],
        "kubia-v1-kbtsk\nthis": [
            1122085995
        ],
        "kubia-v1-2321o\n..\nnoteif": [
            1122085995
        ],
        "to\naccess": [
            1122085995
        ],
        "5\n9.2.2performing": [
            1122085995
        ],
        "kubectl\nnext": [
            1122085995
        ],
        "change\nthe": [
            1122085995
        ],
        "“this": [
            1122085995
        ],
        "v2”:\n": [
            1122085995
        ],
        "v2": [
            1122085995
        ],
        "\\n\");\nthis": [
            1122085995
        ],
        "luksa/kubia:v2": [
            1122085995
        ],
        "yourself\nlisting": [
            1122085995
        ],
        "curl\npushing": [
            1122085995
        ],
        "tag\nmodifying": [
            1122085995
        ],
        "pushing": [
            1122085995
        ],
        "idea\nbut": [
            1122085995
        ],
        "\nlatest": [
            1122085995
        ],
        "tag\nthat’s": [
            1122085995
        ],
        "tagging": [
            1122085995
        ],
        "latest)": [
            1122085995
        ],
        "image\nwill": [
            1122085995
        ],
        "same\nimage": [
            1122085995
        ],
        "images)\nthat": [
            1122085995
        ],
        "picked": [
            1122085995
        ],
        "them\nto": [
            1122085995
        ],
        "the\nold": [
            1122085995
        ],
        "version\nwill": [
            1122085995
        ],
        "container’s\nimagepullpolicy": [
            1122085995
        ],
        "\nimagepullpolicy": [
            1122085995
        ],
        "tag\nif": [
            1122085995
        ],
        "at\nall)": [
            1122085995
        ],
        "other\ntag": [
            1122085995
        ],
        "\nifnotpresent": [
            1122085995
        ],
        "properly\nif": [
            1122085995
        ],
        "sure\nyou": [
            1122085995
        ],
        "tag\n": [
            1122085995
        ],
        "\n\n257performing": [
            1122085995
        ],
        "replicationcontroller\nkeep": [
            1122085995
        ],
        "rolling\nupdate": [
            1122085995
        ],
        "rolling-update": [
            1122085995
        ],
        "to\nreplace": [
            1122085995
        ],
        "per-\nforming": [
            1122085995
        ],
        "update\n$": [
            1122085995
        ],
        "kubia-v2": [
            1122085995
        ],
        "--image=luksa/kubia:v2\ncreated": [
            1122085995
        ],
        "kubia-v2\nscaling": [
            1122085995
        ],
        "(keep": [
            1122085995
        ],
        "pods)\n..\nbecause": [
            1122085995
        ],
        "2\nof": [
            1122085995
        ],
        "\nkubia-v2\nand": [
            1122085995
        ],
        "\nkubia-v2": [
            1122085995
        ],
        "95.\nthe": [
            1122085995
        ],
        "\nluksa/kubia:v2": [
            1122085995
        ],
        "image\nand": [
            1122085995
        ],
        "0\n": [
            1122085995
        ],
        "kubia-v2\nname:": [
            1122085995
        ],
        "kubia-v2\nnamespace:": [
            1122085995
        ],
        "default\nimage(s):": [
            1122085995
        ],
        "\nselector:": [
            1122085995
        ],
        "app=kubiadeployment=757d16a0f02f6a5c387f2b5edb62b155\nlabels:": [
            1122085995
        ],
        "\nreplicas:": [
            1122085995
        ],
        "\n..\nlisting": [
            1122085995
        ],
        "initiating": [
            1122085995
        ],
        "kubectl\nlisting": [
            1122085995
        ],
        "update\npod:": [
            1122085995
        ],
        "v1no": [
            1122085995
        ],
        "yetpod:": [
            1122085995
        ],
        "v1\nreplicationcontroller:": [
            1122085995
        ],
        "kubia-v1\nimage:kubia/v1\nreplicas:": [
            1122085995
        ],
        "3\nreplicationcontroller:": [
            1122085995
        ],
        "kubia-v2\nimage:kubia/v2\nreplicas:": [
            1122085995
        ],
        "0\nfigure": [
            1122085995
        ],
        "update\nthe": [
            1122085995
        ],
        "\nrefers": [
            1122085995
        ],
        "image\ninitially": [
            1122085995
        ],
        "\nnumber": [
            1122085995
        ],
        "zero\n": [
            1122085995
        ],
        "\n\n258chapter": [
            1122085995
        ],
        "declaratively\nunderstanding": [
            1122085995
        ],
        "commences\nkubectl": [
            1122085995
        ],
        "copying": [
            1122085995
        ],
        "and\nchanging": [
            1122085995
        ],
        "simple\napp=kubia": [
            1122085995
        ],
        "in\norder": [
            1122085995
        ],
        "replicationcontroller\n": [
            1122085995
        ],
        "new\nand": [
            1122085995
        ],
        "pods\ncreated": [
            1122085995
        ],
        "the\napp=kubia": [
            1122085995
        ],
        "replicationcontrol-\nler’s": [
            1122085995
        ],
        "\napp=kubia?": [
            1122085995
        ],
        "well:\n$": [
            1122085995
        ],
        "kubia-v1\nname:": [
            1122085995
        ],
        "kubia-v1\nnamespace:": [
            1122085995
        ],
        "luksa/kubia:v1\nselector:": [
            1122085995
        ],
        "app=kubiadeployment=3ddd307978b502a5b975ed4045ae4964-orig": [
            1122085995
        ],
        "\nokay": [
            1122085995
        ],
        "selec-\ntor": [
            1122085995
        ],
        "label?\nno": [
            1122085995
        ],
        "modify-\ning": [
            1122085995
        ],
        "labels\nkubia-v1-m33mv": [
            1122085995
        ],
        "app=kubiadeployment=3ddd..\nkubia-v1-nmzw9": [
            1122085995
        ],
        "app=kubiadeployment=3ddd..\nkubia-v1-cdtey": [
            1122085995
        ],
        "app=kubiadeployment=3ddd..\nif": [
            1122085995
        ],
        "their\nlabels": [
            1122085995
        ],
        "selectors\nreplicationcontroller:": [
            1122085995
        ],
        "kubia-v1\nreplicas:": [
            1122085995
        ],
        "3\nselector:app=kubia\ndeployment=3ddd..\nreplicationcontroller:": [
            1122085995
        ],
        "kubia-v2\nreplicas:": [
            1122085995
        ],
        "0\nselector:app=kubia\ndeployment=757d..\ndeployment:": [
            1122085995
        ],
        "3ddd..\napp:": [
            1122085995
        ],
        "v1\ndeployment:": [
            1122085995
        ],
        "v1\nfigure": [
            1122085995
        ],
        "\nupdate\n": [
            1122085995
        ],
        "\n\n259performing": [
            1122085995
        ],
        "replicationcontroller\nkubectl": [
            1122085995
        ],
        "now\nimagine": [
            1122085995
        ],
        "mistake\nhere": [
            1122085995
        ],
        "kill": [
            1122085995
        ],
        "pods—pods": [
            1122085995
        ],
        "actively": [
            1122085995
        ],
        "clients!\nreplacing": [
            1122085995
        ],
        "replicationcontrollers\nafter": [
            1122085995
        ],
        "new\ncontroller": [
            1122085995
        ],
        "scales\ndown": [
            1122085995
        ],
        "printed\nby": [
            1122085995
        ],
        "\nkubectl:\nscaling": [
            1122085995
        ],
        "1\nscaling": [
            1122085995
        ],
        "2\nbecause": [
            1122085995
        ],
        "targeting": [
            1122085995
        ],
        "iterations:\nthis": [
            1122085995
        ],
        "kubia-v2-nmzw9": [
            1122085995
        ],
        "kubia-v1-2321o\nthis": [
            1122085995
        ],
        "\n..\nfigure": [
            1122085995
        ],
        "system\nas": [
            1122085995
        ],
        "continues": [
            1122085995
        ],
        "bigger\npercentage": [
            1122085995
        ],
        "v1\npods": [
            1122085995
        ],
        "original\nrequests": [
            1122085995
        ],
        "version\nreplicationcontroller:": [
            1122085995
        ],
        "2\nselector:app=kubia\ndeployment=3ddd..\nreplicationcontroller:": [
            1122085995
        ],
        "1\nselector:app=kubia\ndeployment=757d..\ndeployment:": [
            1122085995
        ],
        "757d..\napp:": [
            1122085995
        ],
        "v2\ncurl\nservice\nselector:app=kubia\nfigure": [
            1122085995
        ],
        "redirecting": [
            1122085995
        ],
        "\nrolling": [
            1122085995
        ],
        "update\n": [
            1122085995
        ],
        "\n\n260chapter": [
            1122085995
        ],
        "declaratively\nreplicationcontroller": [
            1122085995
        ],
        "will\ndelete": [
            1122085995
        ],
        "listing\n...\nscaling": [
            1122085995
        ],
        "2\nscaling": [
            1122085995
        ],
        "3\nscaling": [
            1122085995
        ],
        "0\nupdate": [
            1122085995
        ],
        "kubia-v1\nreplicationcontroller": [
            1122085995
        ],
        "kubia-v1\"": [
            1122085995
        ],
        "kubia-v2\"\nyou’re": [
            1122085995
        ],
        "all\nthroughout": [
            1122085995
        ],
        "gotten": [
            1122085995
        ],
        "\n92.3understanding": [
            1122085995
        ],
        "obsolete\nat": [
            1122085995
        ],
        "updates\nthan": [
            1122085995
        ],
        "bet-\nter": [
            1122085995
        ],
        "introduced?": [
            1122085995
        ],
        "starters": [
            1122085995
        ],
        "created\nokay": [
            1122085995
        ],
        "perfectly": [
            1122085995
        ],
        "my\nreplicationcontroller\ns": [
            1122085995
        ],
        "go\naround": [
            1122085995
        ],
        "yelling": [
            1122085995
        ],
        "“who’s": [
            1122085995
        ],
        "messing": [
            1122085995
        ],
        "controllers!?!?”": [
            1122085995
        ],
        "importantly": [
            1122085995
        ],
        "paid": [
            1122085995
        ],
        "used\nyou": [
            1122085995
        ],
        "\n--v": [
            1122085995
        ],
        "triggering\nthe": [
            1122085995
        ],
        "update:\n$": [
            1122085995
        ],
        "--image=luksa/kubia:v2": [
            1122085995
        ],
        "--v": [
            1122085995
        ],
        "6\ntipusing": [
            1122085995
        ],
        "server\nusing": [
            1122085995
        ],
        "to\n/api/v1/namespaces/default/replicationcontrollers/kubia-v1\nwhich": [
            1122085995
        ],
        "resource\nthese": [
            1122085995
        ],
        "shows\nlisting": [
            1122085995
        ],
        "rolling-update\n": [
            1122085995
        ],
        "\n\n261using": [
            1122085995
        ],
        "declaratively\nthat": [
            1122085995
        ],
        "\ntipuse": [
            1122085995
        ],
        "commands\nto": [
            1122085995
        ],
        "\nbut": [
            1122085995
        ],
        "client\ninstead": [
            1122085995
        ],
        "server?": [
            1122085995
        ],
        "you\nlost": [
            1122085995
        ],
        "update?": [
            1122085995
        ],
        "interrupted": [
            1122085995
        ],
        "mid-way": [
            1122085995
        ],
        "in\nan": [
            1122085995
        ],
        "intermediate": [
            1122085995
        ],
        "state\n": [
            1122085995
        ],
        "is\nbecause": [
            1122085995
        ],
        "stressed": [
            1122085995
        ],
        "about\nyou": [
            1122085995
        ],
        "that\nstate": [
            1122085995
        ],
        "deployed\nand": [
            1122085995
        ],
        "one—you": [
            1122085995
        ],
        "replicas\nand": [
            1122085995
        ],
        "defini-\ntions": [
            1122085995
        ],
        "deployment\nwhich": [
            1122085995
        ],
        "\n93using": [
            1122085995
        ],
        "declaratively\na": [
            1122085995
        ],
        "and\nupdating": [
            1122085995
        ],
        "or\na": [
            1122085995
        ],
        "lower-level": [
            1122085995
        ],
        "concepts\n": [
            1122085995
        ],
        "underneath\n(eventually": [
            1122085995
        ],
        "replica-\nsets": [
            1122085995
        ],
        "pods\nare": [
            1122085995
        ],
        "deployment’s": [
            1122085995
        ],
        "deployment\ndirectly": [
            1122085995
        ],
        "98).\nyou": [
            1122085995
        ],
        "complicate": [
            1122085995
        ],
        "object\non": [
            1122085995
        ],
        "suffices": [
            1122085995
        ],
        "set\nof": [
            1122085995
        ],
        "demonstrates\nwhen": [
            1122085995
        ],
        "and\npodsreplicasetdeployment\nfigure": [
            1122085995
        ],
        "supervises": [
            1122085995
        ],
        "\ndeployment’s": [
            1122085995
        ],
        "\n\n262chapter": [
            1122085995
        ],
        "declaratively\ncoordinate": [
            1122085995
        ],
        "dance": [
            1122085995
        ],
        "stepping": [
            1122085995
        ],
        "each\nother’s": [
            1122085995
        ],
        "toes": [
            1122085995
        ],
        "coordinating": [
            1122085995
        ],
        "resource\ntakes": [
            1122085995
        ],
        "(it’s": [
            1122085995
        ],
        "process\nrunning": [
            1122085995
        ],
        "that;": [
            1122085995
        ],
        "11)\n": [
            1122085995
        ],
        "constructs": [
            1122085995
        ],
        "app\nmuch": [
            1122085995
        ],
        "deployment\nresource": [
            1122085995
        ],
        "pages\n9.3.1creating": [
            1122085995
        ],
        "deployment\ncreating": [
            1122085995
        ],
        "a\ndeployment": [
            1122085995
        ],
        "deployment\nstrategy": [
            1122085995
        ],
        "manifest\nlet’s": [
            1122085995
        ],
        "this\nchapter": [
            1122085995
        ],
        "replicationcontroller\nas": [
            1122085995
        ],
        "the\nmodified": [
            1122085995
        ],
        "apps/v1beta1": [
            1122085995
        ],
        "luksa/kubia:v1\n": [
            1122085995
        ],
        "nodejs\nnoteyou’ll": [
            1122085995
        ],
        "extensions/\nv1beta1\n": [
            1122085995
        ],
        "and\ndifferent": [
            1122085995
        ],
        "version\nbecause": [
            1122085995
        ],
        "\nkubia-v1": [
            1122085995
        ],
        "version\nstuff": [
            1122085995
        ],
        "version\nlisting": [
            1122085995
        ],
        "kubia-deployment-v1yaml\ndeployments": [
            1122085995
        ],
        "v1beta1\nyou’ve": [
            1122085995
        ],
        "deployment\nthere’s": [
            1122085995
        ],
        "deployment\n": [
            1122085995
        ],
        "\n\n263using": [
            1122085995
        ],
        "declaratively\ncreating": [
            1122085995
        ],
        "resource\nbefore": [
            1122085995
        ],
        "replicationcontrollers\nand": [
            1122085995
        ],
        "the\n--all": [
            1122085995
        ],
        "--all\nyou’re": [
            1122085995
        ],
        "deployment:": [
            1122085995
        ],
        "kubia-deployment-v1yaml": [
            1122085995
        ],
        "--record\ndeployment": [
            1122085995
        ],
        "created\ntipbe": [
            1122085995
        ],
        "--record": [
            1122085995
        ],
        "history": [
            1122085995
        ],
        "later\ndisplaying": [
            1122085995
        ],
        "rollout\nyou": [
            1122085995
        ],
        "deployment\ncommands": [
            1122085995
        ],
        "additional\ncommand": [
            1122085995
        ],
        "status:\n$": [
            1122085995
        ],
        "kubia\ndeployment": [
            1122085995
        ],
        "out\naccording": [
            1122085995
        ],
        "age\nkubia-1506449474-otnnh": [
            1122085995
        ],
        "14s\nkubia-1506449474-vmn7s": [
            1122085995
        ],
        "14s\nkubia-1506449474-xis6m": [
            1122085995
        ],
        "14s\nunderstanding": [
            1122085995
        ],
        "pods\ntake": [
            1122085995
        ],
        "replicationcontroller\nto": [
            1122085995
        ],
        "ran-\ndomly": [
            1122085995
        ],
        "\nkubia-v1-m33mv)": [
            1122085995
        ],
        "numeric": [
            1122085995
        ],
        "names\nwhat": [
            1122085995
        ],
        "exactly?\n": [
            1122085995
        ],
        "hashed": [
            1122085995
        ],
        "deploy-\nment": [
            1122085995
        ],
        "deployment\ndoesn’t": [
            1122085995
        ],
        "managing\nto": [
            1122085995
        ],
        "deployment:\n$": [
            1122085995
        ],
        "replicasets\nname": [
            1122085995
        ],
        "age\nkubia-1506449474": [
            1122085995
        ],
        "10s\nthe": [
            1122085995
        ],
        "hash": [
            1122085995
        ],
        "see\nlater": [
            1122085995
        ],
        "replicasets—one": [
            1122085995
        ],
        "\n\n264chapter": [
            1122085995
        ],
        "declaratively\ntemplate": [
            1122085995
        ],
        "deployment\nto": [
            1122085995
        ],
        "(possibly": [
            1122085995
        ],
        "existing)": [
            1122085995
        ],
        "pod\ntemplate\naccessing": [
            1122085995
        ],
        "service\nwith": [
            1122085995
        ],
        "good-enough": [
            1122085995
        ],
        "should\nuse": [
            1122085995
        ],
        "hasn’t\nbeen": [
            1122085995
        ],
        "things\nwith": [
            1122085995
        ],
        "superior": [
            1122085995
        ],
        "will\nbecome": [
            1122085995
        ],
        "through\na": [
            1122085995
        ],
        "compares": [
            1122085995
        ],
        "replicationcontroller\n9.3.2updating": [
            1122085995
        ],
        "deployment\npreviously": [
            1122085995
        ],
        "explicitly\ntell": [
            1122085995
        ],
        "even\nhad": [
            1122085995
        ],
        "old\none": [
            1122085995
        ],
        "original\nreplicationcontroller": [
            1122085995
        ],
        "had\nto": [
            1122085995
        ],
        "roll-\ning": [
            1122085995
        ],
        "thing\nyou": [
            1122085995
        ],
        "and\nkubernetes": [
            1122085995
        ],
        "what’s\ndefined": [
            1122085995
        ],
        "or\ndown": [
            1122085995
        ],
        "new\ndesired": [
            1122085995
        ],
        "state\nunderstanding": [
            1122085995
        ],
        "strategies\nhow": [
            1122085995
        ],
        "governed": [
            1122085995
        ],
        "strategy": [
            1122085995
        ],
        "(the\nstrategy": [
            1122085995
        ],
        "\nrollingupdate)": [
            1122085995
        ],
        "which\ndeletes": [
            1122085995
        ],
        "a\nreplicationcontroller’s": [
            1122085995
        ],
        "about\nthis": [
            1122085995
        ],
        "91.1).\n": [
            1122085995
        ],
        "\nrecreate": [
            1122085995
        ],
        "are\ncreated": [
            1122085995
        ],
        "involve": [
            1122085995
        ],
        "app\nbecomes": [
            1122085995
        ],
        "unavailable\n": [
            1122085995
        ],
        "\n\n265using": [
            1122085995
        ],
        "rollingupdate": [
            1122085995
        ],
        "one\nwhile": [
            1122085995
        ],
        "throughout\nthe": [
            1122085995
        ],
        "requests\nthis": [
            1122085995
        ],
        "lower": [
            1122085995
        ],
        "above\nor": [
            1122085995
        ],
        "only\nwhen": [
            1122085995
        ],
        "time\nslowing": [
            1122085995
        ],
        "purposes\nin": [
            1122085995
        ],
        "down\nthe": [
            1122085995
        ],
        "a\nrolling": [
            1122085995
        ],
        "fashion": [
            1122085995
        ],
        "\nminreadyseconds": [
            1122085995
        ],
        "for\nnow": [
            1122085995
        ],
        "command\n$": [
            1122085995
        ],
        "{spec\":": [
            1122085995
        ],
        "{minreadyseconds\":": [
            1122085995
        ],
        "10}}\nkubia\"": [
            1122085995
        ],
        "patched\ntipthe": [
            1122085995
        ],
        "property\nor": [
            1122085995
        ],
        "defi-\nnition": [
            1122085995
        ],
        "doesn’t\ncause": [
            1122085995
        ],
        "template\nchanging": [
            1122085995
        ],
        "trigger": [
            1122085995
        ],
        "indi-\nvidual": [
            1122085995
        ],
        "way\ntriggering": [
            1122085995
        ],
        "update\nif": [
            1122085995
        ],
        "progresses": [
            1122085995
        ],
        "again\nin": [
            1122085995
        ],
        "replace\nthe": [
            1122085995
        ],
        "service):\n$": [
            1122085995
        ],
        "done\nto": [
            1122085995
        ],
        "or\nusing": [
            1122085995
        ],
        "\npatch": [
            1122085995
        ],
        "image\ncommand": [
            1122085995
        ],
        "container\n(replicationcontrollers": [
            1122085995
        ],
        "modify\nyour": [
            1122085995
        ],
        "nodejs=luksa/kubia:v2\ndeployment": [
            1122085995
        ],
        "updated\nwhen": [
            1122085995
        ],
        "\nnodejs": [
            1122085995
        ],
        "(from\n:v1)": [
            1122085995
        ],
        "99.\n": [
            1122085995
        ],
        "\n\n266chapter": [
            1122085995
        ],
        "declaratively\nways": [
            1122085995
        ],
        "resources\nover": [
            1122085995
        ],
        "existing\nobject": [
            1122085995
        ],
        "refresh": [
            1122085995
        ],
        "memory\nall": [
            1122085995
        ],
        "process\nimage": [
            1122085995
        ],
        "registry\npod": [
            1122085995
        ],
        "template\ndeployment\nkubectl": [
            1122085995
        ],
        "image..\nluksa/kubia:v2\ncontainer:\nnodejs\n:v1\n:v2\nimage": [
            1122085995
        ],
        "template\ndeployment\ncontainer:\nnodejs\n:v1:v2\nfigure": [
            1122085995
        ],
        "image\ntable": [
            1122085995
        ],
        "kubernetes\nmethodwhat": [
            1122085995
        ],
        "does\nkubectl": [
            1122085995
        ],
        "editopens": [
            1122085995
        ],
        "updated\nexample:": [
            1122085995
        ],
        "kubia\nkubectl": [
            1122085995
        ],
        "patchmodifies": [
            1122085995
        ],
        "object\nexample:": [
            1122085995
        ],
        "\n{template\":": [
            1122085995
        ],
        "{containers\":": [
            1122085995
        ],
        "[{name\":": [
            1122085995
        ],
        "\nnodejs\"": [
            1122085995
        ],
        "image\":": [
            1122085995
        ],
        "luksa/kubia:v2\"}]}}}}\nkubectl": [
            1122085995
        ],
        "applymodifies": [
            1122085995
        ],
        "applying": [
            1122085995
        ],
        "\njson": [
            1122085995
        ],
        "\nit’s": [
            1122085995
        ],
        "\ncase": [
            1122085995
        ],
        "patch)\nexample:": [
            1122085995
        ],
        "kubia-deployment-v2yaml\nkubectl": [
            1122085995
        ],
        "replacereplaces": [
            1122085995
        ],
        "con-\ntrast": [
            1122085995
        ],
        "\nexist;": [
            1122085995
        ],
        "error\nexample:": [
            1122085995
        ],
        "imagechanges": [
            1122085995
        ],
        "replicaset\nexample:": [
            1122085995
        ],
        "\nnodejs=luksa/kubia:v2\n": [
            1122085995
        ],
        "\n\n267using": [
            1122085995
        ],
        "declaratively\nif": [
            1122085995
        ],
        "then\nmore": [
            1122085995
        ],
        "pod\ns": [
            1122085995
        ],
        "remain-\ning": [
            1122085995
        ],
        "per-\nformed": [
            1122085995
        ],
        "\nkubectl\nunderstanding": [
            1122085995
        ],
        "awesomeness": [
            1122085995
        ],
        "deployments\nlet’s": [
            1122085995
        ],
        "version—by": [
            1122085995
        ],
        "single\nfield!": [
            1122085995
        ],
        "performed\nthe": [
            1122085995
        ],
        "you\nused": [
            1122085995
        ],
        "than\nhaving": [
            1122085995
        ],
        "waiting\naround": [
            1122085995
        ],
        "completed\nnotebe": [
            1122085995
        ],
        "secret)": [
            1122085995
        ],
        "an\nupdate": [
            1122085995
        ],
        "con-\nfig": [
            1122085995
        ],
        "references\nthe": [
            1122085995
        ],
        "replicaset\nwas": [
            1122085995
        ],
        "scaled\ndown": [
            1122085995
        ],
        "910).\nyou": [
            1122085995
        ],
        "24m\nkubia-1581357123": [
            1122085995
        ],
        "23m\npods:": [
            1122085995
        ],
        "v1\nreplicaset:": [
            1122085995
        ],
        "v1\nreplicas:": [
            1122085995
        ],
        "--\nbeforeafter\nreplicaset:": [
            1122085995
        ],
        "v2\nreplicas:": [
            1122085995
        ],
        "++\ndeployment\npods:": [
            1122085995
        ],
        "v2\nreplicaset:": [
            1122085995
        ],
        "v2\ndeployment\nfigure": [
            1122085995
        ],
        "\n\n268chapter": [
            1122085995
        ],
        "declaratively\nsimilar": [
            1122085995
        ],
        "new\nreplicaset": [
            1122085995
        ],
        "what\nthe": [
            1122085995
        ],
        "inactive": [
            1122085995
        ],
        "them\ndirectly": [
            1122085995
        ],
        "resource;": [
            1122085995
        ],
        "underlying\nreplicasets": [
            1122085995
        ],
        "multiple\nreplicationcontrollers": [
            1122085995
        ],
        "apparent": [
            1122085995
        ],
        "a\nrollout": [
            1122085995
        ],
        "rollout\nprocess": [
            1122085995
        ],
        "now\n9.3.3rolling": [
            1122085995
        ],
        "deployment\nyou’re": [
            1122085995
        ],
        "3\nfirst": [
            1122085995
        ],
        "app\nin": [
            1122085995
        ],
        "four\nrequests": [
            1122085995
        ],
        "fifth": [
            1122085995
        ],
        "onward": [
            1122085995
        ],
        "internal\nserver": [
            1122085995
        ],
        "(http": [
            1122085995
        ],
        "500)": [
            1122085995
        ],
        "with\nall": [
            1122085995
        ],
        "bold\nconst": [
            1122085995
        ],
        "require(os');\nvar": [
            1122085995
        ],
        "requestcount": [
            1122085995
        ],
        "0;\nconsolelog(kubia": [
            1122085995
        ],
        "(++requestcount": [
            1122085995
        ],
        ">=": [
            1122085995
        ],
        "5)": [
            1122085995
        ],
        "responsewritehead(500);\n": [
            1122085995
        ],
        "responseend(some": [
            1122085995
        ],
        "occurred!": [
            1122085995
        ],
        "\noshostname()": [
            1122085995
        ],
        "\\n\");\n": [
            1122085995
        ],
        "return;\n": [
            1122085995
        ],
        "v3": [
            1122085995
        ],
        "httpcreateserver(handler);\nwww.listen(8080);": [
            1122085995
        ],
        "error\nwith": [
            1122085995
        ],
        "“some": [
            1122085995
        ],
        "occurred..”\nlisting": [
            1122085995
        ],
        "version):": [
            1122085995
        ],
        "v3/appjs\n": [
            1122085995
        ],
        "\n\n269using": [
            1122085995
        ],
        "declaratively\ndeploying": [
            1122085995
        ],
        "3\ni’ve": [
            1122085995
        ],
        "luksa/kubia:v3": [
            1122085995
        ],
        "this\nnew": [
            1122085995
        ],
        "again:": [
            1122085995
        ],
        "nodejs=luksa/kubia:v3\ndeployment": [
            1122085995
        ],
        "updated\nyou": [
            1122085995
        ],
        "kubia\nwaiting": [
            1122085995
        ],
        "finish:": [
            1122085995
        ],
        "updated..\nwaiting": [
            1122085995
        ],
        "termination..\ndeployment": [
            1122085995
        ],
        "out\nthe": [
            1122085995
        ],
        "errors\n$": [
            1122085995
        ],
        "kubia-1914148340-lalmx\nthis": [
            1122085995
        ],
        "kubia-1914148340-bz35w\nthis": [
            1122085995
        ],
        "kubia-1914148340-w0voh\n..\nthis": [
            1122085995
        ],
        "kubia-1914148340-w0voh\nsome": [
            1122085995
        ],
        "kubia-1914148340-lalmx\nsome": [
            1122085995
        ],
        "kubia-1914148340-bz35w\nsome": [
            1122085995
        ],
        "kubia-1914148340-w0voh\nundoing": [
            1122085995
        ],
        "93.6": [
            1122085995
        ],
        "automatically\nbut": [
            1122085995
        ],
        "luckily\ndeployments": [
            1122085995
        ],
        "roll": [
            1122085995
        ],
        "undo": [
            1122085995
        ],
        "back\nthis": [
            1122085995
        ],
        "rolls": [
            1122085995
        ],
        "\ntipthe": [
            1122085995
        ],
        "in\nprogress": [
            1122085995
        ],
        "roll-\nout": [
            1122085995
        ],
        "again\nlisting": [
            1122085995
        ],
        "\n\n270chapter": [
            1122085995
        ],
        "declaratively\ndisplaying": [
            1122085995
        ],
        "history\nrolling": [
            1122085995
        ],
        "as\nyou’ll": [
            1122085995
        ],
        "rollout\ncompletes": [
            1122085995
        ],
        "revi-\nsion": [
            1122085995
        ],
        "kubia\ndeployments": [
            1122085995
        ],
        "kubia\":\nrevision": [
            1122085995
        ],
        "change-cause\n2": [
            1122085995
        ],
        "nodejs=luksa/kubia:v2\n3": [
            1122085995
        ],
        "nodejs=luksa/kubia:v3\nremember": [
            1122085995
        ],
        "deploy-\nment?": [
            1122085995
        ],
        "\nchange-cause": [
            1122085995
        ],
        "empty\nmaking": [
            1122085995
        ],
        "revision\nrolling": [
            1122085995
        ],
        "revision\nyou": [
            1122085995
        ],
        "--to-revision=1\nremember": [
            1122085995
        ],
        "time?": [
            1122085995
        ],
        "represents": [
            1122085995
        ],
        "911.": [
            1122085995
        ],
        "that\nspecific": [
            1122085995
        ],
        "specific\nrevision": [
            1122085995
        ],
        "it\nbut": [
            1122085995
        ],
        "cluttering": [
            1122085995
        ],
        "length": [
            1122085995
        ],
        "\nrevisionhistorylimit": [
            1122085995
        ],
        "revision\nare": [
            1122085995
        ],
        "pre-\nserved)": [
            1122085995
        ],
        "\ndeployment\nv1": [
            1122085995
        ],
        "replicaset\nreplicaset\npods:": [
            1122085995
        ],
        "v1\nreplicasetreplicasetreplicaset\nrevision": [
            1122085995
        ],
        "2revision": [
            1122085995
        ],
        "4revision": [
            1122085995
        ],
        "3revision": [
            1122085995
        ],
        "1\nrevision": [
            1122085995
        ],
        "historycurrent": [
            1122085995
        ],
        "revision\nfigure": [
            1122085995
        ],
        "history\n": [
            1122085995
        ],
        "\n\n271using": [
            1122085995
        ],
        "declaratively\nnotethe": [
            1122085995
        ],
        "extensions/v1beta1": [
            1122085995
        ],
        "default\nrevisionhistorylimit": [
            1122085995
        ],
        "10\n9.3.4controlling": [
            1122085995
        ],
        "rollout\nwhen": [
            1122085995
        ],
        "tracked": [
            1122085995
        ],
        "kubectl\nrollout\n": [
            1122085995
        ],
        "it\nbecame": [
            1122085995
        ],
        "continued": [
            1122085995
        ],
        "and\nold": [
            1122085995
        ],
        "maxsurge": [
            1122085995
        ],
        "maxunavailable": [
            1122085995
        ],
        "strategy\ntwo": [
            1122085995
        ],
        "\nmaxsurge": [
            1122085995
        ],
        "the\nrollingupdate": [
            1122085995
        ],
        "sub-property": [
            1122085995
        ],
        "strategy:\n": [
            1122085995
        ],
        "rollingupdate:\n": [
            1122085995
        ],
        "maxsurge:": [
            1122085995
        ],
        "1\n": [
            1122085995
        ],
        "maxunavailable:": [
            1122085995
        ],
        "rollingupdate\nwhat": [
            1122085995
        ],
        "92.\nbecause": [
            1122085995
        ],
        "properties\ndefault": [
            1122085995
        ],
        "strategy\ntable": [
            1122085995
        ],
        "update\npropertywhat": [
            1122085995
        ],
        "does\nmaxsurgedetermines": [
            1122085995
        ],
        "\ncount": [
            1122085995
        ],
        "\n25%": [
            1122085995
        ],
        "\nset": [
            1122085995
        ],
        "\ntime": [
            1122085995
        ],
        "converting": [
            1122085995
        ],
        "percentage": [
            1122085995
        ],
        "rounded": [
            1122085995
        ],
        "\nabsolute": [
            1122085995
        ],
        "allowed)\nmaxunavailabledetermines": [
            1122085995
        ],
        "relative": [
            1122085995
        ],
        "avail-\nable": [
            1122085995
        ],
        "75%": [
            1122085995
        ],
        "\ndown": [
            1122085995
        ],
        "\none": [
            1122085995
        ],
        "\nalso": [
            1122085995
        ],
        "percentage\n": [
            1122085995
        ],
        "\n\n272chapter": [
            1122085995
        ],
        "declaratively\nmaxunavailable": [
            1122085995
        ],
        "pods\nhad": [
            1122085995
        ],
        "912.\nunderstanding": [
            1122085995
        ],
        "property\nthe": [
            1122085995
        ],
        "defaults—it": [
            1122085995
        ],
        "both\nmaxsurge": [
            1122085995
        ],
        "max-\nsurge\n": [
            1122085995
        ],
        "0)": [
            1122085995
        ],
        "this\nmakes": [
            1122085995
        ],
        "unwind": [
            1122085995
        ],
        "913.\nv1\nnumber\nof": [
            1122085995
        ],
        "pods\n3\n4\n2\n1\ntime\nv1\n3": [
            1122085995
        ],
        "available\n1": [
            1122085995
        ],
        "unavailable\ncreate\none\nv2": [
            1122085995
        ],
        "pod\n4": [
            1122085995
        ],
        "available\n3": [
            1122085995
        ],
        "unavailable\n4": [
            1122085995
        ],
        "unavailable\nmaxsurge=": [
            1122085995
        ],
        "1\nmaxunavailable=": [
            1122085995
        ],
        "0\ndesired": [
            1122085995
        ],
        "3\n3": [
            1122085995
        ],
        "available\nv2\nv1v1v2v2\nv1v1\nv1\nv1\nv1\nv1\nv1v1\nv1\nv1v2v2v2v2\nv2v2v2\nv2\nv1\nv2\nv2v2v2\n4": [
            1122085995
        ],
        "available\nwait\nuntil\nit’s\navailable\ndelete\none": [
            1122085995
        ],
        "v1\npod": [
            1122085995
        ],
        "and\ncreate": [
            1122085995
        ],
        "one\nv2": [
            1122085995
        ],
        "pod\nwait\nuntil\nit’s\navailable\ndelete\none": [
            1122085995
        ],
        "pod\nwait\nuntil\nit’s\navailable\ndelete\nlast\nv1": [
            1122085995
        ],
        "\nv1\nnumber\nof": [
            1122085995
        ],
        "pods\n3\n4\n2\n1\ntime\nv1\n2": [
            1122085995
        ],
        "available\n2": [
            1122085995
        ],
        "unavailable\n3": [
            1122085995
        ],
        "available\nmaxsurge=": [
            1122085995
        ],
        "1\ndesired": [
            1122085995
        ],
        "3\nv1v1\nv1\nv1\nv1\nv2\nv2\nv2\nv2\nv2v2\nv2\nv2\nv2\nv2\nwait": [
            1122085995
        ],
        "until\nboth": [
            1122085995
        ],
        "are\navailable\ndelete\ntwo": [
            1122085995
        ],
        "pod\ndelete": [
            1122085995
        ],
        "two\nv2": [
            1122085995
        ],
        "pods\nwait\nuntil": [
            1122085995
        ],
        "it’s\navailable\nfigure": [
            1122085995
        ],
        "maxsurge=1": [
            1122085995
        ],
        "maxunavailable=1\n": [
            1122085995
        ],
        "\n\n273using": [
            1122085995
        ],
        "declaratively\nin": [
            1122085995
        ],
        "three\nonly": [
            1122085995
        ],
        "immediately\ndeletes": [
            1122085995
        ],
        "and\nthat": [
            1122085995
        ],
        "exceeded": [
            1122085995
        ],
        "this\ncase—three": [
            1122085995
        ],
        "\nmaxsurge)": [
            1122085995
        ],
        "remaining": [
            1122085995
        ],
        "grasp": [
            1122085995
        ],
        "\nmaxunavailable": [
            1122085995
        ],
        "second\ncolumn": [
            1122085995
        ],
        "one\nthat": [
            1122085995
        ],
        "(3": [
            1122085995
        ],
        "minus": [
            1122085995
        ],
        "pods\navailable": [
            1122085995
        ],
        "one\n9.3.5pausing": [
            1122085995
        ],
        "process\nafter": [
            1122085995
        ],
        "bug\nand": [
            1122085995
        ],
        "apprehensive": [
            1122085995
        ],
        "out\nacross": [
            1122085995
        ],
        "\nv4": [
            1122085995
        ],
        "your\nusers": [
            1122085995
        ],
        "everything’s": [
            1122085995
        ],
        "okay": [
            1122085995
        ],
        "with\nnew": [
            1122085995
        ],
        "another\noption": [
            1122085995
        ],
        "paused": [
            1122085995
        ],
        "during\nthe": [
            1122085995
        ],
        "proceeding": [
            1122085995
        ],
        "rollout\npausing": [
            1122085995
        ],
        "rollout\ni’ve": [
            1122085995
        ],
        "v4": [
            1122085995
        ],
        "\nluksa/kubia:v4": [
            1122085995
        ],
        "(within": [
            1122085995
        ],
        "seconds)": [
            1122085995
        ],
        "pause": [
            1122085995
        ],
        "rollout:\n$": [
            1122085995
        ],
        "nodejs=luksa/kubia:v4\ndeployment": [
            1122085995
        ],
        "updated\n$": [
            1122085995
        ],
        "paused\na": [
            1122085995
        ],
        "be\nrunning": [
            1122085995
        ],
        "redirected\nto": [
            1122085995
        ],
        "a\ntechnique": [
            1122085995
        ],
        "minimizing": [
            1122085995
        ],
        "it\naffecting": [
            1122085995
        ],
        "replace\nonly": [
            1122085995
        ],
        "\n\n274chapter": [
            1122085995
        ],
        "declaratively\nis": [
            1122085995
        ],
        "pods\nor": [
            1122085995
        ],
        "\nresuming": [
            1122085995
        ],
        "rollout\nin": [
            1122085995
        ],
        "pausing": [
            1122085995
        ],
        "portion": [
            1122085995
        ],
        "will\nhit": [
            1122085995
        ],
        "confident": [
            1122085995
        ],
        "resume": [
            1122085995
        ],
        "pods\nwith": [
            1122085995
        ],
        "ones:\n$": [
            1122085995
        ],
        "resumed\nobviously": [
            1122085995
        ],
        "process\nisn’t": [
            1122085995
        ],
        "upgrade": [
            1122085995
        ],
        "appropriately": [
            1122085995
        ],
        "rollouts\npausing": [
            1122085995
        ],
        "from\nkicking": [
            1122085995
        ],
        "changes\nonce": [
            1122085995
        ],
        "the\nrollout": [
            1122085995
        ],
        "start\nnoteif": [
            1122085995
        ],
        "you\nresume": [
            1122085995
        ],
        "deployment\n9.3.6blocking": [
            1122085995
        ],
        "versions\nbefore": [
            1122085995
        ],
        "deployment\nat": [
            1122085995
        ],
        "93.2?": [
            1122085995
        ],
        "see\nit": [
            1122085995
        ],
        "not\nslowing": [
            1122085995
        ],
        "fun": [
            1122085995
        ],
        "applicability": [
            1122085995
        ],
        "minreadyseconds\nthe": [
            1122085995
        ],
        "minreadyseconds": [
            1122085995
        ],
        "be\nready": [
            1122085995
        ],
        "treated": [
            1122085995
        ],
        "property?)": [
            1122085995
        ],
        "ready\nwhen": [
            1122085995
        ],
        "func-\ntioning": [
            1122085995
        ],
        "have\npassed": [
            1122085995
        ],
        "blocked\n": [
            1122085995
        ],
        "usually\nyou’d": [
            1122085995
        ],
        "report-\ning": [
            1122085995
        ],
        "\n\n275using": [
            1122085995
        ],
        "staging": [
            1122085995
        ],
        "an\nairbag": [
            1122085995
        ],
        "saves": [
            1122085995
        ],
        "buggy": [
            1122085995
        ],
        "slip": [
            1122085995
        ],
        "prevented": [
            1122085995
        ],
        "\nv3": [
            1122085995
        ],
        "how\ndefining": [
            1122085995
        ],
        "fully\nyou’re": [
            1122085995
        ],
        "before\nyou": [
            1122085995
        ],
        "pretend": [
            1122085995
        ],
        "you’re\nupgrading": [
            1122085995
        ],
        "wish": [
            1122085995
        ],
        "straight": [
            1122085995
        ],
        "fol-\nlows": [
            1122085995
        ],
        "assumes": [
            1122085995
        ],
        "until\nnow": [
            1122085995
        ],
        "explicit": [
            1122085995
        ],
        "returning\nerrors": [
            1122085995
        ],
        "and\nshouldn’t": [
            1122085995
        ],
        "deployment\n(you’ll": [
            1122085995
        ],
        "\nkubia-deployment-v3-with-readinesscheckyaml)": [
            1122085995
        ],
        "apps/v1beta1\nkind:": [
            1122085995
        ],
        "deployment\nmetadata:\n": [
            1122085995
        ],
        "minreadyseconds:": [
            1122085995
        ],
        "rollingupdate\n": [
            1122085995
        ],
        "luksa/kubia:v3\nlisting": [
            1122085995
        ],
        "kubia-deployment-v3-with-\nreadinesscheckyaml\nyou’re": [
            1122085995
        ],
        "10\nyou’re": [
            1122085995
        ],
        "\nreplace": [
            1122085995
        ],
        "\n\n276chapter": [
            1122085995
        ],
        "nodejs\n": [
            1122085995
        ],
        "readinessprobe:\n": [
            1122085995
        ],
        "periodseconds:": [
            1122085995
        ],
        "\nupdating": [
            1122085995
        ],
        "kubia-deployment-v3-with-readinesscheckyaml": [
            1122085995
        ],
        "configured\nthe": [
            1122085995
        ],
        "the\nyaml": [
            1122085995
        ],
        "definition\nand": [
            1122085995
        ],
        "\nreplicas": [
            1122085995
        ],
        "existing\ndeployment": [
            1122085995
        ],
        "usually\nwhat": [
            1122085995
        ],
        "\napply": [
            1122085995
        ],
        "kick": [
            1122085995
        ],
        "again\nfollow": [
            1122085995
        ],
        "updated..\nbecause": [
            1122085995
        ],
        "it\noccasionally": [
            1122085995
        ],
        "kubia-1765119474-jvslk\nthis": [
            1122085995
        ],
        "kubia-1765119474-xk5g3\nthis": [
            1122085995
        ],
        "kubia-1765119474-pmb26\nthis": [
            1122085995
        ],
        "kubia-1765119474-xk5g3\n..\nnope": [
            1122085995
        ],
        "there?": [
            1122085995
        ],
        "age\nkubia-1163142519-7ws0i": [
            1122085995
        ],
        "30s\nkubia-1765119474-jvslk": [
            1122085995
        ],
        "9m\nkubia-1765119474-pmb26": [
            1122085995
        ],
        "9m\nkubia-1765119474-xk5g3": [
            1122085995
        ],
        "8m\nyou’re": [
            1122085995
        ],
        "second\nthe": [
            1122085995
        ],
        "\nagainst": [
            1122085995
        ],
        "\n\n277using": [
            1122085995
        ],
        "declaratively\naha!": [
            1122085995
        ],
        "blessing)!": [
            1122085995
        ],
        "shown\nas": [
            1122085995
        ],
        "guess": [
            1122085995
        ],
        "expecting": [
            1122085995
        ],
        "happened?\nunderstanding": [
            1122085995
        ],
        "out\nas": [
            1122085995
        ],
        "(you\nset": [
            1122085995
        ],
        "spec)": [
            1122085995
        ],
        "began": [
            1122085995
        ],
        "500\nfrom": [
            1122085995
        ],
        "914).\nby": [
            1122085995
        ],
        "been\nmarked": [
            1122085995
        ],
        "and\nthat’s": [
            1122085995
        ],
        "not\nfunctioning": [
            1122085995
        ],
        "properly\nbut": [
            1122085995
        ],
        "process?": [
            1122085995
        ],
        "one\nnew": [
            1122085995
        ],
        "thankfully": [
            1122085995
        ],
        "ready\nfor": [
            1122085995
        ],
        "new\npods": [
            1122085995
        ],
        "\nmaxunavailable\nproperty": [
            1122085995
        ],
        "\nservice\ncurl\npod:": [
            1122085995
        ],
        "v3\n(unhealthy)\npod:": [
            1122085995
        ],
        "3\ndeployment\nreplicas:": [
            1122085995
        ],
        "3\nrollingupdate:\nmaxsurge:": [
            1122085995
        ],
        "1\nmaxunavailable:": [
            1122085995
        ],
        "0\nreplicaset:": [
            1122085995
        ],
        "v3\nreplicas:": [
            1122085995
        ],
        "1\nrequests": [
            1122085995
        ],
        "failed\nreadiness": [
            1122085995
        ],
        "probe\nfigure": [
            1122085995
        ],
        "blocked": [
            1122085995
        ],
        "\n\n278chapter": [
            1122085995
        ],
        "continued\nreplacing": [
            1122085995
        ],
        "non-working\nservice": [
            1122085995
        ],
        "weren’t": [
            1122085995
        ],
        "the\nreadiness": [
            1122085995
        ],
        "no\nnegative": [
            1122085995
        ],
        "impact": [
            1122085995
        ],
        "experienced": [
            1122085995
        ],
        "server\nerror": [
            1122085995
        ],
        "the\nfaulty": [
            1122085995
        ],
        "3\ntipif": [
            1122085995
        ],
        "minreadyseconds\nproperly": [
            1122085995
        ],
        "invo-\ncation": [
            1122085995
        ],
        "succeeds": [
            1122085995
        ],
        "failing\nshortly": [
            1122085995
        ],
        "appropriately\nconfiguring": [
            1122085995
        ],
        "rollout\nby": [
            1122085995
        ],
        "as\nfailed": [
            1122085995
        ],
        "a\nprogressdeadlineexceeded": [
            1122085995
        ],
        "condition": [
            1122085995
        ],
        "kubia\n..\nconditions:\n": [
            1122085995
        ],
        "reason\n": [
            1122085995
        ],
        "----": [
            1122085995
        ],
        "------\n": [
            1122085995
        ],
        "minimumreplicasavailable\n": [
            1122085995
        ],
        "progressing": [
            1122085995
        ],
        "progressdeadlineexceeded": [
            1122085995
        ],
        "the\nprogressdeadlineseconds": [
            1122085995
        ],
        "spec\nnotethe": [
            1122085995
        ],
        "deadline\naborting": [
            1122085995
        ],
        "rollout\nbecause": [
            1122085995
        ],
        "undoing": [
            1122085995
        ],
        "back\nnotein": [
            1122085995
        ],
        "aborted": [
            1122085995
        ],
        "\nprogressdeadlineseconds": [
            1122085995
        ],
        "exceeded\nlisting": [
            1122085995
        ],
        "\ntook": [
            1122085995
        ],
        "\nmake": [
            1122085995
        ],
        "progress\n": [
            1122085995
        ],
        "\n\n279summary\n94summary\nthis": [
            1122085995
        ],
        "declarative\napproach": [
            1122085995
        ],
        "you’ve\nread": [
            1122085995
        ],
        "to\nperform": [
            1122085995
        ],
        "replicationcontroller\ncreate": [
            1122085995
        ],
        "replicasets\nupdate": [
            1122085995
        ],
        "specification\nroll": [
            1122085995
        ],
        "revision\nstill": [
            1122085995
        ],
        "history\nabort": [
            1122085995
        ],
        "mid-way\npause": [
            1122085995
        ],
        "behaves\nin": [
            1122085995
        ],
        "ones\ncontrol": [
            1122085995
        ],
        "maxunavailable\nproperties\nuse": [
            1122085995
        ],
        "faulty": [
            1122085995
        ],
        "automatically\nin": [
            1122085995
        ],
        "deployment-specific": [
            1122085995
        ],
        "to\nuse": [
            1122085995
        ],
        "separator": [
            1122085995
        ],
        "file\nturn": [
            1122085995
        ],
        "kubectl’s": [
            1122085995
        ],
        "the\ncurtains\nyou": [
            1122085995
        ],
        "update\nthem": [
            1122085995
        ],
        "storage?": [
            1122085995
        ],
        "our\nnext": [
            1122085995
        ],
        "\n\n280\nstatefulsets:\ndeploying": [
            1122085995
        ],
        "replicated\nstateful": [
            1122085995
        ],
        "single-instance": [
            1122085995
        ],
        "stateless": [
            1122085995
        ],
        "utilizing": [
            1122085995
        ],
        "repli-\ncated": [
            1122085995
        ],
        "instance\nthat": [
            1122085995
        ],
        "through\npersistentvolumes": [
            1122085995
        ],
        "employ": [
            1122085995
        ],
        "pod?\nthis": [
            1122085995
        ],
        "covers\ndeploying": [
            1122085995
        ],
        "applications\nproviding": [
            1122085995
        ],
        "pod\nguaranteeing": [
            1122085995
        ],
        "replicas\nstarting": [
            1122085995
        ],
        "\npredictable": [
            1122085995
        ],
        "order\ndiscovering": [
            1122085995
        ],
        "records\n": [
            1122085995
        ],
        "\n\n281replicating": [
            1122085995
        ],
        "pods\n101": [
            1122085995
        ],
        "replicating": [
            1122085995
        ],
        "pods\nreplicasets": [
            1122085995
        ],
        "replicas\ndon’t": [
            1122085995
        ],
        "apart": [
            1122085995
        ],
        "101).\nbecause": [
            1122085995
        ],
        "stamp": [
            1122085995
        ],
        "out\nmultiple": [
            1122085995
        ],
        "each\ninstance": [
            1122085995
        ],
        "storage—at": [
            1122085995
        ],
        "to\nbe": [
            1122085995
        ],
        "honest": [
            1122085995
        ],
        "store\npossible": [
            1122085995
        ],
        "\n101.1": [
            1122085995
        ],
        "each\nhow": [
            1122085995
        ],
        "storage\nvolume?": [
            1122085995
        ],
        "use\nthem": [
            1122085995
        ],
        "use?\ncreating": [
            1122085995
        ],
        "persistentvolume-\nclaim": [
            1122085995
        ],
        "man-\nually": [
            1122085995
        ],
        "failure)\ntherefore": [
            1122085995
        ],
        "viable": [
            1122085995
        ],
        "option\nusing": [
            1122085995
        ],
        "instance\ninstead": [
            1122085995
        ],
        "each\npod": [
            1122085995
        ],
        "102).\n": [
            1122085995
        ],
        "or\naccidental": [
            1122085995
        ],
        "deletions": [
            1122085995
        ],
        "cumbersome": [
            1122085995
        ],
        "single\nreplicaset": [
            1122085995
        ],
        "you\npersistent\nvolume\nclaim\npersistent\nvolume\nreplicaset\npod\npod\npod\nfigure": [
            1122085995
        ],
        "persistentvolume\n": [
            1122085995
        ],
        "\n\n282chapter": [
            1122085995
        ],
        "10statefulsets:": [
            1122085995
        ],
        "applications\ncouldn’t": [
            1122085995
        ],
        "count—you’d": [
            1122085995
        ],
        "maybe\nuse": [
            1122085995
        ],
        "volume?": [
            1122085995
        ],
        "a\nseparate": [
            1122085995
        ],
        "103).\nbecause": [
            1122085995
        ],
        "instance\nautomatically": [
            1122085995
        ],
        "create)": [
            1122085995
        ],
        "used\nby": [
            1122085995
        ],
        "coordination": [
            1122085995,
            186247402
        ],
        "between\nthe": [
            1122085995
        ],
        "bottleneck\n10.1.2": [
            1122085995
        ],
        "instance\nhas": [
            1122085995
        ],
        "long-lived": [
            1122085995
        ],
        "with\npvc": [
            1122085995
        ],
        "a1pv": [
            1122085995
        ],
        "a1\nreplicaset": [
            1122085995
        ],
        "a1\npod": [
            1122085995
        ],
        "a1-xyz\npvc": [
            1122085995
        ],
        "a2pv": [
            1122085995
        ],
        "a2\nreplicaset": [
            1122085995
        ],
        "a2\npod": [
            1122085995
        ],
        "a2-xzy\npvc": [
            1122085995
        ],
        "a3pv": [
            1122085995
        ],
        "a3\nreplicaset": [
            1122085995
        ],
        "a3\npod": [
            1122085995
        ],
        "a3-zyx\nfigure": [
            1122085995
        ],
        "instance\npersistent\nvolume\nclaim\npersistentvolume\nreplicaset\npod\npod\npod\napp\napp\napp\n/data/1/\n/data/3/\n/data/2/\nfigure": [
            1122085995
        ],
        "\n\n283replicating": [
            1122085995
        ],
        "pods\nnew": [
            1122085995
        ],
        "instance’s": [
            1122085995
        ],
        "a\ncompletely": [
            1122085995
        ],
        "problems\n": [
            1122085995
        ],
        "mandate": [
            1122085995
        ],
        "identity?": [
            1122085995
        ],
        "is\nfairly": [
            1122085995
        ],
        "adminis-\ntrator": [
            1122085995
        ],
        "members": [
            1122085995
        ],
        "hostnames)": [
            1122085995
        ],
        "in\neach": [
            1122085995
        ],
        "member’s": [
            1122085995
        ],
        "whole\napplication": [
            1122085995
        ],
        "reconfigured": [
            1122085995
        ],
        "instance\na": [
            1122085995
        ],
        "individual\nmember": [
            1122085995
        ],
        "member": [
            1122085995
        ],
        "through\nits": [
            1122085995
        ],
        "(rather": [
            1122085995
        ],
        "ip)": [
            1122085995
        ],
        "techniques": [
            1122085995
        ],
        "the\nsetup": [
            1122085995
        ],
        "covering": [
            1122085995
        ],
        "cluster)\nthe": [
            1122085995
        ],
        "ugly": [
            1122085995
        ],
        "pods\ncan’t": [
            1122085995
        ],
        "stable\nip)": [
            1122085995
        ],
        "self-register": [
            1122085995
        ],
        "\npvc": [
            1122085995
        ],
        "a1-xzy\nservice": [
            1122085995
        ],
        "a1\nservice": [
            1122085995
        ],
        "a\npvc": [
            1122085995
        ],
        "a2-xzy\nservice": [
            1122085995
        ],
        "a2\npvc": [
            1122085995
        ],
        "a3-zyx\nservice": [
            1122085995
        ],
        "a3\nfigure": [
            1122085995
        ],
        "\nindividual": [
            1122085995
        ],
        "respectively\n": [
            1122085995
        ],
        "\n\n284chapter": [
            1122085995
        ],
        "proper\nclean": [
            1122085995
        ],
        "is\nthrough": [
            1122085995
        ],
        "\n102": [
            1122085995
        ],
        "statefulsets\ninstead": [
            1122085995
        ],
        "statefulset\nresource": [
            1122085995
        ],
        "tailored": [
            1122085995
        ],
        "non-fungible": [
            1122085995
        ],
        "individuals": [
            1122085995
        ],
        "name\nand": [
            1122085995
        ],
        "\n102.1": [
            1122085995
        ],
        "replicasets\nto": [
            1122085995
        ],
        "or\nreplicationcontrollers": [
            1122085995
        ],
        "analogy": [
            1122085995
        ],
        "widely\nused": [
            1122085995
        ],
        "field\nunderstanding": [
            1122085995
        ],
        "pets": [
            1122085995
        ],
        "cattle": [
            1122085995
        ],
        "analogy\nyou": [
            1122085995
        ],
        "we\ncan": [
            1122085995
        ],
        "\nnotestatefulsets": [
            1122085995
        ],
        "petsets": [
            1122085995
        ],
        "the\npets": [
            1122085995
        ],
        "here\nwe": [
            1122085995
        ],
        "and\ntake": [
            1122085995
        ],
        "cattle\nand": [
            1122085995
        ],
        "pay": [
            1122085995
        ],
        "replace\nunhealthy": [
            1122085995
        ],
        "farmer\nreplaces": [
            1122085995
        ],
        "heads": [
            1122085995
        ],
        "it\ndoesn’t": [
            1122085995
        ],
        "dies—you": [
            1122085995
        ],
        "won’t\nnotice": [
            1122085995
        ],
        "pet": [
            1122085995
        ],
        "a\npet": [
            1122085995
        ],
        "buy": [
            1122085995
        ],
        "lost\npet": [
            1122085995
        ],
        "one\ncomparing": [
            1122085995
        ],
        "replicationcontrollers\npod": [
            1122085995
        ],
        "cattle\nbecause": [
            1122085995
        ],
        "pod\nreplica": [
            1122085995
        ],
        "pod\ninstance": [
            1122085995
        ],
        "fails)": [
            1122085995
        ],
        "resur-\nrected": [
            1122085995
        ],
        "network\nidentity": [
            1122085995
        ],
        "are\nmanaged": [
            1122085995
        ],
        "\n\n285understanding": [
            1122085995
        ],
        "statefulsets\n": [
            1122085995
        ],
        "their\nidentity": [
            1122085995
        ],
        "a\nstatefulset": [
            1122085995
        ],
        "how\nmany": [
            1122085995
        ],
        "cookie-cutter": [
            1122085995
        ],
        "anal-\nogy?)": [
            1122085995
        ],
        "aren’t\nexact": [
            1122085995
        ],
        "volumes—in": [
            1122085995
        ],
        "words\nstorage": [
            1122085995
        ],
        "state)—which": [
            1122085995
        ],
        "differentiates": [
            1122085995
        ],
        "pods\nalso": [
            1122085995
        ],
        "predictable": [
            1122085995
        ],
        "stable)": [
            1122085995
        ],
        "getting\na": [
            1122085995
        ],
        "\n102.2": [
            1122085995
        ],
        "identity\neach": [
            1122085995
        ],
        "ordinal": [
            1122085995
        ],
        "(zero-based)": [
            1122085995
        ],
        "which\nis": [
            1122085995
        ],
        "derive": [
            1122085995
        ],
        "is\nderived": [
            1122085995
        ],
        "statefulset’s": [
            1122085995
        ],
        "rather\nthan": [
            1122085995
        ],
        "next\nfigure\nintroducing": [
            1122085995
        ],
        "service\nbut": [
            1122085995
        ],
        "regu-\nlar": [
            1122085995
        ],
        "addressable": [
            1122085995
        ],
        "whereas\nstateless": [
            1122085995
        ],
        "oper-\nate": [
            1122085995
        ],
        "hold\ndifferent": [
            1122085995
        ],
        "governing\nheadless": [
            1122085995
        ],
        "other\nclients": [
            1122085995
        ],
        "govern-\ning": [
            1122085995
        ],
        "pods\nreplicaset": [
            1122085995
        ],
        "a\npoda-fewrb\npoda-jwqec\npoda-dsfwx\nstatefulset": [
            1122085995
        ],
        "a\npoda-1\npoda-2\npoda-0\nfigure": [
            1122085995
        ],
        "\nunlike": [
            1122085995
        ],
        "replicaset\n": [
            1122085995
        ],
        "\n\n286chapter": [
            1122085995
        ],
        "applications\nis": [
            1122085995
        ],
        "a-0": [
            1122085995
        ],
        "\na-0foo.default.svc.cluster.local.": [
            1122085995
        ],
        "a\nreplicaset\n": [
            1122085995
        ],
        "\nfoodefault.svc.cluster.local": [
            1122085995
        ],
        "we’ll\nexplain": [
            1122085995
        ],
        "members\nof": [
            1122085995
        ],
        "statefulset\nreplacing": [
            1122085995
        ],
        "pets\nwhen": [
            1122085995
        ],
        "(because": [
            1122085995
        ],
        "pod\nwas": [
            1122085995
        ],
        "pod\nobject": [
            1122085995
        ],
        "instance—similar\nto": [
            1122085995
        ],
        "contrast": [
            1122085995
        ],
        "disappeared": [
            1122085995
        ],
        "between\nreplicasets": [
            1122085995
        ],
        "illustrated": [
            1122085995
        ],
        "106).\nnode": [
            1122085995
        ],
        "1node": [
            1122085995
        ],
        "2\nreplicaset": [
            1122085995
        ],
        "breplicaset": [
            1122085995
        ],
        "b\nstatefulset\nstatefulset": [
            1122085995
        ],
        "a-0pod": [
            1122085995
        ],
        "a-1poda-0poda-0\npod": [
            1122085995
        ],
        "a-1\nnode": [
            1122085995
        ],
        "fails\nstatefulset": [
            1122085995
        ],
        "2\nreplicaset\nnode": [
            1122085995
        ],
        "b-fdawrpod": [
            1122085995
        ],
        "b-jkbde\npodb-fdawr\npodb-rsqkw\npod": [
            1122085995
        ],
        "b-jkbde\nfigure": [
            1122085995
        ],
        "\n\n287understanding": [
            1122085995
        ],
        "statefulsets\nthe": [
            1122085995
        ],
        "early\non": [
            1122085995
        ],
        "reachable\nunder": [
            1122085995
        ],
        "statefulset\nscaling": [
            1122085995
        ],
        "unused": [
            1122085995
        ],
        "index\nif": [
            1122085995
        ],
        "indexes": [
            1122085995
        ],
        "know\nwhat": [
            1122085995
        ],
        "replicaset\nwhere": [
            1122085995
        ],
        "specify\nwhich": [
            1122085995
        ],
        "future)\nscaling": [
            1122085995
        ],
        "highest": [
            1122085995
        ],
        "index\nfirst": [
            1122085995
        ],
        "107).": [
            1122085995
        ],
        "scale-down": [
            1122085995
        ],
        "predictable\nbecause": [
            1122085995
        ],
        "rapid": [
            1122085995
        ],
        "scale-downs": [
            1122085995
        ],
        "stateful-\nsets": [
            1122085995
        ],
        "example\nmay": [
            1122085995
        ],
        "replicated\ndata": [
            1122085995
        ],
        "two\nnodes": [
            1122085995
        ],
        "exactly\nthose": [
            1122085995
        ],
        "sequential": [
            1122085995
        ],
        "time\nto": [
            1122085995
        ],
        "(single)\nlost": [
            1122085995
        ],
        "copy\n": [
            1122085995
        ],
        "permit": [
            1122085995
        ],
        "once\n10.2.3": [
            1122085995
        ],
        "instance\nyou’ve": [
            1122085995
        ],
        "what\nabout": [
            1122085995
        ],
        "state-\nful": [
            1122085995
        ],
        "(replaced": [
            1122085995
        ],
        "as\nbefore)": [
            1122085995
        ],
        "this?\npod\na-0\npod\na-1\npod\na-2\nstatefulset": [
            1122085995
        ],
        "a\nreplicas:3\npod\na-0\npod\na-1\npod\na-2\nstatefulset": [
            1122085995
        ],
        "a\nreplicas:2\npod\na-0\npod\na-1\nstatefulset": [
            1122085995
        ],
        "a\nreplicas:1\nscale": [
            1122085995
        ],
        "downscale": [
            1122085995
        ],
        "down\nfigure": [
            1122085995
        ],
        "\n\n288chapter": [
            1122085995
        ],
        "the\npersistentvolumeclaim": [
            1122085995
        ],
        "map\nto": [
            1122085995
        ],
        "one-to-one": [
            1122085995
        ],
        "because\nall": [
            1122085995
        ],
        "stamped": [
            1122085995
        ],
        "refer\nto": [
            1122085995
        ],
        "persistentvolumeclaim?": [
            1122085995
        ],
        "claims?": [
            1122085995
        ],
        "surely": [
            1122085995
        ],
        "you\nplan": [
            1122085995
        ],
        "upfront?": [
            1122085995
        ],
        "not\nteaming": [
            1122085995
        ],
        "templates": [
            1122085995
        ],
        "templates\nthe": [
            1122085995
        ],
        "cre-\nating": [
            1122085995
        ],
        "claim\ntemplates": [
            1122085995
        ],
        "108).\nthe": [
            1122085995
        ],
        "up-front": [
            1122085995
        ],
        "explained\nat": [
            1122085995
        ],
        "persistentvolumeclaims\nscaling": [
            1122085995
        ],
        "deletes\nonly": [
            1122085995
        ],
        "consider\nwhat": [
            1122085995
        ],
        "it\nwas": [
            1122085995
        ],
        "the\ndata": [
            1122085995
        ],
        "stateful-\nset": [
            1122085995
        ],
        "catastrophic—especially": [
            1122085995
        ],
        "triggering": [
            1122085995
        ],
        "as\ndecreasing": [
            1122085995
        ],
        "to\ndelete": [
            1122085995
        ],
        "persistentvolume\npvc": [
            1122085995
        ],
        "a-0pv\npod": [
            1122085995
        ],
        "a-0\npvc": [
            1122085995
        ],
        "a-1pv\npod": [
            1122085995
        ],
        "a-1\npvc": [
            1122085995
        ],
        "a-2pv\npod": [
            1122085995
        ],
        "a-2\nstatefulset": [
            1122085995
        ],
        "a\npod\ntemplate\nvolume": [
            1122085995
        ],
        "claim\ntemplate\nfigure": [
            1122085995
        ],
        "\n\n289understanding": [
            1122085995
        ],
        "statefulsets\nreattaching": [
            1122085995
        ],
        "subse-\nquent": [
            1122085995
        ],
        "scale-up": [
            1122085995
        ],
        "reattach": [
            1122085995
        ],
        "109).": [
            1122085995
        ],
        "accidentally\nscale": [
            1122085995
        ],
        "new\npod": [
            1122085995
        ],
        "name)\n10.2.4": [
            1122085995
        ],
        "guarantees\nas": [
            1122085995
        ],
        "storage\nstatefulsets": [
            1122085995
        ],
        "guarantees": [
            1122085995
        ],
        "implications": [
            1122085995
        ],
        "storage\nwhile": [
            1122085995
        ],
        "fungible": [
            1122085995
        ],
        "how\na": [
            1122085995
        ],
        "and\nhostname": [
            1122085995
        ],
        "pod\nmanually)": [
            1122085995
        ],
        "a\nreplacement": [
            1122085995
        ],
        "iden-\ntity": [
            1122085995
        ],
        "storage\npod\na-0\npod\na-1\nstatefulset": [
            1122085995
        ],
        "a\nreplicas:2\nscale\ndown\nscale\nup\nnew": [
            1122085995
        ],
        "created\nwith": [
            1122085995
        ],
        "before\npvc": [
            1122085995
        ],
        "is\nre-attached\npvc\na-0\npv\npvc\na-1\npv\npod\na-0\nstatefulset": [
            1122085995
        ],
        "a\nreplicas:1\npvc\na-0\npv\npvc\na-1\npv\npod\na-0\npod": [
            1122085995
        ],
        "deleted\npod\na-1\nstatefulset": [
            1122085995
        ],
        "a\nreplicas:2\npvc\na-0\npv\npvc\na-1\npvc": [
            1122085995
        ],
        "not\nbeen": [
            1122085995
        ],
        "deleted\npv\nfigure": [
            1122085995
        ],
        "down;": [
            1122085995
        ],
        "\nreattach": [
            1122085995
        ],
        "up\n": [
            1122085995
        ],
        "\n\n290chapter": [
            1122085995
        ],
        "applications\nso": [
            1122085995
        ],
        "pods\nmanaged": [
            1122085995
        ],
        "to\nwork": [
            1122085995
        ],
        "at-most-one": [
            1122085995
        ],
        "semantics\nkubernetes": [
            1122085995
        ],
        "never\nrunning": [
            1122085995
        ],
        "guarantee": [
            1122085995
        ],
        "semantics": [
            1122085995
        ],
        "fail-\nures": [
            1122085995
        ],
        "that\nhowever,": [
            1122085995
        ],
        "way\n10.3": [
            1122085995
        ],
        "statefulset\nto": [
            1122085995
        ],
        "data\nstore": [
            1122085995
        ],
        "fancy—more": [
            1122085995
        ],
        "stone": [
            1122085995
        ],
        "\n103.1": [
            1122085995
        ],
        "image\nyou’ll": [
            1122085995
        ],
        "you’ll\nexpand": [
            1122085995
        ],
        "listing\n...\nconst": [
            1122085995
        ],
        "datafile": [
            1122085995
        ],
        "/var/data/kubiatxt\";\n...\nvar": [
            1122085995
        ],
        "(requestmethod": [
            1122085995
        ],
        "==": [
            1122085995
        ],
        "post')": [
            1122085995
        ],
        "fscreatewritestream(datafile);": [
            1122085995
        ],
        "fileon(open'": [
            1122085995
        ],
        "(fd)": [
            1122085995
        ],
        "requestpipe(file);": [
            1122085995
        ],
        "consolelog(new": [
            1122085995
        ],
        "stored);": [
            1122085995
        ],
        "responsewritehead(200);": [
            1122085995
        ],
        "responseend(data": [
            1122085995
        ],
        "\\n\");": [
            1122085995
        ],
        "});\n": [
            1122085995
        ],
        "fileexists(datafile)": [
            1122085995
        ],
        "fsreadfilesync(datafile": [
            1122085995
        ],
        "utf8')": [
            1122085995
        ],
        "yet;": [
            1122085995
        ],
        "responsewrite(youve": [
            1122085995
        ],
        "}\n};\nlisting": [
            1122085995
        ],
        "kubia-pet-image/appjs\non": [
            1122085995
        ],
        "\nbody": [
            1122085995
        ],
        "\ndata": [
            1122085995
        ],
        "file\non": [
            1122085995
        ],
        "\nother": [
            1122085995
        ],
        "\n\n291using": [
            1122085995
        ],
        "statefulset\nvar": [
            1122085995
        ],
        "httpcreateserver(handler);\nwww.listen(8080);\nwhenever": [
            1122085995
        ],
        "\n/var/data/kubiatxt.": [
            1122085995
        ],
        "(contents": [
            1122085995
        ],
        "first\nversion": [
            1122085995
        ],
        "listing\nand": [
            1122085995
        ],
        "before\nfrom": [
            1122085995
        ],
        "appjs\"]\ngo": [
            1122085995
        ],
        "dockerio/luksa/kubia-pet.\n10.3.2": [
            1122085995
        ],
        "three)": [
            1122085995
        ],
        "objects:\npersistentvolumes": [
            1122085995
        ],
        "persistentvolumes)\na": [
            1122085995
        ],
        "statefulset\nthe": [
            1122085995
        ],
        "itself\nfor": [
            1122085995
        ],
        "will\nbind": [
            1122085995
        ],
        "section)": [
            1122085995
        ],
        "volumes\nyou’ll": [
            1122085995
        ],
        "more\nthan": [
            1122085995
        ],
        "chapter06/\npersistent-volumes-hostpathyaml": [
            1122085995
        ],
        "actual\ngce": [
            1122085995
        ],
        "pv-a\n$": [
            1122085995
        ],
        "pv-b\n$": [
            1122085995
        ],
        "pv-c\nnotemake": [
            1122085995
        ],
        "are\nrunning": [
            1122085995
        ],
        "in\nlisting": [
            1122085995
        ],
        "kubia-pet-image/dockerfile\n": [
            1122085995
        ],
        "\n\n292chapter": [
            1122085995
        ],
        "applications\nthen": [
            1122085995
        ],
        "persistent-volumes-gcepdyaml": [
            1122085995
        ],
        "file\nwhich": [
            1122085995
        ],
        "v1\nitems:\n-": [
            1122085995
        ],
        "v1\n": [
            1122085995
        ],
        "kind:": [
            1122085995
        ],
        "pv-a": [
            1122085995
        ],
        "capacity:\n": [
            1122085995
        ],
        "readwriteonce\n": [
            1122085995
        ],
        "nfs4": [
            1122085995
        ],
        "pv-b\n": [
            1122085995
        ],
        "..\nnotein": [
            1122085995
        ],
        "same\nyaml": [
            1122085995
        ],
        "delimiting": [
            1122085995
        ],
        "three-dash": [
            1122085995
        ],
        "\nlist": [
            1122085995
        ],
        "equivalent\nthis": [
            1122085995
        ],
        "\npv-a": [
            1122085995
        ],
        "pv-b": [
            1122085995
        ],
        "pv-c": [
            1122085995
        ],
        "clus-\nters": [
            1122085995
        ],
        "similar\ncreating": [
            1122085995
        ],
        "headless\nservice": [
            1122085995
        ],
        "persistentvolumes:": [
            1122085995
        ],
        "persistent-volumes-gcepdyaml\nlisting": [
            1122085995
        ],
        "statefulset:": [
            1122085995
        ],
        "kubia-service-headlessyaml\nfile": [
            1122085995
        ],
        "\nvolumes\npersistent": [
            1122085995
        ],
        "\nare": [
            1122085995
        ],
        "pv-c\ncapacity": [
            1122085995
        ],
        "mebibyte\nwhen": [
            1122085995
        ],
        "again\nthe": [
            1122085995
        ],
        "\nstorage": [
            1122085995
        ],
        "mechanism\nname": [
            1122085995
        ],
        "\nservice\nthe": [
            1122085995
        ],
        "\n\n293using": [
            1122085995
        ],
        "statefulset\n": [
            1122085995
        ],
        "http\n": [
            1122085995
        ],
        "80\nyou’re": [
            1122085995
        ],
        "will\nenable": [
            1122085995
        ],
        "later)": [
            1122085995
        ],
        "statefulset\ncreating": [
            1122085995
        ],
        "manifest\nnow": [
            1122085995
        ],
        "statefulset\nmetadata:\n": [
            1122085995
        ],
        "luksa/kubia-pet\n": [
            1122085995
        ],
        "/var/data": [
            1122085995
        ],
        "volumeclaimtemplates:\n": [
            1122085995
        ],
        "manifests\nyou’ve": [
            1122085995
        ],
        "\nvolumeclaimtemplates": [
            1122085995
        ],
        "defin-\ning": [
            1122085995
        ],
        "a\nclaim": [
            1122085995
        ],
        "previous\nlisting": [
            1122085995
        ],
        "kubia-statefulsetyaml\nall": [
            1122085995
        ],
        "service\npods": [
            1122085995
        ],
        "label\nthe": [
            1122085995
        ],
        "\nmount": [
            1122085995
        ],
        "path\nthe": [
            1122085995
        ],
        "\ntemplate\n": [
            1122085995
        ],
        "\n\n294chapter": [
            1122085995
        ],
        "applications\npod": [
            1122085995
        ],
        "statefulset\ncreated": [
            1122085995
        ],
        "pod\ncreating": [
            1122085995
        ],
        "statefulset\nyou’ll": [
            1122085995
        ],
        "kubia-statefulsetyaml": [
            1122085995
        ],
        "created\nnow": [
            1122085995
        ],
        "age\nkubia-0": [
            1122085995
        ],
        "1s\nnotice": [
            1122085995
        ],
        "strange?": [
            1122085995
        ],
        "cre-\nates": [
            1122085995
        ],
        "first\none": [
            1122085995
        ],
        "stateful\napps": [
            1122085995
        ],
        "race": [
            1122085995
        ],
        "safer": [
            1122085995
        ],
        "up\nthe": [
            1122085995
        ],
        "rest\n": [
            1122085995
        ],
        "progressing:\n$": [
            1122085995
        ],
        "8s\nkubia-1": [
            1122085995
        ],
        "2s\nsee": [
            1122085995
        ],
        "being\nstarted": [
            1122085995
        ],
        "the\nstatefulset": [
            1122085995
        ],
        "constructed": [
            1122085995
        ],
        "template\n$": [
            1122085995
        ],
        "kubia-0": [
            1122085995
        ],
        "\n\n295using": [
            1122085995
        ],
        "default-token-r2m41\n": [
            1122085995
        ],
        "data-kubia-0": [
            1122085995
        ],
        "secret:\n": [
            1122085995
        ],
        "default-token-r2m41\nthe": [
            1122085995
        ],
        "persistentvolumeclaim\nand": [
            1122085995
        ],
        "persistentvolumeclaims\nnow": [
            1122085995
        ],
        "age\ndata-kubia-0": [
            1122085995
        ],
        "37s\ndata-kubia-1": [
            1122085995
        ],
        "37s\nthe": [
            1122085995
        ],
        "name\ndefined": [
            1122085995
        ],
        "\nvolumeclaimtemplate": [
            1122085995
        ],
        "the\nclaims’": [
            1122085995
        ],
        "template\n10.3.3": [
            1122085995
        ],
        "head-\nless": [
            1122085995
        ],
        "but\nthat": [
            1122085995
        ],
        "pod)\n": [
            1122085995
        ],
        "directly:": [
            1122085995
        ],
        "piggybacking": [
            1122085995
        ],
        "another\npod": [
            1122085995
        ],
        "you’ll\ntry": [
            1122085995
        ],
        "\ncommunicating": [
            1122085995
        ],
        "server\none": [
            1122085995
        ],
        "\nkubia-0": [
            1122085995
        ],
        "url:\n<apiserverhost>:<port>/api/v1/namespaces/default/pods/kubia-0/proxy/<path>\nbecause": [
            1122085995
        ],
        "is\ncumbersome": [
            1122085995
        ],
        "each\nrequest)": [
            1122085995
        ],
        "the\nthe": [
            1122085995
        ],
        "\nspecified": [
            1122085995
        ],
        "manifest\nthe": [
            1122085995
        ],
        "statefulset\nthe": [
            1122085995
        ],
        "\n\n296chapter": [
            1122085995
        ],
        "applications\napi": [
            1122085995
        ],
        "the\nproxy": [
            1122085995
        ],
        "1270.0.1:8001\nnow": [
            1122085995
        ],
        "use\nlocalhost:8001": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\nyouve": [
            1122085995
        ],
        "kubia-0\ndata": [
            1122085995
        ],
        "yet\nthe": [
            1122085995
        ],
        "that\nlast": [
            1122085995
        ],
        "slash": [
            1122085995
        ],
        "follows": [
            1122085995
        ],
        "redirects\nby": [
            1122085995
        ],
        "communicating": [
            1122085995
        ],
        "different\nproxies": [
            1122085995
        ],
        "prox-\nied": [
            1122085995
        ],
        "1010.\nthe": [
            1122085995
        ],
        "post\nrequests": [
            1122085995
        ],
        "same\nproxy": [
            1122085995
        ],
        "body\ninto": [
            1122085995
        ],
        "-x": [
            1122085995
        ],
        "hey": [
            1122085995
        ],
        "there!": [
            1122085995
        ],
        "greeting": [
            1122085995
        ],
        "submitted": [
            1122085995
        ],
        "kubia-0\n➥": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\ndata": [
            1122085995
        ],
        "kubia-0\nkubectl": [
            1122085995
        ],
        "proxycurl\nget": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\nget": [
            1122085995
        ],
        "192168.99.100:8443/api/v1/namespaces/default/pods/kubia-0/proxy/\nauthorization:": [
            1122085995
        ],
        "<token>\nget": [
            1122085995
        ],
        "17217.0.3:8080/\napi": [
            1122085995
        ],
        "server\npod:": [
            1122085995
        ],
        "kubia-0\n192168.99.100\n172.17.0.3\nlocalhost\nfigure": [
            1122085995
        ],
        "proxy\n": [
            1122085995
        ],
        "\n\n297using": [
            1122085995
        ],
        "stored\ndata": [
            1122085995
        ],
        "kubia-0\nokay": [
            1122085995
        ],
        "kubia-1": [
            1122085995
        ],
        "pod)\nsays:\n$": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/\nyouve": [
            1122085995
        ],
        "kubia-1\ndata": [
            1122085995
        ],
        "yet\nas": [
            1122085995
        ],
        "persisted?": [
            1122085995
        ],
        "out\ndeleting": [
            1122085995
        ],
        "reattached": [
            1122085995
        ],
        "storage\nyou’re": [
            1122085995
        ],
        "kubia-0\npod": [
            1122085995
        ],
        "kubia-0\"": [
            1122085995
        ],
        "deleted\nif": [
            1122085995
        ],
        "terminating:": [
            1122085995
        ],
        "3m\nkubia-1": [
            1122085995
        ],
        "3m\nas": [
            1122085995
        ],
        "the\nstatefulset:\n$": [
            1122085995
        ],
        "6s\nkubia-1": [
            1122085995
        ],
        "4m\n$": [
            1122085995
        ],
        "9s\nkubia-1": [
            1122085995
        ],
        "4m\nlet": [
            1122085995
        ],
        "remind": [
            1122085995
        ],
        "pod’s\nwhole": [
            1122085995
        ],
        "storage)": [
            1122085995
        ],
        "new\nnode": [
            1122085995
        ],
        "1011).": [
            1122085995
        ],
        "before\n": [
            1122085995
        ],
        "\n\n298chapter": [
            1122085995
        ],
        "applications\nwith": [
            1122085995
        ],
        "in\nits": [
            1122085995
        ],
        "incarnation": [
            1122085995
        ],
        "hostname\nand": [
            1122085995
        ],
        "data?": [
            1122085995
        ],
        "confirm:\n$": [
            1122085995
        ],
        "kubia-0\nthe": [
            1122085995
        ],
        "before\nconfirming": [
            1122085995
        ],
        "the\nexact": [
            1122085995
        ],
        "extended": [
            1122085995
        ],
        "period\nshould": [
            1122085995
        ],
        "it\nimmediately": [
            1122085995
        ],
        "but\nleaves": [
            1122085995
        ],
        "untouched": [
            1122085995
        ],
        "state-\nfulset": [
            1122085995
        ],
        "gradu-\nally—similar": [
            1122085995
        ],
        "ini-\ntially": [
            1122085995
        ],
        "ordinal\nnumber": [
            1122085995
        ],
        "the\nsecond": [
            1122085995
        ],
        "non-headless": [
            1122085995
        ],
        "non-\nheadless": [
            1122085995
        ],
        "pods\nthrough": [
            1122085995
        ],
        "directly\nnode": [
            1122085995
        ],
        "1\npod:kubia-0\npod:": [
            1122085995
        ],
        "kubia-1\ndelete": [
            1122085995
        ],
        "kubia-0\nstoragestorage\nstorage\npod:": [
            1122085995
        ],
        "kubia-1\nstorage\nnode": [
            1122085995
        ],
        "1\nkubia-0": [
            1122085995
        ],
        "rescheduled\nnode": [
            1122085995
        ],
        "2\nstorage\npod:": [
            1122085995
        ],
        "kubia-1\nstorage\npod:kubia-0\nfigure": [
            1122085995
        ],
        "retains": [
            1122085995
        ],
        "\n\n299discovering": [
            1122085995
        ],
        "kubia-public\nspec:\n": [
            1122085995
        ],
        "8080\nbecause": [
            1122085995
        ],
        "not\na": [
            1122085995
        ],
        "necessarily\nconnecting": [
            1122085995
        ],
        "cluster-internal": [
            1122085995
        ],
        "server\ninstead": [
            1122085995
        ],
        "piggyback": [
            1122085995
        ],
        "way\nyou’ve": [
            1122085995
        ],
        "uri": [
            1122085995
        ],
        "proxy-ing": [
            1122085995
        ],
        "formed": [
            1122085995
        ],
        "this:\n/api/v1/namespaces/<namespace>/services/<service": [
            1122085995
        ],
        "name>/proxy/<path>\ntherefore": [
            1122085995
        ],
        "running):\n$": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/default/services/kubia-\n➥": [
            1122085995
        ],
        "public/proxy/\nyouve": [
            1122085995
        ],
        "yet\nlikewise": [
            1122085995
        ],
        "(inside": [
            1122085995
        ],
        "kubia-public": [
            1122085995
        ],
        "to\nand": [
            1122085995
        ],
        "you’ll\nimprove": [
            1122085995
        ],
        "next\n10.4": [
            1122085995
        ],
        "statefulset\nwe": [
            1122085995
        ],
        "clus-\ntered": [
            1122085995
        ],
        "discovery—the": [
            1122085995
        ],
        "each\nmember": [
            1122085995
        ],
        "that\nhelp": [
            1122085995
        ],
        "undesirable\nlisting": [
            1122085995
        ],
        "kubia-service-publicyaml\n": [
            1122085995
        ],
        "\n\n300chapter": [
            1122085995
        ],
        "api?": [
            1122085995
        ],
        "existing\nwell-known": [
            1122085995
        ],
        "possible?": [
            1122085995
        ],
        "domain\nname": [
            1122085995
        ],
        "(dns)?": [
            1122085995
        ],
        "probably\nunderstand": [
            1122085995
        ],
        "mx": [
            1122085995
        ],
        "lesser-known": [
            1122085995
        ],
        "of\ndns": [
            1122085995
        ],
        "record\nintroducing": [
            1122085995
        ],
        "records\nsrv": [
            1122085995
        ],
        "specific\nservice": [
            1122085995
        ],
        "back-\ning": [
            1122085995
        ],
        "\ndig": [
            1122085995
        ],
        "dns\nlookup": [
            1122085995
        ],
        "use:\n$": [
            1122085995
        ],
        "srvlookup": [
            1122085995
        ],
        "--rm": [
            1122085995
        ],
        "--restart=never": [
            1122085995
        ],
        "kubiadefault.svc.cluster.local\nthe": [
            1122085995
        ],
        "one-off": [
            1122085995
        ],
        "(--restart=never)": [
            1122085995
        ],
        "is\nattached": [
            1122085995
        ],
        "(\n-it)": [
            1122085995
        ],
        "(--rm)": [
            1122085995
        ],
        "following\ncommand:\ndig": [
            1122085995
        ],
        "out\n...\n;;": [
            1122085995
        ],
        "section:\nkd.s.c.l.": [
            1122085995
        ],
        "kubia-0kubia.default.svc.cluster.local.\nk.d.s.c.l.": [
            1122085995
        ],
        "kubia-1kubia.default.svc.cluster.local.\n;;": [
            1122085995
        ],
        "section:\nkubia-0kubia.default.svc.cluster.local.": [
            1122085995
        ],
        "17217.0.4\nkubia-1.kubia.default.svc.cluster.local.": [
            1122085995
        ],
        "17217.0.6\n...\nnotei’ve": [
            1122085995
        ],
        "shorten": [
            1122085995
        ],
        "single\nline": [
            1122085995
        ],
        "\nkubiad.s.c.l": [
            1122085995
        ],
        "kubiadefault.svc.cluster.local.\nthe": [
            1122085995
        ],
        "\nanswer": [
            1122085995
        ],
        "is\nperform": [
            1122085995
        ],
        "performed\nlike": [
            1122085995
        ],
        "this:\ndnsresolvesrv(kubia.default.svc.cluster.local\"": [
            1122085995
        ],
        "callbackfunction);\nyou’ll": [
            1122085995
        ],
        "peers\nlisting": [
            1122085995
        ],
        "\n\n301discovering": [
            1122085995
        ],
        "statefulset\nnotethe": [
            1122085995
        ],
        "kubia-1\n10.4.1": [
            1122085995
        ],
        "dns\nyour": [
            1122085995
        ],
        "completely\nindependently": [
            1122085995
        ],
        "others—no": [
            1122085995
        ],
        "get\nthem": [
            1122085995
        ],
        "next\n": [
            1122085995
        ],
        "\nkubia-\npublic\n": [
            1122085995
        ],
        "data\nentries": [
            1122085995
        ],
        "many\nrequests": [
            1122085995
        ],
        "cluster\nnodes": [
            1122085995
        ],
        "this\n": [
            1122085995
        ],
        "full\nsource": [
            1122085995
        ],
        "archive;": [
            1122085995
        ],
        "important\nparts)\n...\nconst": [
            1122085995
        ],
        "require(dns');\nconst": [
            1122085995
        ],
        "/var/data/kubiatxt\";\nconst": [
            1122085995
        ],
        "servicename": [
            1122085995
        ],
        "kubiadefault.svc.cluster.local\";\nconst": [
            1122085995
        ],
        "8080;\n..\nvar": [
            1122085995
        ],
        "(requesturl": [
            1122085995
        ],
        "/data')": [
            1122085995
        ],
        "yet;\n": [
            1122085995
        ],
        "responseend(data);\n": [
            1122085995
        ],
        "responsewrite(data": [
            1122085995
        ],
        "cluster:\\n);\n": [
            1122085995
        ],
        "dnsresolvesrv(servicename": [
            1122085995
        ],
        "(err": [
            1122085995
        ],
        "addresses)": [
            1122085995
        ],
        "(err)": [
            1122085995
        ],
        "responseend(could": [
            1122085995
        ],
        "records:": [
            1122085995
        ],
        "err);\n": [
            1122085995
        ],
        "numresponses": [
            1122085995
        ],
        "0;\n": [
            1122085995
        ],
        "(addresseslength": [
            1122085995
        ],
        "responseend(no": [
            1122085995
        ],
        "discovered);\n": [
            1122085995
        ],
        "{\nlisting": [
            1122085995
        ],
        "sample": [
            1122085995
        ],
        "kubia-pet-peers-image/appjs\nthe": [
            1122085995
        ],
        "\nperforms": [
            1122085995
        ],
        "\nlookup": [
            1122085995
        ],
        "\nsrv": [
            1122085995
        ],
        "\n\n302chapter": [
            1122085995
        ],
        "addressesforeach(function": [
            1122085995
        ],
        "(item)": [
            1122085995
        ],
        "requestoptions": [
            1122085995
        ],
        "itemname": [
            1122085995
        ],
        "/data'\n": [
            1122085995
        ],
        "};\n": [
            1122085995
        ],
        "httpget(requestoptions": [
            1122085995
        ],
        "(returneddata)": [
            1122085995
        ],
        "numresponses++;\n": [
            1122085995
        ],
        "responsewrite(-": [
            1122085995
        ],
        "returneddata);\n": [
            1122085995
        ],
        "responsewrite(\\n\");\n": [
            1122085995
        ],
        "(numresponses": [
            1122085995
        ],
        "addresseslength)": [
            1122085995
        ],
        "responseend();\n": [
            1122085995
        ],
        "}\n};\n..\nfigure": [
            1122085995
        ],
        "as\nsimple": [
            1122085995
        ],
        "possible)": [
            1122085995
        ],
        "on\neach": [
            1122085995
        ],
        "them\nthe": [
            1122085995
        ],
        "dockerio/\nluksa/kubia-pet-peers\n.\n10.4.2": [
            1122085995
        ],
        "statefulset\nyour": [
            1122085995
        ],
        "\npointed": [
            1122085995
        ],
        "\nthen": [
            1122085995
        ],
        "contacted": [
            1122085995
        ],
        "data\ncurl\ndns\n1.get": [
            1122085995
        ],
        "/\n4get": [
            1122085995
        ],
        "/data\n5get": [
            1122085995
        ],
        "/data\n2": [
            1122085995
        ],
        "lookup\n6": [
            1122085995
        ],
        "collated": [
            1122085995
        ],
        "data\nkubia-0kubia-1\nkubia-2\n3get": [
            1122085995
        ],
        "/data\nfigure": [
            1122085995
        ],
        "\n\n303discovering": [
            1122085995
        ],
        "statefulset\nupdate": [
            1122085995
        ],
        "would\nbe": [
            1122085995
        ],
        "option):\n$": [
            1122085995
        ],
        "change\nspecreplicas": [
            1122085995
        ],
        "spectemplate.spec.containers.image": [
            1122085995
        ],
        "attri-\nbute": [
            1122085995
        ],
        "(\nluksa/kubia-pet-peers": [
            1122085995
        ],
        "luksa/kubia-\npet\n)": [
            1122085995
        ],
        "were\nrunning": [
            1122085995
        ],
        "\nkubia-2": [
            1122085995
        ],
        "start-\ning": [
            1122085995
        ],
        "25m\nkubia-1": [
            1122085995
        ],
        "26m\nkubia-2": [
            1122085995
        ],
        "rep-\nlicas?": [
            1122085995
        ],
        "judging": [
            1122085995
        ],
        "expected\nbecause": [
            1122085995
        ],
        "deployments\nso": [
            1122085995
        ],
        "delete\nthe": [
            1122085995
        ],
        "new\ntemplate:\n$": [
            1122085995
        ],
        "kubia-1\npod": [
            1122085995
        ],
        "kubia-1\"": [
            1122085995
        ],
        "deleted\nnotestarting": [
            1122085995
        ],
        "rolling\nupdates": [
            1122085995
        ],
        "statefulset’s\nspecupdatestrategy": [
            1122085995
        ],
        "more\ninformation\n10.4.3": [
            1122085995
        ],
        "store\nonce": [
            1122085995
        ],
        "shiny": [
            1122085995
        ],
        "as\nexpected": [
            1122085995
        ],
        "shining": [
            1122085995
        ],
        "\\\n➥": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/\ndata": [
            1122085995
        ],
        "kubia-1\n$": [
            1122085995
        ],
        "weather": [
            1122085995
        ],
        "sweet": [
            1122085995
        ],
        "kubia-0\nnow": [
            1122085995
        ],
        "\n\n304chapter": [
            1122085995
        ],
        "applications\n$": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/default/services\n➥": [
            1122085995
        ],
        "/kubia-public/proxy/\nyouve": [
            1122085995
        ],
        "kubia-2\ndata": [
            1122085995
        ],
        "node:\n-": [
            1122085995
        ],
        "kubia-0kubia.default.svc.cluster.local:": [
            1122085995
        ],
        "sweet\n-": [
            1122085995
        ],
        "kubia-1kubia.default.svc.cluster.local:": [
            1122085995
        ],
        "shining\n-": [
            1122085995
        ],
        "kubia-2kubia.default.svc.cluster.local:": [
            1122085995
        ],
        "yet\nnice!": [
            1122085995
        ],
        "reaches": [
            1122085995
        ],
        "discovers": [
            1122085995
        ],
        "its\npeers": [
            1122085995
        ],
        "gathers": [
            1122085995
        ],
        "you\nscale": [
            1122085995
        ],
        "servicing": [
            1122085995
        ],
        "find\nall": [
            1122085995
        ],
        "how\ninstances": [
            1122085995
        ],
        "horizontal\nscaling": [
            1122085995
        ],
        "ease\n10.5": [
            1122085995
        ],
        "\nfailures\nin": [
            1122085995
        ],
        "102.4": [
            1122085995
        ],
        "stateful\npod": [
            1122085995
        ],
        "fails\nabruptly": [
            1122085995
        ],
        "know\nwhether": [
            1122085995
        ],
        "still\nreachable": [
            1122085995
        ],
        "master\n": [
            1122085995
        ],
        "is\nno": [
            1122085995
        ],
        "the\nadmin": [
            1122085995
        ],
        "(doing": [
            1122085995
        ],
        "deletes\nall": [
            1122085995
        ],
        "node)\n": [
            1122085995
        ],
        "statefulsets\nand": [
            1122085995
        ],
        "disconnected": [
            1122085995
        ],
        "network\n10.5.1": [
            1122085995
        ],
        "disconnecting": [
            1122085995
        ],
        "shutting\ndown": [
            1122085995
        ],
        "\neth0": [
            1122085995
        ],
        "instead\nshutting": [
            1122085995
        ],
        "adapter\nto": [
            1122085995
        ],
        "gke-kubia-default-pool-32a2cac8-m0g1\nlisting": [
            1122085995
        ],
        "\n\n305understanding": [
            1122085995
        ],
        "failures\nthen": [
            1122085995
        ],
        "down\nyour": [
            1122085995
        ],
        "continue\nchecking": [
            1122085995
        ],
        "master\nwith": [
            1122085995
        ],
        "\nnotready": [
            1122085995
        ],
        "this\nwhen": [
            1122085995
        ],
        "shows\n$": [
            1122085995
        ],
        "version\ngke-kubia-default-pool-32a2cac8-596v": [
            1122085995
        ],
        "v16.2\ngke-kubia-default-pool-32a2cac8-m0g1": [
            1122085995
        ],
        "v16.2\ngke-kubia-default-pool-32a2cac8-sgl7": [
            1122085995
        ],
        "v16.2\nbecause": [
            1122085995
        ],
        "the\nstatus": [
            1122085995
        ],
        "15m\nkubia-1": [
            1122085995
        ],
        "14m\nkubia-2": [
            1122085995
        ],
        "13m\nas": [
            1122085995
        ],
        "(and\nstill": [
            1122085995
        ],
        "down\nunderstanding": [
            1122085995
        ],
        "unknown\nif": [
            1122085995
        ],
        "statuses": [
            1122085995
        ],
        "pod\nwould": [
            1122085995
        ],
        "configurable)": [
            1122085995
        ],
        "plane)": [
            1122085995
        ],
        "evicts": [
            1122085995
        ],
        "pod\nby": [
            1122085995
        ],
        "ter-\nminating": [
            1122085995
        ],
        "(because\nyou": [
            1122085995
        ],
        "network)": [
            1122085995
        ],
        "keep\nrunning\nlisting": [
            1122085995
        ],
        "observing": [
            1122085995
        ],
        "notready\nlisting": [
            1122085995
        ],
        "notready\n": [
            1122085995
        ],
        "\n\n306chapter": [
            1122085995
        ],
        "kubia-0\nname:": [
            1122085995
        ],
        "kubia-0\nnamespace:": [
            1122085995
        ],
        "gke-kubia-default-pool-32a2cac8-m0g1/10132.0.2\n...\nstatus:": [
            1122085995
        ],
        "(expires": [
            1122085995
        ],
        "tue": [
            1122085995
        ],
        "15:06:09": [
            1122085995
        ],
        "+0200)\nreason:": [
            1122085995
        ],
        "nodelost\nmessage:": [
            1122085995
        ],
        "gke-kubia-default-pool-32a2cac8-m0g1": [
            1122085995
        ],
        "unresponsive\nthe": [
            1122085995
        ],
        "nodelost": [
            1122085995
        ],
        "termina-\ntion": [
            1122085995
        ],
        "unresponsive\nnotewhat’s": [
            1122085995
        ],
        "plane’s": [
            1122085995
        ],
        "reality\nthe": [
            1122085995
        ],
        "all\n10.5.2": [
            1122085995
        ],
        "handle\nclients": [
            1122085995
        ],
        "as\nmentioned": [
            1122085995
        ],
        "\ndeleting": [
            1122085995
        ],
        "way\ndelete": [
            1122085995
        ],
        "deleted\nall": [
            1122085995
        ],
        "confirm:": [
            1122085995
        ],
        "13m\nthat’s": [
            1122085995
        ],
        "it\nwhy": [
            1122085995
        ],
        "name—\nthis": [
            1122085995
        ],
        "\nage": [
            1122085995
        ],
        "be\nmerely": [
            1122085995
        ],
        "seconds\nlisting": [
            1122085995
        ],
        "\n\n307summary\nunderstanding": [
            1122085995
        ],
        "con-\ntrol": [
            1122085995
        ],
        "evict": [
            1122085995
        ],
        "\nterminating\nthe": [
            1122085995
        ],
        "\nforcibly": [
            1122085995
        ],
        "--force": [
            1122085995
        ],
        "--grace-period": [
            1122085995
        ],
        "0\nwarning:": [
            1122085995
        ],
        "confirmation": [
            1122085995
        ],
        "\ncluster": [
            1122085995
        ],
        "indefinitely\npod": [
            1122085995
        ],
        "deleted\nyou": [
            1122085995
        ],
        "dis-\nplayed": [
            1122085995
        ],
        "finally\nsee": [
            1122085995
        ],
        "20m\nkubia-2": [
            1122085995
        ],
        "19m\nwarningdon’t": [
            1122085995
        ],
        "forcibly": [
            1122085995
        ],
        "forever)": [
            1122085995
        ],
        "online\nyou": [
            1122085995
        ],
        "terminal\nby": [
            1122085995
        ],
        "issuing": [
            1122085995
        ],
        "<node": [
            1122085995
        ],
        "name>\n106": [
            1122085995
        ],
        "summary\nthis": [
            1122085995
        ],
        "concludes": [
            1122085995
        ],
        "chapter\nhas": [
            1122085995
        ],
        "to\ngive": [
            1122085995
        ],
        "storage\nprovide": [
            1122085995
        ],
        "pod\ncreate": [
            1122085995
        ],
        "service\nscale": [
            1122085995
        ],
        "statefulset\ndiscover": [
            1122085995
        ],
        "dns\n": [
            1122085995
        ],
        "\n\n308chapter": [
            1122085995
        ],
        "applications\nconnect": [
            1122085995
        ],
        "names\nforcibly": [
            1122085995
        ],
        "pods\nnow": [
            1122085995
        ],
        "major": [
            1122085995
        ],
        "and\nmanage": [
            1122085995
        ],
        "chapter\nyou’ll": [
            1122085995
        ],
        "and\nkeep": [
            1122085995
        ],
        "\n\n309\nunderstanding\nkubernetes": [
            1122085995
        ],
        "internals\nby": [
            1122085995
        ],
        "spent": [
            1122085995
        ],
        "time\nexplaining": [
            1122085995
        ],
        "opinion": [
            1122085995
        ],
        "to\ngo": [
            1122085995
        ],
        "scheduled\nor": [
            1122085995
        ],
        "deployed\nresources": [
            1122085995
        ],
        "implemented\nthis": [
            1122085995
        ],
        "covers\nwhat": [
            1122085995
        ],
        "cluster\nwhat": [
            1122085995
        ],
        "it\nhow": [
            1122085995
        ],
        "pod\nwhat": [
            1122085995
        ],
        "is\nhow": [
            1122085995
        ],
        "works\nhow": [
            1122085995
        ],
        "work\nhow": [
            1122085995
        ],
        "high-availability": [
            1122085995
        ],
        "achieved\n": [
            1122085995
        ],
        "\n\n310chapter": [
            1122085995
        ],
        "11understanding": [
            1122085995
        ],
        "internals\n111": [
            1122085995
        ],
        "architecture\nbefore": [
            1122085995
        ],
        "the\ncomponents": [
            1122085995
        ],
        "parts:\nthe": [
            1122085995
        ],
        "plane\nthe": [
            1122085995
        ],
        "nodes\nlet’s": [
            1122085995
        ],
        "them\ncomponents": [
            1122085995
        ],
        "refresh\nyour": [
            1122085995
        ],
        "storage\nthe": [
            1122085995
        ],
        "server\nthe": [
            1122085995
        ],
        "scheduler\nthe": [
            1122085995
        ],
        "manager\nthese": [
            1122085995
        ],
        "runs\nthe": [
            1122085995
        ],
        "\ncomponents": [
            1122085995
        ],
        "each\nworker": [
            1122085995
        ],
        "node:\nthe": [
            1122085995
        ],
        "kubelet\nthe": [
            1122085995
        ],
        "(kube-proxy)\nthe": [
            1122085995
        ],
        "others)\nadd-on": [
            1122085995
        ],
        "components\nbeside": [
            1122085995
        ],
        "discussed\nso": [
            1122085995
        ],
        "includes\nthe": [
            1122085995
        ],
        "dashboard\nan": [
            1122085995
        ],
        "controller\nheapster": [
            1122085995
        ],
        "14\nthe": [
            1122085995
        ],
        "this\nchapter)\n111.1": [
            1122085995
        ],
        "components\nthe": [
            1122085995
        ],
        "111.\n": [
            1122085995
        ],
        "\n\n311understanding": [
            1122085995
        ],
        "architecture\nto": [
            1122085995
        ],
        "running\nbut": [
            1122085995
        ],
        "components\nyou’ll": [
            1122085995
        ],
        "communicate\nkubernetes": [
            1122085995
        ],
        "don’t\ntalk": [
            1122085995
        ],
        "communicates\nwith": [
            1122085995
        ],
        "instead\nmodify": [
            1122085995
        ],
        "always\ninitiated": [
            1122085995
        ],
        "111.": [
            1122085995
        ],
        "connect\nto": [
            1122085995
        ],
        "fetch": [
            1122085995
        ],
        "command\nnotethe": [
            1122085995
        ],
        "attaches\nto": [
            1122085995
        ],
        "one\nrunning": [
            1122085995
        ],
        "components\nalthough": [
            1122085995
        ],
        "node\nthe": [
            1122085995
        ],
        "there\nchecking": [
            1122085995
        ],
        "componentstatus": [
            1122085995
        ],
        "the\nhealth": [
            1122085995
        ],
        "and\ntheir": [
            1122085995
        ],
        "\nkubectl:\n$": [
            1122085995
        ],
        "componentstatuses\nname": [
            1122085995
        ],
        "error\nscheduler": [
            1122085995
        ],
        "ok\ncontroller-manager": [
            1122085995
        ],
        "ok\netcd-0": [
            1122085995
        ],
        "{health\":": [
            1122085995
        ],
        "true\"}\ncontrol": [
            1122085995
        ],
        "(master": [
            1122085995
        ],
        "node)worker": [
            1122085995
        ],
        "node(s)\netcd\napi": [
            1122085995
        ],
        "server\nkube-proxy\nkubelet\nscheduler\ncontroller\nmanager\ncontroller\nruntime\nfigure": [
            1122085995
        ],
        "\nplane": [
            1122085995
        ],
        "\n\n312chapter": [
            1122085995
        ],
        "internals\ncan": [
            1122085995
        ],
        "ensure\nhigh": [
            1122085995
        ],
        "sched-\nuler": [
            1122085995
        ],
        "time—with": [
            1122085995
        ],
        "in\nstandby": [
            1122085995
        ],
        "mode\nhow": [
            1122085995
        ],
        "111).": [
            1122085995
        ],
        "surprised\nto": [
            1122085995
        ],
        "the\ncontrol": [
            1122085995
        ],
        "with\nkubeadm": [
            1122085995
        ],
        "b\n$": [
            1122085995
        ],
        "custom-columns=pod:metadatanamenode:spec.nodename": [
            1122085995
        ],
        "--sort-by": [
            1122085995
        ],
        "specnodename": [
            1122085995
        ],
        "kube-system\npod": [
            1122085995
        ],
        "node\nkube-controller-manager-master": [
            1122085995
        ],
        "\nkube-dns-2334855451-37d9k": [
            1122085995
        ],
        "\netcd-master": [
            1122085995
        ],
        "\nkube-apiserver-master": [
            1122085995
        ],
        "\nkube-scheduler-master": [
            1122085995
        ],
        "\nkube-flannel-ds-tgj9k": [
            1122085995
        ],
        "node1": [
            1122085995
        ],
        "\nkube-proxy-ny3xm": [
            1122085995
        ],
        "\nkube-flannel-ds-0eek8": [
            1122085995
        ],
        "node2": [
            1122085995
        ],
        "\nkube-proxy-sp362": [
            1122085995
        ],
        "\nkube-flannel-ds-r5yf4": [
            1122085995
        ],
        "node3": [
            1122085995
        ],
        "\nkube-proxy-og9ac": [
            1122085995
        ],
        "kube-proxy\nand": [
            1122085995
        ],
        "flannel": [
            1122085995
        ],
        "overlay": [
            1122085995
        ],
        "about\nflannel": [
            1122085995
        ],
        "\ntipas": [
            1122085995
        ],
        "columns\nwith": [
            1122085995
        ],
        "custom-columns": [
            1122085995
        ],
        "sort": [
            1122085995
        ],
        "--sort-by\nnow": [
            1122085995
        ],
        "lowest": [
            1122085995
        ],
        "com-\nponent": [
            1122085995
        ],
        "plane—the": [
            1122085995
        ],
        "storage\n11.1.2": [
            1122085995
        ],
        "etcd\nall": [
            1122085995
        ],
        "book—pods": [
            1122085995
        ],
        "replicationcontrollers\nservices,": [
            1122085995
        ],
        "on—need": [
            1122085995
        ],
        "so\ntheir": [
            1122085995
        ],
        "survive": [
            1122085995
        ],
        "etcd\nlisting": [
            1122085995
        ],
        "pods\netcd": [
            1122085995
        ],
        "\ncontroller": [
            1122085995
        ],
        "master\nthe": [
            1122085995
        ],
        "kube": [
            1122085995
        ],
        "\nflannel": [
            1122085995
        ],
        "\n\n313understanding": [
            1122085995
        ],
        "architecture\nwhich": [
            1122085995
        ],
        "distributed\nyou": [
            1122085995
        ],
        "performance\n": [
            1122085995
        ],
        "indirectly": [
            1122085995
        ],
        "this\nbrings": [
            1122085995
        ],
        "robust": [
            1122085995
        ],
        "optimistic": [
            1122085995
        ],
        "locking": [
            1122085995
        ],
        "as\nvalidation;": [
            1122085995
        ],
        "other\ncomponents": [
            1122085995
        ],
        "worth": [
            1122085995
        ],
        "emphasizing": [
            1122085995
        ],
        "that\netcd": [
            1122085995
        ],
        "metadata\nhow": [
            1122085995
        ],
        "etcd\nas": [
            1122085995
        ],
        "3\nis": [
            1122085995
        ],
        "recommended": [
            1122085995
        ],
        "hier-\narchical": [
            1122085995
        ],
        "each\nkey": [
            1122085995
        ],
        "a\ncorresponding": [
            1122085995
        ],
        "format\nremains": [
            1122085995
        ],
        "(keys": [
            1122085995
        ],
        "slashes)": [
            1122085995
        ],
        "being\ngrouped": [
            1122085995
        ],
        "/registry": [
            1122085995
        ],
        "/registry\n$": [
            1122085995
        ],
        "etcdctl": [
            1122085995
        ],
        "/registry\n/registry/configmaps\n/registry/daemonsets\n/registry/deployments\n/registry/events\n/registry/namespaces\n/registry/pods\n..\nabout": [
            1122085995
        ],
        "concurrency": [
            1122085995
        ],
        "control\noptimistic": [
            1122085995
        ],
        "(sometimes": [
            1122085995
        ],
        "locking)": [
            1122085995
        ],
        "a\nmethod": [
            1122085995
        ],
        "or\nupdated": [
            1122085995
        ],
        "lock": [
            1122085995
        ],
        "the\nversion": [
            1122085995
        ],
        "read\nthe": [
            1122085995
        ],
        "rejected\nand": [
            1122085995
        ],
        "re-read": [
            1122085995
        ],
        "succeeds\nall": [
            1122085995
        ],
        "\nmetadataresourceversion": [
            1122085995
        ],
        "clients\nneed": [
            1122085995
        ],
        "doesn’t\nmatch": [
            1122085995
        ],
        "rejects": [
            1122085995
        ],
        "update\nlisting": [
            1122085995
        ],
        "\n\n314chapter": [
            1122085995
        ],
        "internals\nyou’ll": [
            1122085995
        ],
        "given\nprefix": [
            1122085995
        ],
        "\netcdctl": [
            1122085995
        ],
        "--prefix=true\nthe": [
            1122085995
        ],
        "/registry/pods": [
            1122085995
        ],
        "directory\n$": [
            1122085995
        ],
        "/registry/pods\n/registry/pods/default\n/registry/pods/kube-system\nas": [
            1122085995
        ],
        "infer": [
            1122085995
        ],
        "the\nkube-system": [
            1122085995
        ],
        "/registry/pods/default": [
            1122085995
        ],
        "/registry/pods/default\n/registry/pods/default/kubia-159041347-xk0vc\n/registry/pods/default/kubia-159041347-wt6ga\n/registry/pods/default/kubia-159041347-hp2o5\neach": [
            1122085995
        ],
        "key-value\nentries": [
            1122085995
        ],
        "them\n$": [
            1122085995
        ],
        "/registry/pods/default/kubia-159041347-wt6ga\n{kind\":\"pod\"\"apiversion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\",\n\"generatename\":\"kubia-159041347-\",\"namespace\":\"default\",\"selflink\":..\nyou’ll": [
            1122085995
        ],
        "the\napi": [
            1122085995
        ],
        "representation": [
            1122085995
        ],
        "of\netcd’s": [
            1122085995
        ],
        "hierarchical": [
            1122085995
        ],
        "right?\nwarningprior": [
            1122085995
        ],
        "secret\nresource": [
            1122085995
        ],
        "encrypted)": [
            1122085995
        ],
        "direct\naccess": [
            1122085995
        ],
        "are\nencrypted": [
            1122085995
        ],
        "securely\nensuring": [
            1122085995
        ],
        "consistency": [
            1122085995
        ],
        "validity": [
            1122085995
        ],
        "objects\nremember": [
            1122085995
        ],
        "google’s": [
            1122085995
        ],
        "are\nwhat": [
            1122085995
        ],
        "components\naccess": [
            1122085995
        ],
        "adhere": [
            1122085995
        ],
        "directory\nlisting": [
            1122085995
        ],
        "namespace\nlisting": [
            1122085995
        ],
        "\n\n315understanding": [
            1122085995
        ],
        "architecture\nthe": [
            1122085995
        ],
        "adhering": [
            1122085995
        ],
        "inconsistent": [
            1122085995
        ],
        "improves": [
            1122085995
        ],
        "go\nthrough": [
            1122085995
        ],
        "exists\nif": [
            1122085995
        ],
        "always\nvalid": [
            1122085995
        ],
        "clustered\nfor": [
            1122085995
        ],
        "etcd\nmultiple": [
            1122085995
        ],
        "system\nneeds": [
            1122085995
        ],
        "raft": [
            1122085995
        ],
        "consensus\nalgorithm": [
            1122085995
        ],
        "is\neither": [
            1122085995
        ],
        "majority": [
            1122085995
        ],
        "agrees": [
            1122085995
        ],
        "previ-\nously": [
            1122085995
        ],
        "actual\ncurrent": [
            1122085995
        ],
        "is\nthe": [
            1122085995
        ],
        "instances)": [
            1122085995
        ],
        "algorithm": [
            1122085995
        ],
        "quorum)": [
            1122085995
        ],
        "progress\nto": [
            1122085995
        ],
        "splits": [
            1122085995
        ],
        "diverge": [
            1122085995
        ],
        "previous\nstate": [
            1122085995
        ],
        "obvi-\nously": [
            1122085995
        ],
        "can’t\nwhen": [
            1122085995
        ],
        "reconnect": [
            1122085995
        ],
        "112).\nclients(s)clients(s)clients(s)\netcd-0\netcd-1etcd-2\nthe": [
            1122085995
        ],
        "know\nthere": [
            1122085995
        ],
        "nodes\nin": [
            1122085995
        ],
        "cluster\netcd-0\netcd-1\nthese": [
            1122085995
        ],
        "know\nthey": [
            1122085995
        ],
        "quorum\nand": [
            1122085995
        ],
        "state\nchanges": [
            1122085995
        ],
        "clients\netcd-2\nthis": [
            1122085995
        ],
        "does\nnot": [
            1122085995
        ],
        "quorum": [
            1122085995
        ],
        "not\nallow": [
            1122085995
        ],
        "changes\nnetwork\nsplit\nfigure": [
            1122085995
        ],
        "split-brain": [
            1122085995
        ],
        "scenario": [
            1122085995
        ],
        "(quorum)": [
            1122085995
        ],
        "\nstate": [
            1122085995
        ],
        "changes\n": [
            1122085995
        ],
        "\n\n316chapter": [
            1122085995
        ],
        "internals\nwhy": [
            1122085995
        ],
        "odd": [
            1122085995
        ],
        "number\netcd": [
            1122085995
        ],
        "know\nwhy": [
            1122085995
        ],
        "requires\nboth": [
            1122085995
        ],
        "cluster\ncan’t": [
            1122085995
        ],
        "worse\nthan": [
            1122085995
        ],
        "fail-\ning": [
            1122085995
        ],
        "instances\none": [
            1122085995
        ],
        "(of": [
            1122085995
        ],
        "two)": [
            1122085995
        ],
        "need\nthree": [
            1122085995
        ],
        "(two": [
            1122085995
        ],
        "enough)": [
            1122085995
        ],
        "three-": [
            1122085995
        ],
        "four-instance": [
            1122085995
        ],
        "possibility": [
            1122085995
        ],
        "(compared": [
            1122085995
        ],
        "nodes)\n": [
            1122085995
        ],
        "seven": [
            1122085995
        ],
        "can\nhandle": [
            1122085995
        ],
        "two-": [
            1122085995
        ],
        "\n111.3": [
            1122085995
        ],
        "components\nand": [
            1122085995
        ],
        "delete)\ninterface": [
            1122085995
        ],
        "stores\nthat": [
            1122085995
        ],
        "etcd\n": [
            1122085995
        ],
        "performs\nvalidation": [
            1122085995
        ],
        "improperly": [
            1122085995
        ],
        "(which\nthey": [
            1122085995
        ],
        "han-\ndles": [
            1122085995
        ],
        "overridden": [
            1122085995
        ],
        "clients\nin": [
            1122085995
        ],
        "concurrent": [
            1122085995
        ],
        "updates\n": [
            1122085995
        ],
        "been\nusing": [
            1122085995
        ],
        "post\nrequest": [
            1122085995
        ],
        "paragraphs\napi": [
            1122085995
        ],
        "server\netcd\nauthentication\nplugin": [
            1122085995
        ],
        "1\nauthentication\nplugin": [
            1122085995
        ],
        "2\nauthentication\nplugin": [
            1122085995
        ],
        "3\nclient\n()kubectl\nhttp": [
            1122085995
        ],
        "post\nrequest\nauthorization\nplugin": [
            1122085995
        ],
        "1\nauthorization\nplugin": [
            1122085995
        ],
        "2\nauthorization\nplugin": [
            1122085995
        ],
        "3\nadmission\ncontrol": [
            1122085995
        ],
        "1\nadmission\ncontrol": [
            1122085995
        ],
        "2\nadmission\ncontrol": [
            1122085995
        ],
        "3\nresource\nvalidation\nfigure": [
            1122085995
        ],
        "\n\n317understanding": [
            1122085995
        ],
        "architecture\nauthenticating": [
            1122085995
        ],
        "plugins\nfirst": [
            1122085995
        ],
        "plugins": [
            1122085995
        ],
        "inspecting": [
            1122085995
        ],
        "extracted": [
            1122085995
        ],
        "cli-\nent’s": [
            1122085995
        ],
        "\nauthorization": [
            1122085995
        ],
        "extracts": [
            1122085995
        ],
        "belongs\nto": [
            1122085995
        ],
        "stage": [
            1122085995
        ],
        "authorization\nauthorizing": [
            1122085995
        ],
        "plugins\nbesides": [
            1122085995
        ],
        "more\nauthorization": [
            1122085995
        ],
        "authenticated": [
            1122085995
        ],
        "can\nperform": [
            1122085995
        ],
        "creating\npods": [
            1122085995
        ],
        "consults": [
            1122085995
        ],
        "whether\nthe": [
            1122085995
        ],
        "user\ncan": [
            1122085995
        ],
        "stage\nvalidating": [
            1122085995
        ],
        "admission": [
            1122085995
        ],
        "plugins\nif": [
            1122085995
        ],
        "sent\nthrough": [
            1122085995
        ],
        "admission\ncontrol": [
            1122085995
        ],
        "reasons": [
            1122085995
        ],
        "they\nmay": [
            1122085995
        ],
        "default\nvalues": [
            1122085995
        ],
        "which\naren’t": [
            1122085995
        ],
        "reject": [
            1122085995
        ],
        "resource\npasses": [
            1122085995
        ],
        "plugins\nnotewhen": [
            1122085995
        ],
        "control\nexamples": [
            1122085995
        ],
        "include\nalwayspullimages—overrides": [
            1122085995
        ],
        "forcing\nthe": [
            1122085995
        ],
        "deployed\nserviceaccount—applies": [
            1122085995
        ],
        "specify\nit": [
            1122085995
        ],
        "explicitly\nnamespacelifecycle—prevents": [
            1122085995
        ],
        "namespaces\nresourcequota—ensures": [
            1122085995
        ],
        "cpu\nand": [
            1122085995
        ],
        "allotted": [
            1122085995
        ],
        "this\nin": [
            1122085995
        ],
        "14\nyou’ll": [
            1122085995
        ],
        "documen-\ntation": [
            1122085995
        ],
        "https://kubernetesio/docs/admin/admission-controllers/.\n": [
            1122085995
        ],
        "\n\n318chapter": [
            1122085995
        ],
        "internals\nvalidating": [
            1122085995
        ],
        "persistently\nafter": [
            1122085995
        ],
        "server\nthen": [
            1122085995
        ],
        "validates": [
            1122085995
        ],
        "client\n11.1.4": [
            1122085995
        ],
        "\nchanges\nthe": [
            1122085995
        ],
        "is\nenable": [
            1122085995
        ],
        "observe": [
            1122085995
        ],
        "task\nit": [
            1122085995
        ],
        "metadata\n": [
            1122085995
        ],
        "modifications": [
            1122085995
        ],
        "the\nwatched": [
            1122085995
        ],
        "watching": [
            1122085995
        ],
        "clients\ncan": [
            1122085995
        ],
        "etcd\nand": [
            1122085995
        ],
        "relayed": [
            1122085995
        ],
        "moment\none": [
            1122085995
        ],
        "watching\nresources": [
            1122085995
        ],
        "poll": [
            1122085995
        ],
        "--watch\nflag": [
            1122085995
        ],
        "modification": [
            1122085995
        ],
        "--watch\nname": [
            1122085995
        ],
        "age\nlisting": [
            1122085995
        ],
        "deleted\nvarious\nclients\nkubectl\napi": [
            1122085995
        ],
        "server\n1get": [
            1122085995
        ],
        "/../pods?watch=true\n2.post": [
            1122085995
        ],
        "/../pods/pod-xyz\n5.": [
            1122085995
        ],
        "watchers\n3": [
            1122085995
        ],
        "object\nin": [
            1122085995
        ],
        "etcd\n4": [
            1122085995
        ],
        "modification\nnotification\netcd\nfigure": [
            1122085995
        ],
        "\nwatchers\n": [
            1122085995
        ],
        "\n\n319understanding": [
            1122085995
        ],
        "architecture\nkubia-159041347-14j3i": [
            1122085995
        ],
        "0s\nkubia-159041347-14j3i": [
            1122085995
        ],
        "1s\nkubia-159041347-14j3i": [
            1122085995
        ],
        "3s\nkubia-159041347-14j3i": [
            1122085995
        ],
        "5s\nkubia-159041347-14j3i": [
            1122085995
        ],
        "9s\nkubia-159041347-14j3i": [
            1122085995
        ],
        "17s\nkubia-159041347-14j3i": [
            1122085995
        ],
        "17s\nyou": [
            1122085995
        ],
        "--watch\nthe": [
            1122085995
        ],
        "plane\ncomponent": [
            1122085995
        ],
        "about\n11.1.5": [
            1122085995
        ],
        "scheduler\nyou’ve": [
            1122085995
        ],
        "should\nrun": [
            1122085995
        ],
        "afar": [
            1122085995
        ],
        "looks\nsimple": [
            1122085995
        ],
        "mech-\nanism": [
            1122085995
        ],
        "that\nnode)": [
            1122085995
        ],
        "(again": [
            1122085995
        ],
        "previously)": [
            1122085995
        ],
        "actual\ntask": [
            1122085995
        ],
        "simplest\nscheduler": [
            1122085995
        ],
        "on\nthat": [
            1122085995
        ],
        "tech-\nniques": [
            1122085995
        ],
        "anticipate": [
            1122085995
        ],
        "be\nscheduled": [
            1122085995
        ],
        "maximize": [
            1122085995
        ],
        "hard-\nware": [
            1122085995
        ],
        "kubernetes’\ndefault": [
            1122085995
        ],
        "algorithm\nthe": [
            1122085995
        ],
        "selection": [
            1122085995
        ],
        "115:\nfiltering": [
            1122085995
        ],
        "to\nprioritizing": [
            1122085995
        ],
        "nodes\nhave": [
            1122085995
        ],
        "score": [
            1122085995
        ],
        "across\nall": [
            1122085995
        ],
        "evenly\n": [
            1122085995
        ],
        "\n\n320chapter": [
            1122085995
        ],
        "internals\nfinding": [
            1122085995
        ],
        "nodes\nto": [
            1122085995
        ],
        "passes": [
            1122085995
        ],
        "predicate": [
            1122085995
        ],
        "things\nsuch": [
            1122085995
        ],
        "as\ncan": [
            1122085995
        ],
        "fulfill": [
            1122085995
        ],
        "resources?": [
            1122085995
        ],
        "14\nis": [
            1122085995
        ],
        "(is": [
            1122085995
        ],
        "pres-\nsure": [
            1122085995
        ],
        "condition)?": [
            1122085995
        ],
        "\nif": [
            1122085995
        ],
        "node?\ndoes": [
            1122085995
        ],
        "defined)?\nif": [
            1122085995
        ],
        "(discussed": [
            1122085995
        ],
        "13)\nis": [
            1122085995
        ],
        "same\nvolume?\ndoes": [
            1122085995
        ],
        "tolerate": [
            1122085995
        ],
        "node?": [
            1122085995
        ],
        "16\ndoes": [
            1122085995
        ],
        "rules?": [
            1122085995
        ],
        "yes\nwould": [
            1122085995
        ],
        "break": [
            1122085995
        ],
        "16\nall": [
            1122085995
        ],
        "eligible": [
            1122085995
        ],
        "ends": [
            1122085995
        ],
        "any\nof": [
            1122085995
        ],
        "definition\nselecting": [
            1122085995
        ],
        "pod\neven": [
            1122085995
        ],
        "a\nbetter": [
            1122085995
        ],
        "eli-\ngible": [
            1122085995
        ],
        "isn’t\nrunning": [
            1122085995
        ],
        "favor": [
            1122085995
        ],
        "3node": [
            1122085995
        ],
        "4\nnode": [
            1122085995
        ],
        "5..\nfind": [
            1122085995
        ],
        "acceptable\nnodes\nnode": [
            1122085995
        ],
        "5..\nprioritize": [
            1122085995
        ],
        "nodes\nand": [
            1122085995
        ],
        "the\ntop": [
            1122085995
        ],
        "one\nnode": [
            1122085995
        ],
        "3\nnode": [
            1122085995
        ],
        "4\nfigure": [
            1122085995
        ],
        "\n\n321understanding": [
            1122085995
        ],
        "relinquish": [
            1122085995
        ],
        "the\ncloud": [
            1122085995
        ],
        "provider": [
            1122085995
        ],
        "money": [
            1122085995
        ],
        "pods\nconsider": [
            1122085995
        ],
        "you’d\nwant": [
            1122085995
        ],
        "barely": [
            1122085995
        ],
        "dent": [
            1122085995
        ],
        "nodes\nby": [
            1122085995
        ],
        "force": [
            1122085995
        ],
        "be\nspread": [
            1122085995
        ],
        "anti-\naffinity": [
            1122085995
        ],
        "it\ndepends": [
            1122085995
        ],
        "multitude": [
            1122085995
        ],
        "suit": [
            1122085995
        ],
        "replaced\nwith": [
            1122085995
        ],
        "altogether": [
            1122085995
        ],
        "cluster\nwithout": [
            1122085995
        ],
        "manually\nusing": [
            1122085995
        ],
        "schedulers\ninstead": [
            1122085995
        ],
        "schedulers\nthen": [
            1122085995
        ],
        "particular\npod": [
            1122085995
        ],
        "\nschedulername": [
            1122085995
        ],
        "so\nare": [
            1122085995
        ],
        "default-scheduler": [
            1122085995
        ],
        "ignored": [
            1122085995
        ],
        "another\nscheduler": [
            1122085995
        ],
        "schedulers": [
            1122085995
        ],
        "options\n11.1.6": [
            1122085995
        ],
        "manager\nas": [
            1122085995
        ],
        "in\netcd": [
            1122085995
        ],
        "notify": [
            1122085995
        ],
        "system\nconverges": [
            1122085995
        ],
        "control-\nlers": [
            1122085995
        ],
        "split\nup": [
            1122085995
        ],
        "imple-\nmentation": [
            1122085995
        ],
        "the\nreplication": [
            1122085995
        ],
        "resources)\nreplicaset": [
            1122085995
        ],
        "controllers\n": [
            1122085995
        ],
        "\n\n322chapter": [
            1122085995
        ],
        "internals\ndeployment": [
            1122085995
        ],
        "controller\nstatefulset": [
            1122085995
        ],
        "controller\nnode": [
            1122085995
        ],
        "controller\nservice": [
            1122085995
        ],
        "controller\nendpoints": [
            1122085995
        ],
        "controller\nnamespace": [
            1122085995
        ],
        "controller\npersistentvolume": [
            1122085995
        ],
        "controller\nothers\nwhat": [
            1122085995
        ],
        "list\nyou": [
            1122085995
        ],
        "resources\nare": [
            1122085995
        ],
        "are\nthe": [
            1122085995
        ],
        "deployed\nresources\nunderstanding": [
            1122085995
        ],
        "it\ncontrollers": [
            1122085995
        ],
        "to\nresources": [
            1122085995
        ],
        "(deployments": [
            1122085995
        ],
        "change\nwhether": [
            1122085995
        ],
        "object\nmost": [
            1122085995
        ],
        "(to": [
            1122085995
        ],
        "example)\n": [
            1122085995
        ],
        "reconciles": [
            1122085995
        ],
        "state\nwith": [
            1122085995
        ],
        "(specified": [
            1122085995
        ],
        "resource’s": [
            1122085995
        ],
        "new\nactual": [
            1122085995
        ],
        "watches": [
            1122085995
        ],
        "controller\nwon’t": [
            1122085995
        ],
        "miss": [
            1122085995
        ],
        "re-list": [
            1122085995
        ],
        "sure\nthey": [
            1122085995
        ],
        "missed": [
            1122085995
        ],
        "anything\n": [
            1122085995
        ],
        "con-\ntrollers": [
            1122085995
        ],
        "watch\nmechanism": [
            1122085995
        ],
        "111.3": [
            1122085995
        ],
        "responsible": [
            1122085995
        ],
        "briefly": [
            1122085995
        ],
        "in-depth\nview": [
            1122085995
        ],
        "sidebar\nexplains": [
            1122085995
        ],
        "started\na": [
            1122085995
        ],
        "controllers’": [
            1122085995
        ],
        "code\nif": [
            1122085995
        ],
        "strongly": [
            1122085995
        ],
        "encour-\nage": [
            1122085995
        ],
        "tips:\nthe": [
            1122085995
        ],
        "https://githubcom/kubernetes/\nkubernetes/blob/master/pkg/controller.\neach": [
            1122085995
        ],
        "constructor": [
            1122085995
        ],
        "\ninformer": [
            1122085995
        ],
        "is\nbasically": [
            1122085995
        ],
        "listener": [
            1122085995
        ],
        "usually\n": [
            1122085995
        ],
        "\n\n323understanding": [
            1122085995
        ],
        "manager\nthe": [
            1122085995
        ],
        "the\nreplication": [
            1122085995
        ],
        "4\nit’s": [
            1122085995
        ],
        "man-\nager": [
            1122085995
        ],
        "review": [
            1122085995
        ],
        "be\nthought": [
            1122085995
        ],
        "iteration": [
            1122085995
        ],
        "replica\ncount": [
            1122085995
        ],
        "but\nis": [
            1122085995
        ],
        "the\ndesired": [
            1122085995
        ],
        "116).": [
            1122085995
        ],
        "such\nchanges": [
            1122085995
        ],
        "recheck": [
            1122085995
        ],
        "act\naccordingly\n": [
            1122085995
        ],
        "informer": [
            1122085995
        ],
        "listens": [
            1122085995
        ],
        "con-\nstructor": [
            1122085995
        ],
        "watching\nnext": [
            1122085995
        ],
        "\nworker()": [
            1122085995
        ],
        "invoked\neach": [
            1122085995
        ],
        "stored\nin": [
            1122085995
        ],
        "\nsynchandler": [
            1122085995
        ],
        "the\nconstructor": [
            1122085995
        ],
        "that\nfunction": [
            1122085995
        ],
        "magic": [
            1122085995
        ],
        "happens\ncontroller": [
            1122085995
        ],
        "manager\nwatches\ncreates": [
            1122085995
        ],
        "and\ndeletes\nreplication\nmanager\napi": [
            1122085995
        ],
        "server\nreplicationcontroller\nresources\npod": [
            1122085995
        ],
        "resources\nother": [
            1122085995
        ],
        "resources\nfigure": [
            1122085995
        ],
        "\nobjects\n": [
            1122085995
        ],
        "\n\n324chapter": [
            1122085995
        ],
        "internals\nnew": [
            1122085995
        ],
        "lets": [
            1122085995
        ],
        "manipulating": [
            1122085995
        ],
        "objects\nthrough": [
            1122085995
        ],
        "operate\nthe": [
            1122085995
        ],
        "controllers\nthe": [
            1122085995
        ],
        "manager\ndescribed": [
            1122085995
        ],
        "job\ncontrollers": [
            1122085995
        ],
        "their\ncontainers": [
            1122085995
        ],
        "in\nsync": [
            1122085995
        ],
        "pods)\nit": [
            1122085995
        ],
        "old\npods": [
            1122085995
        ],
        "directly\nthe": [
            1122085995
        ],
        "related\ncontrollers": [
            1122085995
        ],
        "control-\nler": [
            1122085995
        ],
        "instantiates": [
            1122085995
        ],
        "instance\nthe": [
            1122085995
        ],
        "cluster’s": [
            1122085995
        ],
        "sync\nwith": [
            1122085995
        ],
        "node’s\nhealth": [
            1122085995
        ],
        "objects\nthey’re": [
            1122085995
        ],
        "users\nthrough": [
            1122085995
        ],
        "controller\nin": [
            1122085995
        ],
        "types\nexist": [
            1122085995
        ],
        "releasing": [
            1122085995
        ],
        "a\nloadbalancer-type": [
            1122085995
        ],
        "\n\n325understanding": [
            1122085995
        ],
        "controller\nyou’ll": [
            1122085995
        ],
        "(ips": [
            1122085995
        ],
        "ports)": [
            1122085995
        ],
        "when\nservices": [
            1122085995
        ],
        "controller\ncreates": [
            1122085995
        ],
        "is\ndeleted\nthe": [
            1122085995
        ],
        "controller\nremember": [
            1122085995
        ],
        "3)?": [
            1122085995
        ],
        "belong\nto": [
            1122085995
        ],
        "resources\nbelonging": [
            1122085995
        ],
        "persistentvolumeclaims\nonce": [
            1122085995
        ],
        "appropriate\npersistentvolume": [
            1122085995
        ],
        "persistentvolume\ncontroller": [
            1122085995
        ],
        "pops": [
            1122085995
        ],
        "smallest": [
            1122085995
        ],
        "matching\nthe": [
            1122085995
        ],
        "declared": [
            1122085995
        ],
        "requested\ncontroller": [
            1122085995
        ],
        "modifies\nand": [
            1122085995
        ],
        "deletes\nendpoints\ncontroller\napi": [
            1122085995
        ],
        "server\nservice": [
            1122085995
        ],
        "resources\npod": [
            1122085995
        ],
        "resources\nendpoints": [
            1122085995
        ],
        "\n\n326chapter": [
            1122085995
        ],
        "internals\nin": [
            1122085995
        ],
        "each\naccess": [
            1122085995
        ],
        "ascending": [
            1122085995
        ],
        "unbound\nand": [
            1122085995
        ],
        "reclaimed": [
            1122085995
        ],
        "(left": [
            1122085995
        ],
        "emptied)\ncontroller": [
            1122085995
        ],
        "controllers\nwork": [
            1122085995
        ],
        "issue": [
            1122085995
        ],
        "of\ninstructions": [
            1122085995
        ],
        "controller\nupdates": [
            1122085995
        ],
        "proxies\nalso": [
            1122085995
        ],
        "handles": [
            1122085995
        ],
        "to\nfully": [
            1122085995
        ],
        "unfold": [
            1122085995
        ],
        "next\n11.1.7": [
            1122085995
        ],
        "does\nin": [
            1122085995
        ],
        "node(s)": [
            1122085995
        ],
        "exactly?\nunderstanding": [
            1122085995
        ],
        "kubelet’s": [
            1122085995
        ],
        "job\nin": [
            1122085995
        ],
        "nutshell": [
            1122085995
        ],
        "a\nworker": [
            1122085995
        ],
        "register": [
            1122085995
        ],
        "node\nresource": [
            1122085995
        ],
        "for\npods": [
            1122085995
        ],
        "this\nby": [
            1122085995
        ],
        "coreos’": [
            1122085995
        ],
        "else)": [
            1122085995
        ],
        "resource\nconsumption": [
            1122085995
        ],
        "restart-\ning": [
            1122085995
        ],
        "is\ndeleted": [
            1122085995
        ],
        "terminated\nrunning": [
            1122085995
        ],
        "server\nalthough": [
            1122085995
        ],
        "manifests\nfrom": [
            1122085995
        ],
        "118.": [
            1122085995
        ],
        "natively": [
            1122085995
        ],
        "pod\nmanifests": [
            1122085995
        ],
        "manage\n": [
            1122085995
        ],
        "\n\n327understanding": [
            1122085995
        ],
        "architecture\nthem": [
            1122085995
        ],
        "but\ndoing": [
            1122085995
        ],
        "method\n11.1.8": [
            1122085995
        ],
        "proxy\nbeside": [
            1122085995
        ],
        "non-pod": [
            1122085995
        ],
        "\nwhy": [
            1122085995
        ],
        "proxy\nthe": [
            1122085995
        ],
        "userspace": [
            1122085995
        ],
        "an\nactual": [
            1122085995
        ],
        "inter-\ncept": [
            1122085995
        ],
        "destined": [
            1122085995
        ],
        "\niptables": [
            1122085995
        ],
        "rules\n(\niptables": [
            1122085995
        ],
        "kernel’s": [
            1122085995
        ],
        "filtering": [
            1122085995
        ],
        "features)": [
            1122085995
        ],
        "to\nredirect": [
            1122085995
        ],
        "diagram": [
            1122085995
        ],
        "\nuserspace": [
            1122085995
        ],
        "proxy\nmode": [
            1122085995
        ],
        "119.\ncontainer": [
            1122085995
        ],
        "..)\nkubelet\napi": [
            1122085995
        ],
        "server\nworker": [
            1122085995
        ],
        "node\nruns": [
            1122085995
        ],
        "monitors\nand": [
            1122085995
        ],
        "manages\ncontainers\npod": [
            1122085995
        ],
        "resource\ncontainer": [
            1122085995
        ],
        "b\ncontainer": [
            1122085995
        ],
        "c\npod": [
            1122085995
        ],
        "(file)\nlocal": [
            1122085995
        ],
        "directory\ncontainer": [
            1122085995
        ],
        "c\nfigure": [
            1122085995
        ],
        "specs": [
            1122085995
        ],
        "directory\nclient\nkube-proxy\nconfigures:iptables\nredirect": [
            1122085995
        ],
        "server\niptables\npod\nfigure": [
            1122085995
        ],
        "mode\n": [
            1122085995
        ],
        "\n\n328chapter": [
            1122085995
        ],
        "internals\nthe": [
            1122085995
        ],
        "much\nbetter": [
            1122085995
        ],
        "a\nrandomly": [
            1122085995
        ],
        "server\nthis": [
            1122085995
        ],
        "1110.\nthe": [
            1122085995
        ],
        "the\nkube-proxy": [
            1122085995
        ],
        "mode\ndoesn’t—it": [
            1122085995
        ],
        "only\nfive": [
            1122085995
        ],
        "only\none": [
            1122085995
        ],
        "problem\nisn’t": [
            1122085995
        ],
        "apparent\n": [
            1122085995
        ],
        "115.": [
            1122085995
        ],
        "\n111.9": [
            1122085995
        ],
        "add-ons\nwe’ve": [
            1122085995
        ],
        "not\nalways": [
            1122085995
        ],
        "exposing\nmultiple": [
            1122085995
        ],
        "web\ndashboard": [
            1122085995
        ],
        "deployed\nthese": [
            1122085995
        ],
        "submitting\nyaml": [
            1122085995
        ],
        "book\nsome": [
            1122085995
        ],
        "repli-\ncationcontroller": [
            1122085995
        ],
        "the\ndashboard": [
            1122085995
        ],
        "\nclient\nconfigures:iptables\nredirect": [
            1122085995
        ],
        "pod\n(no": [
            1122085995
        ],
        "in-between)\niptables\npod\nkube-proxy\nfigure": [
            1122085995
        ],
        "\n\n329understanding": [
            1122085995
        ],
        "architecture\n$": [
            1122085995
        ],
        "age\ndefault-http-backend": [
            1122085995
        ],
        "6d\nkubernetes-dashboard": [
            1122085995
        ],
        "6d\nnginx-ingress-controller": [
            1122085995
        ],
        "6d\nthe": [
            1122085995
        ],
        "age\nkube-dns": [
            1122085995
        ],
        "6d\nlet’s": [
            1122085995
        ],
        "work\nhow": [
            1122085995
        ],
        "works\nall": [
            1122085995
        ],
        "specified\nas": [
            1122085995
        ],
        "\nnameserver": [
            1122085995
        ],
        "/etc/resolvconf": [
            1122085995
        ],
        "its\nclients": [
            1122085995
        ],
        "(fairly)": [
            1122085995
        ],
        "invalid\nhow": [
            1122085995
        ],
        "(most)": [
            1122085995
        ],
        "work\nunlike": [
            1122085995
        ],
        "reverse\nproxy": [
            1122085995
        ],
        "thus\nneeds": [
            1122085995
        ],
        "mechanism)": [
            1122085995
        ],
        "controllers\nforward": [
            1122085995
        ],
        "preservation": [
            1122085995
        ],
        "cases\nusing": [
            1122085995
        ],
        "add-ons\nyou’ve": [
            1122085995
        ],
        "client\nconnections": [
            1122085995
        ],
        "minikube\nlisting": [
            1122085995
        ],
        "\n\n330chapter": [
            1122085995
        ],
        "internals\n": [
            1122085995
        ],
        "perform\nthe": [
            1122085995
        ],
        "this\nand": [
            1122085995
        ],
        "chapters\n11.1.10": [
            1122085995
        ],
        "together\nyou’ve": [
            1122085995
        ],
        "small\nloosely": [
            1122085995
        ],
        "separation": [
            1122085995
        ],
        "system\nsynchronized": [
            1122085995
        ],
        "coordinated\ndance": [
            1122085995
        ],
        "unfolds": [
            1122085995
        ],
        "\n112": [
            1122085995
        ],
        "cooperate\nyou": [
            1122085995
        ],
        "comprised": [
            1122085995
        ],
        "of\nnow": [
            1122085995
        ],
        "solidify": [
            1122085995
        ],
        "hap-\npens": [
            1122085995
        ],
        "directly\nyou’re": [
            1122085995
        ],
        "must\nhappen": [
            1122085995
        ],
        "started\n11.2.1": [
            1122085995
        ],
        "involved\neven": [
            1122085995
        ],
        "types\nthis": [
            1122085995
        ],
        "1111.": [
            1122085995
        ],
        "a\npart": [
            1122085995
        ],
        "because\nit’s": [
            1122085995
        ],
        "place\nwhere": [
            1122085995
        ],
        "stored\nmaster": [
            1122085995
        ],
        "node\ncontroller": [
            1122085995
        ],
        "manager\nwatches\ndeployment\ncontroller\nscheduler\nreplicaset\ncontroller\napi": [
            1122085995
        ],
        "server\ndeployments\npods\nreplicasets\nwatches\nwatches\nnode": [
            1122085995
        ],
        "x\nwatches\ndocker\nkubelet\nfigure": [
            1122085995
        ],
        "\n\n331how": [
            1122085995
        ],
        "cooperate\n112.2": [
            1122085995
        ],
        "events\nimagine": [
            1122085995
        ],
        "you’re\nabout": [
            1122085995
        ],
        "submit": [
            1122085995
        ],
        "chain\nof": [
            1122085995
        ],
        "1112.\nthe": [
            1122085995
        ],
        "replicaset\nall": [
            1122085995
        ],
        "after\nit’s": [
            1122085995
        ],
        "discussed\nearlier": [
            1122085995
        ],
        "more\nreplicasets": [
            1122085995
        ],
        "is\ndetected": [
            1122085995
        ],
        "speci-\nfication": [
            1122085995
        ],
        "involves": [
            1122085995
        ],
        "resource\nthrough": [
            1122085995
        ],
        "all\nmaster": [
            1122085995
        ],
        "node\ncontroller\nmanager\n2": [
            1122085995
        ],
        "notification\nthrough": [
            1122085995
        ],
        "watch\n3": [
            1122085995
        ],
        "creates\nreplicaset\n4": [
            1122085995
        ],
        "notification\n5": [
            1122085995
        ],
        "pod\n6": [
            1122085995
        ],
        "watch\n7": [
            1122085995
        ],
        "node\n1": [
            1122085995
        ],
        "deployment\nresource\ndeployment\ncontroller\nscheduler\nkubectl\nreplicaset\ncontroller\napi": [
            1122085995
        ],
        "server\ndeployment": [
            1122085995
        ],
        "a\ndeployments\nreplicasets\npod": [
            1122085995
        ],
        "a\npods\nreplicaset": [
            1122085995
        ],
        "x\n8": [
            1122085995
        ],
        "watch\n9": [
            1122085995
        ],
        "containers\ndocker\n10": [
            1122085995
        ],
        "runs\ncontainers\ncontainer(s)\nkubelet\nfigure": [
            1122085995
        ],
        "\n\n332chapter": [
            1122085995
        ],
        "resources\nthe": [
            1122085995
        ],
        "which\nwatches": [
            1122085995
        ],
        "creations": [
            1122085995
        ],
        "consideration": [
            1122085995
        ],
        "match\nthe": [
            1122085995
        ],
        "copied": [
            1122085995
        ],
        "replicaset)": [
            1122085995
        ],
        "pods\nthese": [
            1122085995
        ],
        "import-\nant": [
            1122085995
        ],
        "thing—they": [
            1122085995
        ],
        "\nnodename": [
            1122085995
        ],
        "encounters": [
            1122085995
        ],
        "chooses": [
            1122085995
        ],
        "the\nbest": [
            1122085995
        ],
        "now\nincludes": [
            1122085995
        ],
        "tangible\nexcept": [
            1122085995
        ],
        "containers\nhaven’t": [
            1122085995
        ],
        "down-\nloaded": [
            1122085995
        ],
        "can\nfinally": [
            1122085995
        ],
        "inspects": [
            1122085995
        ],
        "docker\nor": [
            1122085995
        ],
        "container\nruntime": [
            1122085995
        ],
        "containers\n11.2.3": [
            1122085995
        ],
        "events\nboth": [
            1122085995
        ],
        "emit": [
            1122085995
        ],
        "as\nthey": [
            1122085995
        ],
        "pertaining": [
            1122085995
        ],
        "specific\nresources": [
            1122085995
        ],
        "events\n": [
            1122085995
        ],
        "they’re\nnot": [
            1122085995
        ],
        "temporal": [
            1122085995
        ],
        "\n--watch": [
            1122085995
        ],
        "is\nmuch": [
            1122085995
        ],
        "eyes": [
            1122085995
        ],
        "emitted": [
            1122085995
        ],
        "previously\n(some": [
            1122085995
        ],
        "heavily": [
            1122085995
        ],
        "legible\nin": [
            1122085995
        ],
        "page)\n": [
            1122085995
        ],
        "\n\n333understanding": [
            1122085995
        ],
        "--watch\n": [
            1122085995
        ],
        "\n..": [
            1122085995
        ],
        "scalingreplicaset": [
            1122085995
        ],
        "deployment-controller": [
            1122085995
        ],
        "kubia-193": [
            1122085995
        ],
        "3\n..": [
            1122085995
        ],
        "replicaset-controller": [
            1122085995
        ],
        "kubia-193-w7ll2\n..": [
            1122085995
        ],
        "kubia-193-tpg6j": [
            1122085995
        ],
        "node1\n..": [
            1122085995
        ],
        "kubia-193-39590\n..": [
            1122085995
        ],
        "kubia-193-tpg6j\n..": [
            1122085995
        ],
        "kubia-193-39590": [
            1122085995
        ],
        "node2\n..": [
            1122085995
        ],
        "kubia-193-w7ll2": [
            1122085995
        ],
        "machine\n..": [
            1122085995
        ],
        "13da752\n..": [
            1122085995
        ],
        "8850184\n..\nas": [
            1122085995
        ],
        "acting": [
            1122085995
        ],
        "reason\ncolumn": [
            1122085995
        ],
        "line)": [
            1122085995
        ],
        "details\nabout": [
            1122085995
        ],
        "done\n11.3": [
            1122085995
        ],
        "is\nwith": [
            1122085995
        ],
        "it?\n": [
            1122085995
        ],
        "investigative": [
            1122085995
        ],
        "snuck": [
            1122085995
        ],
        "if\nnot": [
            1122085995
        ],
        "--image=nginx\ndeployment": [
            1122085995
        ],
        "nginx\"": [
            1122085995
        ],
        "single\nlisting": [
            1122085995
        ],
        "\n\n334chapter": [
            1122085995
        ],
        "internals\nnode": [
            1122085995
        ],
        "name>\n": [
            1122085995
        ],
        "\ndocker\nps\n": [
            1122085995
        ],
        "listing\ndocker@minikubevm:~$": [
            1122085995
        ],
        "created\nc917a6f3c3f7": [
            1122085995
        ],
        "-g": [
            1122085995
        ],
        "\n98b8bf797174": [
            1122085995
        ],
        "gcrio/.../pause:3.0": [
            1122085995
        ],
        "/pause\"": [
            1122085995
        ],
        "ago\n..\nnotei’ve": [
            1122085995
        ],
        "listing—this\nincludes": [
            1122085995
        ],
        "rows": [
            1122085995
        ],
        "containers\nthat": [
            1122085995
        ],
        "judging\nfrom": [
            1122085995
        ],
        "\npause\")": [
            1122085995
        ],
        "was\ncreated": [
            1122085995
        ],
        "role?\n": [
            1122085995
        ],
        "pod\ntogether": [
            1122085995
        ],
        "other\nlinux": [
            1122085995
        ],
        "namespaces?": [
            1122085995
        ],
        "sole\npurpose": [
            1122085995
        ],
        "user-defined": [
            1122085995
        ],
        "pod\nthen": [
            1122085995
        ],
        "1113).\nactual": [
            1122085995
        ],
        "die": [
            1122085995
        ],
        "pod—the\ncontainer": [
            1122085995
        ],
        "the\ninfrastructure": [
            1122085995
        ],
        "meantime": [
            1122085995
        ],
        "recreates": [
            1122085995
        ],
        "pod’s\ncontainers\nlisting": [
            1122085995
        ],
        "containers\npod\ncontainer": [
            1122085995
        ],
        "infrastructure\ncontainer\ncontainer": [
            1122085995
        ],
        "b\nuses": [
            1122085995
        ],
        "from\nuses": [
            1122085995
        ],
        "from\nfigure": [
            1122085995
        ],
        "two-container": [
            1122085995
        ],
        "\nsharing": [
            1122085995
        ],
        "namespaces\n": [
            1122085995
        ],
        "\n\n335inter-pod": [
            1122085995
        ],
        "networking\n114": [
            1122085995
        ],
        "networking\nby": [
            1122085995
        ],
        "communicate\nwith": [
            1122085995
        ],
        "nat-less": [
            1122085995
        ],
        "kubernetes\nachieve": [
            1122085995
        ],
        "this?": [
            1122085995
        ],
        "or\nby": [
            1122085995
        ],
        "(cni)": [
            1122085995
        ],
        "\n114.1": [
            1122085995
        ],
        "like\nkubernetes": [
            1122085995
        ],
        "does\nmandate": [
            1122085995
        ],
        "precise": [
            1122085995
        ],
        "containers)": [
            1122085995
        ],
        "the\nnetwork": [
            1122085995
        ],
        "as\nits": [
            1122085995
        ],
        "in\nquestion": [
            1122085995
        ],
        "1114.": [
            1122085995
        ],
        "(sends": [
            1122085995
        ],
        "b\nthe": [
            1122085995
        ],
        "translation": [
            1122085995
        ],
        "(nat)": [
            1122085995
        ],
        "by\npod": [
            1122085995
        ],
        "destination": [
            1122085995
        ],
        "unchanged\nthis": [
            1122085995
        ],
        "pods\nsimple": [
            1122085995
        ],
        "absence": [
            1122085995
        ],
        "101.1.1\nsrcip:10.1.1.1\ndstip:10.1.2.1\nsrcip:10.1.1.1\ndstip:10.1.2.1\npacket\nnode": [
            1122085995
        ],
        "101.2.1\nsrcip:10.1.1.1\ndstip:10.1.2.1\npacket\nnetwork\nno": [
            1122085995
        ],
        "(ips\nare": [
            1122085995
        ],
        "preserved)\nfigure": [
            1122085995
        ],
        "mandates": [
            1122085995
        ],
        "\nnetwork\n": [
            1122085995
        ],
        "\n\n336chapter": [
            1122085995
        ],
        "y": [
            1122085995
        ],
        "noti-\nfication": [
            1122085995
        ],
        "it\n“hey,": [
            1122085995
        ],
        "12.3.4;": [
            1122085995
        ],
        "address”\nthe": [
            1122085995
        ],
        "received\nip": [
            1122085995
        ],
        "extends": [
            1122085995
        ],
        "pod-\nto-node": [
            1122085995
        ],
        "node-to-pod": [
            1122085995
        ],
        "be\nchanged": [
            1122085995
        ],
        "is\nchanged": [
            1122085995
        ],
        "according\nto": [
            1122085995
        ],
        "we’re\nnot": [
            1122085995
        ],
        "\n114.2": [
            1122085995
        ],
        "works\nin": [
            1122085995
        ],
        "and\nheld": [
            1122085995
        ],
        "then\nuse": [
            1122085995
        ],
        "connected\nto": [
            1122085995
        ],
        "1115.": [
            1122085995
        ],
        "next\nenabling": [
            1122085995
        ],
        "node\nbefore": [
            1122085995
        ],
        "ethernet": [
            1122085995
        ],
        "veth\npair)": [
            1122085995
        ],
        "host’s\nnamespace": [
            1122085995
        ],
        "\nvethxxx": [
            1122085995
        ],
        "node)\nwhereas": [
            1122085995
        ],
        "renamed\neth0": [
            1122085995
        ],
        "pipe": [
            1122085995
        ],
        "network\ndevices": [
            1122085995
        ],
        "cable)—what": [
            1122085995
        ],
        "vice-versa": [
            1122085995
        ],
        "\nnode\npod": [
            1122085995
        ],
        "a\neth0\n101.1.1\nveth123\npod": [
            1122085995
        ],
        "b\neth0\n101.1.2\nveth234\nbridge\n10.1.1.0/24\nthis": [
            1122085995
        ],
        "a’s\nveth": [
            1122085995
        ],
        "pair\nthis": [
            1122085995
        ],
        "b’s\nveth": [
            1122085995
        ],
        "pair\nfigure": [
            1122085995
        ],
        "\nconnected": [
            1122085995
        ],
        "bridge": [
            1122085995
        ],
        "\nvirtual": [
            1122085995
        ],
        "pairs\n": [
            1122085995
        ],
        "\n\n337inter-pod": [
            1122085995
        ],
        "networking\n": [
            1122085995
        ],
        "is\nassigned": [
            1122085995
        ],
        "bridge’s": [
            1122085995
        ],
        "application\nrunning": [
            1122085995
        ],
        "namespace)": [
            1122085995
        ],
        "veth": [
            1122085995
        ],
        "that’s\nconnected": [
            1122085995
        ],
        "b’s": [
            1122085995
        ],
        "node\nare": [
            1122085995
        ],
        "bridges": [
            1122085995
        ],
        "\nenabling": [
            1122085995
        ],
        "nodes\nyou": [
            1122085995
        ],
        "with\noverlay": [
            1122085995
        ],
        "underlay": [
            1122085995
        ],
        "networks": [
            1122085995
        ],
        "routing": [
            1122085995
        ],
        "bridges\nacross": [
            1122085995
        ],
        "ranges": [
            1122085995
        ],
        "bridge\non": [
            1122085995
        ],
        "101.1.0/24": [
            1122085995
        ],
        "using\n101.2.0/24": [
            1122085995
        ],
        "exist\n": [
            1122085995
        ],
        "con-\nnected": [
            1122085995
        ],
        "tables": [
            1122085995
        ],
        "all\npackets": [
            1122085995
        ],
        "101.2.0/24": [
            1122085995
        ],
        "routing\ntables": [
            1122085995
        ],
        "a\n": [
            1122085995
        ],
        "then\nnode": [
            1122085995
        ],
        "a\nnetwork\neth0\n101.1.1\nveth123\npod": [
            1122085995
        ],
        "b\neth0\n101.1.2\nveth234\nbridge\n10.1.1.0/24\neth0\n10.100.0.1\nnode": [
            1122085995
        ],
        "b\npod": [
            1122085995
        ],
        "c\neth0\n101.2.1\nveth345\npod": [
            1122085995
        ],
        "d\neth0\n101.2.2\nveth456\nbridge\n10.1.2.0/24\neth0\n10.100.0.2\nfigure": [
            1122085995
        ],
        "\nsomehow\n": [
            1122085995
        ],
        "\n\n338chapter": [
            1122085995
        ],
        "internals\nthrough": [
            1122085995
        ],
        "adapter": [
            1122085995
        ],
        "other\nnode’s": [
            1122085995
        ],
        "veth\npair": [
            1122085995
        ],
        "without\nany": [
            1122085995
        ],
        "routers": [
            1122085995
        ],
        "between;": [
            1122085995
        ],
        "because\nthey": [
            1122085995
        ],
        "con-\nfigured": [
            1122085995
        ],
        "difficult\nand": [
            1122085995
        ],
        "(sdn)": [
            1122085995
        ],
        "nodes\nappear": [
            1122085995
        ],
        "sent\nfrom": [
            1122085995
        ],
        "de-encapsulated": [
            1122085995
        ],
        "origi-\nnal": [
            1122085995
        ],
        "form\n11.4.3": [
            1122085995
        ],
        "interface\nto": [
            1122085995
        ],
        "container\nnetwork": [
            1122085995
        ],
        "cni": [
            1122085995
        ],
        "include\ncalico\nflannel\nromana\nweave": [
            1122085995
        ],
        "net": [
            1122085995
        ],
        "\nand": [
            1122085995
        ],
        "others\nwe’re": [
            1122085995
        ],
        "plugins;": [
            1122085995
        ],
        "about\nthem": [
            1122085995
        ],
        "https://kubernetesio/docs/concepts/cluster-administration/addons/.\n": [
            1122085995
        ],
        "provided\non": [
            1122085995
        ],
        "plugin’s": [
            1122085995
        ],
        "deploy\na": [
            1122085995
        ],
        "ties": [
            1122085995
        ],
        "node\nbut": [
            1122085995
        ],
        "\n--network-plugin=cni": [
            1122085995
        ],
        "\n115": [
            1122085995
        ],
        "implemented\nin": [
            1122085995
        ],
        "long-\nlived": [
            1122085995
        ],
        "truly\nunderstand": [
            1122085995
        ],
        "behave\nthe": [
            1122085995
        ],
        "\n\n339how": [
            1122085995
        ],
        "implemented\n115.1": [
            1122085995
        ],
        "kube-proxy\neverything": [
            1122085995
        ],
        "was\ncalled": [
            1122085995
        ],
        "better-performing": [
            1122085995
        ],
        "mode\nreplaced": [
            1122085995
        ],
        "want\n": [
            1122085995
        ],
        "rele-\nvant": [
            1122085995
        ],
        "paragraphs\n": [
            1122085995
        ],
        "clients\n(usually": [
            1122085995
        ],
        "virtual—it’s": [
            1122085995
        ],
        "as\neither": [
            1122085995
        ],
        "packet\nleaves": [
            1122085995
        ],
        "(or\nmultiple": [
            1122085995
        ],
        "services)": [
            1122085995
        ],
        "itself\ndoesn’t": [
            1122085995
        ],
        "\n115.2": [
            1122085995
        ],
        "iptables\nwhen": [
            1122085995
        ],
        "agents": [
            1122085995
        ],
        "makes\nthat": [
            1122085995
        ],
        "few\niptables": [
            1122085995
        ],
        "ip/port": [
            1122085995
        ],
        "is\nintercepted": [
            1122085995
        ],
        "watches\nfor": [
            1122085995
        ],
        "me\nrefresh": [
            1122085995
        ],
        "back\nthe": [
            1122085995
        ],
        "that’s\nwhy": [
            1122085995
        ],
        "endpoints\nobject": [
            1122085995
        ],
        "1117.\n": [
            1122085995
        ],
        "\nkube-proxy": [
            1122085995
        ],
        "pod\nreaches": [
            1122085995
        ],
        "the\npacket": [
            1122085995
        ],
        "figure)": [
            1122085995
        ],
        "packet’s": [
            1122085995
        ],
        "the\nexample": [
            1122085995
        ],
        "17230.0.1:80).": [
            1122085995
        ],
        "\n\n340chapter": [
            1122085995
        ],
        "internals\npacket": [
            1122085995
        ],
        "a’s": [
            1122085995
        ],
        "them\nsays": [
            1122085995
        ],
        "17230.0.1": [
            1122085995
        ],
        "port\nequal": [
            1122085995
        ],
        "and\nport": [
            1122085995
        ],
        "destination\nip": [
            1122085995
        ],
        "101.2.1": [
            1122085995
        ],
        "b2’s": [
            1122085995
        ],
        "specified\nin": [
            1122085995
        ],
        "packet\nto": [
            1122085995
        ],
        "understand\n": [
            1122085995
        ],
        "anode": [
            1122085995
        ],
        "b\napi": [
            1122085995
        ],
        "server\npod": [
            1122085995
        ],
        "apod": [
            1122085995
        ],
        "b1pod": [
            1122085995
        ],
        "b2pod": [
            1122085995
        ],
        "b3\npacket": [
            1122085995
        ],
        "x\nsource:\n101.1.1\ndestination:\n172.30.0.1:80\n10.1.2.1:8080\niptables\nservice": [
            1122085995
        ],
        "b\n17230.0.1:80\nconfigures\niptables\npacket": [
            1122085995
        ],
        "x\nsource:\n101.1.1\ndestination:\n172.30.0.1:80\nkube-proxy\nendpoints": [
            1122085995
        ],
        "101.1.1\npod": [
            1122085995
        ],
        "b1\nip:": [
            1122085995
        ],
        "101.1.2\npod": [
            1122085995
        ],
        "b2\nip:": [
            1122085995
        ],
        "101.2.1\npod": [
            1122085995
        ],
        "b3\nip:": [
            1122085995
        ],
        "101.2.2\nwatches": [
            1122085995
        ],
        "endpoints\nfigure": [
            1122085995
        ],
        "\nmodified": [
            1122085995
        ],
        "\n\n341running": [
            1122085995
        ],
        "clusters\n116": [
            1122085995
        ],
        "clusters\none": [
            1122085995
        ],
        "infrastructure\nfailures": [
            1122085995
        ],
        "at\nwhat’s": [
            1122085995
        ],
        "next\n11.6.1": [
            1122085995
        ],
        "available\nwhen": [
            1122085995
        ],
        "keeps\nrunning": [
            1122085995
        ],
        "replicas;": [
            1122085995
        ],
        "by\nkubernetes": [
            1122085995
        ],
        "likelihood": [
            1122085995
        ],
        "downtime\nthis": [
            1122085995
        ],
        "the\nreplica": [
            1122085995
        ],
        "that\ndoesn’t": [
            1122085995
        ],
        "instantaneously": [
            1122085995
        ],
        "notice\nthat": [
            1122085995
        ],
        "containers\nthere": [
            1122085995
        ],
        "inevitably": [
            1122085995
        ],
        "leader-election": [
            1122085995
        ],
        "non-horizontally": [
            1122085995
        ],
        "apps\nto": [
            1122085995
        ],
        "the\nactive": [
            1122085995
        ],
        "fast-acting": [
            1122085995
        ],
        "lease": [
            1122085995
        ],
        "election": [
            1122085995,
            1941223023
        ],
        "app\ninstances": [
            1122085995
        ],
        "are\nwaiting": [
            1122085995
        ],
        "leaders": [
            1122085995,
            1043891123
        ],
        "be\nactive": [
            1122085995
        ],
        "others\nare": [
            1122085995
        ],
        "instances\nare": [
            1122085995
        ],
        "unpredictable": [
            1122085995
        ],
        "due\nto": [
            1122085995
        ],
        "conditions\n": [
            1122085995
        ],
        "incorporated": [
            1122085995
        ],
        "a\nsidecar": [
            1122085995
        ],
        "elec-\ntion": [
            1122085995
        ],
        "https://githubcom/kubernetes/contrib/tree/master/election.\n": [
            1122085995
        ],
        "kubernetes\ntakes": [
            1122085995
        ],
        "fails?": [
            1122085995
        ],
        "servers\nrunning": [
            1122085995
        ],
        "down?": [
            1122085995
        ],
        "available?\n": [
            1122085995
        ],
        "\n\n342chapter": [
            1122085995
        ],
        "internals\n116.2": [
            1122085995
        ],
        "components:\netcd": [
            1122085995
        ],
        "kept\napi": [
            1122085995
        ],
        "server\ncontroller": [
            1122085995
        ],
        "run\nscheduler\nwithout": [
            1122085995
        ],
        "1118\nshows": [
            1122085995
        ],
        "cluster\nrunning": [
            1122085995
        ],
        "cluster\nbecause": [
            1122085995
        ],
        "ability\nto": [
            1122085995
        ],
        "(three": [
            1122085995
        ],
        "as\nexplained": [
            1122085995
        ],
        "by\nincluding": [
            1122085995
        ],
        "etcd\ninstances": [
            1122085995
        ],
        "reached": [
            1122085995
        ],
        "when\nrunning": [
            1122085995
        ],
        "three-machine": [
            1122085995
        ],
        "and\nwrite": [
            1122085995
        ],
        "fault": [
            1122085995
        ],
        "tolerance": [
            1122085995
        ],
        "three\nnode": [
            1122085995
        ],
        "1\nkubelet\nnode": [
            1122085995
        ],
        "2\nkubelet\nnode": [
            1122085995
        ],
        "3\nkubelet\nnode": [
            1122085995
        ],
        "4\nkubelet\nnode": [
            1122085995
        ],
        "5\nkubelet\n..\nnode": [
            1122085995
        ],
        "n\nkubelet\nload\nbalancer\nmaster": [
            1122085995
        ],
        "3\netcd\napi": [
            1122085995
        ],
        "server\nscheduler\ncontroller\nmanager\n[standing-by][standing-by]\nmaster": [
            1122085995
        ],
        "2\netcd\napi": [
            1122085995
        ],
        "1\netcd\napi": [
            1122085995
        ],
        "server\nscheduler\ncontroller\nmanager\n[active][active]\nfigure": [
            1122085995
        ],
        "highly-available": [
            1122085995
        ],
        "\n\n343running": [
            1122085995
        ],
        "clusters\nnode": [
            1122085995
        ],
        "nec-\nessary": [
            1122085995
        ],
        "impacting": [
            1122085995
        ],
        "performance\nrunning": [
            1122085995
        ],
        "server\nmaking": [
            1122085995
        ],
        "(almost\ncompletely)": [
            1122085995
        ],
        "(all": [
            1122085995
        ],
        "cache": [
            1122085995
        ],
        "other\nat": [
            1122085995
        ],
        "collocated": [
            1122085995
        ],
        "the\netcd": [
            1122085995
        ],
        "so\nclients": [
            1122085995
        ],
        "(\nkubectl": [
            1122085995
        ],
        "kubelets)\nalways": [
            1122085995
        ],
        "scheduler\ncompared": [
            1122085995
        ],
        "simultaneously": [
            1122085995
        ],
        "simple\nbecause": [
            1122085995
        ],
        "when\nit": [
            1122085995
        ],
        "racing": [
            1122085995
        ],
        "which\ncould": [
            1122085995
        ],
        "undesired": [
            1122085995
        ],
        "(creating": [
            1122085995
        ],
        "one\ninstance": [
            1122085995
        ],
        "controlled": [
            1122085995
        ],
        "\n--leader-elect": [
            1122085995
        ],
        "to\ntrue)": [
            1122085995
        ],
        "elected": [
            1122085995
        ],
        "only\nthe": [
            1122085995
        ],
        "waiting\nfor": [
            1122085995
        ],
        "elect": [
            1122085995
        ],
        "leader\nwhich": [
            1122085995
        ],
        "never\noperating": [
            1122085995
        ],
        "1119).\nmaster": [
            1122085995
        ],
        "3\nscheduler\ncontroller\nmanager\n[standing-by][standing-by]\nmaster": [
            1122085995
        ],
        "1\nscheduler\ncontroller\nmanager\n[active][active]\nmaster": [
            1122085995
        ],
        "2\nscheduler\ncontroller\nmanager\n[standing-by][standing-by]\nonly": [
            1122085995
        ],
        "manager\nare": [
            1122085995
        ],
        "reacting": [
            1122085995
        ],
        "api\nresources": [
            1122085995
        ],
        "created\nupdated,": [
            1122085995
        ],
        "deleted\nthese": [
            1122085995
        ],
        "managers\nand": [
            1122085995
        ],
        "doing\nanything": [
            1122085995
        ],
        "to\nbecome": [
            1122085995
        ],
        "leaders\nonly": [
            1122085995
        ],
        "scheduler\nis": [
            1122085995
        ],
        "pods\nfigure": [
            1122085995
        ],
        "active;": [
            1122085995
        ],
        "by\n": [
            1122085995
        ],
        "\n\n344chapter": [
            1122085995
        ],
        "and\netcd": [
            1122085995
        ],
        "directly;": [
            1122085995
        ],
        "load\nbalancer\nunderstanding": [
            1122085995
        ],
        "components\nwhat": [
            1122085995
        ],
        "resource—the": [
            1122085995
        ],
        "(abused": [
            1122085995
        ],
        "term)\n": [
            1122085995
        ],
        "used\nbecause": [
            1122085995
        ],
        "any\nother": [
            1122085995
        ],
        "use\nconfigmaps": [
            1122085995
        ],
        "let’s\ntake": [
            1122085995
        ],
        "later\nupdate)": [
            1122085995
        ],
        "\nkube-scheduler": [
            1122085995
        ],
        "kube-\nsystem\n": [
            1122085995
        ],
        "kube-scheduler": [
            1122085995
        ],
        "control-planealpha.kubernetes.io/leader:": [
            1122085995
        ],
        "{holderidentity\":\n": [
            1122085995
        ],
        "minikube\"\"leasedurationseconds\":15,\"acquiretime\":\n": [
            1122085995
        ],
        "2017-05-27t18:54:53z\"\"renewtime\":\"2017-05-28t13:07:49z\",\n": [
            1122085995
        ],
        "leadertransitions\":0}\n": [
            1122085995
        ],
        "2017-05-27t18:54:53z\n": [
            1122085995
        ],
        "kube-scheduler\n": [
            1122085995
        ],
        "kube-system\n": [
            1122085995
        ],
        "654059\"\n": [
            1122085995
        ],
        "/api/v1/namespaces/kube-system/endpoints/kube-scheduler\n": [
            1122085995
        ],
        "f847bd14-430d-11e7-9720-080027f8fa4e\nsubsets:": [
            1122085995
        ],
        "[]\nthe": [
            1122085995
        ],
        "control-planealpha.kubernetes.io/leader": [
            1122085995
        ],
        "part\nas": [
            1122085995
        ],
        "\nholderidentity": [
            1122085995
        ],
        "putting": [
            1122085995
        ],
        "becomes\nthe": [
            1122085995
        ],
        "winner\n": [
            1122085995
        ],
        "earlier?": [
            1122085995
        ],
        "ensures\nthat": [
            1122085995
        ],
        "them\nsucceeds": [
            1122085995
        ],
        "whether\nit": [
            1122085995
        ],
        "(every": [
            1122085995
        ],
        "sec-\nonds": [
            1122085995
        ],
        "fails\nlisting": [
            1122085995
        ],
        "leader-election\n": [
            1122085995
        ],
        "\n\n345summary\nother": [
            1122085995
        ],
        "become\nthe": [
            1122085995
        ],
        "right?\n117": [
            1122085995
        ],
        "summary\nhopefully": [
            1122085995
        ],
        "inner": [
            1122085995
        ],
        "workings": [
            1122085995
        ],
        "you\nwhat": [
            1122085995
        ],
        "is\nresponsible": [
            1122085995
        ],
        "for\nhow": [
            1122085995
        ],
        "controller\nmanager": [
            1122085995
        ],
        "life\nhow": [
            1122085995
        ],
        "pod\nhow": [
            1122085995
        ],
        "connected\nso": [
            1122085995
        ],
        "other\nhow": [
            1122085995
        ],
        "by\nconfiguring": [
            1122085995
        ],
        "node\nhow": [
            1122085995
        ],
        "available\nnext": [
            1122085995
        ],
        "whole\n": [
            1122085995
        ],
        "\n\n346\nsecuring": [
            1122085995
        ],
        "server\nin": [
            1122085995
        ],
        "to\nauthenticate": [
            1122085995
        ],
        "config-\nure": [
            1122085995
        ],
        "subjects": [
            1122085995
        ],
        "\n121": [
            1122085995
        ],
        "authentication\nin": [
            1122085995
        ],
        "more\nauthentication": [
            1122085995
        ],
        "plugins)": [
            1122085995
        ],
        "a\nrequest": [
            1122085995
        ],
        "authentication\nthis": [
            1122085995
        ],
        "authentication\nwhat": [
            1122085995
        ],
        "used\nunderstanding": [
            1122085995
        ],
        "\n(rbac)": [
            1122085995
        ],
        "plugin\nusing": [
            1122085995
        ],
        "rolebindings\nusing": [
            1122085995
        ],
        "clusterrolebindings\nunderstanding": [
            1122085995
        ],
        "bindings\n": [
            1122085995
        ],
        "\n\n347understanding": [
            1122085995
        ],
        "authentication\nplugins": [
            1122085995
        ],
        "who’s": [
            1122085995
        ],
        "returns\nthe": [
            1122085995
        ],
        "server\ncore": [
            1122085995
        ],
        "contin-\nues": [
            1122085995
        ],
        "phase": [
            1122085995
        ],
        "client\nusing": [
            1122085995
        ],
        "methods:\nfrom": [
            1122085995
        ],
        "certificate\nfrom": [
            1122085995
        ],
        "header\nbasic": [
            1122085995
        ],
        "authentication\nothers\nthe": [
            1122085995
        ],
        "starting\nthe": [
            1122085995
        ],
        "\n121.1": [
            1122085995
        ],
        "groups\nan": [
            1122085995
        ],
        "group(s)": [
            1122085995
        ],
        "authenticated\nuser": [
            1122085995
        ],
        "anywhere;": [
            1122085995
        ],
        "not\nunderstanding": [
            1122085995
        ],
        "users\nkubernetes": [
            1122085995
        ],
        "distinguishes": [
            1122085995
        ],
        "server:\nactual": [
            1122085995
        ],
        "(users)\npods": [
            1122085995
        ],
        "them)\nboth": [
            1122085995
        ],
        "aforementioned": [
            1122085995
        ],
        "sign\non": [
            1122085995
        ],
        "(sso)": [
            1122085995
        ],
        "resource\nrepresents": [
            1122085995
        ],
        "service-\naccounts": [
            1122085995
        ],
        "cluster\nadministrators": [
            1122085995
        ],
        "http://\nkubernetesio/docs/admin.\nunderstanding": [
            1122085995
        ],
        "groups\nboth": [
            1122085995
        ],
        "said\nthat": [
            1122085995
        ],
        "id\ngroups": [
            1122085995
        ],
        "grant": [
            1122085995
        ],
        "to\ngrant": [
            1122085995
        ],
        "\n\n348chapter": [
            1122085995
        ],
        "12securing": [
            1122085995
        ],
        "strings": [
            1122085995
        ],
        "arbitrary\ngroup": [
            1122085995
        ],
        "built-in": [
            1122085995
        ],
        "meaning:\nthe": [
            1122085995
        ],
        "system:unauthenticated": [
            1122085995
        ],
        "the\nauthentication": [
            1122085995
        ],
        "client\nthe": [
            1122085995
        ],
        "system:authenticated": [
            1122085995
        ],
        "was\nauthenticated": [
            1122085995
        ],
        "successfully\nthe": [
            1122085995
        ],
        "system:serviceaccounts": [
            1122085995
        ],
        "encompasses": [
            1122085995
        ],
        "the\nsystem\nthe": [
            1122085995
        ],
        "system:serviceaccounts:<namespace>": [
            1122085995
        ],
        "namespace\n12.1.2": [
            1122085995
        ],
        "serviceaccounts\nlet’s": [
            1122085995
        ],
        "server\nrequires": [
            1122085995
        ],
        "file\n/var/run/secrets/kubernetesio/serviceaccount/token": [
            1122085995
        ],
        "represent?": [
            1122085995
        ],
        "service-\naccount": [
            1122085995
        ],
        "file\nholds": [
            1122085995
        ],
        "serviceaccount’s": [
            1122085995
        ],
        "serviceaccount\nand": [
            1122085995
        ],
        "usernames": [
            1122085995
        ],
        "this:\nsystem:serviceaccount:<namespace>:<service": [
            1122085995
        ],
        "name>\nthe": [
            1122085995
        ],
        "which\ndetermine": [
            1122085995
        ],
        "performed\nby": [
            1122085995
        ],
        "serviceaccount\n": [
            1122085995
        ],
        "request\nunderstanding": [
            1122085995
        ],
        "resource\nserviceaccounts": [
            1122085995
        ],
        "are\nscoped": [
            1122085995
        ],
        "created\nfor": [
            1122085995
        ],
        "(that’s": [
            1122085995
        ],
        "along)": [
            1122085995
        ],
        "resources:\n$": [
            1122085995
        ],
        "sa\nname": [
            1122085995
        ],
        "1d\nnotethe": [
            1122085995
        ],
        "sa\n": [
            1122085995
        ],
        "\n\n349understanding": [
            1122085995
        ],
        "authentication\nas": [
            1122085995
        ],
        "exactly\none": [
            1122085995
        ],
        "namespace\nunderstanding": [
            1122085995
        ],
        "tie": [
            1122085995
        ],
        "authorization\nyou": [
            1122085995
        ],
        "account’s": [
            1122085995
        ],
        "serviceaccount\nin": [
            1122085995
        ],
        "resources\neach": [
            1122085995
        ],
        "bearing": [
            1122085995
        ],
        "the\nsystem-wide": [
            1122085995
        ],
        "(rbac)": [
            1122085995
        ],
        "plugin\nwhich": [
            1122085995
        ],
        "rbac\nplugin": [
            1122085995
        ],
        "use\n12.1.3": [
            1122085995
        ],
        "serviceaccounts\nwe’ve": [
            1122085995
        ],
        "additional\nones": [
            1122085995
        ],
        "cluster\nmetadata": [
            1122085995
        ],
        "constrained": [
            1122085995
        ],
        "retrieve\nor": [
            1122085995
        ],
        "resource\nmetadata": [
            1122085995
        ],
        "objects’\nmetadata": [
            1122085995
        ],
        "own\nserviceaccount": [
            1122085995
        ],
        "\npod\nnamespace:": [
            1122085995
        ],
        "foo\nservice-\naccount:\ndefault\npodpod\nnamespace:": [
            1122085995
        ],
        "baz\npod\nnamespace:": [
            1122085995
        ],
        "bar\npodpod\nnot": [
            1122085995
        ],
        "possible\nservice-\naccount:\ndefault\nanother\nservice-\naccount\nmultiple": [
            1122085995
        ],
        "serviceaccount\nfigure": [
            1122085995
        ],
        "\n\n350chapter": [
            1122085995
        ],
        "secrets\nand": [
            1122085995
        ],
        "serviceaccount\ncreating": [
            1122085995
        ],
        "create\nserviceaccount\n": [
            1122085995
        ],
        "foo:\n$": [
            1122085995
        ],
        "foo\nserviceaccount": [
            1122085995
        ],
        "sa": [
            1122085995
        ],
        "foo\nname:": [
            1122085995
        ],
        "foo\nnamespace:": [
            1122085995
        ],
        "<none>\nimage": [
            1122085995
        ],
        "\nmountable": [
            1122085995
        ],
        "foo-token-qzq7j": [
            1122085995
        ],
        "\ntokens:": [
            1122085995
        ],
        "the\nserviceaccount": [
            1122085995
        ],
        "foo-\ntoken-qzq7j\n": [
            1122085995
        ],
        "and\ntoken)": [
            1122085995
        ],
        "be\ndifferent)": [
            1122085995
        ],
        "foo-token-qzq7j\n..\nca.crt:": [
            1122085995
        ],
        "bytes\nnamespace:": [
            1122085995
        ],
        "bytes\ntoken:": [
            1122085995
        ],
        "eyjhbgcioijsuzi1niisinr5cci6ikpxvcj9..\nnoteyou’ve": [
            1122085995
        ],
        "(jwt)": [
            1122085995
        ],
        "authentica-\ntion": [
            1122085995
        ],
        "jwt": [
            1122085995
        ],
        "tokens\nunderstanding": [
            1122085995
        ],
        "mountable": [
            1122085995
        ],
        "secrets\nthe": [
            1122085995
        ],
        "serviceaccount\nwith": [
            1122085995
        ],
        "can\nmount": [
            1122085995
        ],
        "only\nlisting": [
            1122085995
        ],
        "secret\nthese": [
            1122085995
        ],
        "\nautomatically": [
            1122085995
        ],
        "serviceaccount\npods": [
            1122085995
        ],
        "enforced\nauthentication": [
            1122085995
        ],
        "token(s)": [
            1122085995
        ],
        "\ninside": [
            1122085995
        ],
        "\n\n351understanding": [
            1122085995
        ],
        "authentication\nallow": [
            1122085995
        ],
        "anno-\ntation:": [
            1122085995
        ],
        "\nkubernetesio/enforce-mountable-secrets=true\".": [
            1122085995
        ],
        "annotated": [
            1122085995
        ],
        "mount\nonly": [
            1122085995
        ],
        "secrets—they": [
            1122085995
        ],
        "secret\nunderstanding": [
            1122085995
        ],
        "secrets\na": [
            1122085995
        ],
        "examined": [
            1122085995
        ],
        "for\npulling": [
            1122085995
        ],
        "which\nincludes": [
            1122085995
        ],
        "7\napiversion:": [
            1122085995
        ],
        "serviceaccount\nmetadata:\n": [
            1122085995
        ],
        "my-service-account\nimagepullsecrets:\n-": [
            1122085995
        ],
        "my-dockerhub-secret\na": [
            1122085995
        ],
        "mountable\nsecrets": [
            1122085995
        ],
        "add\nthem": [
            1122085995
        ],
        "\n121.4": [
            1122085995
        ],
        "is\ndone": [
            1122085995
        ],
        "\nspecserviceaccountname\nfield": [
            1122085995
        ],
        "server’s\nrest": [
            1122085995
        ],
        "which\nused": [
            1122085995
        ],
        "minutes\nago": [
            1122085995
        ],
        "sa-image-pull-secretsyaml\n": [
            1122085995
        ],
        "\n\n352chapter": [
            1122085995
        ],
        "curl-custom-sa\nspec:\n": [
            1122085995
        ],
        "luksa/kubectl-proxy:16.2\nto": [
            1122085995
        ],
        "curl-custom-sa": [
            1122085995
        ],
        "/var/run/secrets/kubernetesio/serviceaccount/token\neyjhbgcioijsuzi1niisinr5cci6ikpxvcj9...\nyou": [
            1122085995
        ],
        "token\nstring": [
            1122085995
        ],
        "122.": [
            1122085995
        ],
        "server\nlet’s": [
            1122085995
        ],
        "previously\nthe": [
            1122085995
        ],
        "test\nthe": [
            1122085995
        ],
        "\nlocalhost:8001": [
            1122085995
        ],
        "localhost:8001/api/v1/pods\n{\n": [
            1122085995
        ],
        "/api/v1/pods\"\n": [
            1122085995
        ],
        "433895\"\n": [
            1122085995
        ],
        "..\nokay": [
            1122085995
        ],
        "custom\nserviceaccount": [
            1122085995
        ],
        "gave": [
            1122085995
        ],
        "as\ninstructed": [
            1122085995
        ],
        "non-default": [
            1122085995
        ],
        "curl-custom-sayaml\nlisting": [
            1122085995
        ],
        "container(s)\nlisting": [
            1122085995
        ],
        "serviceaccount\nthis": [
            1122085995
        ],
        "\n\n353securing": [
            1122085995
        ],
        "control\n": [
            1122085995
        ],
        "additional\nserviceaccounts": [
            1122085995
        ],
        "is\nallowed": [
            1122085995
        ],
        "to\nenforce": [
            1122085995
        ],
        "practically": [
            1122085995
        ],
        "the\nrbac": [
            1122085995
        ],
        "next\n12.2": [
            1122085995
        ],
        "securing": [
            1122085995
        ],
        "control\nstarting": [
            1122085995
        ],
        "16.0": [
            1122085995
        ],
        "ramped": [
            1122085995
        ],
        "in\nearlier": [
            1122085995
        ],
        "acquire": [
            1122085995
        ],
        "around\nyou’ll": [
            1122085995
        ],
        "demos": [
            1122085995
        ],
        "traversal": [
            1122085995
        ],
        "traversal)": [
            1122085995
        ],
        "attack": [
            1122085995
        ],
        "directory)": [
            1122085995
        ],
        "malicious": [
            1122085995
        ],
        "insecure": [
            1122085995
        ],
        "18.0": [
            1122085995
        ],
        "graduated": [
            1122085995
        ],
        "ga": [
            1122085995
        ],
        "(general\navailability)": [
            1122085995
        ],
        "when\ndeploying": [
            1122085995
        ],
        "kubadm": [
            1122085995
        ],
        "b)": [
            1122085995
        ],
        "unau-\nthorized": [
            1122085995
        ],
        "viewing": [
            1122085995
        ],
        "you\ngrant": [
            1122085995
        ],
        "privileges": [
            1122085995
        ],
        "8)": [
            1122085995
        ],
        "manage\nauthorization": [
            1122085995
        ],
        "rbac-specific": [
            1122085995
        ],
        "resources\nnotein": [
            1122085995
        ],
        "authorization\nplugins": [
            1122085995
        ],
        "attribute-based": [
            1122085995
        ],
        "(abac)": [
            1122085995
        ],
        "web-\nhook": [
            1122085995
        ],
        "standard\nthough\n12.2.1": [
            1122085995
        ],
        "plugin\nthe": [
            1122085995
        ],
        "check\nwhether": [
            1122085995
        ],
        "http\nrequests": [
            1122085995
        ],
        "certificate)\nunderstanding": [
            1122085995
        ],
        "actions\nbut": [
            1122085995
        ],
        "delete\nand": [
            1122085995
        ],
        "specific\nrest": [
            1122085995
        ],
        "on\nhere": [
            1122085995
        ],
        "kubernetes:\nget": [
            1122085995
        ],
        "pods\ncreate": [
            1122085995
        ],
        "\n\n354chapter": [
            1122085995
        ],
        "server\nupdate": [
            1122085995
        ],
        "secrets\nand": [
            1122085995
        ],
        "(get": [
            1122085995
        ],
        "update)": [
            1122085995
        ],
        "(\nget": [
            1122085995
        ],
        "post\nput)": [
            1122085995
        ],
        "mapping": [
            1122085995
        ],
        "121).": [
            1122085995
        ],
        "the\nnouns": [
            1122085995
        ],
        "(pods": [
            1122085995
        ],
        "secrets)": [
            1122085995
        ],
        "deter-\nmines": [
            1122085995
        ],
        "verb": [
            1122085995
        ],
        "requested\nresource": [
            1122085995
        ],
        "not\nnotethe": [
            1122085995
        ],
        "which\nare": [
            1122085995
        ],
        "chapter\nbesides": [
            1122085995
        ],
        "also\napply": [
            1122085995
        ],
        "\nmyservice)\nand": [
            1122085995
        ],
        "non-resource": [
            1122085995
        ],
        "paths\nbecause": [
            1122085995
        ],
        "\n/api\npath": [
            1122085995
        ],
        "/healthz)": [
            1122085995
        ],
        "factor\nin": [
            1122085995
        ],
        "may\nbe": [
            1122085995
        ],
        "serviceaccounts)": [
            1122085995
        ],
        "associated\nwith": [
            1122085995
        ],
        "certain\nresources": [
            1122085995
        ],
        "allows\nthem": [
            1122085995
        ],
        "user’s": [
            1122085995
        ],
        "permission": [
            1122085995
        ],
        "update\nsecrets": [
            1122085995
        ],
        "requests\non": [
            1122085995
        ],
        "next\ntable": [
            1122085995
        ],
        "verbs\nhttp": [
            1122085995
        ],
        "methodverb": [
            1122085995
        ],
        "resourceverb": [
            1122085995
        ],
        "collection\nget": [
            1122085995
        ],
        "head": [
            1122085995
        ],
        "watching)list": [
            1122085995
        ],
        "watch)\npost": [
            1122085995
        ],
        "createn/a\nput": [
            1122085995
        ],
        "updaten/a\npatch": [
            1122085995
        ],
        "patchn/a\ndelete": [
            1122085995
        ],
        "deletecollection\n": [
            1122085995
        ],
        "\n\n355securing": [
            1122085995
        ],
        "control\n122.2": [
            1122085995
        ],
        "be\ngrouped": [
            1122085995
        ],
        "groups:\nroles": [
            1122085995
        ],
        "which\nresources\nrolebindings": [
            1122085995
        ],
        "specific\nusers": [
            1122085995
        ],
        "serviceaccounts\nroles": [
            1122085995
        ],
        "bindings": [
            1122085995
        ],
        "122).\nthe": [
            1122085995
        ],
        "clusterrole": [
            1122085995
        ],
        "rolebinding": [
            1122085995
        ],
        "a\nclusterrolebinding": [
            1122085995
        ],
        "resources\nwhereas": [
            1122085995
        ],
        "(not\nnamespaced)": [
            1122085995
        ],
        "123.\n": [
            1122085995
        ],
        "roles)": [
            1122085995
        ],
        "cluster-\nroles": [
            1122085995
        ],
        "rolebindings\nare": [
            1122085995
        ],
        "now\n": [
            1122085995
        ],
        "\nwhat?\nrole\nbinding\nsome\nresources\nother\nresources\nrole\ndoesn’t": [
            1122085995
        ],
        "allow\ndoing": [
            1122085995
        ],
        "anything\nwith": [
            1122085995
        ],
        "resources\nuser": [
            1122085995
        ],
        "a\nwho?\nadmins": [
            1122085995
        ],
        "group\nallows": [
            1122085995
        ],
        "users\nto": [
            1122085995
        ],
        "access\nservice-\naccount:\nx\nfigure": [
            1122085995
        ],
        "subjects\n": [
            1122085995
        ],
        "\n\n356chapter": [
            1122085995
        ],
        "server\nsetting": [
            1122085995
        ],
        "exercise\nbefore": [
            1122085995
        ],
        "you’re\nusing": [
            1122085995
        ],
        "if\none": [
            1122085995
        ],
        "allowed\nnoteif": [
            1122085995
        ],
        "autho-\nrization": [
            1122085995
        ],
        "\n--no-enable-legacy-authorization\noption": [
            1122085995
        ],
        "\n--extra-config=apiserverauthorization.mode=rbac\nif": [
            1122085995
        ],
        "followed": [
            1122085995
        ],
        "now’s": [
            1122085995
        ],
        "re-enable": [
            1122085995
        ],
        "permissive-binding\nto": [
            1122085995
        ],
        "per-namespace": [
            1122085995
        ],
        "behaves\n": [
            1122085995
        ],
        "(based": [
            1122085995
        ],
        "image)": [
            1122085995
        ],
        "kubectl\nexec\n": [
            1122085995
        ],
        "aspect": [
            1122085995
        ],
        "security\nnamespace": [
            1122085995
        ],
        "c\nnamespaced\nresources\ncluster-level\nresources\nrolebinding\nrolebinding\nrole\nnamespace": [
            1122085995
        ],
        "b\nnamespaced\nresources\nrolebindingrole\nnamespace": [
            1122085995
        ],
        "a\nnamespaced\nresources\nrolebindingrole\ncluster": [
            1122085995
        ],
        "(resources": [
            1122085995
        ],
        "namespaced)\nclusterrolebindingclusterrole\nfigure": [
            1122085995
        ],
        "namespaced;": [
            1122085995
        ],
        "\n\n357securing": [
            1122085995
        ],
        "control\ncreating": [
            1122085995
        ],
        "pods\nyou’re": [
            1122085995
        ],
        "namespace\nbar": [
            1122085995
        ],
        "foo\nnamespace": [
            1122085995
        ],
        "--image=luksa/kubectl-proxy": [
            1122085995
        ],
        "foo\ndeployment": [
            1122085995
        ],
        "test\"": [
            1122085995
        ],
        "bar\nnamespace": [
            1122085995
        ],
        "bar\"": [
            1122085995
        ],
        "bar\ndeployment": [
            1122085995
        ],
        "terminals": [
            1122085995
        ],
        "terminal)": [
            1122085995
        ],
        "namespace\nfoo": [
            1122085995
        ],
        "foo\nname": [
            1122085995
        ],
        "age\ntest-145485760-ttq36": [
            1122085995
        ],
        "1m\nthen": [
            1122085995
        ],
        "test-145485760-ttq36": [
            1122085995
        ],
        "sh\n/": [
            1122085995
        ],
        "#\ndo": [
            1122085995
        ],
        "pods\nto": [
            1122085995
        ],
        "namespace:\n/": [
            1122085995
        ],
        "#": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/foo/services\nuser": [
            1122085995
        ],
        "system:serviceaccount:foo:default\"": [
            1122085995
        ],
        "foo\"\nyou’re": [
            1122085995
        ],
        "localhost:8001": [
            1122085995
        ],
        "is\nlistening": [
            1122085995
        ],
        "responded": [
            1122085995
        ],
        "you’re\nseeing": [
            1122085995
        ],
        "resource\nlisting": [
            1122085995
        ],
        "\n\n358chapter": [
            1122085995
        ],
        "server\n122.3": [
            1122085995
        ],
        "rolebindings\na": [
            1122085995
        ],
        "restful\nresources)": [
            1122085995
        ],
        "\nget": [
            1122085995
        ],
        "list\nservices": [
            1122085995
        ],
        "namespace\napiversion:": [
            1122085995
        ],
        "rbacauthorization.k8s.io/v1\nkind:": [
            1122085995
        ],
        "role\nmetadata:\n": [
            1122085995
        ],
        "service-reader\nrules:\n-": [
            1122085995
        ],
        "apigroups:": [
            1122085995
        ],
        "[\"]": [
            1122085995
        ],
        "verbs:": [
            1122085995
        ],
        "[get\"": [
            1122085995
        ],
        "list\"]": [
            1122085995
        ],
        "[services\"]": [
            1122085995
        ],
        "\nwarningthe": [
            1122085995
        ],
        "plural": [
            1122085995
        ],
        "that\neach": [
            1122085995
        ],
        "field\n(along": [
            1122085995
        ],
        "version)": [
            1122085995
        ],
        "spec-\nify": [
            1122085995
        ],
        "\napigroup": [
            1122085995
        ],
        "you’re\nallowing": [
            1122085995
        ],
        "rules\nnotein": [
            1122085995
        ],
        "their\nnames": [
            1122085995
        ],
        "\nresourcenames": [
            1122085995
        ],
        "field\nfigure": [
            1122085995
        ],
        "role:": [
            1122085995
        ],
        "service-readeryaml\nroles": [
            1122085995
        ],
        "\nomitted": [
            1122085995
        ],
        "used)\nservices": [
            1122085995
        ],
        "apigroup": [
            1122085995
        ],
        "hence": [
            1122085995
        ],
        "\"\ngetting": [
            1122085995
        ],
        "allowed\nthis": [
            1122085995
        ],
        "pertains": [
            1122085995
        ],
        "\n(plural": [
            1122085995
        ],
        "used!)\nallows": [
            1122085995
        ],
        "getting\nallows": [
            1122085995
        ],
        "listing\nservices\nrole:\nservice-reader\nservices\nnamespace:": [
            1122085995
        ],
        "foonamespace:": [
            1122085995
        ],
        "bar\ndoes": [
            1122085995
        ],
        "in\nother": [
            1122085995
        ],
        "namespaces\nfigure": [
            1122085995
        ],
        "service-reader": [
            1122085995
        ],
        "\n\n359securing": [
            1122085995
        ],
        "role\ncreate": [
            1122085995
        ],
        "service-readeryaml": [
            1122085995
        ],
        "foo\nrole": [
            1122085995
        ],
        "service-reader\"": [
            1122085995
        ],
        "created\nnotethe": [
            1122085995
        ],
        "--namespace\nnote": [
            1122085995
        ],
        "have\ncluster-admin": [
            1122085995
        ],
        "cluster-admin-binding": [
            1122085995
        ],
        "--user=youremail@address.com\ninstead": [
            1122085995
        ],
        "the\nrole": [
            1122085995
        ],
        "namespace:\n$": [
            1122085995
        ],
        "--verb=get": [
            1122085995
        ],
        "--verb=list": [
            1122085995
        ],
        "--resource=services": [
            1122085995
        ],
        "bar\nrole": [
            1122085995
        ],
        "created\nthese": [
            1122085995
        ],
        "respectively)": [
            1122085995
        ],
        "command\nagain)": [
            1122085995
        ],
        "respec-\ntive": [
            1122085995
        ],
        "\nbinding": [
            1122085995
        ],
        "serviceaccount\na": [
            1122085995
        ],
        "perform\nthem": [
            1122085995
        ],
        "serviceaccounts)\n": [
            1122085995
        ],
        "bind\nthe": [
            1122085995
        ],
        "--role=service-reader": [
            1122085995
        ],
        "--serviceaccount=foo:default": [
            1122085995
        ],
        "foo\nrolebinding": [
            1122085995
        ],
        "binds\nthe": [
            1122085995
        ],
        "\nservice-reader": [
            1122085995
        ],
        "125.\nnoteto": [
            1122085995
        ],
        "--user\nargument": [
            1122085995
        ],
        "--group\n": [
            1122085995
        ],
        "\n\n360chapter": [
            1122085995
        ],
        "rolebinding\nmetadata:\n": [
            1122085995
        ],
        "test\n": [
            1122085995
        ],
        "..\nroleref:\n": [
            1122085995
        ],
        "apigroup:": [
            1122085995
        ],
        "rbacauthorization.k8s.io\n": [
            1122085995
        ],
        "\nsubjects:\n-": [
            1122085995
        ],
        "the\nroleref": [
            1122085995
        ],
        "property)": [
            1122085995
        ],
        "groups)": [
            1122085995
        ],
        "rolebinding\nbinds": [
            1122085995
        ],
        "pod\n/": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/foo/services\n{\n": [
            1122085995
        ],
        "servicelist\"\n": [
            1122085995
        ],
        "/api/v1/namespaces/foo/services\"\nlisting": [
            1122085995
        ],
        "role\nlisting": [
            1122085995
        ],
        "server\nnamespace:": [
            1122085995
        ],
        "foo\nrole:\nservice-reader\nget": [
            1122085995
        ],
        "list\ndefault": [
            1122085995
        ],
        "serviceaccount\nis": [
            1122085995
        ],
        "namespace\nservices\nrolebinding:\ntest\nservice-\naccount:\ndefault\nfigure": [
            1122085995
        ],
        "role\nthis": [
            1122085995
        ],
        "role\nand": [
            1122085995
        ],
        "\n\n361securing": [
            1122085995
        ],
        "24906\"\n": [
            1122085995
        ],
        "[]": [
            1122085995
        ],
        "\n}\nincluding": [
            1122085995
        ],
        "rolebinding\nthe": [
            1122085995
        ],
        "obviously\nalso": [
            1122085995
        ],
        "foo\nthen": [
            1122085995
        ],
        "listing\nsubjects:\n-": [
            1122085995
        ],
        "other\nterminal": [
            1122085995
        ],
        "summarize\nwhat": [
            1122085995
        ],
        "as\ndepicted": [
            1122085995
        ],
        "126.\nlisting": [
            1122085995
        ],
        "namespace\nthe": [
            1122085995
        ],
        "exist\nyou’re": [
            1122085995
        ],
        "\nserviceaccount": [
            1122085995
        ],
        "namespace\nnamespace:": [
            1122085995
        ],
        "list\nboth": [
            1122085995
        ],
        "are\nallowed": [
            1122085995
        ],
        "services\nin": [
            1122085995
        ],
        "namespace\nservices\nnamespace:": [
            1122085995
        ],
        "bar\nrolebinding:\ntest\nservice-\naccount:\ndefault\nservice-\naccount:\ndefault\nfigure": [
            1122085995
        ],
        "role\n": [
            1122085995
        ],
        "\n\n362chapter": [
            1122085995
        ],
        "server\n122.4": [
            1122085995
        ],
        "clusterrolebindings\nroles": [
            1122085995
        ],
        "also\nexist:": [
            1122085995
        ],
        "is\nin": [
            1122085995
        ],
        "prob-\nably": [
            1122085995
        ],
        "need)": [
            1122085995
        ],
        "namespace\nwhen": [
            1122085995
        ],
        "at\nall": [
            1122085995
        ],
        "also\nmentioned": [
            1122085995
        ],
        "resources\n(\n/healthz": [
            1122085995
        ],
        "non-\nresource": [
            1122085995
        ],
        "non-namespaced\nresources": [
            1122085995
        ],
        "redefine": [
            1122085995
        ],
        "them\nallowing": [
            1122085995
        ],
        "resources\nas": [
            1122085995
        ],
        "first\nyou’ll": [
            1122085995
        ],
        "\npv-reader:\n$": [
            1122085995
        ],
        "pv-reader": [
            1122085995
        ],
        "--verb=getlist": [
            1122085995
        ],
        "--resource=persistentvolumes\nclusterrole": [
            1122085995
        ],
        "pv-reader\"": [
            1122085995
        ],
        "clusterrole’s": [
            1122085995
        ],
        "clusterrole\nmetadata:": [
            1122085995
        ],
        "39932\"": [
            1122085995
        ],
        "e9ac1099-30e2-11e7-955c-080027e6b159": [
            1122085995
        ],
        "definition\nclusterroles": [
            1122085995
        ],
        "\nno": [
            1122085995
        ],
        "field\n": [
            1122085995
        ],
        "\n\n363securing": [
            1122085995
        ],
        "control\nrules:\n-": [
            1122085995
        ],
        "pod\ncan": [
            1122085995
        ],
        "where\nyou’re": [
            1122085995
        ],
        "localhost:8001/api/v1/persistentvolumes\nuser": [
            1122085995
        ],
        "scope\nnotethe": [
            1122085995
        ],
        "aren’t\nnamespaced": [
            1122085995
        ],
        "to\nbind": [
            1122085995
        ],
        "pv-test": [
            1122085995
        ],
        "--clusterrole=pv-reader": [
            1122085995
        ],
        "pv-test\"": [
            1122085995
        ],
        "created\ncan": [
            1122085995
        ],
        "now?\n/": [
            1122085995
        ],
        "scope\nhmm": [
            1122085995
        ],
        "rolebinding’s": [
            1122085995
        ],
        "listing\ncan": [
            1122085995
        ],
        "anything)": [
            1122085995
        ],
        "it?\n$": [
            1122085995
        ],
        "pv-test\n": [
            1122085995
        ],
        "clusterrole\nin": [
            1122085995
        ],
        "\nrules": [
            1122085995
        ],
        "\nlike": [
            1122085995
        ],
        "\nregular": [
            1122085995
        ],
        "role\nthe": [
            1122085995
        ],
        "\npv-reader": [
            1122085995
        ],
        "clusterrole\n": [
            1122085995
        ],
        "\n\n364chapter": [
            1122085995
        ],
        "server\nsubjects:\n-": [
            1122085995
        ],
        "the\ncorrect": [
            1122085995
        ],
        "wrong?\nalthough": [
            1122085995
        ],
        "for\ncluster-level": [
            1122085995
        ],
        "(non-namespaced)": [
            1122085995
        ],
        "clusterrolebinding\n": [
            1122085995
        ],
        "role-\nbinding": [
            1122085995
        ],
        "pv-test\nrolebinding": [
            1122085995
        ],
        "clusterrolebinding:\n$": [
            1122085995
        ],
        "--serviceaccount=foo:default\nclusterrolebinding": [
            1122085995
        ],
        "command\nand": [
            1122085995
        ],
        "(need": [
            1122085995
        ],
        "now:\n/": [
            1122085995
        ],
        "localhost:8001/api/v1/persistentvolumes\n{\n": [
            1122085995
        ],
        "persistentvolumelist\"\n": [
            1122085995
        ],
        "v1\"\n..\nthe": [
            1122085995
        ],
        "foocluster-level": [
            1122085995
        ],
        "resources\nclusterrole:\npv-reader\nget": [
            1122085995
        ],
        "list\npersistent\nvolumes\nrolebinding:\npv-test\ndefault": [
            1122085995
        ],
        "list\npersistentvolumes\nservice-\naccount:\ndefault\nfigure": [
            1122085995
        ],
        "cluster-\nlevel": [
            1122085995
        ],
        "\n\n365securing": [
            1122085995
        ],
        "control\nyou": [
            1122085995
        ],
        "can!": [
            1122085995
        ],
        "when\ngranting": [
            1122085995
        ],
        "resources\ntipremember": [
            1122085995
        ],
        "resources\neven": [
            1122085995
        ],
        "clusterrolebinding\nallowing": [
            1122085995
        ],
        "urls\nwe’ve": [
            1122085995
        ],
        "these\nurls": [
            1122085995
        ],
        "granted": [
            1122085995
        ],
        "explicitly;": [
            1122085995
        ],
        "client’s\nrequest": [
            1122085995
        ],
        "\nsystem:discovery\nclusterrole": [
            1122085995
        ],
        "identically": [
            1122085995
        ],
        "among\nother": [
            1122085995
        ],
        "122.5).": [
            1122085995
        ],
        "\nsystem:discovery": [
            1122085995
        ],
        "system:discovery": [
            1122085995
        ],
        "clusterrole\nmetadata:\n": [
            1122085995
        ],
        "system:discovery\n": [
            1122085995
        ],
        "..\nrules:\n-": [
            1122085995
        ],
        "nonresourceurls:": [
            1122085995
        ],
        "/api": [
            1122085995
        ],
        "/api/*": [
            1122085995
        ],
        "/apis": [
            1122085995
        ],
        "/apis/*": [
            1122085995
        ],
        "/healthz": [
            1122085995
        ],
        "/swaggerapi/*": [
            1122085995
        ],
        "/version": [
            1122085995
        ],
        "clusterrole\nnamespace:": [
            1122085995
        ],
        "list\npersistent\nvolumes\nclusterrolebinding:\npv-test\ndefault": [
            1122085995
        ],
        "in\nfoo": [
            1122085995
        ],
        "allowed\nto": [
            1122085995
        ],
        "persistentvolumes\nservice-\naccount:\ndefault\nfigure": [
            1122085995
        ],
        "resources\ninstead": [
            1122085995
        ],
        "\nurls\n": [
            1122085995
        ],
        "\n\n366chapter": [
            1122085995
        ],
        "(field": [
            1122085995
        ],
        "nonresource-\nurls\n": [
            1122085995
        ],
        "field)": [
            1122085995
        ],
        "http\nmethod": [
            1122085995
        ],
        "urls\nnotefor": [
            1122085995
        ],
        "and\npatch": [
            1122085995
        ],
        "in\nlowercase\nas": [
            1122085995
        ],
        "bound\nwith": [
            1122085995
        ],
        "effect\nthe": [
            1122085995
        ],
        "cluster-\nrolebinding": [
            1122085995
        ],
        "clusterrolebinding\nmetadata:\n": [
            1122085995
        ],
        "clusterrole\nas": [
            1122085995
        ],
        "\nsystem:authenticated": [
            1122085995
        ],
        "system:unauthenti-\ncated\n": [
            1122085995
        ],
        "\nnotegroups": [
            1122085995
        ],
        "used\nin": [
            1122085995
        ],
        "\n/api": [
            1122085995
        ],
        "(through\nthe": [
            1122085995
        ],
        "serviceaccount)\nlisting": [
            1122085995
        ],
        "clusterrolebinding\nonly": [
            1122085995
        ],
        "urls\nthis": [
            1122085995
        ],
        "clusterrole\nit": [
            1122085995
        ],
        "\nunauthenticated": [
            1122085995
        ],
        "\n(that": [
            1122085995
        ],
        "everyone)\n": [
            1122085995
        ],
        "\n\n367securing": [
            1122085995
        ],
        "control\nand": [
            1122085995
        ],
        "(making\nyou": [
            1122085995
        ],
        "unauthenticated": [
            1122085995
        ],
        "user):\n$": [
            1122085995
        ],
        "https://$(minikube": [
            1122085995
        ],
        "ip):8443/api": [
            1122085995
        ],
        "-k\n{\n": [
            1122085995
        ],
        "apiversions\"\n": [
            1122085995
        ],
        "..\nyou’ve": [
            1122085995
        ],
        "cluster-level\nresources": [
            1122085995
        ],
        "used\nwith": [
            1122085995
        ],
        "role-\nbinding’s": [
            1122085995
        ],
        "namespace\nusing": [
            1122085995
        ],
        "namespaces\nclusterroles": [
            1122085995
        ],
        "clusterrolebindings\nthey": [
            1122085995
        ],
        "already\nstarted": [
            1122085995
        ],
        "\nview\nwhich": [
            1122085995
        ],
        "view\n": [
            1122085995
        ],
        "apigroups:\n": [
            1122085995
        ],
        "\"\n": [
            1122085995
        ],
        "replicationcontrollers/scale": [
            1122085995
        ],
        "\n..\nthis": [
            1122085995
        ],
        "rule\nallows": [
            1122085995
        ],
        "persistent-\nvolumeclaims": [
            1122085995
        ],
        "you’re\nlooking": [
            1122085995
        ],
        "role)": [
            1122085995
        ],
        "this\nclusterrole": [
            1122085995
        ],
        "do?\nlisting": [
            1122085995
        ],
        "clusterrole\nthis": [
            1122085995
        ],
        "\nthese": [
            1122085995
        ],
        "\nthey’re": [
            1122085995
        ],
        "\nresources)\nas": [
            1122085995
        ],
        "\nsuggests": [
            1122085995
        ],
        "\n\n368chapter": [
            1122085995
        ],
        "either)": [
            1122085995
        ],
        "cluster-\nrole": [
            1122085995
        ],
        "all\nnamespaces": [
            1122085995
        ],
        "the\nbinding": [
            1122085995
        ],
        "both\noptions": [
            1122085995
        ],
        "place:\n/": [
            1122085995
        ],
        "localhost:8001/api/v1/pods\nuser": [
            1122085995
        ],
        "\nscope/": [
            1122085995
        ],
        "#\n/": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/foo/pods\nuser": [
            1122085995
        ],
        "\nfoo\"\nwith": [
            1122085995
        ],
        "sec-\nond": [
            1122085995
        ],
        "either\n": [
            1122085995
        ],
        "serviceaccount:\n$": [
            1122085995
        ],
        "view-test": [
            1122085995
        ],
        "--clusterrole=view": [
            1122085995
        ],
        "view-test\"": [
            1122085995
        ],
        "namespace?\n/": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/foo/pods\n{\n": [
            1122085995
        ],
        "..\nit": [
            1122085995
        ],
        "namespaces\nthe": [
            1122085995
        ],
        "well:\n/": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/bar/pods\n{\n": [
            1122085995
        ],
        "pods\nacross": [
            1122085995
        ],
        "/api/v1/pods": [
            1122085995
        ],
        "path:\n/": [
            1122085995
        ],
        "\n\n369securing": [
            1122085995
        ],
        "control\nas": [
            1122085995
        ],
        "com-\nbining": [
            1122085995
        ],
        "resources\nallows": [
            1122085995
        ],
        "129.\nnow": [
            1122085995
        ],
        "rolebinding\nfirst": [
            1122085995
        ],
        "view-test\nclusterrolebinding": [
            1122085995
        ],
        "deleted\nnext": [
            1122085995
        ],
        "\nview": [
            1122085995
        ],
        "pod\naccess": [
            1122085995
        ],
        "..\nnamespace:": [
            1122085995
        ],
        "foo\ncluster-level\nresources\nnamespace:": [
            1122085995
        ],
        "bar\npodspods\ndefault\nserviceaccount\nin": [
            1122085995
        ],
        "namespace\nis": [
            1122085995
        ],
        "to\nview": [
            1122085995
        ],
        "namespace\nclusterrole:\nview\nallows": [
            1122085995
        ],
        "getting\nlisting,": [
            1122085995
        ],
        "watching\nclusterrolebinding:\nview-test\npods\nservices,\nendpoints,\nconfigmaps,\n..\nservice-\naccount:\ndefault\nfigure": [
            1122085995
        ],
        "grants": [
            1122085995
        ],
        "\nnamespaces\n": [
            1122085995
        ],
        "\n\n370chapter": [
            1122085995
        ],
        "server\n/": [
            1122085995
        ],
        "localhost:8001/api/v1/namespaces/bar/pods\nuser": [
            1122085995
        ],
        "\nbar\"\n/": [
            1122085995
        ],
        "\nscope\nas": [
            1122085995
        ],
        "visualized": [
            1122085995
        ],
        "1210.\nsummarizing": [
            1122085995
        ],
        "combinations\nwe’ve": [
            1122085995
        ],
        "remember\nwhen": [
            1122085995
        ],
        "cat-\negorizing": [
            1122085995
        ],
        "122.\ntable": [
            1122085995
        ],
        "types\nfor": [
            1122085995
        ],
        "accessingrole": [
            1122085995
        ],
        "usebinding": [
            1122085995
        ],
        "use\ncluster-level": [
            1122085995
        ],
        "(nodes": [
            1122085995
        ],
        "..)clusterrole": [
            1122085995
        ],
        "clusterrolebinding\nnon-resource": [
            1122085995
        ],
        "(/api": [
            1122085995
        ],
        "clusterrolebinding\nnamespaced": [
            1122085995
        ],
        "\nacross": [
            1122085995
        ],
        "namespaces)\nclusterrole": [
            1122085995
        ],
        "(reus-\ning": [
            1122085995
        ],
        "rolebinding\nnamespaced": [
            1122085995
        ],
        "\n(role": [
            1122085995
        ],
        "namespace)\nrole": [
            1122085995
        ],
        "rolebinding\nnamespace:": [
            1122085995
        ],
        "foo\ncluster-level": [
            1122085995
        ],
        "resources\nnamespace:": [
            1122085995
        ],
        "bar\npodspods\nclusterrole:\nview\nallows": [
            1122085995
        ],
        "watching\nrolebinding:\nview-test\npods\nservices,\nendpoints,\nconfigmaps,\n..\ndefault": [
            1122085995
        ],
        "foo\ndespite": [
            1122085995
        ],
        "clusterrole\nservice-\naccount:\ndefault\nfigure": [
            1122085995
        ],
        "\nrolebinding’s": [
            1122085995
        ],
        "\n\n371securing": [
            1122085995
        ],
        "control\nhopefully": [
            1122085995
        ],
        "relationships": [
            1122085995
        ],
        "clearer\nnow": [
            1122085995
        ],
        "may\nclear": [
            1122085995
        ],
        "pre-configured": [
            1122085995
        ],
        "section\n12.2.5": [
            1122085995
        ],
        "clusterrolebindings\nkubernetes": [
            1122085995
        ],
        "and\nbindings": [
            1122085995
        ],
        "mistakenly": [
            1122085995
        ],
        "clusterrolebindings\nname": [
            1122085995
        ],
        "age\ncluster-admin": [
            1122085995
        ],
        "1d\nsystem:basic-user": [
            1122085995
        ],
        "1d\nsystem:controller:attachdetach-controller": [
            1122085995
        ],
        "1d\n..\nsystem:controller:ttl-controller": [
            1122085995
        ],
        "1d\nsystem:discovery": [
            1122085995
        ],
        "1d\nsystem:kube-controller-manager": [
            1122085995
        ],
        "1d\nsystem:kube-dns": [
            1122085995
        ],
        "1d\nsystem:kube-scheduler": [
            1122085995
        ],
        "1d\nsystem:node": [
            1122085995
        ],
        "1d\nsystem:node-proxier": [
            1122085995
        ],
        "1d\n$": [
            1122085995
        ],
        "clusterroles\nname": [
            1122085995
        ],
        "age\nadmin": [
            1122085995
        ],
        "1d\ncluster-admin": [
            1122085995
        ],
        "1d\nedit": [
            1122085995
        ],
        "1d\nsystem:auth-delegator": [
            1122085995
        ],
        "1d\nsystem:heapster": [
            1122085995
        ],
        "1d\nsystem:kube-aggregator": [
            1122085995
        ],
        "1d\nsystem:node-bootstrapper": [
            1122085995
        ],
        "1d\nsystem:node-problem-detector": [
            1122085995
        ],
        "1d\nsystem:persistent-volume-provisioner": [
            1122085995
        ],
        "1d\nview": [
            1122085995
        ],
        "1d\nlisting": [
            1122085995
        ],
        "clusterroles\n": [
            1122085995
        ],
        "\n\n372chapter": [
            1122085995
        ],
        "clusterroles\nthey’re": [
            1122085995
        ],
        "pods\nallowing": [
            1122085995
        ],
        "clusterrole\nyou": [
            1122085995
        ],
        "read-\ning": [
            1122085995
        ],
        "you’re\nprobably": [
            1122085995
        ],
        "secrets?": [
            1122085995
        ],
        "masquerade": [
            1122085995
        ],
        "additional\nprivileges": [
            1122085995
        ],
        "(privilege": [
            1122085995
        ],
        "escalation)": [
            1122085995
        ],
        "\nallowing": [
            1122085995
        ],
        "clusterrole\nnext": [
            1122085995
        ],
        "namespace\nbut": [
            1122085995
        ],
        "viewing\nor": [
            1122085995
        ],
        "rolebindings—again": [
            1122085995
        ],
        "privilege": [
            1122085995
        ],
        "escalation\ngranting": [
            1122085995
        ],
        "clusterrole\ncomplete": [
            1122085995
        ],
        "resourcequotas": [
            1122085995
        ],
        "14)": [
            1122085995
        ],
        "the\nnamespace": [
            1122085995
        ],
        "\nedit": [
            1122085995
        ],
        "namespace\nnoteto": [
            1122085995
        ],
        "escalation": [
            1122085995
        ],
        "that\nrole": [
            1122085995
        ],
        "scope)": [
            1122085995
        ],
        "cluster-\nadmin\n": [
            1122085995
        ],
        "doesn’t\nallow": [
            1122085995
        ],
        "namespace’s": [
            1122085995
        ],
        "namespace\nresource": [
            1122085995
        ],
        "rolebinding\nthat": [
            1122085995
        ],
        "\ncluster-admin": [
            1122085995
        ],
        "the\nrolebinding": [
            1122085995
        ],
        "created\n": [
            1122085995
        ],
        "complete\ncontrol": [
            1122085995
        ],
        "\ncluster-admin\nclusterrole": [
            1122085995
        ],
        "rolebinding\nunderstanding": [
            1122085995
        ],
        "clusterroles\nthe": [
            1122085995
        ],
        "which\nstart": [
            1122085995
        ],
        "\nsystem:": [
            1122085995
        ],
        "kubernetes\ncomponents": [
            1122085995
        ],
        "\nsystem:kube-scheduler": [
            1122085995
        ],
        "\nsystem:node": [
            1122085995
        ],
        "and\nso": [
            1122085995
        ],
        "\n\n373summary\n": [
            1122085995
        ],
        "(they’re": [
            1122085995
        ],
        "prefixed\nwith": [
            1122085995
        ],
        "controller:)": [
            1122085995
        ],
        "binds\nit": [
            1122085995
        ],
        "\nsystem:kube-scheduler\nclusterrolebinding": [
            1122085995
        ],
        "the\nsystem:kube-scheduler": [
            1122085995
        ],
        "\n122.6": [
            1122085995
        ],
        "granting": [
            1122085995
        ],
        "wisely\nby": [
            1122085995
        ],
        "than\nthose": [
            1122085995
        ],
        "previous\nexamples": [
            1122085995
        ],
        "urls)": [
            1122085995
        ],
        "can’t\neven": [
            1122085995
        ],
        "a\nbad": [
            1122085995
        ],
        "permis-\nsions": [
            1122085995
        ],
        "(principle": [
            1122085995
        ],
        "least\nprivilege)\ncreating": [
            1122085995
        ],
        "pod\nit’s": [
            1122085995
        ],
        "rep-\nlicas)": [
            1122085995
        ],
        "associate": [
            1122085995
        ],
        "clusterrole)": [
            1122085995
        ],
        "a\nrolebinding": [
            1122085995
        ],
        "pods\nwhile": [
            1122085995
        ],
        "serviceaccounts\nand": [
            1122085995
        ],
        "\nserviceaccountname": [
            1122085995
        ],
        "necessary\npermissions": [
            1122085995
        ],
        "\nexpecting": [
            1122085995
        ],
        "compromised\nyour": [
            1122085995
        ],
        "intruder": [
            1122085995
        ],
        "today’s\ncomplex": [
            1122085995
        ],
        "vulnerabilities": [
            1122085995
        ],
        "unwanted": [
            1122085995
        ],
        "persons": [
            1122085995
        ],
        "to\neventually": [
            1122085995
        ],
        "should\nalways": [
            1122085995
        ],
        "damage\n12.3": [
            1122085995
        ],
        "foundation": [
            1122085995
        ],
        "following:\nclients": [
            1122085995
        ],
        "pods\napplications": [
            1122085995
        ],
        "\nboth": [
            1122085995
        ],
        "\n\n374chapter": [
            1122085995
        ],
        "server\nby": [
            1122085995
        ],
        "automatically\nadditional": [
            1122085995
        ],
        "pod\nserviceaccounts": [
            1122085995
        ],
        "of\nsecrets": [
            1122085995
        ],
        "pod\na": [
            1122085995
        ],
        "pod\nroles": [
            1122085995
        ],
        "resources\nrolebindings": [
            1122085995
        ],
        "users\ngroups,": [
            1122085995
        ],
        "serviceaccounts\neach": [
            1122085995
        ],
        "clusterrolebindings\nin": [
            1122085995
        ],
        "protect": [
            1122085995
        ],
        "\n\n375\nsecuring": [
            1122085995
        ],
        "network\nin": [
            1122085995
        ],
        "attacker\ngets": [
            1122085995
        ],
        "their\ncode": [
            1122085995
        ],
        "real\ndamage?": [
            1122085995
        ],
        "node\nthey’re": [
            1122085995
        ],
        "pods\nrunning": [
            1122085995
        ],
        "users\nrunning": [
            1122085995
        ],
        "privileged": [
            1122085995
        ],
        "containers\nadding": [
            1122085995
        ],
        "dropping": [
            1122085995
        ],
        "\ncapabilities\ndefining": [
            1122085995
        ],
        "do\nsecuring": [
            1122085995
        ],
        "\n\n376chapter": [
            1122085995
        ],
        "13securing": [
            1122085995
        ],
        "network\nthe": [
            1122085995
        ],
        "use\nto": [
            1122085995
        ],
        "communicate\n13.1": [
            1122085995
        ],
        "pod\ncontainers": [
            1122085995
        ],
        "isolate\ntheir": [
            1122085995
        ],
        "default\nnamespaces": [
            1122085995
        ],
        "only\nprocesses": [
            1122085995
        ],
        "inter-process\ncommunication": [
            1122085995
        ],
        "(ipc)\n13.1.1": [
            1122085995
        ],
        "pod\ncertain": [
            1122085995
        ],
        "(usually": [
            1122085995
        ],
        "namespaces\nallowing": [
            1122085995
        ],
        "manipulate": [
            1122085995
        ],
        "node-level": [
            1122085995
        ],
        "network\nadapters": [
            1122085995
        ],
        "\nhostnetwork": [
            1122085995
        ],
        "spec\nto": [
            1122085995
        ],
        "\ntrue\n": [
            1122085995
        ],
        "131.": [
            1122085995
        ],
        "and\nif": [
            1122085995
        ],
        "port\nyou": [
            1122085995
        ],
        "pod-with-host-network\nlisting": [
            1122085995
        ],
        "pod-with-host-networkyaml\nnode\npod": [
            1122085995
        ],
        "network\nnamespace\neth0lo\neth0docker0loeth1\nnode’s": [
            1122085995
        ],
        "network\nnamespace\npod": [
            1122085995
        ],
        "b\nhostnetwork:": [
            1122085995
        ],
        "true\nfigure": [
            1122085995
        ],
        "\nhostnetwork:": [
            1122085995
        ],
        "\ntrue": [
            1122085995
        ],
        "\n\n377using": [
            1122085995
        ],
        "hostnetwork:": [
            1122085995
        ],
        "alpine\n": [
            1122085995
        ],
        "[/bin/sleep\"": [
            1122085995
        ],
        "999999\"]\nafter": [
            1122085995
        ],
        "example)\n$": [
            1122085995
        ],
        "pod-with-host-network": [
            1122085995
        ],
        "ifconfig\ndocker0": [
            1122085995
        ],
        "encap:ethernet": [
            1122085995
        ],
        "hwaddr": [
            1122085995
        ],
        "02:42:14:08:23:47\n": [
            1122085995
        ],
        "inet": [
            1122085995
        ],
        "addr:17217.0.1": [
            1122085995
        ],
        "bcast:00.0.0": [
            1122085995
        ],
        "mask:255255.0.0\n": [
            1122085995
        ],
        "..\neth0": [
            1122085995
        ],
        "08:00:27:f8:fa:4e\n": [
            1122085995
        ],
        "addr:100.2.15": [
            1122085995
        ],
        "bcast:100.2.255": [
            1122085995
        ],
        "mask:255255.255.0\n": [
            1122085995
        ],
        "..\nlo": [
            1122085995
        ],
        "encap:local": [
            1122085995
        ],
        "loopback\n": [
            1122085995
        ],
        "addr:1270.0.1": [
            1122085995
        ],
        "mask:2550.0.0\n": [
            1122085995
        ],
        "..\nveth1178d4f": [
            1122085995
        ],
        "1e:03:8d:d6:e1:2c\n": [
            1122085995
        ],
        "inet6": [
            1122085995
        ],
        "addr:": [
            1122085995
        ],
        "fe80::1c03:8dff:fed6:e12c/64": [
            1122085995
        ],
        "scope:link\n": [
            1122085995
        ],
        "broadcast": [
            1122085995
        ],
        "multicast": [
            1122085995
        ],
        "mtu:1500": [
            1122085995
        ],
        "metric:1\n..\nwhen": [
            1122085995
        ],
        "that\nthose": [
            1122085995
        ],
        "they\nweren’t": [
            1122085995
        ],
        "pod\n13.1.2": [
            1122085995
        ],
        "\nnamespace\na": [
            1122085995
        ],
        "but\nstill": [
            1122085995
        ],
        "\nhostport": [
            1122085995
        ],
        "property\nin": [
            1122085995
        ],
        "\nspeccontainers.ports": [
            1122085995
        ],
        "service\nthey’re": [
            1122085995
        ],
        "132.\n": [
            1122085995
        ],
        "a\nconnection": [
            1122085995
        ],
        "node\nwhereas": [
            1122085995
        ],
        "with\npods": [
            1122085995
        ],
        "pods\nwhereas": [
            1122085995
        ],
        "run\nsuch": [
            1122085995
        ],
        "figure)\nlisting": [
            1122085995
        ],
        "\n\n378chapter": [
            1122085995
        ],
        "network\nit’s": [
            1122085995
        ],
        "bind\nto": [
            1122085995
        ],
        "133.": [
            1122085995
        ],
        "scheduled\n(one": [
            1122085995
        ],
        "pending)\nnode": [
            1122085995
        ],
        "1\ntwo": [
            1122085995
        ],
        "using\nhostport\nport\n8080\nport\n9000\nnode": [
            1122085995
        ],
        "2\nport\n8080\nport\n9000\nnode": [
            1122085995
        ],
        "same\nnodeport\nservice\nport\n8080\nnode": [
            1122085995
        ],
        "2\nport\n8080\nnode": [
            1122085995
        ],
        "3\nport\n88\nport\n88\nport\n88\nservice\n()iptables\nservice\n()iptables\nservice\n()iptables\nfigure": [
            1122085995
        ],
        "hostport": [
            1122085995
        ],
        "service\nnode": [
            1122085995
        ],
        "1\nport\n8080\nhost\nport\n9000\nhost\nport\n9000\npod": [
            1122085995
        ],
        "3\nport\n8080\nhost\nport\n9000\nnode": [
            1122085995
        ],
        "3\npod": [
            1122085995
        ],
        "4\nport\n8080\ncannot": [
            1122085995
        ],
        "bound\nonly": [
            1122085995
        ],
        "single\nreplica": [
            1122085995
        ],
        "node\nfigure": [
            1122085995
        ],
        "\n\n379using": [
            1122085995
        ],
        "9000\napiversion:": [
            1122085995
        ],
        "kubia-hostport\nspec:\n": [
            1122085995
        ],
        "hostport:": [
            1122085995
        ],
        "tcp\nafter": [
            1122085995
        ],
        "that\nport": [
            1122085995
        ],
        "5\nthe": [
            1122085995
        ],
        "two\nreplicas": [
            1122085995
        ],
        "this—it’s": [
            1122085995
        ],
        "16\n13.1.3": [
            1122085995
        ],
        "namespaces\nsimilar": [
            1122085995
        ],
        "hostnetwork": [
            1122085995
        ],
        "hostipc": [
            1122085995
        ],
        "properties\nwhen": [
            1122085995
        ],
        "ipc\nnamespaces": [
            1122085995
        ],
        "pod-with-host-pid-and-ipc\nspec:\n": [
            1122085995
        ],
        "hostpid:": [
            1122085995
        ],
        "hostipc:": [
            1122085995
        ],
        "999999\"]\nlisting": [
            1122085995
        ],
        "space:": [
            1122085995
        ],
        "kubia-hostportyaml\nlisting": [
            1122085995
        ],
        "namespaces:": [
            1122085995
        ],
        "pod-with-host-pid-and-ipcyaml\nthe": [
            1122085995
        ],
        "\nreached": [
            1122085995
        ],
        "ip\nit": [
            1122085995
        ],
        "on\nyou": [
            1122085995
        ],
        "\nnamespace\nyou": [
            1122085995
        ],
        "\nipc": [
            1122085995
        ],
        "\n\n380chapter": [
            1122085995
        ],
        "network\nyou’ll": [
            1122085995
        ],
        "pod\nand": [
            1122085995
        ],
        "pod-with-host-pid-and-ipc": [
            1122085995
        ],
        "aux\npid": [
            1122085995
        ],
        "0:01": [
            1122085995
        ],
        "/usr/lib/systemd/systemd": [
            1122085995
        ],
        "--switched-root": [
            1122085995
        ],
        "--system": [
            1122085995
        ],
        "[kthreadd]\n": [
            1122085995
        ],
        "[ksoftirqd/0]\n": [
            1122085995
        ],
        "[kworker/0:0h]\n": [
            1122085995
        ],
        "[kworker/u2:0]\n": [
            1122085995
        ],
        "[migration/0]\n": [
            1122085995
        ],
        "[rcu_bh]\n": [
            1122085995
        ],
        "[rcu_sched]\n": [
            1122085995
        ],
        "[watchdog/0]\n..\nby": [
            1122085995
        ],
        "also\ncommunicate": [
            1122085995
        ],
        "inter-process\ncommunication\n13.2": [
            1122085995
        ],
        "context\nbesides": [
            1122085995
        ],
        "security-related\nfeatures": [
            1122085995
        ],
        "\nsecurity-\ncontext\n": [
            1122085995
        ],
        "the\nspec": [
            1122085995
        ],
        "containers\nunderstanding": [
            1122085995
        ],
        "context\nconfiguring": [
            1122085995
        ],
        "things:\nspecify": [
            1122085995
        ],
        "id)": [
            1122085995
        ],
        "run\nprevent": [
            1122085995
        ],
        "runs\nas": [
            1122085995
        ],
        "prevent\ncontainers": [
            1122085995
        ],
        "root)\nrun": [
            1122085995
        ],
        "kernel\nconfigure": [
            1122085995
        ],
        "fine-grained": [
            1122085995
        ],
        "capabilities—in": [
            1122085995
        ],
        "privi-\nleged": [
            1122085995
        ],
        "mode\nset": [
            1122085995
        ],
        "selinux": [
            1122085995
        ],
        "(security": [
            1122085995
        ],
        "enhanced": [
            1122085995
        ],
        "linux)": [
            1122085995
        ],
        "a\ncontainer\nprevent": [
            1122085995
        ],
        "filesystem\nwe’ll": [
            1122085995
        ],
        "\n\n381configuring": [
            1122085995
        ],
        "context\nrunning": [
            1122085995
        ],
        "context\nfirst": [
            1122085995
        ],
        "context:\n$": [
            1122085995
        ],
        "pod-with-defaults": [
            1122085995
        ],
        "--image": [
            1122085995
        ],
        "alpine": [
            1122085995
        ],
        "--restart": [
            1122085995
        ],
        "/bin/sleep": [
            1122085995
        ],
        "999999\npod": [
            1122085995
        ],
        "pod-with-defaults\"": [
            1122085995
        ],
        "it\nbelongs": [
            1122085995
        ],
        "\nid": [
            1122085995
        ],
        "id\nuid=0(root)": [
            1122085995
        ],
        "gid=0(root)": [
            1122085995
        ],
        "groups=0(root)": [
            1122085995
        ],
        "1(bin)": [
            1122085995
        ],
        "2(daemon)": [
            1122085995
        ],
        "3(sys)": [
            1122085995
        ],
        "4(adm)": [
            1122085995
        ],
        "\n6(disk)": [
            1122085995
        ],
        "10(wheel)": [
            1122085995
        ],
        "11(floppy)": [
            1122085995
        ],
        "20(dialout)": [
            1122085995
        ],
        "26(tape)": [
            1122085995
        ],
        "27(video)\nthe": [
            1122085995
        ],
        "(uid)": [
            1122085995
        ],
        "(gid)": [
            1122085995
        ],
        "(also\nroot)": [
            1122085995
        ],
        "\nnotewhat": [
            1122085995
        ],
        "\nuser": [
            1122085995
        ],
        "directive": [
            1122085995
        ],
        "root\nnow": [
            1122085995
        ],
        "user\n13.2.1": [
            1122085995
        ],
        "user\nto": [
            1122085995
        ],
        "\nsecuritycontextrunasuser": [
            1122085995
        ],
        "you’ll\nmake": [
            1122085995
        ],
        "\nguest": [
            1122085995
        ],
        "is\n405": [
            1122085995
        ],
        "pod-as-user-guest\nspec:\n": [
            1122085995
        ],
        "999999\"]\n": [
            1122085995
        ],
        "securitycontext:\n": [
            1122085995
        ],
        "runasuser:": [
            1122085995
        ],
        "runasuser": [
            1122085995
        ],
        "pod-as-user-guest": [
            1122085995
        ],
        "id\nuid=405(guest)": [
            1122085995
        ],
        "gid=100(users)\nlisting": [
            1122085995
        ],
        "user:": [
            1122085995
        ],
        "pod-as-user-guestyaml\nyou": [
            1122085995
        ],
        "(id": [
            1122085995
        ],
        "user)\n": [
            1122085995
        ],
        "\n\n382chapter": [
            1122085995
        ],
        "network\nas": [
            1122085995
        ],
        "\n132.2": [
            1122085995
        ],
        "root\nwhat": [
            1122085995
        ],
        "root?": [
            1122085995
        ],
        "\nuser\ndaemon\n": [
            1122085995
        ],
        "daemon\nuser": [
            1122085995
        ],
        "attacker": [
            1122085995
        ],
        "different\nimage": [
            1122085995
        ],
        "tag?": [
            1122085995
        ],
        "attacker’s": [
            1122085995
        ],
        "user\nwhen": [
            1122085995
        ],
        "download\nthe": [
            1122085995
        ],
        "as\nroot": [
            1122085995
        ],
        "non-root\nit": [
            1122085995
        ],
        "pod’s\ncontainer": [
            1122085995
        ],
        "non-root": [
            1122085995
        ],
        "pod-run-as-non-root\nspec:\n": [
            1122085995
        ],
        "securitycontext:": [
            1122085995
        ],
        "runasnonroot:": [
            1122085995
        ],
        "pod-run-as-non-root\nname": [
            1122085995
        ],
        "\npod-run-as-non-root": [
            1122085995
        ],
        "runasnonroot": [
            1122085995
        ],
        "tampers": [
            1122085995
        ],
        "far\n13.2.3": [
            1122085995
        ],
        "mode\nsometimes": [
            1122085995
        ],
        "protected": [
            1122085995
        ],
        "to\nregular": [
            1122085995
        ],
        "root:": [
            1122085995
        ],
        "pod-run-as-non-rootyaml\nthis": [
            1122085995
        ],
        "\nnon-root": [
            1122085995
        ],
        "user\n": [
            1122085995
        ],
        "\n\n383configuring": [
            1122085995
        ],
        "context\n": [
            1122085995
        ],
        "node’s\niptables": [
            1122085995
        ],
        "the\ninstructions": [
            1122085995
        ],
        "cluster\nnode": [
            1122085995
        ],
        "the\nspecial": [
            1122085995
        ],
        "privileged\nmode": [
            1122085995
        ],
        "security-\ncontext\n": [
            1122085995
        ],
        "pod-privileged\nspec:\n": [
            1122085995
        ],
        "privileged:": [
            1122085995
        ],
        "non-privileged": [
            1122085995
        ],
        "you\nran": [
            1122085995
        ],
        "/dev\nwhich": [
            1122085995
        ],
        "on\ndisk": [
            1122085995
        ],
        "are\nvisible": [
            1122085995
        ],
        "\npod-with-defaults\npod)": [
            1122085995
        ],
        "/dev": [
            1122085995
        ],
        "/dev\ncore": [
            1122085995
        ],
        "stderr": [
            1122085995
        ],
        "urandom\nfd": [
            1122085995
        ],
        "ptmx": [
            1122085995
        ],
        "zero\nfull": [
            1122085995
        ],
        "pts": [
            1122085995
        ],
        "stdout\nfuse": [
            1122085995
        ],
        "termination-log\nmqueue": [
            1122085995
        ],
        "shm": [
            1122085995
        ],
        "tty\nthe": [
            1122085995
        ],
        "see\n$": [
            1122085995
        ],
        "pod-privileged": [
            1122085995
        ],
        "/dev\nautofs": [
            1122085995
        ],
        "snd": [
            1122085995
        ],
        "tty46\nbsg": [
            1122085995
        ],
        "sr0": [
            1122085995
        ],
        "tty47\nlisting": [
            1122085995
        ],
        "pod-privilegedyaml\nlisting": [
            1122085995
        ],
        "pod\nthis": [
            1122085995
        ],
        "\nrun": [
            1122085995
        ],
        "\nmode\n": [
            1122085995
        ],
        "\n\n384chapter": [
            1122085995
        ],
        "network\nbtrfs-control": [
            1122085995
        ],
        "tty48\ncore": [
            1122085995
        ],
        "tty49\ncpu": [
            1122085995
        ],
        "tty5\ncpu_dma_latency": [
            1122085995
        ],
        "termination-log": [
            1122085995
        ],
        "tty50\nfd": [
            1122085995
        ],
        "tty51\nfull": [
            1122085995
        ],
        "tty0": [
            1122085995
        ],
        "tty52\nfuse": [
            1122085995
        ],
        "tty1": [
            1122085995
        ],
        "tty53\nhpet": [
            1122085995
        ],
        "tty10": [
            1122085995
        ],
        "tty54\nhwrng": [
            1122085995
        ],
        "tty11": [
            1122085995
        ],
        "tty55\n..": [
            1122085995
        ],
        "..\ni": [
            1122085995
        ],
        "evident\nthat": [
            1122085995
        ],
        "sees\nall": [
            1122085995
        ],
        "raspberry": [
            1122085995
        ],
        "pi": [
            1122085995
        ],
        "leds": [
            1122085995
        ],
        "it\n13.2.4": [
            1122085995
        ],
        "unlimited": [
            1122085995
        ],
        "power": [
            1122085995,
            1118639836
        ],
        "traditional": [
            1122085995
        ],
        "distinguished": [
            1122085995
        ],
        "privileged\nand": [
            1122085995
        ],
        "unprivileged": [
            1122085995
        ],
        "more\nfine-grained": [
            1122085995
        ],
        "capabilities\n": [
            1122085995
        ],
        "a\nmuch": [
            1122085995
        ],
        "perspective)": [
            1122085995
        ],
        "kernel\nfeatures": [
            1122085995
        ],
        "container\nor": [
            1122085995
        ],
        "fine-tune": [
            1122085995
        ],
        "and\nlimit": [
            1122085995
        ],
        "potential": [
            1122085995
        ],
        "intrusion": [
            1122085995
        ],
        "attacker\n": [
            1122085995
        ],
        "clock’s": [
            1122085995
        ],
        "\npod-with-\ndefaults\n": [
            1122085995
        ],
        "+%t": [
            1122085995
        ],
        "12:00:00\"\ndate:": [
            1122085995
        ],
        "cant": [
            1122085995
        ],
        "permitted\nif": [
            1122085995
        ],
        "capabil-\nity": [
            1122085995
        ],
        "\ncap_sys_time": [
            1122085995
        ],
        "pod-add-settime-capability\nspec:\n": [
            1122085995
        ],
        "alpine\nlisting": [
            1122085995
        ],
        "cap_sys_time": [
            1122085995
        ],
        "capability:": [
            1122085995
        ],
        "pod-add-settime-capabilityyaml\n": [
            1122085995
        ],
        "\n\n385configuring": [
            1122085995
        ],
        "capabilities:": [
            1122085995
        ],
        "add:": [
            1122085995
        ],
        "sys_time": [
            1122085995
        ],
        "\nnotelinux": [
            1122085995
        ],
        "cap_": [
            1122085995
        ],
        "when\nspecifying": [
            1122085995
        ],
        "prefix\nif": [
            1122085995
        ],
        "changed\nsuccessfully:\n$": [
            1122085995
        ],
        "pod-add-settime-capability": [
            1122085995
        ],
        "12:00:00\"\n12:00:00\n$": [
            1122085995
        ],
        "date\nsun": [
            1122085995
        ],
        "12:00:03": [
            1122085995
        ],
        "2017\nwarningif": [
            1122085995
        ],
        "unusable": [
            1122085995
        ],
        "(ntp)": [
            1122085995
        ],
        "to\nreboot": [
            1122085995
        ],
        "node\nrunning": [
            1122085995
        ],
        "can\nget": [
            1122085995
        ],
        "12:00:07": [
            1122085995
        ],
        "2017\nadding": [
            1122085995
        ],
        "privileges\nwith": [
            1122085995
        ],
        "\nprivileged:": [
            1122085995
        ],
        "admittedly": [
            1122085995
        ],
        "what\neach": [
            1122085995
        ],
        "capability": [
            1122085995
        ],
        "does\ntipyou’ll": [
            1122085995
        ],
        "pages\n13.2.5": [
            1122085995
        ],
        "container\nyou’ve": [
            1122085995
        ],
        "oth-\nerwise": [
            1122085995
        ],
        "\ncap_chown": [
            1122085995
        ],
        "the\nownership": [
            1122085995
        ],
        "ownership": [
            1122085995
        ],
        "/tmp": [
            1122085995
        ],
        "\npod-with-defaults": [
            1122085995
        ],
        "chown": [
            1122085995
        ],
        "/tmp\n$": [
            1122085995
        ],
        "tmp\ndrwxrwxrwt": [
            1122085995
        ],
        "15:18": [
            1122085995
        ],
        "tmp\ncapabilities": [
            1122085995
        ],
        "dropped": [
            1122085995
        ],
        "securitycontext": [
            1122085995
        ],
        "property\nyou’re": [
            1122085995
        ],
        "\nsys_time": [
            1122085995
        ],
        "capability\n": [
            1122085995
        ],
        "\n\n386chapter": [
            1122085995
        ],
        "network\nto": [
            1122085995
        ],
        "it\nunder": [
            1122085995
        ],
        "\nsecuritycontextcapabilities.drop": [
            1122085995
        ],
        "pod-drop-chown-capability\nspec:\n": [
            1122085995
        ],
        "capabilities:\n": [
            1122085995
        ],
        "drop:": [
            1122085995
        ],
        "owner": [
            1122085995
        ],
        "/tmp\ndirectory": [
            1122085995
        ],
        "pod-drop-chown-capability": [
            1122085995
        ],
        "/tmp\nchown:": [
            1122085995
        ],
        "/tmp:": [
            1122085995
        ],
        "permitted\nyou’re": [
            1122085995
        ],
        "at\none": [
            1122085995
        ],
        "more\n13.2.6": [
            1122085995
        ],
        "filesystem\nyou": [
            1122085995
        ],
        "vulnerability": [
            1122085995
        ],
        "allow-\ning": [
            1122085995
        ],
        "served": [
            1122085995
        ],
        "vul-\nnerability": [
            1122085995
        ],
        "attacks": [
            1122085995
        ],
        "thwarted": [
            1122085995
        ],
        "\nsecuritycontextreadonlyrootfilesystem": [
            1122085995
        ],
        "pod-with-readonly-filesystem\nlisting": [
            1122085995
        ],
        "pod-drop-chown-capabilityyaml\nlisting": [
            1122085995
        ],
        "filesystem:": [
            1122085995
        ],
        "pod-with-readonly-filesystemyaml\nyou’re": [
            1122085995
        ],
        "ownership\n": [
            1122085995
        ],
        "\n\n387configuring": [
            1122085995
        ],
        "context\nspec:\n": [
            1122085995
        ],
        "readonlyrootfilesystem:": [
            1122085995
        ],
        "/volume": [
            1122085995
        ],
        "my-volume\n": [
            1122085995
        ],
        "emptydir:\nwhen": [
            1122085995
        ],
        "fails:\n$": [
            1122085995
        ],
        "pod-with-readonly-filesystem": [
            1122085995
        ],
        "/new-file\ntouch:": [
            1122085995
        ],
        "/new-file:": [
            1122085995
        ],
        "system\non": [
            1122085995
        ],
        "allowed:\n$": [
            1122085995
        ],
        "/volume/newfile\n$": [
            1122085995
        ],
        "/volume/newfile\n-rw-r--r--": [
            1122085995
        ],
        "19:11": [
            1122085995
        ],
        "/mountedvolume/newfile\nas": [
            1122085995
        ],
        "you’ll\nprobably": [
            1122085995
        ],
        "on-disk": [
            1122085995
        ],
        "caches": [
            1122085995
        ],
        "on)\ntipto": [
            1122085995
        ],
        "\nreadonlyrootfilesystem": [
            1122085995
        ],
        "true\nsetting": [
            1122085995
        ],
        "level\nin": [
            1122085995
        ],
        "sev-\neral": [
            1122085995
        ],
        "\npodspec.security-\ncontext\n": [
            1122085995
        ],
        "be\noverridden": [
            1122085995
        ],
        "set\nadditional": [
            1122085995
        ],
        "next\n13.2.7": [
            1122085995
        ],
        "users\nin": [
            1122085995
        ],
        "pod’s\ncontainers": [
            1122085995
        ],
        "full\naccess": [
            1122085995
        ],
        "we\nexplained": [
            1122085995
        ],
        "(per-\nhaps": [
            1122085995
        ],
        "\ncan’t": [
            1122085995
        ],
        "to..\n...but": [
            1122085995
        ],
        "\nallowed": [
            1122085995
        ],
        "becase": [
            1122085995
        ],
        "\n\n388chapter": [
            1122085995
        ],
        "network\nunder": [
            1122085995
        ],
        "supplemental": [
            1122085995
        ],
        "ids\nthey’re": [
            1122085995
        ],
        "properties:\nfsgroup\nsupplementalgroups\nwhat": [
            1122085995
        ],
        "containers\nsharing": [
            1122085995
        ],
        "volume\napiversion:": [
            1122085995
        ],
        "pod-with-shared-volume-fsgroup\nspec:\n": [
            1122085995
        ],
        "fsgroup:": [
            1122085995
        ],
        "supplementalgroups:": [
            1122085995
        ],
        "[666": [
            1122085995
        ],
        "777]": [
            1122085995
        ],
        "shared-volume": [
            1122085995
        ],
        "/volume\n": [
            1122085995
        ],
        "second\n": [
            1122085995
        ],
        "emptydir:\nafter": [
            1122085995
        ],
        "group\nids": [
            1122085995
        ],
        "as:\n$": [
            1122085995
        ],
        "pod-with-shared-volume-fsgroup": [
            1122085995
        ],
        "id\nuid=1111": [
            1122085995
        ],
        "groups=555666,777\nlisting": [
            1122085995
        ],
        "1314fsgroup": [
            1122085995
        ],
        "&": [
            1122085995
        ],
        "pod-with-shared-volume-fsgroupyaml\nthe": [
            1122085995
        ],
        "level\nthe": [
            1122085995
        ],
        "\nruns": [
            1122085995
        ],
        "1111\nboth": [
            1122085995
        ],
        "\nvolume\nthe": [
            1122085995
        ],
        "second\ncontainer\nruns": [
            1122085995
        ],
        "user\nid": [
            1122085995
        ],
        "2222\n": [
            1122085995
        ],
        "\n\n389restricting": [
            1122085995
        ],
        "\n0(root)": [
            1122085995
        ],
        "are\nalso": [
            1122085995
        ],
        "\nfsgroup": [
            1122085995
        ],
        "volume\nwill": [
            1122085995
        ],
        "\n555": [
            1122085995
        ],
        "here:\n/": [
            1122085995
        ],
        "volume\ndrwxrwsrwx": [
            1122085995
        ],
        "12:23": [
            1122085995
        ],
        "id\n1111": [
            1122085995
        ],
        "as)": [
            1122085995
        ],
        "555:\n/": [
            1122085995
        ],
        "/volume/foo\n/": [
            1122085995
        ],
        "/volume\ntotal": [
            1122085995
        ],
        "4\n-rw-r--r--": [
            1122085995
        ],
        "12:25": [
            1122085995
        ],
        "foo\nthis": [
            1122085995
        ],
        "creates\nfiles": [
            1122085995
        ],
        "the\nvolume:\n/": [
            1122085995
        ],
        "/tmp/foo\n/": [
            1122085995
        ],
        "/tmp\ntotal": [
            1122085995
        ],
        "12:41": [
            1122085995
        ],
        "foo\nas": [
            1122085995
        ],
        "used)": [
            1122085995
        ],
        "the\nsupplementalgroups": [
            1122085995
        ],
        "asso-\nciated": [
            1122085995
        ],
        "con-\ntext": [
            1122085995
        ],
        "restrict": [
            1122085995
        ],
        "so\n13.3": [
            1122085995
        ],
        "restricting": [
            1122085995
        ],
        "can\ndo": [
            1122085995
        ],
        "or\nall": [
            1122085995
        ],
        "previously\ndescribed": [
            1122085995
        ],
        "podsecuritypolicy\nresources\n13.3.1": [
            1122085995
        ],
        "resource\npodsecuritypolicy": [
            1122085995
        ],
        "what\nsecurity-related": [
            1122085995
        ],
        "upholding\nthe": [
            1122085995
        ],
        "\n\n390chapter": [
            1122085995
        ],
        "network\npodsecuritypolicy": [
            1122085995
        ],
        "explained\nadmission": [
            1122085995
        ],
        "11)\nnotethe": [
            1122085995
        ],
        "enabled\nin": [
            1122085995
        ],
        "sidebar\nwhen": [
            1122085995
        ],
        "admis-\nsion": [
            1122085995
        ],
        "podsecurity-\npolicies": [
            1122085995
        ],
        "into\netcd;": [
            1122085995
        ],
        "rejected": [
            1122085995
        ],
        "pod\nresource": [
            1122085995
        ],
        "policy\nunderstanding": [
            1122085995
        ],
        "do\na": [
            1122085995
        ],
        "following:\nwhether": [
            1122085995
        ],
        "namespaces\nwhich": [
            1122085995
        ],
        "to\nwhat": [
            1122085995
        ],
        "as\nwhether": [
            1122085995
        ],
        "created\nenabling": [
            1122085995
        ],
        "minikube\ni’m": [
            1122085995
        ],
        "v019.0": [
            1122085995
        ],
        "doesn’t\nenable": [
            1122085995
        ],
        "authorization\nwhich": [
            1122085995
        ],
        "authenticating\nas": [
            1122085995
        ],
        "plugin\nwhere": [
            1122085995
        ],
        "similar)\ncommand": [
            1122085995
        ],
        "using:": [
            1122085995
        ],
        "--extra-config": [
            1122085995
        ],
        "apiserverauthentication.passwordfile.\n➥": [
            1122085995
        ],
        "basicauthfile=/etc/kubernetes/passwd": [
            1122085995
        ],
        "--extra-config=apiserver\n➥": [
            1122085995
        ],
        "authorizationmode=rbac": [
            1122085995
        ],
        "--extra-config=apiservergenericserverrun\n➥": [
            1122085995
        ],
        "optionsadmissioncontrol=namespacelifecyclelimitranger,service\n➥": [
            1122085995
        ],
        "accountpersistentvolumelabel,defaultstorageclass,resourcequota,\n➥": [
            1122085995
        ],
        "defaulttolerationsecondspodsecuritypolicy\nthe": [
            1122085995
        ],
        "<<eof": [
            1122085995
        ],
        "tee": [
            1122085995
        ],
        "/etc/kubernetes/passwd\npasswordalice,1000,basic-user\npassword,bob,2000,privileged-user\neof\nyou’ll": [
            1122085995
        ],
        "in\nchapter13/minikube-with-rbac-and-psp-enabledsh.\n": [
            1122085995
        ],
        "\n\n391restricting": [
            1122085995
        ],
        "pods\nwhich": [
            1122085995
        ],
        "dropped\nwhat": [
            1122085995
        ],
        "use\nwhether": [
            1122085995
        ],
        "not\nwhich": [
            1122085995
        ],
        "as\nwhich": [
            1122085995
        ],
        "use\nif": [
            1122085995
        ],
        "previous\nlist": [
            1122085995
        ],
        "podsecuritypolicy\nthe": [
            1122085995
        ],
        "from\nusing": [
            1122085995
        ],
        "privileged\ncontainers": [
            1122085995
        ],
        "(except": [
            1122085995
        ],
        "10000-11000": [
            1122085995
        ],
        "13000-\n14000)": [
            1122085995
        ],
        "selinux\ngroups": [
            1122085995
        ],
        "as\napiversion:": [
            1122085995
        ],
        "podsecuritypolicy\nmetadata:\n": [
            1122085995
        ],
        "default\nspec:\n": [
            1122085995
        ],
        "hostports:": [
            1122085995
        ],
        "min:": [
            1122085995
        ],
        "max:": [
            1122085995
        ],
        "rule:": [
            1122085995
        ],
        "runasany": [
            1122085995
        ],
        "selinux:": [
            1122085995
        ],
        "*'": [
            1122085995
        ],
        "\nmost": [
            1122085995
        ],
        "if\nyou’ve": [
            1122085995
        ],
        "podsecuritypolicy:": [
            1122085995
        ],
        "pod-security-policyyaml\ncontainers": [
            1122085995
        ],
        "\nhost’s": [
            1122085995
        ],
        "namespace\nthey": [
            1122085995
        ],
        "\n10000": [
            1122085995
        ],
        "(inclusive)": [
            1122085995
        ],
        "14000\ncontainers": [
            1122085995
        ],
        "mode\ncontainers": [
            1122085995
        ],
        "filesystem\ncontainers": [
            1122085995
        ],
        "group\nthey": [
            1122085995
        ],
        "\nselinux": [
            1122085995
        ],
        "want\nall": [
            1122085995
        ],
        "\n\n392chapter": [
            1122085995
        ],
        "used\nearlier": [
            1122085995
        ],
        "example\n$": [
            1122085995
        ],
        "pod-privilegedyaml\nerror": [
            1122085995
        ],
        "(forbidden):": [
            1122085995
        ],
        "pod-privilegedyaml\":\npods": [
            1122085995
        ],
        "pod-privileged\"": [
            1122085995
        ],
        "forbidden:": [
            1122085995
        ],
        "validate": [
            1122085995
        ],
        "\nsecurity": [
            1122085995
        ],
        "policy:": [
            1122085995
        ],
        "[speccontainers[0].securitycontext.privileged:": [
            1122085995
        ],
        "invalid": [
            1122085995
        ],
        "\nvalue:": [
            1122085995
        ],
        "true:": [
            1122085995
        ],
        "allowed]\nlikewise": [
            1122085995
        ],
        "pol-\nicy": [
            1122085995
        ],
        "(containers": [
            1122085995
        ],
        "write\nto": [
            1122085995
        ],
        "volumes)\n13.3.2": [
            1122085995
        ],
        "\npolicies\nthe": [
            1122085995
        ],
        "and\ngroups": [
            1122085995
        ],
        "\nrunasany": [
            1122085995
        ],
        "runas-\nuser\n": [
            1122085995
        ],
        "of\nallowed": [
            1122085995
        ],
        "\nmustrunas": [
            1122085995
        ],
        "mustrunas": [
            1122085995
        ],
        "rule\nlet’s": [
            1122085995
        ],
        "\n2–10": [
            1122085995
        ],
        "20–\n30\n": [
            1122085995
        ],
        "inclusive)": [
            1122085995
        ],
        "resource\n": [
            1122085995
        ],
        "runasuser:\n": [
            1122085995
        ],
        "mustrunas\n": [
            1122085995
        ],
        "ranges:\n": [
            1122085995
        ],
        "fsgroup:\n": [
            1122085995
        ],
        "supplementalgroups:\n": [
            1122085995
        ],
        "as:": [
            1122085995
        ],
        "psp-must-run-asyaml\nadd": [
            1122085995
        ],
        "min": [
            1122085995
        ],
        "max": [
            1122085995
        ],
        "id\nmultiple": [
            1122085995
        ],
        "\nsupported—here": [
            1122085995
        ],
        "2–10": [
            1122085995
        ],
        "20–30": [
            1122085995
        ],
        "(inclusive)\n": [
            1122085995
        ],
        "\n\n393restricting": [
            1122085995
        ],
        "podsecurity-\npolicy": [
            1122085995
        ],
        "psp-must-run-asyaml": [
            1122085995
        ],
        "\nnotechanging": [
            1122085995
        ],
        "enforced": [
            1122085995
        ],
        "pods\ndeploying": [
            1122085995
        ],
        "policy’s": [
            1122085995
        ],
        "range\nif": [
            1122085995
        ],
        "pod-as-user-guestyaml": [
            1122085995
        ],
        "\n405": [
            1122085995
        ],
        "pod-as-user-guestyaml\nerror": [
            1122085995
        ],
        "pod-as-user-guestyaml\"\n:": [
            1122085995
        ],
        "pod-as-user-guest\"": [
            1122085995
        ],
        "[securitycontextrunasuser:": [
            1122085995
        ],
        "405:": [
            1122085995
        ],
        "uid": [
            1122085995
        ],
        "allowed:": [
            1122085995
        ],
        "[{2": [
            1122085995
        ],
        "2}]]\nokay": [
            1122085995
        ],
        "direc-\ntive": [
            1122085995
        ],
        "dockerfile)?\ndeploying": [
            1122085995
        ],
        "out-of-range": [
            1122085995
        ],
        "id\ni’ve": [
            1122085995
        ],
        "the\nbook": [
            1122085995
        ],
        "docker-\nfile": [
            1122085995
        ],
        "/appjs\nuser": [
            1122085995
        ],
        "appjs\"]\ni": [
            1122085995
        ],
        "luksa/kubia-run-as-user-5": [
            1122085995
        ],
        "run-as-5": [
            1122085995
        ],
        "never\npod": [
            1122085995
        ],
        "run-as-5\"": [
            1122085995
        ],
        "created\nunlike": [
            1122085995
        ],
        "container\nlet’s": [
            1122085995
        ],
        "id\nuid=2(bin)": [
            1122085995
        ],
        "gid=2(bin)": [
            1122085995
        ],
        "groups=2(bin)\nas": [
            1122085995
        ],
        "id\nhardcoded": [
            1122085995
        ],
        "image\nlisting": [
            1122085995
        ],
        "directive:": [
            1122085995
        ],
        "kubia-run-as-user-5/dockerfile\ncontainers": [
            1122085995
        ],
        "5\n": [
            1122085995
        ],
        "\n\n394chapter": [
            1122085995
        ],
        "network\nusing": [
            1122085995
        ],
        "mustrunasnonroot": [
            1122085995
        ],
        "field\nfor": [
            1122085995
        ],
        "used:": [
            1122085995
        ],
        "the\nname": [
            1122085995
        ],
        "(zero": [
            1122085995
        ],
        "root\nuser’s": [
            1122085995
        ],
        "explained\nwhy": [
            1122085995
        ],
        "earlier\n13.3.3": [
            1122085995
        ],
        "capabilities\nas": [
            1122085995
        ],
        "a\nmore": [
            1122085995
        ],
        "kernel\ncapabilities": [
            1122085995
        ],
        "can\nor": [
            1122085995
        ],
        "use:\nallowedcapabilities\ndefaultaddcapabilities\nrequireddropcapabilities\nwe’ll": [
            1122085995
        ],
        "fields\nrelated": [
            1122085995
        ],
        "capabilities\napiversion:": [
            1122085995
        ],
        "podsecuritypolicy\nspec:\n": [
            1122085995
        ],
        "allowedcapabilities:": [
            1122085995
        ],
        "defaultaddcapabilities:": [
            1122085995
        ],
        "requireddropcapabilities:": [
            1122085995
        ],
        "sys_admin": [
            1122085995
        ],
        "sys_module": [
            1122085995
        ],
        "..\nnotethe": [
            1122085995
        ],
        "administrative": [
            1122085995
        ],
        "operations\nand": [
            1122085995
        ],
        "\nsys_module": [
            1122085995
        ],
        "loading": [
            1122085995
        ],
        "unloading": [
            1122085995
        ],
        "kernel\nmodules\nspecifying": [
            1122085995
        ],
        "allowedcapabilities": [
            1122085995
        ],
        "authors": [
            1122085995
        ],
        "can\nadd": [
            1122085995
        ],
        "\nsecuritycontextcapabilities": [
            1122085995
        ],
        "pod-\nsecuritypolicy": [
            1122085995
        ],
        "been\nable": [
            1122085995
        ],
        "1318.\nlisting": [
            1122085995
        ],
        "psp-capabilitiesyaml\nallow": [
            1122085995
        ],
        "\nadd": [
            1122085995
        ],
        "\ncapability\nautomatically": [
            1122085995
        ],
        "\ncapability": [
            1122085995
        ],
        "container\nrequire": [
            1122085995
        ],
        "\ndrop": [
            1122085995
        ],
        "\n\n395restricting": [
            1122085995
        ],
        "pods\nadding": [
            1122085995
        ],
        "defaultaddcapabilities": [
            1122085995
        ],
        "to\nevery": [
            1122085995
        ],
        "have\nthose": [
            1122085995
        ],
        "capa-\nbility": [
            1122085995
        ],
        "\nchown": [
            1122085995
        ],
        "example)\ndropping": [
            1122085995
        ],
        "requireddropcapabilities": [
            1122085995
        ],
        "a\nsomewhat": [
            1122085995
        ],
        "capabilities\nlisted": [
            1122085995
        ],
        "\nsecurity-\ncontextcapabilities.drop\n": [
            1122085995
        ],
        "listed\nin": [
            1122085995
        ],
        "\nrequireddropcapabilities": [
            1122085995
        ],
        "rejected:\n$": [
            1122085995
        ],
        "pod-add-sysadmin-capabilityyaml\nerror": [
            1122085995
        ],
        "pod-add-sysadmin-\ncapabilityyaml\":": [
            1122085995
        ],
        "pod-add-sysadmin-capability\"": [
            1122085995
        ],
        "[capabilitiesadd:": [
            1122085995
        ],
        "sys_admin\":": [
            1122085995
        ],
        "added]\n133.4": [
            1122085995
        ],
        "users\ncan": [
            1122085995
        ],
        "persistentvolume-\nclaim\n": [
            1122085995
        ],
        "emptydir\n": [
            1122085995
        ],
        "downwardapi\n": [
            1122085995
        ],
        "persistentvolumeclaim\nif": [
            1122085995
        ],
        "type\ndefined": [
            1122085995
        ],
        "union": [
            1122085995,
            186247402
        ],
        "used)\nlisting": [
            1122085995
        ],
        "psp": [
            1122085995
        ],
        "\npsp-volumesyaml\n": [
            1122085995
        ],
        "\n\n396chapter": [
            1122085995
        ],
        "network\n133.5": [
            1122085995
        ],
        "groups\nwe": [
            1122085995
        ],
        "it\ncan’t": [
            1122085995
        ],
        "applied": [
            1122085995
        ],
        "always\napplies": [
            1122085995
        ],
        "unus-\nable": [
            1122085995
        ],
        "pods\nshouldn’t\n": [
            1122085995
        ],
        "mecha-\nnism": [
            1122085995
        ],
        "clusterrole\nresources": [
            1122085995
        ],
        "those\nclusterroles": [
            1122085995
        ],
        "additional\npodsecuritypolicy\ncreating": [
            1122085995
        ],
        "deployed\nyou’ll": [
            1122085995
        ],
        "runasany\n": [
            1122085995
        ],
        "selinux:\n": [
            1122085995
        ],
        "*'\nafter": [
            1122085995
        ],
        "psp\nname": [
            1122085995
        ],
        "priv": [
            1122085995
        ],
        "..\nprivileged": [
            1122085995
        ],
        "psp\nlisting": [
            1122085995
        ],
        "users:": [
            1122085995
        ],
        "psp-privilegedyaml\nthe": [
            1122085995
        ],
        "privileged”\nit": [
            1122085995
        ],
        "\n\n397restricting": [
            1122085995
        ],
        "currently\nlogged": [
            1122085995
        ],
        "any\npolicy": [
            1122085995
        ],
        "accept\nyour": [
            1122085995
        ],
        "alice": [
            1122085995
        ],
        "bob": [
            1122085995
        ],
        "want\nalice": [
            1122085995
        ],
        "restricted": [
            1122085995
        ],
        "(non-privileged)": [
            1122085995
        ],
        "to\nalso": [
            1122085995
        ],
        "default\npodsecuritypolicy": [
            1122085995
        ],
        "both\nusing": [
            1122085995
        ],
        "ref-\nerencing": [
            1122085995
        ],
        "policies\nyou’ll": [
            1122085995
        ],
        "\npsp-default": [
            1122085995
        ],
        "psp-default": [
            1122085995
        ],
        "--verb=use": [
            1122085995
        ],
        "--resource=podsecuritypolicies": [
            1122085995
        ],
        "--resource-name=default\nclusterrole": [
            1122085995
        ],
        "psp-default\"": [
            1122085995
        ],
        "created\nnoteyou’re": [
            1122085995
        ],
        "similar\nas": [
            1122085995
        ],
        "\n--resource-name": [
            1122085995
        ],
        "psp-\nprivileged\n": [
            1122085995
        ],
        "policy:\n$": [
            1122085995
        ],
        "psp-privileged": [
            1122085995
        ],
        "--verb=use\n➥": [
            1122085995
        ],
        "--resource-name=privileged\nclusterrole": [
            1122085995
        ],
        "psp-privileged\"": [
            1122085995
        ],
        "pre-\nvious": [
            1122085995
        ],
        "are)": [
            1122085995
        ],
        "(namespaced)": [
            1122085995
        ],
        "place\nauthenticated": [
            1122085995
        ],
        "group:\n$": [
            1122085995
        ],
        "psp-all-users": [
            1122085995
        ],
        "--clusterrole=psp-default": [
            1122085995
        ],
        "--group=system:authenticated\nclusterrolebinding": [
            1122085995
        ],
        "psp-all-users\"": [
            1122085995
        ],
        "\n\n398chapter": [
            1122085995
        ],
        "bob:\n$": [
            1122085995
        ],
        "psp-bob": [
            1122085995
        ],
        "--clusterrole=psp-privileged": [
            1122085995
        ],
        "--user=bob\nclusterrolebinding": [
            1122085995
        ],
        "psp-bob\"": [
            1122085995
        ],
        "pod-\nsecuritypolicies": [
            1122085995
        ],
        "bob\nshould": [
            1122085995
        ],
        "true\ncreating": [
            1122085995
        ],
        "kubectl\nbut": [
            1122085995
        ],
        "authenticated\nas": [
            1122085995
        ],
        "currently?": [
            1122085995
        ],
        "multiple\nclusters": [
            1122085995
        ],
        "bare\ncommands": [
            1122085995
        ],
        "\nkubectl’s": [
            1122085995
        ],
        "two\ncommands:\n$": [
            1122085995
        ],
        "set-credentials": [
            1122085995
        ],
        "--username=alice": [
            1122085995
        ],
        "--password=password\nuser": [
            1122085995
        ],
        "alice\"": [
            1122085995
        ],
        "set\n$": [
            1122085995
        ],
        "--username=bob": [
            1122085995
        ],
        "bob\"": [
            1122085995
        ],
        "set\nit": [
            1122085995
        ],
        "and\npassword": [
            1122085995
        ],
        "users\n(other": [
            1122085995
        ],
        "on)\ncreating": [
            1122085995
        ],
        "user\nyou": [
            1122085995
        ],
        "--user": [
            1122085995
        ],
        "pod-privilegedyaml\":": [
            1122085995
        ],
        "allowed]\nas": [
            1122085995
        ],
        "pod-privilegedyaml\npod": [
            1122085995
        ],
        "\n\n399isolating": [
            1122085995
        ],
        "network\nand": [
            1122085995
        ],
        "control\nplugin": [
            1122085995
        ],
        "users\n13.4": [
            1122085995
        ],
        "network\nup": [
            1122085995
        ],
        "options\nthat": [
            1122085995
        ],
        "remainder": [
            1122085995
        ],
        "chapter\nwe’ll": [
            1122085995
        ],
        "can\ntalk": [
            1122085995
        ],
        "networking\nplugin": [
            1122085995
        ],
        "configure\nnetwork": [
            1122085995
        ],
        "networkpolicy": [
            1122085995
        ],
        "either\nwhich": [
            1122085995
        ],
        "destinations": [
            1122085995
        ],
        "accessed\nfrom": [
            1122085995
        ],
        "egress": [
            1122085995
        ],
        "respec-\ntively": [
            1122085995
        ],
        "ip\nblock": [
            1122085995
        ],
        "classless": [
            1122085995
        ],
        "inter-domain": [
            1122085995
        ],
        "(cidr)": [
            1122085995
        ],
        "example\n192168.1.0/24).": [
            1122085995
        ],
        "options\nnoteingress": [
            1122085995
        ],
        "ingress\nresource": [
            1122085995
        ],
        "5\n13.4.1": [
            1122085995
        ],
        "namespace\nby": [
            1122085995
        ],
        "\ndefault-deny": [
            1122085995
        ],
        "all\nclients": [
            1122085995
        ],
        "networkingk8s.io/v1\nkind:": [
            1122085995
        ],
        "networkpolicy\nmetadata:\n": [
            1122085995
        ],
        "default-deny\nspec:\n": [
            1122085995
        ],
        "podselector:": [
            1122085995
        ],
        "default-deny": [
            1122085995
        ],
        "networkpolicy:": [
            1122085995
        ],
        "network-policy-default-denyyaml\nempty": [
            1122085995
        ],
        "\n\n400chapter": [
            1122085995
        ],
        "network\nnotethe": [
            1122085995
        ],
        "inter-pod\nconnectivity\n13.4.2": [
            1122085995
        ],
        "who\ncan": [
            1122085995
        ],
        "this\nthrough": [
            1122085995
        ],
        "postgresql": [
            1122085995
        ],
        "web-\nserver": [
            1122085995
        ],
        "postgres-netpolicy\nspec:\n": [
            1122085995
        ],
        "from:": [
            1122085995
        ],
        "webserver": [
            1122085995
        ],
        "app=webserver": [
            1122085995
        ],
        "\napp=database": [
            1122085995
        ],
        "anything\nother": [
            1122085995
        ],
        "134.\n": [
            1122085995
        ],
        "con-\nnecting": [
            1122085995
        ],
        "well\n": [
            1122085995
        ],
        "postgres": [
            1122085995
        ],
        "network-policy-postgresyaml\nthis": [
            1122085995
        ],
        "secures": [
            1122085995
        ],
        "\naccess": [
            1122085995
        ],
        "label\nit": [
            1122085995
        ],
        "\napp=webserver": [
            1122085995
        ],
        "label\nconnections": [
            1122085995
        ],
        "allowed\n": [
            1122085995
        ],
        "\n\n401isolating": [
            1122085995
        ],
        "network\n134.3": [
            1122085995
        ],
        "namespaces\nnow": [
            1122085995
        ],
        "tenants": [
            1122085995
        ],
        "tenant": [
            1122085995
        ],
        "a\nlabel": [
            1122085995
        ],
        "man-\nning": [
            1122085995
        ],
        "\ntenant:": [
            1122085995
        ],
        "their\nnamespaces": [
            1122085995
        ],
        "shopping": [
            1122085995
        ],
        "cart": [
            1122085995
        ],
        "tenants\nto": [
            1122085995
        ],
        "shoppingcart-netpolicy\nspec:\n": [
            1122085995
        ],
        "shopping-cart": [
            1122085995
        ],
        "ingress:\n": [
            1122085995
        ],
        "from:\n": [
            1122085995
        ],
        "namespaceselector:": [
            1122085995
        ],
        "tenant:": [
            1122085995
        ],
        "pod(s):": [
            1122085995
        ],
        "network-policy-cartyaml\napp:": [
            1122085995
        ],
        "database\npod:\ndatabase\nport\n5432\nport\n9876\napp:": [
            1122085995
        ],
        "webserver\npod:\nwebserver\npod": [
            1122085995
        ],
        "selector:\napp=webserver\npod": [
            1122085995
        ],
        "selector:\napp=database\napp:": [
            1122085995
        ],
        "webserver\npod:\nwebserver\nother": [
            1122085995
        ],
        "pods\nnetworkpolicy:": [
            1122085995
        ],
        "postgres-netpolicy\nfigure": [
            1122085995
        ],
        "\nport\nthis": [
            1122085995
        ],
        "\nlabeled": [
            1122085995
        ],
        "microservice=": [
            1122085995
        ],
        "\nshopping-cart\nonly": [
            1122085995
        ],
        "tenant=manning": [
            1122085995
        ],
        "\n\n402chapter": [
            1122085995
        ],
        "network\nthis": [
            1122085995
        ],
        "tenant:\nmanning\n": [
            1122085995
        ],
        "135.\nif": [
            1122085995
        ],
        "(perhaps": [
            1122085995
        ],
        "partner": [
            1122085995
        ],
        "companies)": [
            1122085995
        ],
        "networkpolicy\nresource": [
            1122085995
        ],
        "networkpolicy\nnotein": [
            1122085995
        ],
        "labels\n(or": [
            1122085995
        ],
        "able\nto": [
            1122085995
        ],
        "circumvent": [
            1122085995
        ],
        "\nnamespaceselector-based": [
            1122085995
        ],
        "rules\n13.4.4": [
            1122085995
        ],
        "notation\ninstead": [
            1122085995
        ],
        "pod-": [
            1122085995
        ],
        "pods\ntargeted": [
            1122085995
        ],
        "\nshopping-cart": [
            1122085995
        ],
        "acces-\nsible": [
            1122085995
        ],
        "192168.1.1": [
            1122085995
        ],
        "ipblock:": [
            1122085995
        ],
        "cidr:": [
            1122085995
        ],
        "192168.1.0/24": [
            1122085995
        ],
        "network-policy-cidryaml\napp:": [
            1122085995
        ],
        "shopping-cart\npod:\nshopping-cart\nport\n80\nnamespace": [
            1122085995
        ],
        "selector:\ntenant=manning\npod": [
            1122085995
        ],
        "selector:\napp=shopping-cart\nother": [
            1122085995
        ],
        "pods\npods\nnetworkpolicy:\nshoppingcart-netpolicy\nnamespace:": [
            1122085995
        ],
        "manninga\nnamespace:": [
            1122085995
        ],
        "ecommerce-ltd\nother": [
            1122085995
        ],
        "namespaces\ntenant:": [
            1122085995
        ],
        "manning\npods\nnamespace:": [
            1122085995
        ],
        "manningb\ntenant:": [
            1122085995
        ],
        "manning\nfigure": [
            1122085995
        ],
        "namespaceselector": [
            1122085995
        ],
        "\nclients": [
            1122085995
        ],
        "\n\n403summary\n134.5": [
            1122085995
        ],
        "inbound": [
            1122085995
        ],
        "that\nmatch": [
            1122085995
        ],
        "networkpolicy’s": [
            1122085995
        ],
        "limit\ntheir": [
            1122085995
        ],
        "egress:": [
            1122085995
        ],
        "app=webserver\nlabel": [
            1122085995
        ],
        "app=database": [
            1122085995
        ],
        "(neither\nother": [
            1122085995
        ],
        "the\ncluster)\n13.5": [
            1122085995
        ],
        "summary\nin": [
            1122085995
        ],
        "from\nother": [
            1122085995
        ],
        "that\npods": [
            1122085995
        ],
        "own\ncontainers": [
            1122085995
        ],
        "image\ncontainers": [
            1122085995
        ],
        "node’s\ndevices": [
            1122085995
        ],
        "pods\ncontainers": [
            1122085995
        ],
        "volumes)\ncluster-level": [
            1122085995
        ],
        "from\ncreating": [
            1122085995
        ],
        "compromise": [
            1122085995
        ],
        "node\npodsecuritypolicy": [
            1122085995
        ],
        "rbac’s\nclusterroles": [
            1122085995
        ],
        "clusterrolebindings\nnetworkpolicy": [
            1122085995
        ],
        "outbound\ntraffic\nin": [
            1122085995
        ],
        "be\nconstrained": [
            1122085995
        ],
        "network-policy-egressyaml\nthis": [
            1122085995
        ],
        "limits\nthe": [
            1122085995
        ],
        "pods’\noutbound\ntraffic\nwebserver": [
            1122085995
        ],
        "\nconnect": [
            1122085995
        ],
        "\n\n404\nmanaging": [
            1122085995
        ],
        "pods’\n": [
            1122085995
        ],
        "resources\nup": [
            1122085995
        ],
        "memory\nthey’re": [
            1122085995
        ],
        "how\nmuch": [
            1122085995
        ],
        "con-\nsume": [
            1122085995
        ],
        "parameters\nmakes": [
            1122085995
        ],
        "covers\nrequesting": [
            1122085995
        ],
        "\ncomputational": [
            1122085995
        ],
        "memory\nunderstanding": [
            1122085995
        ],
        "\npods\nsetting": [
            1122085995
        ],
        "namespace\nlimiting": [
            1122085995
        ],
        "\n\n405requesting": [
            1122085995
        ],
        "containers\n141": [
            1122085995
        ],
        "containers\nwhen": [
            1122085995
        ],
        "consume\n(known": [
            1122085995
        ],
        "limits)": [
            1122085995
        ],
        "lim-\nits": [
            1122085995
        ],
        "\n141.1": [
            1122085995
        ],
        "requests\nlet’s": [
            1122085995
        ],
        "spec-\nified": [
            1122085995
        ],
        "requests-pod\nspec:\n": [
            1122085995
        ],
        "[dd\"": [
            1122085995
        ],
        "if=/dev/zero\"": [
            1122085995
        ],
        "of=/dev/null\"]\n": [
            1122085995
        ],
        "200m": [
            1122085995
        ],
        "10mi": [
            1122085995
        ],
        "one-fifth": [
            1122085995
        ],
        "(200": [
            1122085995
        ],
        "mil-\nlicores)": [
            1122085995
        ],
        "pods/containers": [
            1122085995
        ],
        "sufficiently": [
            1122085995
        ],
        "single\ncpu": [
            1122085995
        ],
        "much\ncpu": [
            1122085995
        ],
        "may\nnot": [
            1122085995
        ],
        "demand": [
            1122085995
        ],
        "processes\nexists": [
            1122085995
        ],
        "cpu)": [
            1122085995
        ],
        "low-priority": [
            1122085995
        ],
        "aren’t\ntime-critical": [
            1122085995
        ],
        "requests\n": [
            1122085995
        ],
        "mebibytes": [
            1122085995
        ],
        "container\nby": [
            1122085995
        ],
        "expect-\ning": [
            1122085995
        ],
        "do\n": [
            1122085995
        ],
        "pro-\ncess’": [
            1122085995
        ],
        "requests-podyaml\nyou’re": [
            1122085995
        ],
        "\nmillicores": [
            1122085995
        ],
        "1/5": [
            1122085995
        ],
        "\nsingle": [
            1122085995
        ],
        "core’s": [
            1122085995
        ],
        "time)\nthe": [
            1122085995
        ],
        "also\nrequests": [
            1122085995
        ],
        "mebibytes\nof": [
            1122085995
        ],
        "memory\n": [
            1122085995
        ],
        "\n\n406chapter": [
            1122085995
        ],
        "14managing": [
            1122085995
        ],
        "resources\n$": [
            1122085995
        ],
        "requests-pod": [
            1122085995
        ],
        "top\nmem:": [
            1122085995
        ],
        "1288116k": [
            1122085995
        ],
        "760368k": [
            1122085995
        ],
        "9196k": [
            1122085995
        ],
        "shrd": [
            1122085995
        ],
        "25748k": [
            1122085995
        ],
        "buff": [
            1122085995
        ],
        "814840k": [
            1122085995
        ],
        "cached\ncpu:": [
            1122085995
        ],
        "91%": [
            1122085995
        ],
        "usr": [
            1122085995
        ],
        "421%": [
            1122085995
        ],
        "00%": [
            1122085995
        ],
        "nic": [
            1122085995
        ],
        "484%": [
            1122085995
        ],
        "idle": [
            1122085995
        ],
        "io": [
            1122085995
        ],
        "irq": [
            1122085995
        ],
        "02%": [
            1122085995
        ],
        "sirq\nload": [
            1122085995
        ],
        "average:": [
            1122085995
        ],
        "079": [
            1122085995
        ],
        "052": [
            1122085995
        ],
        "029": [
            1122085995
        ],
        "2/481": [
            1122085995
        ],
        "10\n": [
            1122085995
        ],
        "ppid": [
            1122085995
        ],
        "%vsz": [
            1122085995
        ],
        "r": [
            1122085995
        ],
        "dd": [
            1122085995
        ],
        "/dev/zero": [
            1122085995
        ],
        "/dev/null\n": [
            1122085995
        ],
        "top\nthe": [
            1122085995
        ],
        "can\nbut": [
            1122085995
        ],
        "thread": [
            1122085995
        ],
        "vm\nwhich": [
            1122085995
        ],
        "cores": [
            1122085995
        ],
        "why\nthe": [
            1122085995
        ],
        "fifty": [
            1122085995
        ],
        "container\nis": [
            1122085995
        ],
        "millicores": [
            1122085995
        ],
        "is\nexpected": [
            1122085995
        ],
        "you’d\nneed": [
            1122085995
        ],
        "spec-\nifying": [
            1122085995
        ],
        "pod\n14.1.2": [
            1122085995
        ],
        "scheduling\nby": [
            1122085995
        ],
        "resources\nyour": [
            1122085995
        ],
        "allocate": [
            1122085995
        ],
        "enough\nunallocated": [
            1122085995
        ],
        "meet": [
            1122085995
        ],
        "of\nunallocated": [
            1122085995
        ],
        "not\nschedule": [
            1122085995
        ],
        "amount\nrequired": [
            1122085995
        ],
        "pod\nunderstanding": [
            1122085995
        ],
        "node\nwhat’s": [
            1122085995
        ],
        "surprising": [
            1122085995
        ],
        "scheduling\nbut": [
            1122085995
        ],
        "node\neven": [
            1122085995
        ],
        "scheduling\nanother": [
            1122085995
        ],
        "given\nto": [
            1122085995
        ],
        "141.": [
            1122085995
        ],
        "together\nthey’ve": [
            1122085995
        ],
        "80%": [
            1122085995
        ],
        "60%": [
            1122085995
        ],
        "d\nshown": [
            1122085995
        ],
        "it\nrequests": [
            1122085995
        ],
        "20%": [
            1122085995
        ],
        "fact\nthat": [
            1122085995
        ],
        "70%": [
            1122085995
        ],
        "difference\nlisting": [
            1122085995
        ],
        "\n\n407requesting": [
            1122085995
        ],
        "to\nexclude": [
            1122085995
        ],
        "prioritizes": [
            1122085995
        ],
        "the\nconfigured": [
            1122085995
        ],
        "prioritization": [
            1122085995
        ],
        "rank\nnodes": [
            1122085995
        ],
        "requested:": [
            1122085995
        ],
        "\nleastrequestedpriority": [
            1122085995
        ],
        "and\nmostrequestedpriority": [
            1122085995
        ],
        "prefers": [
            1122085995
        ],
        "fewer": [
            1122085995
        ],
        "resources\n(with": [
            1122085995
        ],
        "exact\nopposite—it": [
            1122085995
        ],
        "amount\nof": [
            1122085995
        ],
        "consumed\n": [
            1122085995
        ],
        "wonder\nwhy": [
            1122085995
        ],
        "\nmostrequestedpriority": [
            1122085995
        ],
        "however\nthat’s": [
            1122085995
        ],
        "and\nremove": [
            1122085995
        ],
        "whenever": [
            1122085995
        ],
        "\nmost-\nrequestedpriority\n": [
            1122085995
        ],
        "pos-\nsible": [
            1122085995
        ],
        "cpu/memory\nit": [
            1122085995
        ],
        "packed": [
            1122085995
        ],
        "vacant": [
            1122085995
        ],
        "be\nremoved": [
            1122085995
        ],
        "money\ninspecting": [
            1122085995
        ],
        "capacity\nlet’s": [
            1122085995
        ],
        "the\namount": [
            1122085995
        ],
        "node’s\ncapacity": [
            1122085995
        ],
        "through\npod": [
            1122085995
        ],
        "c\nnode\npod": [
            1122085995
        ],
        "aunallocatedcpu": [
            1122085995
        ],
        "requestspod": [
            1122085995
        ],
        "acurrently": [
            1122085995
        ],
        "unusedcpu": [
            1122085995
        ],
        "usagepod": [
            1122085995
        ],
        "bpod": [
            1122085995
        ],
        "c\n0%100%\npod": [
            1122085995
        ],
        "amemory": [
            1122085995
        ],
        "c\ncpu": [
            1122085995
        ],
        "requests\nmemory": [
            1122085995
        ],
        "requests\nunallocated\ncurrently": [
            1122085995
        ],
        "unused\npod": [
            1122085995
        ],
        "d\npod": [
            1122085995
        ],
        "d": [
            1122085995
        ],
        "scheduled;": [
            1122085995
        ],
        "cpu\nrequests": [
            1122085995
        ],
        "cpu\nfigure": [
            1122085995
        ],
        "usage\n": [
            1122085995
        ],
        "\n\n408chapter": [
            1122085995
        ],
        "nodes\nname:": [
            1122085995
        ],
        "minikube\n..\ncapacity:": [
            1122085995
        ],
        "2048484ki": [
            1122085995
        ],
        "\nallocatable:": [
            1122085995
        ],
        "1946084ki": [
            1122085995
        ],
        "\n..\nthe": [
            1122085995
        ],
        "amounts": [
            1122085995
        ],
        "node:\nthe": [
            1122085995
        ],
        "allocatable": [
            1122085995
        ],
        "resources\nof": [
            1122085995
        ],
        "reserved\nfor": [
            1122085995
        ],
        "bases": [
            1122085995
        ],
        "decisions": [
            1122085995
        ],
        "amounts\n": [
            1122085995
        ],
        "cores\nand": [
            1122085995
        ],
        "therefore\nthe": [
            1122085995
        ],
        "800\nmillicores": [
            1122085995
        ],
        "requests-pod-2": [
            1122085995
        ],
        "--image=busybox": [
            1122085995
        ],
        "never\n➥": [
            1122085995
        ],
        "--requests=cpu=800mmemory=20mi'": [
            1122085995
        ],
        "if=/dev/zero": [
            1122085995
        ],
        "of=/dev/null\npod": [
            1122085995
        ],
        "requests-pod-2\"": [
            1122085995
        ],
        "scheduled:\n$": [
            1122085995
        ],
        "requests-pod-2\nname": [
            1122085995
        ],
        "age\nrequests-pod-2": [
            1122085995
        ],
        "3m\nokay": [
            1122085995
        ],
        "node\nyou": [
            1122085995
        ],
        "mil-\nlicores": [
            1122085995
        ],
        "available\nfor": [
            1122085995
        ],
        "of\n1000": [
            1122085995
        ],
        "requests-pod-3": [
            1122085995
        ],
        "--requests=cpu=1memory=20mi'": [
            1122085995
        ],
        "overall": [
            1122085995
        ],
        "\nallocatable": [
            1122085995
        ],
        "\n\n409requesting": [
            1122085995
        ],
        "containers\nnotethis": [
            1122085995
        ],
        "(cpu=1)\ninstead": [
            1122085995
        ],
        "(\ncpu=1000m)\nso": [
            1122085995
        ],
        "way)\nnow": [
            1122085995
        ],
        "requests-pod-3\nname": [
            1122085995
        ],
        "age\nrequests-pod-3": [
            1122085995
        ],
        "4m\neven": [
            1122085995
        ],
        "requests-pod-3\nname:": [
            1122085995
        ],
        "requests-pod-3\nnamespace:": [
            1122085995
        ],
        "\n..\nconditions:\n": [
            1122085995
        ],
        "podscheduled": [
            1122085995
        ],
        "\n..\nevents:\n...": [
            1122085995
        ],
        "failedscheduling": [
            1122085995
        ],
        "predicates::": [
            1122085995
        ],
        "insufficient": [
            1122085995
        ],
        "(1)": [
            1122085995
        ],
        "node\ndue": [
            1122085995
        ],
        "that?": [
            1122085995
        ],
        "exactly\nwhat": [
            1122085995
        ],
        "wrong?\ndetermining": [
            1122085995
        ],
        "scheduled\nyou": [
            1122085995
        ],
        "resource\nuse": [
            1122085995
        ],
        "more\nclosely": [
            1122085995
        ],
        "node\nname:": [
            1122085995
        ],
        "minikube\n..\nnon-terminated": [
            1122085995
        ],
        "(7": [
            1122085995
        ],
        "total)\n": [
            1122085995
        ],
        "requ": [
            1122085995
        ],
        "lim": [
            1122085995
        ],
        "mem": [
            1122085995
        ],
        "lim\n": [
            1122085995
        ],
        "---------": [
            1122085995
        ],
        "----------": [
            1122085995
        ],
        "--------": [
            1122085995
        ],
        "--------\n": [
            1122085995
        ],
        "(10%)": [
            1122085995
        ],
        "(0%)": [
            1122085995
        ],
        "(0%)\nlisting": [
            1122085995
        ],
        "node\nno": [
            1122085995
        ],
        "\nassociated": [
            1122085995
        ],
        "\nbeen": [
            1122085995
        ],
        "scheduled\nscheduling": [
            1122085995
        ],
        "\nfailed": [
            1122085995
        ],
        "\ninsufficient": [
            1122085995
        ],
        "cpu\n": [
            1122085995
        ],
        "\n\n410chapter": [
            1122085995
        ],
        "800m": [
            1122085995
        ],
        "(40%)": [
            1122085995
        ],
        "20mi": [
            1122085995
        ],
        "(1%)": [
            1122085995
        ],
        "(0%)\n": [
            1122085995
        ],
        "dflt-http-b..": [
            1122085995
        ],
        "(1%)\n": [
            1122085995
        ],
        "kube-addon-..": [
            1122085995
        ],
        "5m": [
            1122085995
        ],
        "50mi": [
            1122085995
        ],
        "(2%)": [
            1122085995
        ],
        "kube-dns-26..": [
            1122085995
        ],
        "260m": [
            1122085995
        ],
        "(13%)": [
            1122085995
        ],
        "110mi": [
            1122085995
        ],
        "(5%)": [
            1122085995
        ],
        "170mi": [
            1122085995
        ],
        "(8%)\n": [
            1122085995
        ],
        "kubernetes-..": [
            1122085995
        ],
        "nginx-ingre..": [
            1122085995
        ],
        "(0%)\nallocated": [
            1122085995
        ],
        "(total": [
            1122085995
        ],
        "ie.": [
            1122085995
        ],
        "overcommitted)\n": [
            1122085995
        ],
        "limits\n": [
            1122085995
        ],
        "------------": [
            1122085995
        ],
        "---------------": [
            1122085995
        ],
        "-------------\n": [
            1122085995
        ],
        "1275m": [
            1122085995
        ],
        "(63%)": [
            1122085995
        ],
        "210mi": [
            1122085995
        ],
        "(11%)": [
            1122085995
        ],
        "190mi": [
            1122085995
        ],
        "(9%)\nif": [
            1122085995
        ],
        "have\nbeen": [
            1122085995
        ],
        "you\nrequested": [
            1122085995
        ],
        "eating": [
            1122085995
        ],
        "additional\ncpu": [
            1122085995
        ],
        "culprit": [
            1122085995
        ],
        "plus\nyour": [
            1122085995
        ],
        "your\nthird": [
            1122085995
        ],
        "as\nthat": [
            1122085995
        ],
        "overcommitted": [
            1122085995
        ],
        "\nfreeing": [
            1122085995
        ],
        "scheduled\nthe": [
            1122085995
        ],
        "freed": [
            1122085995
        ],
        "terminates\nthis": [
            1122085995
        ],
        "requests-pod-2\npod": [
            1122085995
        ],
        "age\nrequests-pod": [
            1122085995
        ],
        "2h\nrequests-pod-2": [
            1122085995
        ],
        "1h\nrequests-pod-3": [
            1122085995
        ],
        "1h\n$": [
            1122085995
        ],
        "2h\nrequests-pod-3": [
            1122085995
        ],
        "1h\nin": [
            1122085995
        ],
        "played": [
            1122085995
        ],
        "any\nrole": [
            1122085995
        ],
        "requests\nalso": [
            1122085995
        ],
        "elsewhere—while": [
            1122085995
        ],
        "next\nlisting": [
            1122085995
        ],
        "\n\n411requesting": [
            1122085995
        ],
        "containers\n141.3": [
            1122085995
        ],
        "sharing\nyou": [
            1122085995
        ],
        "disregard": [
            1122085995
        ],
        "pods\nright": [
            1122085995
        ],
        "idle)": [
            1122085995
        ],
        "dis-\ntinguishes": [
            1122085995
        ],
        "so\nthe": [
            1122085995
        ],
        "each\nconsume": [
            1122085995
        ],
        "get?": [
            1122085995
        ],
        "scheduling—they": [
            1122085995
        ],
        "(unused)": [
            1122085995
        ],
        "pod\nrequested": [
            1122085995
        ],
        "cpu\nwill": [
            1122085995
        ],
        "ratio": [
            1122085995
        ],
        "142.": [
            1122085995
        ],
        "pods\nconsume": [
            1122085995
        ],
        "sixth": [
            1122085995
        ],
        "167%": [
            1122085995
        ],
        "the\ncpu": [
            1122085995
        ],
        "sixths": [
            1122085995
        ],
        "833%.\nbut": [
            1122085995
        ],
        "sit-\nting": [
            1122085995
        ],
        "(minus": [
            1122085995
        ],
        "any)": [
            1122085995
        ],
        "all\nit": [
            1122085995
        ],
        "throt-\ntled": [
            1122085995
        ],
        "back\n14.1.4": [
            1122085995
        ],
        "resources\nkubernetes": [
            1122085995
        ],
        "request\nthem": [
            1122085995
        ],
        "opaque": [
            1122085995
        ],
        "integer\nresources": [
            1122085995
        ],
        "18.\npod": [
            1122085995
        ],
        "a:\n200": [
            1122085995
        ],
        "m\ncpu\nrequests\npod": [
            1122085995
        ],
        "b:": [
            1122085995
        ],
        "m800": [
            1122085995
        ],
        "m": [
            1122085995
        ],
        "available\ncpu\nusage\n2000": [
            1122085995
        ],
        "m1000": [
            1122085995
        ],
        "m0": [
            1122085995
        ],
        "m\npod": [
            1122085995
        ],
        "requests\nare": [
            1122085995
        ],
        "1:5": [
            1122085995
        ],
        "ratio\navailable": [
            1122085995
        ],
        "is\ndistributed": [
            1122085995
        ],
        "ratio\npod": [
            1122085995
        ],
        "m\n133": [
            1122085995
        ],
        "m\n(1/6)\n667": [
            1122085995
        ],
        "m\n(5/6)\npod": [
            1122085995
        ],
        "a:\n333": [
            1122085995
        ],
        "m\nfigure": [
            1122085995
        ],
        "\n\n412chapter": [
            1122085995
        ],
        "by\nadding": [
            1122085995
        ],
        "\ncapacity": [
            1122085995
        ],
        "a\npatch": [
            1122085995
        ],
        "exampleorg/my-\nresource\n": [
            1122085995
        ],
        "kubernetesio": [
            1122085995
        ],
        "quantity\nmust": [
            1122085995
        ],
        "integer": [
            1122085995
        ],
        "millis": [
            1122085995
        ],
        "inte-\nger;": [
            1122085995
        ],
        "1000m": [
            1122085995
        ],
        "2000m": [
            1122085995
        ],
        "2)": [
            1122085995
        ],
        "copied\nfrom": [
            1122085995
        ],
        "requested\nquantity": [
            1122085995
        ],
        "\nresourcesrequests": [
            1122085995
        ],
        "--requests\nwhen": [
            1122085995
        ],
        "custom\nresource": [
            1122085995
        ],
        "reduces": [
            1122085995
        ],
        "allocatable\nunits": [
            1122085995
        ],
        "units": [
            1122085995
        ],
        "then\nmakes": [
            1122085995
        ],
        "unallocated\n14.2": [
            1122085995
        ],
        "container\nsetting": [
            1122085995
        ],
        "min-\nimum": [
            1122085995
        ],
        "coin—the\nmaximum": [
            1122085995
        ],
        "\n142.1": [
            1122085995
        ],
        "use\nwe’ve": [
            1122085995
        ],
        "processes\nare": [
            1122085995
        ],
        "sitting": [
            1122085995
        ],
        "memory\na": [
            1122085995
        ],
        "compressible": [
            1122085995
        ],
        "throttled": [
            1122085995
        ],
        "adverse": [
            1122085995
        ],
        "way\nmemory": [
            1122085995
        ],
        "different—it’s": [
            1122085995
        ],
        "incompressible": [
            1122085995
        ],
        "chunk": [
            1122085995
        ],
        "of\nmemory": [
            1122085995
        ],
        "process\nitself": [
            1122085995
        ],
        "may\neat": [
            1122085995
        ],
        "node\nbased": [
            1122085995
        ],
        "usage)": [
            1122085995
        ],
        "malfunction-\ning": [
            1122085995
        ],
        "unusable\ncreating": [
            1122085995
        ],
        "limits\nto": [
            1122085995
        ],
        "for\nevery": [
            1122085995
        ],
        "(along": [
            1122085995
        ],
        "\n\n413limiting": [
            1122085995
        ],
        "container\napiversion:": [
            1122085995
        ],
        "limited-pod\nspec:\n": [
            1122085995
        ],
        "limits:": [
            1122085995
        ],
        "consume\nmore": [
            1122085995
        ],
        "\nnotebecause": [
            1122085995
        ],
        "limits\novercommitting": [
            1122085995
        ],
        "limits\nunlike": [
            1122085995
        ],
        "allocatable\nresource": [
            1122085995
        ],
        "exceed\n100%": [
            1122085995
        ],
        "143).": [
            1122085995
        ],
        "restated": [
            1122085995
        ],
        "overcom-\nmitted": [
            1122085995
        ],
        "consequence—when": [
            1122085995
        ],
        "killed\nyou’ll": [
            1122085995
        ],
        "limits\nspecify": [
            1122085995
        ],
        "limited-podyaml\nspecifying": [
            1122085995
        ],
        "\nlimits": [
            1122085995
        ],
        "container\nthis": [
            1122085995
        ],
        "core\nthe": [
            1122085995
        ],
        "be\nallowed": [
            1122085995
        ],
        "20\nmebibytes": [
            1122085995
        ],
        "memory\nnode\n0%136%100%\npod": [
            1122085995
        ],
        "limitspod": [
            1122085995
        ],
        "b\nunallocated\npod": [
            1122085995
        ],
        "\ncapacity\n": [
            1122085995
        ],
        "\n\n414chapter": [
            1122085995
        ],
        "resources\n142.2": [
            1122085995
        ],
        "exceeding": [
            1122085995
        ],
        "limits\nwhat": [
            1122085995
        ],
        "of\nresources": [
            1122085995
        ],
        "to?": [
            1122085995
        ],
        "natural\nfor": [
            1122085995
        ],
        "i/o\noperation": [
            1122085995
        ],
        "cpu\nlimit": [
            1122085995
        ],
        "its\nlimit": [
            1122085995
        ],
        "\noomkilled": [
            1122085995
        ],
        "oom": [
            1122085995
        ],
        "stands\nfor": [
            1122085995
        ],
        "\nalways": [
            1122085995
        ],
        "it\nkeeps": [
            1122085995
        ],
        "begin": [
            1122085995
        ],
        "delays": [
            1122085995
        ],
        "\ncrashloopbackoff": [
            1122085995
        ],
        "status\nin": [
            1122085995
        ],
        "age\nmemoryhog": [
            1122085995
        ],
        "crashloopbackoff": [
            1122085995
        ],
        "that\nafter": [
            1122085995
        ],
        "crashes\nagain": [
            1122085995
        ],
        "this\ndelay": [
            1122085995
        ],
        "exponentially": [
            1122085995
        ],
        "lim-\nited": [
            1122085995
        ],
        "hits": [
            1122085995
        ],
        "300-second": [
            1122085995
        ],
        "keeps\nrestarting": [
            1122085995
        ],
        "stops\ncrashing": [
            1122085995
        ],
        "pod\nname:": [
            1122085995
        ],
        "memoryhog\n..\ncontainers:\n": [
            1122085995
        ],
        "main:\n": [
            1122085995
        ],
        "oomkilled": [
            1122085995
        ],
        "137\n": [
            1122085995
        ],
        "dec": [
            1122085995
        ],
        "14:55:53": [
            1122085995
        ],
        "+0100\n": [
            1122085995
        ],
        "14:55:58": [
            1122085995
        ],
        "137\nlisting": [
            1122085995
        ],
        "\nkilled": [
            1122085995
        ],
        "(oom)\nthe": [
            1122085995
        ],
        "\nwas": [
            1122085995
        ],
        "\nit": [
            1122085995
        ],
        "oom\n": [
            1122085995
        ],
        "\n\n415limiting": [
            1122085995
        ],
        "14:55:37": [
            1122085995
        ],
        "14:55:50": [
            1122085995
        ],
        "false\n..\nthe": [
            1122085995
        ],
        "killed\nimmediately": [
            1122085995
        ],
        "143.2": [
            1122085995
        ],
        "off-\nguard": [
            1122085995
        ],
        "containers\n14.2.3": [
            1122085995
        ],
        "limits\nif": [
            1122085995
        ],
        "limited-podyaml\npod": [
            1122085995
        ],
        "limited-pod\"": [
            1122085995
        ],
        "the\nchapter": [
            1122085995
        ],
        "limited-pod": [
            1122085995
        ],
        "1450980k": [
            1122085995
        ],
        "597504k": [
            1122085995
        ],
        "22012k": [
            1122085995
        ],
        "65876k": [
            1122085995
        ],
        "857552k": [
            1122085995
        ],
        "400%": [
            1122085995
        ],
        "500%": [
            1122085995
        ],
        "017": [
            1122085995
        ],
        "4/503": [
            1122085995
        ],
        "top\nfirst": [
            1122085995
        ],
        "limit\nis": [
            1122085995
        ],
        "mib": [
            1122085995
        ],
        "strikes": [
            1122085995
        ],
        "odd?\n": [
            1122085995
        ],
        "nowhere": [
            1122085995
        ],
        "near\nthe": [
            1122085995
        ],
        "\ndd": [
            1122085995
        ],
        "on?\nunderstanding": [
            1122085995
        ],
        "container’s\nthe": [
            1122085995
        ],
        "is\nrunning": [
            1122085995
        ],
        "cpu-": [
            1122085995
        ],
        "memory-limited": [
            1122085995
        ],
        "\n\n416chapter": [
            1122085995
        ],
        "unfortunate": [
            1122085995
        ],
        "reserve": [
            1122085995
        ],
        "the\nmaximum": [
            1122085995
        ],
        "heap": [
            1122085995
        ],
        "\n-xmx": [
            1122085995
        ],
        "case\nthe": [
            1122085995
        ],
        "jvm": [
            1122085995
        ],
        "laptop": [
            1122085995
        ],
        "the\ndifference": [
            1122085995
        ],
        "solves": [
            1122085995
        ],
        "wrong\nunfortunately": [
            1122085995
        ],
        "constrains": [
            1122085995
        ],
        "jvm’s": [
            1122085995
        ],
        "off-heap": [
            1122085995
        ],
        "alleviate": [
            1122085995
        ],
        "tak-\ning": [
            1122085995
        ],
        "account\nunderstanding": [
            1122085995
        ],
        "cores\nexactly": [
            1122085995
        ],
        "doesn’t\nmagically": [
            1122085995
        ],
        "is\nconstrain": [
            1122085995
        ],
        "one-core": [
            1122085995
        ],
        "64-core": [
            1122085995
        ],
        "1/64th\nof": [
            1122085995
        ],
        "container’s\nprocesses": [
            1122085995
        ],
        "be\nexecuted": [
            1122085995
        ],
        "cores\n": [
            1122085995
        ],
        "sce-\nnario": [
            1122085995
        ],
        "catastrophic\n": [
            1122085995
        ],
        "threads": [
            1122085995
        ],
        "it’s\ngoing": [
            1122085995
        ],
        "competing": [
            1122085995
        ],
        "(possibly)": [
            1122085995
        ],
        "time\nalso": [
            1122085995
        ],
        "sky-\nrocket": [
            1122085995
        ],
        "and\nuse": [
            1122085995
        ],
        "relying": [
            1122085995
        ],
        "tap": [
            1122085995
        ],
        "files:\n/sys/fs/cgroup/cpu/cpucfs_quota_us\n/sys/fs/cgroup/cpu/cpu.cfs_period_us\n": [
            1122085995
        ],
        "\n\n417understanding": [
            1122085995
        ],
        "classes\n143": [
            1122085995
        ],
        "classes\nwe’ve": [
            1122085995
        ],
        "their\nresource": [
            1122085995
        ],
        "90%": [
            1122085995
        ],
        "suddenly": [
            1122085995
        ],
        "up\nto": [
            1122085995
        ],
        "which\ncontainer": [
            1122085995
        ],
        "killed?": [
            1122085995
        ],
        "can’t\nbe": [
            1122085995
        ],
        "satisfied": [
            1122085995
        ],
        "to\npod": [
            1122085995
        ],
        "b?": [
            1122085995
        ],
        "by\ncategorizing": [
            1122085995
        ],
        "(qos)": [
            1122085995
        ],
        "classes:\nbesteffort": [
            1122085995
        ],
        "priority)\nburstable\nguaranteed": [
            1122085995
        ],
        "highest)\n143.1": [
            1122085995
        ],
        "assignable": [
            1122085995
        ],
        "the\nmanifest": [
            1122085995
        ],
        "derived": [
            1122085995
        ],
        "resource\nrequests": [
            1122085995
        ],
        "how\nassigning": [
            1122085995
        ],
        "besteffort": [
            1122085995
        ],
        "class\nthe": [
            1122085995
        ],
        "class\nthat": [
            1122085995
        ],
        "whatsoever": [
            1122085995
        ],
        "worst\ncase": [
            1122085995
        ],
        "when\nmemory": [
            1122085995
        ],
        "\nbesteffort": [
            1122085995
        ],
        "no\nmemory": [
            1122085995
        ],
        "enough\nmemory": [
            1122085995
        ],
        "available\nassigning": [
            1122085995
        ],
        "class\non": [
            1122085995
        ],
        "containers’": [
            1122085995
        ],
        "pod’s\nclass": [
            1122085995
        ],
        "\nguaranteed": [
            1122085995
        ],
        "true:\nrequests": [
            1122085995
        ],
        "memory\nthey": [
            1122085995
        ],
        "container\nthey": [
            1122085995
        ],
        "resource\nin": [
            1122085995
        ],
        "container)\nbecause": [
            1122085995
        ],
        "limits\nspecifying": [
            1122085995
        ],
        "\n\n418chapter": [
            1122085995
        ],
        "higher\nthan": [
            1122085995
        ],
        "\nassigning": [
            1122085995
        ],
        "burstable": [
            1122085995
        ],
        "pods\nfall": [
            1122085995
        ],
        "limits\ndon’t": [
            1122085995
        ],
        "resource\nrequest": [
            1122085995
        ],
        "container’s\nrequests": [
            1122085995
        ],
        "specified\nburstable": [
            1122085995
        ],
        "limit)": [
            1122085995
        ],
        "needed\nunderstanding": [
            1122085995
        ],
        "class\nall": [
            1122085995
        ],
        "144.\nthinking": [
            1122085995
        ],
        "involves\nmultiple": [
            1122085995
        ],
        "between\nrequests": [
            1122085995
        ],
        "level\n(although": [
            1122085995
        ],
        "\nfiguring": [
            1122085995
        ],
        "class\ntable": [
            1122085995
        ],
        "are\ndefined": [
            1122085995
        ],
        "\nbesteffort\nqos\nrequestslimits\nburstable\nqos\nrequests\nlimits\nguaranteed\nqos\nrequestslimits\nrequests": [
            1122085995
        ],
        "and\nlimits": [
            1122085995
        ],
        "set\nrequests": [
            1122085995
        ],
        "are\nbelow": [
            1122085995
        ],
        "limits\nrequests\nequal": [
            1122085995
        ],
        "limits\nfigure": [
            1122085995
        ],
        "classes\n": [
            1122085995
        ],
        "\n\n419understanding": [
            1122085995
        ],
        "classes\nnoteif": [
            1122085995
        ],
        "where\nrequests": [
            1122085995
        ],
        "the\nlimits": [
            1122085995
        ],
        "limits\nfiguring": [
            1122085995
        ],
        "containers\nfor": [
            1122085995
        ],
        "is\nburstable": [
            1122085995
        ],
        "two-\ncontainer": [
            1122085995
        ],
        "easily\nextend": [
            1122085995
        ],
        "containers\nnotea": [
            1122085995
        ],
        "\nstatusqosclass": [
            1122085995
        ],
        "field\nwe’ve": [
            1122085995
        ],
        "determined": [
            1122085995
        ],
        "they\ndetermine": [
            1122085995
        ],
        "system\ntable": [
            1122085995
        ],
        "limits\ncpu": [
            1122085995
        ],
        "limitsmemory": [
            1122085995
        ],
        "limitscontainer": [
            1122085995
        ],
        "class\nnone": [
            1122085995
        ],
        "setnone": [
            1122085995
        ],
        "setbesteffort\nnone": [
            1122085995
        ],
        "setrequests": [
            1122085995
        ],
        "<": [
            1122085995
        ],
        "limitsburstable\nnone": [
            1122085995
        ],
        "limitsburstable\nrequests": [
            1122085995
        ],
        "limitsnone": [
            1122085995
        ],
        "setburstable\nrequests": [
            1122085995
        ],
        "limitsrequests": [
            1122085995
        ],
        "limitsguaranteed\ntable": [
            1122085995
        ],
        "classcontainer": [
            1122085995
        ],
        "classpod’s": [
            1122085995
        ],
        "class\nbesteffort": [
            1122085995
        ],
        "besteffort\nbesteffort": [
            1122085995
        ],
        "burstable\nbesteffort": [
            1122085995
        ],
        "burstable\nburstable": [
            1122085995
        ],
        "burstable\nguaranteed": [
            1122085995
        ],
        "guaranteed\n": [
            1122085995
        ],
        "\n\n420chapter": [
            1122085995
        ],
        "resources\n143.2": [
            1122085995
        ],
        "low\nwhen": [
            1122085995
        ],
        "gets\nkilled": [
            1122085995
        ],
        "finally\nguaranteed": [
            1122085995
        ],
        "memory\nunderstanding": [
            1122085995
        ],
        "up\nlet’s": [
            1122085995
        ],
        "145.": [
            1122085995
        ],
        "single-container\npods": [
            1122085995
        ],
        "maxed": [
            1122085995
        ],
        "to\nhonor": [
            1122085995
        ],
        "allocation": [
            1122085995
        ],
        "pod\nwill": [
            1122085995
        ],
        "\nburstable": [
            1122085995
        ],
        "pod\nobviously": [
            1122085995
        ],
        "pods’\nprocesses": [
            1122085995
        ],
        "that\nof": [
            1122085995
        ],
        "clearly\nthe": [
            1122085995
        ],
        "other\nunderstanding": [
            1122085995
        ],
        "handled\neach": [
            1122085995
        ],
        "outofmemory": [
            1122085995
        ],
        "(oom)": [
            1122085995
        ],
        "scores": [
            1122085995
        ],
        "memory\nneeds": [
            1122085995
        ],
        "killed\n": [
            1122085995
        ],
        "calculated": [
            1122085995
        ],
        "things:": [
            1122085995
        ],
        "adjustment": [
            1122085995
        ],
        "pods\nexist": [
            1122085995
        ],
        "requested\nbesteffort\nqos": [
            1122085995
        ],
        "a\nfirst": [
            1122085995
        ],
        "line\nto": [
            1122085995
        ],
        "killed\nactual": [
            1122085995
        ],
        "usage\nrequestslimits\nburstable\nqos": [
            1122085995
        ],
        "b\nsecond": [
            1122085995
        ],
        "killed\n90%": [
            1122085995
        ],
        "usedrequestslimits\nburstable\nqos": [
            1122085995
        ],
        "c\nthird": [
            1122085995
        ],
        "killed\n70%": [
            1122085995
        ],
        "usedrequestslimits\nguaranteed\nqos": [
            1122085995
        ],
        "d\nlast": [
            1122085995
        ],
        "killed\n99%": [
            1122085995
        ],
        "usedrequestslimits\nfigure": [
            1122085995
        ],
        "\n\n421setting": [
            1122085995
        ],
        "namespace\nmemory": [
            1122085995
        ],
        "percentage-wise": [
            1122085995
        ],
        "90%\nof": [
            1122085995
        ],
        "megabytes": [
            1122085995
        ],
        "mindful": [
            1122085995
        ],
        "requests\nand": [
            1122085995
        ],
        "\n144": [
            1122085995
        ],
        "\nnamespace\nwe’ve": [
            1122085995
        ],
        "mercy": [
            1122085995
        ],
        "that\ndo": [
            1122085995
        ],
        "on\nevery": [
            1122085995
        ],
        "container\n14.4.1": [
            1122085995
        ],
        "limit-\nrange": [
            1122085995
        ],
        "minimum\nand": [
            1122085995
        ],
        "default\nresource": [
            1122085995
        ],
        "146.\napi": [
            1122085995
        ],
        "server\nvalidation\npod": [
            1122085995
        ],
        "a\nmanifest\n-": [
            1122085995
        ],
        "requests\n-": [
            1122085995
        ],
        "limits\npod": [
            1122085995
        ],
        "b\nmanifest\n-": [
            1122085995
        ],
        "no\nrequests\nor": [
            1122085995
        ],
        "limits\ndefaulting\nrejected": [
            1122085995
        ],
        "because\nrequests": [
            1122085995
        ],
        "are\noutside": [
            1122085995
        ],
        "min/max": [
            1122085995
        ],
        "values\ndefaults\napplied\nnamespace": [
            1122085995
        ],
        "xyz\nlimitrange\npod": [
            1122085995
        ],
        "default\nrequests\n-": [
            1122085995
        ],
        "default\nlimits\npod": [
            1122085995
        ],
        "b\n-": [
            1122085995
        ],
        "limits\n-": [
            1122085995
        ],
        "cpu\n-": [
            1122085995
        ],
        "memory\n-": [
            1122085995
        ],
        "defaulting": [
            1122085995
        ],
        "\n\n422chapter": [
            1122085995
        ],
        "resources\nlimitrange": [
            1122085995
        ],
        "limitranger": [
            1122085995
        ],
        "(we\nexplained": [
            1122085995
        ],
        "use-case": [
            1122085995
        ],
        "is\nto": [
            1122085995
        ],
        "gladly": [
            1122085995
        ],
        "never\nschedule": [
            1122085995
        ],
        "pod/con-\ntainer": [
            1122085995
        ],
        "limitrange\nobject": [
            1122085995
        ],
        "\n144.2": [
            1122085995
        ],
        "object\nlet’s": [
            1122085995
        ],
        "do\nthe": [
            1122085995
        ],
        "resource\napiversion:": [
            1122085995
        ],
        "limitrange\nmetadata:\n": [
            1122085995
        ],
        "50m": [
            1122085995
        ],
        "5mi": [
            1122085995
        ],
        "defaultrequest:": [
            1122085995
        ],
        "default:": [
            1122085995
        ],
        "maxlimitrequestratio:": [
            1122085995
        ],
        "limitsyaml\nspecifies": [
            1122085995
        ],
        "whole\nminimum": [
            1122085995
        ],
        "total\nmaximum": [
            1122085995
        ],
        "limit)\nthe\ncontainer\nlimits": [
            1122085995
        ],
        "are\nspecified\nbelow": [
            1122085995
        ],
        "this\nline\ndefault": [
            1122085995
        ],
        "explicitly\ndefault": [
            1122085995
        ],
        "them\nminimum": [
            1122085995
        ],
        "\nrequests/limits": [
            1122085995
        ],
        "have\nmaximum": [
            1122085995
        ],
        "\n\n423setting": [
            1122085995
        ],
        "10gi": [
            1122085995
        ],
        "containers’\nrequests": [
            1122085995
        ],
        "(\ndefaultrequest)": [
            1122085995
        ],
        "limits\n(\ndefault)": [
            1122085995
        ],
        "of\nlimits": [
            1122085995
        ],
        "\nmaxlimitrequestratio": [
            1122085995
        ],
        "4\nwhich": [
            1122085995
        ],
        "times\ngreater": [
            1122085995
        ],
        "be\naccepted": [
            1122085995
        ],
        "maximum\nratio": [
            1122085995
        ],
        "claim\na": [
            1122085995
        ],
        "of\ncpu": [
            1122085995
        ],
        "single\npvc": [
            1122085995
        ],
        "example\n": [
            1122085995
        ],
        "everything\nbut": [
            1122085995
        ],
        "orga-\nnized": [
            1122085995
        ],
        "for\npvcs": [
            1122085995
        ],
        "consolidated\nwhen": [
            1122085995
        ],
        "pvc\n": [
            1122085995
        ],
        "defaults)": [
            1122085995
        ],
        "modify\nthe": [
            1122085995
        ],
        "afterwards": [
            1122085995
        ],
        "revalidated—the": [
            1122085995
        ],
        "limits\nwill": [
            1122085995
        ],
        "\n144.3": [
            1122085995
        ],
        "enforcing": [
            1122085995
        ],
        "limits\nwith": [
            1122085995
        ],
        "than\nallowed": [
            1122085995
        ],
        "discussion\n": [
            1122085995
        ],
        "2\nlisting": [
            1122085995
        ],
        "limit:": [
            1122085995
        ],
        "limits-pod-too-bigyaml\na": [
            1122085995
        ],
        "\namount": [
            1122085995
        ],
        "request\n": [
            1122085995
        ],
        "\n\n424chapter": [
            1122085995
        ],
        "maximum\nyou": [
            1122085995
        ],
        "yields": [
            1122085995
        ],
        "result:\n$": [
            1122085995
        ],
        "limits-pod-too-bigyaml": [
            1122085995
        ],
        "\nerror": [
            1122085995
        ],
        "limits-pod-too-bigyaml\":": [
            1122085995
        ],
        "too-big\"": [
            1122085995
        ],
        "2]\ni’ve": [
            1122085995
        ],
        "legible": [
            1122085995
        ],
        "the\nerror": [
            1122085995
        ],
        "rejected\nnot": [
            1122085995
        ],
        "encountered": [
            1122085995
        ],
        "rea-\nsons:": [
            1122085995
        ],
        "maximum\nis": [
            1122085995
        ],
        "container\nrequested": [
            1122085995
        ],
        "to\nrequest": [
            1122085995
        ],
        "\n144.4": [
            1122085995
        ],
        "limits\nnow": [
            1122085995
        ],
        "that\ndon’t": [
            1122085995
        ],
        "./chapter03/kubia-manual.yaml\npod": [
            1122085995
        ],
        "created\nbefore": [
            1122085995
        ],
        "any\nresource": [
            1122085995
        ],
        "kubia-manual\nname:": [
            1122085995
        ],
        "kubia-manual\n..\ncontainers:\n": [
            1122085995
        ],
        "200m\n": [
            1122085995
        ],
        "10mi\nthe": [
            1122085995
        ],
        "this\nallows": [
            1122085995
        ],
        "\n\n425limiting": [
            1122085995
        ],
        "namespace\nif": [
            1122085995
        ],
        "qa\nstaging,": [
            1122085995
        ],
        "certain\nnamespaces": [
            1122085995
        ],
        "individual\npod/container": [
            1122085995
        ],
        "eat": [
            1122085995
        ],
        "limitranges": [
            1122085995
        ],
        "protection": [
            1122085995
        ],
        "resource-\nquota": [
            1122085995
        ],
        "next\n14.5": [
            1122085995
        ],
        "namespace\nas": [
            1122085995
        ],
        "also\nneed": [
            1122085995
        ],
        "is\nachieved": [
            1122085995
        ],
        "\n145.1": [
            1122085995
        ],
        "enforces": [
            1122085995
        ],
        "resources\nsimilarly": [
            1122085995
        ],
        "pod\nbeing": [
            1122085995
        ],
        "pod\ncreation": [
            1122085995
        ],
        "created—creating": [
            1122085995
        ],
        "also\nlimit": [
            1122085995
        ],
        "create\ninside": [
            1122085995
        ],
        "dealt": [
            1122085995
        ],
        "let’s\nstart": [
            1122085995
        ],
        "them\ncreating": [
            1122085995
        ],
        "memory\nthe": [
            1122085995
        ],
        "is\ndefined": [
            1122085995
        ],
        "resourcequota\nmetadata:\n": [
            1122085995
        ],
        "cpu-and-mem\nspec:\n": [
            1122085995
        ],
        "hard:\n": [
            1122085995
        ],
        "requestscpu:": [
            1122085995
        ],
        "400m\n": [
            1122085995
        ],
        "requestsmemory:": [
            1122085995
        ],
        "200mi\n": [
            1122085995
        ],
        "limitscpu:": [
            1122085995
        ],
        "600m\n": [
            1122085995
        ],
        "limitsmemory:": [
            1122085995
        ],
        "500mi\nlisting": [
            1122085995
        ],
        "quota-cpu-memoryyaml\n": [
            1122085995
        ],
        "\n\n426chapter": [
            1122085995
        ],
        "totals": [
            1122085995
        ],
        "for\nrequests": [
            1122085995
        ],
        "namespace\ncan": [
            1122085995
        ],
        "are\nset": [
            1122085995
        ],
        "mib\nwhereas": [
            1122085995
        ],
        "mib\n": [
            1122085995
        ],
        "147.\ninspecting": [
            1122085995
        ],
        "usage\nafter": [
            1122085995
        ],
        "kubectl\ndescribe\n": [
            1122085995
        ],
        "quota\nname:": [
            1122085995
        ],
        "cpu-and-mem\nnamespace:": [
            1122085995
        ],
        "hard\n--------": [
            1122085995
        ],
        "----\nlimitscpu": [
            1122085995
        ],
        "600m\nlimitsmemory": [
            1122085995
        ],
        "500mi\nrequestscpu": [
            1122085995
        ],
        "400m\nrequestsmemory": [
            1122085995
        ],
        "200mi\ni": [
            1122085995
        ],
        "amounts\nlisting": [
            1122085995
        ],
        "quota\nlimitrangeresourcequota\nnamespace:": [
            1122085995
        ],
        "foo\npod": [
            1122085995
        ],
        "c\nlimitrangeresourcequota\nnamespace:": [
            1122085995
        ],
        "dpod": [
            1122085995
        ],
        "epod": [
            1122085995
        ],
        "f\nfigure": [
            1122085995
        ],
        "\nnamespace\n": [
            1122085995
        ],
        "\n\n427limiting": [
            1122085995
        ],
        "namespace\ncreating": [
            1122085995
        ],
        "resourcequota\none": [
            1122085995
        ],
        "happen\nin": [
            1122085995
        ],
        "./chapter03/kubia-manual.yaml\nerror": [
            1122085995
        ],
        "./chapter03/kubia-\nmanual.yaml\":": [
            1122085995
        ],
        "quota:": [
            1122085995
        ],
        "cpu-and-\nmem:": [
            1122085995
        ],
        "limitscpulimits.memory,requests.cpu,requests.memory\nwhen": [
            1122085995
        ],
        "(request": [
            1122085995
        ],
        "or\nlimit)": [
            1122085995
        ],
        "(respectively)": [
            1122085995
        ],
        "resource;\notherwise": [
            1122085995
        ],
        "with\ndefaults": [
            1122085995
        ],
        "pods\n14.5.2": [
            1122085995
        ],
        "storage\na": [
            1122085995
        ],
        "be\nclaimed": [
            1122085995
        ],
        "storage\nspec:\n": [
            1122085995
        ],
        "requestsstorage:": [
            1122085995
        ],
        "500gi": [
            1122085995
        ],
        "ssdstorageclass.storage.k8s.io/requests.storage:": [
            1122085995
        ],
        "300gi": [
            1122085995
        ],
        "standardstorageclass.storage.k8s.io/requests.storage:": [
            1122085995
        ],
        "1ti\nin": [
            1122085995
        ],
        "\nrequestsstorage": [
            1122085995
        ],
        "object)": [
            1122085995
        ],
        "can\nrequest": [
            1122085995
        ],
        "storageclass\nindividually": [
            1122085995
        ],
        "storage\n(designated": [
            1122085995
        ],
        "\nssd": [
            1122085995
        ],
        "storageclass)": [
            1122085995
        ],
        "less-performant": [
            1122085995
        ],
        "hdd": [
            1122085995
        ],
        "storage\n(storageclass": [
            1122085995
        ],
        "standard)": [
            1122085995
        ],
        "tib\n14.5.3": [
            1122085995
        ],
        "created\na": [
            1122085995
        ],
        "payment\nlisting": [
            1122085995
        ],
        "quota-storageyaml\nthe": [
            1122085995
        ],
        "\noverall\nthe": [
            1122085995
        ],
        "\n\n428chapter": [
            1122085995
        ],
        "resources\nplan": [
            1122085995
        ],
        "objects\nspec:\n": [
            1122085995
        ],
        "replicationcontrollers:": [
            1122085995
        ],
        "configmaps:": [
            1122085995
        ],
        "persistentvolumeclaims:": [
            1122085995
        ],
        "services:": [
            1122085995
        ],
        "servicesloadbalancers:": [
            1122085995
        ],
        "servicesnodeports:": [
            1122085995
        ],
        "ssdstorageclass.storage.k8s.io/persistentvolumeclaims:": [
            1122085995
        ],
        "to\nfive": [
            1122085995
        ],
        "\nloadbal-\nancer\n-type": [
            1122085995
        ],
        "of\npersistentvolumeclaims": [
            1122085995
        ],
        "storageclass\n": [
            1122085995
        ],
        "objects:": [
            1122085995
        ],
        "\npods\nreplicationcontrollers": [
            1122085995
        ],
        "\nsecrets\nconfigmaps\npersistentvolumeclaims\nservices": [
            1122085995
        ],
        "load-\nbalancer\n": [
            1122085995
        ],
        "(servicesloadbalancers)": [
            1122085995
        ],
        "(ser-\nvicesnodeports\n)": [
            1122085995
        ],
        "\nfinally": [
            1122085995
        ],
        "themselves\nthe": [
            1122085995
        ],
        "published\nso": [
            1122085995
        ],
        "information)\nlisting": [
            1122085995
        ],
        "quota-object-countyaml\nonly": [
            1122085995
        ],
        "\n10": [
            1122085995
        ],
        "\n4": [
            1122085995
        ],
        "namespace\nfive": [
            1122085995
        ],
        "services\nonly": [
            1122085995
        ],
        "storage\nwith": [
            1122085995
        ],
        "\n\n429limiting": [
            1122085995
        ],
        "namespace\n145.4": [
            1122085995
        ],
        "current\nstate": [
            1122085995
        ],
        "scopes": [
            1122085995
        ],
        "are\ncurrently": [
            1122085995
        ],
        "available:": [
            1122085995
        ],
        "notbesteffort": [
            1122085995
        ],
        "notterminating": [
            1122085995
        ],
        "applies\nto": [
            1122085995
        ],
        "guaranteed)": [
            1122085995
        ],
        "(\nterminating": [
            1122085995
        ],
        "notterminating)": [
            1122085995
        ],
        "aren’t)": [
            1122085995
        ],
        "to\nbelieve": [
            1122085995
        ],
        "setting\nthe": [
            1122085995
        ],
        "it’s\nmarked": [
            1122085995
        ],
        "what\na": [
            1122085995
        ],
        "quota’s": [
            1122085995
        ],
        "pods\ncpu/memory": [
            1122085995
        ],
        "notterminating\npods": [
            1122085995
        ],
        "besteffort-notterminating-pods\nspec:\n": [
            1122085995
        ],
        "scopes:": [
            1122085995
        ],
        "hard:": [
            1122085995
        ],
        "class\nwhich": [
            1122085995
        ],
        "\nnotbesteffort": [
            1122085995
        ],
        "pods\ninstead": [
            1122085995
        ],
        "\nrequestscpu": [
            1122085995
        ],
        "requestsmemory": [
            1122085995
        ],
        "limitscpu": [
            1122085995
        ],
        "and\nlimitsmemory.\nnotebefore": [
            1122085995
        ],
        "won’t\nlisting": [
            1122085995
        ],
        "besteffort/notterminating": [
            1122085995
        ],
        "\nquota-scopedyaml\nthis": [
            1122085995
        ],
        "set\nonly": [
            1122085995
        ],
        "\n\n430chapter": [
            1122085995
        ],
        "resources\nneed": [
            1122085995
        ],
        "following\nchapters\n14.6": [
            1122085995
        ],
        "usage\nproperly": [
            1122085995
        ],
        "your\nkubernetes": [
            1122085995
        ],
        "underuti-\nlized": [
            1122085995
        ],
        "be\ncpu-starved": [
            1122085995
        ],
        "killer": [
            1122085995
        ],
        "spot": [
            1122085995
        ],
        "limits?\n": [
            1122085995
        ],
        "the\nexpected": [
            1122085995
        ],
        "levels": [
            1122085995
        ],
        "keep\nmonitoring": [
            1122085995
        ],
        "adjust": [
            1122085995
        ],
        "required\n14.6.1": [
            1122085995
        ],
        "collecting": [
            1122085995
        ],
        "usages\nhow": [
            1122085995
        ],
        "itself\nalready": [
            1122085995
        ],
        "cadvisor": [
            1122085995
        ],
        "of\nresource": [
            1122085995
        ],
        "gathering": [
            1122085995
        ],
        "centrally": [
            1122085995
        ],
        "requires\nyou": [
            1122085995
        ],
        "heapster": [
            1122085995
        ],
        "regular\nkubernetes": [
            1122085995
        ],
        "data\nfrom": [
            1122085995
        ],
        "cadvisors": [
            1122085995
        ],
        "148\nshows": [
            1122085995
        ],
        "flow": [
            1122085995
        ],
        "into\nheapster\nkubelet\ncadvisor\nnode": [
            1122085995
        ],
        "1\npod\npod\nkubelet\ncadvisor\nnode": [
            1122085995
        ],
        "2\npod\nkubelet\ncadvisor\nnode": [
            1122085995
        ],
        "x\npod\nheapster\neach": [
            1122085995
        ],
        "from\ncontainers": [
            1122085995
        ],
        "node\nheapster": [
            1122085995
        ],
        "nodes\nfigure": [
            1122085995
        ],
        "heapster\n": [
            1122085995
        ],
        "\n\n431monitoring": [
            1122085995
        ],
        "usage\nthe": [
            1122085995
        ],
        "arrows": [
            1122085995
        ],
        "flows": [
            1122085995
        ],
        "running\ntherein)": [
            1122085995
        ],
        "anything\nabout": [
            1122085995
        ],
        "cadvisors\nthat": [
            1122085995
        ],
        "collect": [
            1122085995
        ],
        "containers\nenabling": [
            1122085995
        ],
        "heapster\nif": [
            1122085995
        ],
        "heapster\nheapster": [
            1122085995
        ],
        "to\ninstructions": [
            1122085995
        ],
        "https://githubcom/kubernetes/heapster.": [
            1122085995
        ],
        "metrics\nbefore": [
            1122085995
        ],
        "patient": [
            1122085995
        ],
        "nodes\nrunning": [
            1122085995
        ],
        "for\nnodes": [
            1122085995
        ],
        "cpu(cores)": [
            1122085995
        ],
        "cpu%": [
            1122085995
        ],
        "memory(bytes)": [
            1122085995
        ],
        "memory%\nminikube": [
            1122085995
        ],
        "170m": [
            1122085995
        ],
        "8%": [
            1122085995
        ],
        "556mi": [
            1122085995
        ],
        "27%\nthis": [
            1122085995
        ],
        "memory(bytes)\nkube-system": [
            1122085995
        ],
        "influxdb-grafana-2r2w9": [
            1122085995
        ],
        "32mi\nkube-system": [
            1122085995
        ],
        "heapster-40j6d": [
            1122085995
        ],
        "0m": [
            1122085995
        ],
        "18mi\nlisting": [
            1122085995
        ],
        "\n\n432chapter": [
            1122085995
        ],
        "resources\ndefault": [
            1122085995
        ],
        "kubia-3773182134-63bmb": [
            1122085995
        ],
        "9mi\nkube-system": [
            1122085995
        ],
        "kube-dns-v20-z0hq6": [
            1122085995
        ],
        "11mi\nkube-system": [
            1122085995
        ],
        "kubernetes-dashboard-r53mc": [
            1122085995
        ],
        "14mi\nkube-system": [
            1122085995
        ],
        "7m": [
            1122085995
        ],
        "33mi\nthe": [
            1122085995
        ],
        "me\nto": [
            1122085995
        ],
        "warn": [
            1122085995
        ],
        "refuse": [
            1122085995
        ],
        "pod\nw0312": [
            1122085995
        ],
        "22:12:58021885": [
            1122085995
        ],
        "top_podgo:186]": [
            1122085995
        ],
        "\ndefault/kubia-3773182134-63bmb": [
            1122085995
        ],
        "age:": [
            1122085995
        ],
        "1h24m19021873823s\nerror:": [
            1122085995
        ],
        "default/kubia-3773182134-63bmb": [
            1122085995
        ],
        "\n1h24m19021873823s\nif": [
            1122085995
        ],
        "while\nand": [
            1122085995
        ],
        "rerun": [
            1122085995
        ],
        "command—it": [
            1122085995
        ],
        "appear\neventually": [
            1122085995
        ],
        "aggre-\ngates": [
            1122085995
        ],
        "\n--containers": [
            1122085995
        ],
        "\n146.2": [
            1122085995
        ],
        "statistics\nthe": [
            1122085995
        ],
        "usages—it": [
            1122085995
        ],
        "yesterday": [
            1122085995
        ],
        "a\nweek": [
            1122085995
        ],
        "usage\ndata": [
            1122085995
        ],
        "window": [
            1122085995
        ],
        "analyze": [
            1122085995
        ],
        "consump-\ntion": [
            1122085995
        ],
        "periods": [
            1122085995
        ],
        "tools\n": [
            1122085995
        ],
        "google\ncloud": [
            1122085995
        ],
        "cluster\n(either": [
            1122085995
        ],
        "influxdb": [
            1122085995
        ],
        "storing\nstatistics": [
            1122085995
        ],
        "grafana": [
            1122085995
        ],
        "visualizing": [
            1122085995
        ],
        "grafana\ninfluxdb": [
            1122085995
        ],
        "time-series": [
            1122085995
        ],
        "metrics\nand": [
            1122085995
        ],
        "analytics": [
            1122085995
        ],
        "visualization\nsuite": [
            1122085995
        ],
        "nice-looking": [
            1122085995
        ],
        "in\ninfluxdb": [
            1122085995
        ],
        "(an\nexample": [
            1122085995
        ],
        "charts": [
            1122085995
        ],
        "149).\n": [
            1122085995
        ],
        "\n\n433monitoring": [
            1122085995
        ],
        "usage\nrunning": [
            1122085995
        ],
        "cluster\nboth": [
            1122085995
        ],
        "straightforward": [
            1122085995
        ],
        "http://github\ncom/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.\n": [
            1122085995
        ],
        "because\nthey’re": [
            1122085995
        ],
        "add-on\nanalyzing": [
            1122085995
        ],
        "grafana\nto": [
            1122085995
        ],
        "the\ngrafana": [
            1122085995
        ],
        "dashboards": [
            1122085995
        ],
        "find\nout": [
            1122085995
        ],
        "grafana’s": [
            1122085995
        ],
        "cluster-info\n..\nmonitoring-grafana": [
            1122085995
        ],
        "\nhttps://192168.99.100:8443/api/v1/proxy/namespaces/kube-\nsystem/services/monitoring-grafana\nfigure": [
            1122085995
        ],
        "\n\n434chapter": [
            1122085995
        ],
        "resources\nwhen": [
            1122085995
        ],
        "service\nso": [
            1122085995
        ],
        "monitoring-grafana": [
            1122085995
        ],
        "kube-system\nopening": [
            1122085995
        ],
        "kube-system/monitoring-grafana": [
            1122085995
        ],
        "\nbrowser..\na": [
            1122085995
        ],
        "screen": [
            1122085995
        ],
        "the\nright-hand": [
            1122085995
        ],
        "entries:\ncluster\npods\nto": [
            1122085995
        ],
        "there\nyou’ll": [
            1122085995
        ],
        "only\nshow": [
            1122085995
        ],
        "(where\nthey": [
            1122085995
        ],
        "apply)\n": [
            1122085995
        ],
        "resource\nusages": [
            1122085995
        ],
        "alongside\nthe": [
            1122085995
        ],
        "zoom": [
            1122085995
        ],
        "out\nand": [
            1122085995
        ],
        "periods:": [
            1122085995
        ],
        "years\nusing": [
            1122085995
        ],
        "charts\nby": [
            1122085995
        ],
        "you’ve\nset": [
            1122085995
        ],
        "raised": [
            1122085995
        ],
        "lowered": [
            1122085995
        ],
        "chart": [
            1122085995
        ],
        "was\nrequested": [
            1122085995
        ],
        "problematic": [
            1122085995
        ],
        "only\npod": [
            1122085995
        ],
        "as\nmuch": [
            1122085995
        ],
        "running\nfine": [
            1122085995
        ],
        "raise": [
            1122085995
        ],
        "request\nfor": [
            1122085995
        ],
        "opposite": [
            1122085995
        ],
        "won’t\nbe": [
            1122085995
        ],
        "should\ndecrease": [
            1122085995
        ],
        "\n\n435summary\n147": [
            1122085995
        ],
        "everything\nrunning": [
            1122085995
        ],
        "takeaways": [
            1122085995
        ],
        "are\nspecifying": [
            1122085995
        ],
        "cluster\nspecifying": [
            1122085995
        ],
        "starving": [
            1122085995
        ],
        "resources\nunused": [
            1122085995
        ],
        "requests\ncontainers": [
            1122085995
        ],
        "killed\nif": [
            1122085995
        ],
        "memory\nin": [
            1122085995
        ],
        "more\nimportant": [
            1122085995
        ],
        "usage\nactual": [
            1122085995
        ],
        "requested\nthe": [
            1122085995
        ],
        "time\nwill": [
            1122085995
        ],
        "other\napps": [
            1122085995
        ],
        "cpu\nyou": [
            1122085995
        ],
        "request\nactual": [
            1122085995
        ],
        "well\nbelow": [
            1122085995
        ],
        "memory\nyou’ve": [
            1122085995
        ],
        "much\nmemory": [
            1122085995
        ],
        "you’re\nwasting": [
            1122085995
        ],
        "it\nwon’t": [
            1122085995
        ],
        "this\napp": [
            1122085995
        ],
        "memory\nrequest\ncpu": [
            1122085995
        ],
        "request\ncpu": [
            1122085995
        ],
        "usage\nmemory": [
            1122085995
        ],
        "request\nmemory": [
            1122085995
        ],
        "usage\nfigure": [
            1122085995
        ],
        "\n\n436chapter": [
            1122085995
        ],
        "resources\nyou": [
            1122085995
        ],
        "pods\nyou": [
            1122085995
        ],
        "namespace\nto": [
            1122085995
        ],
        "mon-\nitor": [
            1122085995
        ],
        "long-enough": [
            1122085995
        ],
        "period\nin": [
            1122085995
        ],
        "\n\n437\nautomatic": [
            1122085995
        ],
        "scaling\nof": [
            1122085995
        ],
        "nodes\napplications": [
            1122085995
        ],
        "the\nreplicas": [
            1122085995
        ],
        "other\nscalable": [
            1122085995
        ],
        "container’s\nresource": [
            1122085995
        ],
        "(though": [
            1122085995
        ],
        "cre-\nation": [
            1122085995
        ],
        "running)": [
            1122085995
        ],
        "for\ntimes": [
            1122085995
        ],
        "advance": [
            1122085995
        ],
        "changes\ngradually": [
            1122085995
        ],
        "handle\nsudden": [
            1122085995
        ],
        "covers\nconfiguring": [
            1122085995
        ],
        "utilization\nconfiguring": [
            1122085995
        ],
        "metrics\nunderstanding": [
            1122085995
        ],
        "vertical": [
            1122085995
        ],
        "\npossible": [
            1122085995
        ],
        "yet\nunderstanding": [
            1122085995
        ],
        "\n\n438chapter": [
            1122085995
        ],
        "15automatic": [
            1122085995
        ],
        "metric": [
            1122085995
        ],
        "a\ncloud": [
            1122085995
        ],
        "can’t\naccept": [
            1122085995
        ],
        "autoscaling\n": [
            1122085995
        ],
        "rewritten": [
            1122085995
        ],
        "16\nand": [
            1122085995
        ],
        "subject\nonline\n15.1": [
            1122085995
        ],
        "autoscaling\nhorizontal": [
            1122085995
        ],
        "and\nconfigured": [
            1122085995
        ],
        "horizontalpodautoscaler": [
            1122085995
        ],
        "(hpa)": [
            1122085995
        ],
        "controller\nperiodically": [
            1122085995
        ],
        "calculates": [
            1122085995
        ],
        "meet\nthe": [
            1122085995
        ],
        "and\nadjusts": [
            1122085995
        ],
        "(deployment": [
            1122085995
        ],
        "statefulset)": [
            1122085995
        ],
        "\n151.1": [
            1122085995
        ],
        "steps:\nobtain": [
            1122085995
        ],
        "object\ncalculate": [
            1122085995
        ],
        "value\nupdate": [
            1122085995
        ],
        "resource\nlet’s": [
            1122085995
        ],
        "next\nobtaining": [
            1122085995
        ],
        "metrics\nthe": [
            1122085995
        ],
        "the\nmetrics": [
            1122085995
        ],
        "met-\nrics": [
            1122085995
        ],
        "collected": [
            1122085995
        ],
        "aggregated": [
            1122085995
        ],
        "horizontal\npod": [
            1122085995
        ],
        "heapster\nthrough": [
            1122085995
        ],
        "(although": [
            1122085995
        ],
        "the\nconnections": [
            1122085995
        ],
        "direction)\nthis": [
            1122085995
        ],
        "heapster\npod(s)cadvisor(s)\nhorizontal": [
            1122085995
        ],
        "autoscaler(s)\nheapster\nfigure": [
            1122085995
        ],
        "pod(s)": [
            1122085995
        ],
        "horizontalpodautoscaler(s)\n": [
            1122085995
        ],
        "\n\n439horizontal": [
            1122085995
        ],
        "autoscaling\nshould": [
            1122085995
        ],
        "heapster\nadd-on": [
            1122085995
        ],
        "examples\n": [
            1122085995
        ],
        "doing\nso": [
            1122085995
        ],
        "\ncalculating": [
            1122085995
        ],
        "auto-\nscaler": [
            1122085995
        ],
        "statefulset\nresource)": [
            1122085995
        ],
        "it\nneeds": [
            1122085995
        ],
        "average": [
            1122085995
        ],
        "all\nthose": [
            1122085995
        ],
        "input": [
            1122085995
        ],
        "cal-\nculation": [
            1122085995
        ],
        "calculating": [
            1122085995
        ],
        "the\nrequired": [
            1122085995
        ],
        "summing": [
            1122085995
        ],
        "dividing": [
            1122085995
        ],
        "horizontalpodautoscaler\nresource": [
            1122085995
        ],
        "rounding": [
            1122085995
        ],
        "next-larger": [
            1122085995
        ],
        "calculation": [
            1122085995
        ],
        "is\na": [
            1122085995
        ],
        "thrash\naround": [
            1122085995
        ],
        "unstable": [
            1122085995
        ],
        "usage\nand": [
            1122085995
        ],
        "queries-per-second": [
            1122085995
        ],
        "[qps])": [
            1122085995
        ],
        "complicated\nthe": [
            1122085995
        ],
        "then\ntakes": [
            1122085995
        ],
        "target\ncpu": [
            1122085995
        ],
        "qps": [
            1122085995
        ],
        "will\nscale": [
            1122085995
        ],
        "example\na": [
            1122085995
        ],
        "metrics\nprior": [
            1122085995
        ],
        "metrics\nfrom": [
            1122085995
        ],
        "an\naggregated": [
            1122085995
        ],
        "manager\nwith": [
            1122085995
        ],
        "\n--horizontal-pod-autoscaler-use-rest-clients=true": [
            1122085995
        ],
        "servers\nwe’ll": [
            1122085995
        ],
        "aggregation": [
            1122085995
        ],
        "\nselecting": [
            1122085995
        ],
        "adminis-\ntrators": [
            1122085995
        ],
        "the\nappropriate": [
            1122085995
        ],
        "format\n": [
            1122085995
        ],
        "\n\n440chapter": [
            1122085995
        ],
        "nodes\nupdating": [
            1122085995
        ],
        "resource\nthe": [
            1122085995
        ],
        "field\non": [
            1122085995
        ],
        "modifies": [
            1122085995
        ],
        "sub-resource": [
            1122085995
        ],
        "scale\nsub-resource": [
            1122085995
        ],
        "153).\nthis": [
            1122085995
        ],
        "for\ndeployments\nreplicasets\nreplicationcontrollers\nstatefulsets\nthese": [
            1122085995
        ],
        "1\ncpu\nutilization\nqps\npod": [
            1122085995
        ],
        "2pod": [
            1122085995
        ],
        "3\ntarget\ncpu": [
            1122085995
        ],
        "utilization\ntarget": [
            1122085995
        ],
        "qps\nreplicas:": [
            1122085995
        ],
        "4\nreplicas:": [
            1122085995
        ],
        "3\nreplicas:": [
            1122085995
        ],
        "4\n301215\n20\n(15": [
            1122085995
        ],
        "12)": [
            1122085995
        ],
        "20\n(60": [
            1122085995
        ],
        "50)": [
            1122085995
        ],
        "50\nmax(4": [
            1122085995
        ],
        "3)\n50%\n60%90%50%\nfigure": [
            1122085995
        ],
        "metrics\nautoscaler": [
            1122085995
        ],
        "adjusts": [
            1122085995
        ],
        "(++": [
            1122085995
        ],
        "--)\nhorizontal": [
            1122085995
        ],
        "autoscaler\ndeployment": [
            1122085995
        ],
        "replicaset\nstatefulset,": [
            1122085995
        ],
        "or\nreplicationcontroller\nscale\nsub-resource\nfigure": [
            1122085995
        ],
        "sub-resource\n": [
            1122085995
        ],
        "\n\n441horizontal": [
            1122085995
        ],
        "autoscaling\nunderstanding": [
            1122085995
        ],
        "process\nyou": [
            1122085995
        ],
        "154.\nthe": [
            1122085995
        ],
        "heapster\nand": [
            1122085995
        ],
        "indicate": [
            1122085995
        ],
        "direction": [
            1122085995
        ],
        "a\ncontinuous": [
            1122085995
        ],
        "loop;": [
            1122085995
        ],
        "hpa": [
            1122085995
        ],
        "controller)": [
            1122085995
        ],
        "the\nend": [
            1122085995
        ],
        "propagated": [
            1122085995
        ],
        "res-\ncaling": [
            1122085995
        ],
        "observe\nthe": [
            1122085995
        ],
        "next\n15.1.2": [
            1122085995
        ],
        "utilization\nperhaps": [
            1122085995
        ],
        "pods\nproviding": [
            1122085995
        ],
        "cope\nwith": [
            1122085995
        ],
        "(vertical": [
            1122085995
        ],
        "scaling—increas-\ning": [
            1122085995
        ],
        "use)": [
            1122085995
        ],
        "(horizontal": [
            1122085995
        ],
        "scaling—increasing": [
            1122085995
        ],
        "here\nautoscaler": [
            1122085995
        ],
        "adjusts\nreplicas": [
            1122085995
        ],
        "--)\nheapster": [
            1122085995
        ],
        "collects\nmetrics": [
            1122085995
        ],
        "nodes\ncadvisor": [
            1122085995
        ],
        "node\ndeploymentreplicaset\nautoscaler": [
            1122085995
        ],
        "heapster\nkubelet\ncadvisor\nnode": [
            1122085995
        ],
        "2\npod\nnode": [
            1122085995
        ],
        "x\nheapster\nhorizontal": [
            1122085995
        ],
        "pod\nautoscaler\nfigure": [
            1122085995
        ],
        "rescales": [
            1122085995
        ],
        "\n\n442chapter": [
            1122085995
        ],
        "nodes\nwe’re": [
            1122085995
        ],
        "(increasing": [
            1122085995
        ],
        "swamped—perhaps": [
            1122085995
        ],
        "pods\nreaches": [
            1122085995
        ],
        "exactly?\ntipalways": [
            1122085995
        ],
        "definitely": [
            1122085995
        ],
        "never\nabove": [
            1122085995
        ],
        "90%)": [
            1122085995
        ],
        "sudden": [
            1122085995
        ],
        "spikes\nas": [
            1122085995
        ],
        "requests\nspecified": [
            1122085995
        ],
        "consum-\ning": [
            1122085995
        ],
        "request)": [
            1122085995
        ],
        "configured\nfor": [
            1122085995
        ],
        "concerned": [
            1122085995
        ],
        "(the\ncpu": [
            1122085995
        ],
        "or\nindirectly": [
            1122085995
        ],
        "uti-\nlization": [
            1122085995
        ],
        "percentage\ncreating": [
            1122085995
        ],
        "usage\nlet’s": [
            1122085995
        ],
        "pods\nbased": [
            1122085995
        ],
        "autoscaling\npossible": [
            1122085995
        ],
        "nodejs\nlisting": [
            1122085995
        ],
        "set:": [
            1122085995
        ],
        "deploymentyaml\nmanually": [
            1122085995
        ],
        "\n(initial)": [
            1122085995
        ],
        "three\nrunning": [
            1122085995
        ],
        "\nkubia:v1": [
            1122085995
        ],
        "\n\n443horizontal": [
            1122085995
        ],
        "object—it": [
            1122085995
        ],
        "millicores\nof": [
            1122085995
        ],
        "way\nexists—using": [
            1122085995
        ],
        "autoscale": [
            1122085995
        ],
        "--cpu-percent=30": [
            1122085995
        ],
        "--min=1": [
            1122085995
        ],
        "--max=5\ndeployment": [
            1122085995
        ],
        "autoscaled\nthis": [
            1122085995
        ],
        "scal-\ning": [
            1122085995
        ],
        "30%": [
            1122085995
        ],
        "specifying\nthe": [
            1122085995
        ],
        "keep\nadjusting": [
            1122085995
        ],
        "across\napplication": [
            1122085995
        ],
        "well\nlet’s": [
            1122085995
        ],
        "better\nunderstanding": [
            1122085995
        ],
        "hpav2beta1.autoscaling": [
            1122085995
        ],
        "autoscaling/v2beta1": [
            1122085995
        ],
        "maxreplicas:": [
            1122085995
        ],
        "metrics:": [
            1122085995
        ],
        "targetaverageutilization:": [
            1122085995
        ],
        "minreplicas:": [
            1122085995
        ],
        "scaletargetref:": [
            1122085995
        ],
        "definition\nrequesting": [
            1122085995
        ],
        "pod\nhpa": [
            1122085995
        ],
        "group\neach": [
            1122085995
        ],
        "case)\nthe\nminimum\nand\nmaximum\nnumber": [
            1122085995
        ],
        "of\nreplicas\nyou\nspecified\nyou’d": [
            1122085995
        ],
        "\nadjust": [
            1122085995
        ],
        "\nso": [
            1122085995
        ],
        "\nrequested": [
            1122085995
        ],
        "cpu\nthe": [
            1122085995
        ],
        "upon\n": [
            1122085995
        ],
        "\n\n444chapter": [
            1122085995
        ],
        "nodes\nstatus:\n": [
            1122085995
        ],
        "currentmetrics:": [
            1122085995
        ],
        "currentreplicas:": [
            1122085995
        ],
        "desiredreplicas:": [
            1122085995
        ],
        "\nnotemultiple": [
            1122085995
        ],
        "autoscaling/v2beta1\nand": [
            1122085995
        ],
        "autoscaling/v1": [
            1122085995
        ],
        "here\nseeing": [
            1122085995
        ],
        "rescale": [
            1122085995
        ],
        "event\nit": [
            1122085995
        ],
        "them\nbefore": [
            1122085995
        ],
        "resource\nwith": [
            1122085995
        ],
        "targets": [
            1122085995
        ],
        "<unknown>:\n$": [
            1122085995
        ],
        "hpa\nname": [
            1122085995
        ],
        "minpods": [
            1122085995
        ],
        "maxpods": [
            1122085995
        ],
        "replicas\nkubia": [
            1122085995
        ],
        "deployment/kubia": [
            1122085995
        ],
        "<unknown>": [
            1122085995
        ],
        "utilization\nwill": [
            1122085995
        ],
        "deployment\ndown": [
            1122085995
        ],
        "replica:\n$": [
            1122085995
        ],
        "deployment\nname": [
            1122085995
        ],
        "23m\nremember": [
            1122085995
        ],
        "deployment\nthe": [
            1122085995
        ],
        "excess\npods": [
            1122085995
        ],
        "horizontalpod-\nautoscaler": [
            1122085995
        ],
        "hpa\nname:": [
            1122085995
        ],
        "<none>\ncreationtimestamp:": [
            1122085995
        ],
        "sat": [
            1122085995
        ],
        "03": [
            1122085995
        ],
        "jun": [
            1122085995
        ],
        "12:59:57": [
            1122085995
        ],
        "+0200\nreference:": [
            1122085995
        ],
        "deployment/kubia\nmetrics:": [
            1122085995
        ],
        "(": [
            1122085995
        ],
        ")\n": [
            1122085995
        ],
        "request):": [
            1122085995
        ],
        "0%": [
            1122085995
        ],
        "(0)": [
            1122085995
        ],
        "30%\nmin": [
            1122085995
        ],
        "1\nmax": [
            1122085995
        ],
        "5\nlisting": [
            1122085995
        ],
        "autoscaler\n": [
            1122085995
        ],
        "\n\n445horizontal": [
            1122085995
        ],
        "autoscaling\nevents:\nfrom": [
            1122085995
        ],
        "---\nhorizontal-pod-autoscaler": [
            1122085995
        ],
        "successfulrescale": [
            1122085995
        ],
        "size:": [
            1122085995
        ],
        "1;": [
            1122085995
        ],
        "target\nnotethe": [
            1122085995
        ],
        "readable\nturn": [
            1122085995
        ],
        "horizon-\ntal": [
            1122085995
        ],
        "rescaled": [
            1122085995
        ],
        "were\nbelow": [
            1122085995
        ],
        "\ntriggering": [
            1122085995
        ],
        "scale-up\nyou’ve": [
            1122085995
        ],
        "witnessed": [
            1122085995
        ],
        "scale-down)": [
            1122085995
        ],
        "you’ll\nstart": [
            1122085995
        ],
        "should\nsee": [
            1122085995
        ],
        "expose:\n$": [
            1122085995
        ],
        "--port=80": [
            1122085995
        ],
        "--target-port=8080\nservice": [
            1122085995
        ],
        "exposed\nbefore": [
            1122085995
        ],
        "the\nhorizontalpodautoscaler": [
            1122085995
        ],
        "hpadeployment\nevery": [
            1122085995
        ],
        "10s:": [
            1122085995
        ],
        "hpadeployment": [
            1122085995
        ],
        "age\nhpa/kubia": [
            1122085995
        ],
        "45m\nname": [
            1122085995
        ],
        "age\ndeploy/kubia": [
            1122085995
        ],
        "56m\ntiplist": [
            1122085995
        ],
        "comma": [
            1122085995
        ],
        "manually\nrun": [
            1122085995
        ],
        "--watch": [
            1122085995
        ],
        "plain\nkubectl": [
            1122085995
        ],
        "when\nusing": [
            1122085995
        ],
        "load-generating": [
            1122085995
        ],
        "pod\nyou’ll": [
            1122085995
        ],
        "terminal:\n$": [
            1122085995
        ],
        "loadgenerator": [
            1122085995
        ],
        "sh": [
            1122085995
        ],
        "wget": [
            1122085995
        ],
        "-q": [
            1122085995
        ],
        "http://kubiadefault;": [
            1122085995
        ],
        "done\nlisting": [
            1122085995
        ],
        "\n\n446chapter": [
            1122085995
        ],
        "nodes\nthis": [
            1122085995
        ],
        "-it\noption": [
            1122085995
        ],
        "process\nwhich": [
            1122085995
        ],
        "ctrl+c": [
            1122085995
        ],
        "\n--rm": [
            1122085995
        ],
        "deleted\nafterward": [
            1122085995
        ],
        "\n--restart=never": [
            1122085995
        ],
        "unman-\naged": [
            1122085995
        ],
        "need\nthis": [
            1122085995
        ],
        "you\nwere": [
            1122085995
        ],
        "cleans": [
            1122085995
        ],
        "command\nterminates": [
            1122085995
        ],
        "deployment\nas": [
            1122085995
        ],
        "load-generator": [
            1122085995
        ],
        "autoscaler\nincrease": [
            1122085995
        ],
        "jumped\nto": [
            1122085995
        ],
        "108%": [
            1122085995
        ],
        "the\nutilization": [
            1122085995
        ],
        "decreased": [
            1122085995
        ],
        "74%": [
            1122085995
        ],
        "stabilized": [
            1122085995
        ],
        "at\naround": [
            1122085995
        ],
        "26%": [
            1122085995
        ],
        "load-generators\nagain": [
            1122085995
        ],
        "the\nautoscaler": [
            1122085995
        ],
        "(only": [
            1122085995
        ],
        "following\nlisting)\nfrom": [
            1122085995
        ],
        "-------\nh-p-a": [
            1122085995
        ],
        "target\nh-p-a": [
            1122085995
        ],
        "4;": [
            1122085995
        ],
        "(percentage": [
            1122085995
        ],
        "target\ndoes": [
            1122085995
        ],
        "i\nonly": [
            1122085995
        ],
        "100%?": [
            1122085995
        ],
        "container’s\ncpu": [
            1122085995
        ],
        "the\nrequested": [
            1122085995
        ],
        "the\npercentage": [
            1122085995
        ],
        "math": [
            1122085995
        ],
        "concluded": [
            1122085995
        ],
        "that\nfour": [
            1122085995
        ],
        "its\ncpu": [
            1122085995
        ],
        "spiked": [
            1122085995
        ],
        "percent-\nage)": [
            1122085995
        ],
        "divide": [
            1122085995
        ],
        "you\nlisting": [
            1122085995
        ],
        "horizontalpodautoscaler\n": [
            1122085995
        ],
        "\n\n447horizontal": [
            1122085995
        ],
        "autoscaling\nget": [
            1122085995
        ],
        "27%": [
            1122085995
        ],
        "neighborhood": [
            1122085995
        ],
        "target\nvalue": [
            1122085995
        ],
        "was\nunderstanding": [
            1122085995
        ],
        "scaling\nin": [
            1122085995
        ],
        "shot": [
            1122085995
        ],
        "usage\ncould": [
            1122085995
        ],
        "spike": [
            1122085995
        ],
        "(say\n150%)": [
            1122085995
        ],
        "still\nonly": [
            1122085995
        ],
        "double\nthe": [
            1122085995
        ],
        "replicas\nexist": [
            1122085995
        ],
        "can\noccur": [
            1122085995
        ],
        "rescaling\nevent": [
            1122085995
        ],
        "less\nfrequently—every": [
            1122085995
        ],
        "refuses": [
            1122085995
        ],
        "show\nthat": [
            1122085995
        ],
        "should\nmodifying": [
            1122085995
        ],
        "hpa\nresource": [
            1122085995
        ],
        "the\ntargetaverageutilization": [
            1122085995
        ],
        "listing\n...\nspec:\n": [
            1122085995
        ],
        "metrics:\n": [
            1122085995
        ],
        "resource:\n": [
            1122085995
        ],
        "resource\n..\nas": [
            1122085995
        ],
        "be\ndetected": [
            1122085995
        ],
        "acted": [
            1122085995
        ],
        "this\ncase)": [
            1122085995
        ],
        "after\nyou": [
            1122085995
        ],
        "deployment\nlisting": [
            1122085995
        ],
        "resource\nchange": [
            1122085995
        ],
        "60\n": [
            1122085995
        ],
        "\n\n448chapter": [
            1122085995
        ],
        "nodes\n151.3": [
            1122085995
        ],
        "consumption\nyou’ve": [
            1122085995
        ],
        "memory\nusage?": [
            1122085995
        ],
        "memory-based": [
            1122085995
        ],
        "cpu-based": [
            1122085995
        ],
        "autoscal-\ning": [
            1122085995
        ],
        "itself—it": [
            1122085995
        ],
        "done\nby": [
            1122085995
        ],
        "hoping": [
            1122085995
        ],
        "use\nless": [
            1122085995
        ],
        "and\nis": [
            1122085995
        ],
        "reader\n15.1.4": [
            1122085995
        ],
        "metrics\nyou’ve": [
            1122085995
        ],
        "the\nonly": [
            1122085995
        ],
        "usable": [
            1122085995
        ],
        "custom\napp-defined": [
            1122085995
        ],
        "ini-\ntial": [
            1122085995
        ],
        "design": [
            1122085995
        ],
        "beyond": [
            1122085995
        ],
        "cpu-based\nscaling": [
            1122085995
        ],
        "prompted": [
            1122085995
        ],
        "(sig)": [
            1122085995
        ],
        "to\nredesign": [
            1122085995
        ],
        "autoscaler\nwith": [
            1122085995
        ],
        "invite": [
            1122085995
        ],
        "entitled": [
            1122085995
        ],
        "“kubernetes": [
            1122085995
        ],
        "port”": [
            1122085995
        ],
        "at\nhttp://mediumcom/@marko.luksa.": [
            1122085995
        ],
        "i\nencountered": [
            1122085995
        ],
        "luckily\nnewer": [
            1122085995
        ],
        "to\nconfigure": [
            1122085995
        ],
        "how\nwe": [
            1122085995
        ],
        "shows\nhow": [
            1122085995
        ],
        "metric\n...\nspec:\n": [
            1122085995
        ],
        "autoscaling\ndefines": [
            1122085995
        ],
        "metric\nthe": [
            1122085995
        ],
        "\nutilization": [
            1122085995
        ],
        "monitored\nthe": [
            1122085995
        ],
        "\n\n449horizontal": [
            1122085995
        ],
        "autoscaling\nas": [
            1122085995
        ],
        "use\nin": [
            1122085995
        ],
        "metric—\nin": [
            1122085995
        ],
        "an\nhpa": [
            1122085995
        ],
        "object:\nresource\npods\nobject\nunderstanding": [
            1122085995
        ],
        "resource\nmetric": [
            1122085995
        ],
        "seen\nhow": [
            1122085995
        ],
        "types\nunderstanding": [
            1122085995
        ],
        "(including": [
            1122085995
        ],
        "custom)": [
            1122085995
        ],
        "queries-\nper-second": [
            1122085995
        ],
        "(qps)": [
            1122085995
        ],
        "messages": [
            1122085995
        ],
        "broker’s": [
            1122085995
        ],
        "queue": [
            1122085995
        ],
        "the\nmessage": [
            1122085995
        ],
        "broker": [
            1122085995
        ],
        "qps\nmetric": [
            1122085995
        ],
        "listing\nunder": [
            1122085995
        ],
        "field\n...\nspec:\n": [
            1122085995
        ],
        "metricname:": [
            1122085995
        ],
        "targetaveragevalue:": [
            1122085995
        ],
        "other)": [
            1122085995
        ],
        "targeted": [
            1122085995
        ],
        "\n100": [
            1122085995
        ],
        "ingress\nobject": [
            1122085995
        ],
        "latency": [
            1122085995
        ],
        "or\nsomething": [
            1122085995
        ],
        "\nobject\nmetric": [
            1122085995
        ],
        "hpa\nlisting": [
            1122085995
        ],
        "hpa\ndefines": [
            1122085995
        ],
        "\n\n450chapter": [
            1122085995
        ],
        "nodes\ndefinition": [
            1122085995
        ],
        "example\n...\nspec:\n": [
            1122085995
        ],
        "latencymillis": [
            1122085995
        ],
        "target:": [
            1122085995
        ],
        "targetvalue:": [
            1122085995
        ],
        "\n..\nin": [
            1122085995
        ],
        "the\nfrontend": [
            1122085995
        ],
        "pod\nautoscaler": [
            1122085995
        ],
        "rises": [
            1122085995
        ],
        "value\nthe": [
            1122085995
        ],
        "\n151.5": [
            1122085995
        ],
        "autoscaling\nyou": [
            1122085995
        ],
        "of\nautoscaling": [
            1122085995
        ],
        "isn’t\na": [
            1122085995
        ],
        "increasing\nthe": [
            1122085995
        ],
        "linear": [
            1122085995
        ],
        "the\nobserved": [
            1122085995
        ],
        "linear)": [
            1122085995
        ],
        "x\nand": [
            1122085995
        ],
        "somewhere\nclose": [
            1122085995
        ],
        "x/2": [
            1122085995
        ],
        "(qps)\nwhich": [
            1122085995
        ],
        "application\nis": [
            1122085995
        ],
        "pro-\nportionate": [
            1122085995
        ],
        "sure\nto": [
            1122085995
        ],
        "or\ndecreases\n15.1.6": [
            1122085995
        ],
        "replicas\nthe": [
            1122085995
        ],
        "minreplicas": [
            1122085995
        ],
        "field\nto": [
            1122085995
        ],
        "doing\nlisting": [
            1122085995
        ],
        "hpa\nuse": [
            1122085995
        ],
        "object\nthe": [
            1122085995
        ],
        "obtain\nthe\nautoscaler\nshould\nscale": [
            1122085995
        ],
        "value\nof": [
            1122085995
        ],
        "the\nmetric\nstays": [
            1122085995
        ],
        "close\nto": [
            1122085995
        ],
        "this\nthe": [
            1122085995
        ],
        "\n\n451vertical": [
            1122085995
        ],
        "autoscaling\nanything": [
            1122085995
        ],
        "dramatically\nincrease": [
            1122085995
        ],
        "only\nonce": [
            1122085995
        ],
        "idling": [
            1122085995
        ],
        "un-idling": [
            1122085995
        ],
        "until\nthe": [
            1122085995
        ],
        "\n152": [
            1122085995
        ],
        "for\nsuch": [
            1122085995
        ],
        "vertically—give": [
            1122085995
        ],
        "cpu\nand/or": [
            1122085995
        ],
        "pod\nrequests": [
            1122085995
        ],
        "i\nsay": [
            1122085995
        ],
        "“would”": [
            1122085995
        ],
        "requests\nor": [
            1122085995
        ],
        "(well": [
            1122085995
        ],
        "ago)": [
            1122085995
        ],
        "i\nwas": [
            1122085995
        ],
        "support\nproper": [
            1122085995
        ],
        "proposal": [
            1122085995
        ],
        "sadly": [
            1122085995
        ],
        "\n152.1": [
            1122085995
        ],
        "requests\nan": [
            1122085995
        ],
        "experimental": [
            1122085995
        ],
        "if\ntheir": [
            1122085995
        ],
        "initialresources": [
            1122085995
        ],
        "(per\nthe": [
            1122085995
        ],
        "tag)": [
            1122085995
        ],
        "accordingly": [
            1122085995
        ],
        "automatically\n15.2.2": [
            1122085995
        ],
        "running\neventually": [
            1122085995
        ],
        "writing\nthis": [
            1122085995
        ],
        "finalized": [
            1122085995
        ],
        "\n\n452chapter": [
            1122085995
        ],
        "already\nimplemented": [
            1122085995
        ],
        "not\n15.3": [
            1122085995
        ],
        "for\nthem": [
            1122085995
        ],
        "any\nmore": [
            1122085995
        ],
        "encounter": [
            1122085995
        ],
        "the\nproblem": [
            1122085995
        ],
        "node’s\nresources": [
            1122085995
        ],
        "down\nvertically": [
            1122085995
        ],
        "premises": [
            1122085995
        ],
        "physically": [
            1122085995
        ],
        "adding\nadditional": [
            1122085995
        ],
        "clicks": [
            1122085995
        ],
        "autoscaler\n15.3.1": [
            1122085995
        ],
        "autoscaler\nthe": [
            1122085995
        ],
        "nodes\nwhen": [
            1122085995
        ],
        "de-provisions": [
            1122085995
        ],
        "underutilized": [
            1122085995
        ],
        "for\nlonger": [
            1122085995
        ],
        "\nrequesting": [
            1122085995
        ],
        "infrastructure\na": [
            1122085995
        ],
        "can’t\nschedule": [
            1122085995
        ],
        "doing\nthat": [
            1122085995
        ],
        "accommodate": [
            1122085995
        ],
        "if\nthat’s": [
            1122085995
        ],
        "pools)": [
            1122085995
        ],
        "same-sized": [
            1122085995
        ],
        "nodes\n(or": [
            1122085995
        ],
        "say\n“give": [
            1122085995
        ],
        "node”": [
            1122085995
        ],
        "type\n": [
            1122085995
        ],
        "unscheduled": [
            1122085995
        ],
        "such\nnode": [
            1122085995
        ],
        "available\nthe": [
            1122085995
        ],
        "“best”": [
            1122085995
        ],
        "obviously\nneed": [
            1122085995
        ],
        "reacts": [
            1122085995
        ],
        "155.\n": [
            1122085995
        ],
        "\n\n453horizontal": [
            1122085995
        ],
        "contacts": [
            1122085995
        ],
        "and\nregisters": [
            1122085995
        ],
        "down?\nrelinquishing": [
            1122085995
        ],
        "requested\ncpu": [
            1122085995
        ],
        "node\n(apart": [
            1122085995
        ],
        "relinquished\nthe": [
            1122085995
        ],
        "other\nwords": [
            1122085995
        ],
        "autoscaler\nknows": [
            1122085995
        ],
        "unschedula-\nble": [
            1122085995
        ],
        "pods\nbelong": [
            1122085995
        ],
        "first\nmarked": [
            1122085995
        ],
        "unschedulable)\nnode": [
            1122085995
        ],
        "x\nnode": [
            1122085995
        ],
        "x1\n1": [
            1122085995
        ],
        "nodes\n3": [
            1122085995
        ],
        "in\nprevious": [
            1122085995
        ],
        "step\n2": [
            1122085995
        ],
        "node\ntype": [
            1122085995
        ],
        "them\ncluster\nautoscaler\npods\nnode": [
            1122085995
        ],
        "x2\npods\nnode": [
            1122085995
        ],
        "y\nnode": [
            1122085995
        ],
        "y1\npods\nunschedulable\npod\nfigure": [
            1122085995
        ],
        "\nexisting": [
            1122085995
        ],
        "\n\n454chapter": [
            1122085995
        ],
        "nodes\n153.2": [
            1122085995
        ],
        "autoscaler\ncluster": [
            1122085995
        ],
        "on\ngoogle": [
            1122085995
        ],
        "(gke)\ngoogle": [
            1122085995
        ],
        "(gce)\namazon": [
            1122085995
        ],
        "(aws)\nmicrosoft": [
            1122085995
        ],
        "azure\nhow": [
            1122085995
        ],
        "running\nfor": [
            1122085995
        ],
        "like\nthis:\n$": [
            1122085995
        ],
        "--enable-autoscaling": [
            1122085995
        ],
        "--min-nodes=3": [
            1122085995
        ],
        "--max-nodes=5\nif": [
            1122085995
        ],
        "before\nrunning": [
            1122085995
        ],
        "\nkube-upsh:": [
            1122085995
        ],
        "\nkube_enable_cluster_autoscaler=true\nkube_autoscaler_min_nodes=3\nkube_autoscaler_max_nodes=5\nrefer": [
            1122085995
        ],
        "https://githubcom/kubernetes/auto-\nscaler/tree/master/cluster-autoscaler": [
            1122085995
        ],
        "other\nplatforms": [
            1122085995
        ],
        "publishes": [
            1122085995
        ],
        "cluster-autoscaler-\nstatus\n": [
            1122085995
        ],
        "namespace\n15.3.3": [
            1122085995
        ],
        "scale-down\nwhen": [
            1122085995
        ],
        "unexpectedly": [
            1122085995
        ],
        "becom-\ning": [
            1122085995
        ],
        "voluntarily": [
            1122085995
        ],
        "disrupt": [
            1122085995
        ],
        "feature\nmanually": [
            1122085995
        ],
        "cordoning": [
            1122085995
        ],
        "draining": [
            1122085995
        ],
        "drained": [
            1122085995
        ],
        "going\ninto": [
            1122085995
        ],
        "commands:\nkubectl": [
            1122085995
        ],
        "cordon": [
            1122085995
        ],
        "<node>": [
            1122085995
        ],
        "do\nanything": [
            1122085995
        ],
        "node)\nkubectl": [
            1122085995
        ],
        "drain": [
            1122085995
        ],
        "node\nin": [
            1122085995
        ],
        "uncordon": [
            1122085995
        ],
        "again\nwith": [
            1122085995
        ],
        "<node>\n": [
            1122085995
        ],
        "\n\n455horizontal": [
            1122085995
        ],
        "running;\nthis": [
            1122085995
        ],
        "quorum-based": [
            1122085995
        ],
        "keep\nrunning": [
            1122085995
        ],
        "pod-\ndisruptionbudget": [
            1122085995
        ],
        "sounds": [
            1122085995
        ],
        "simplest\nkubernetes": [
            1122085995
        ],
        "number\nspecifying": [
            1122085995
        ],
        "starting\nfrom": [
            1122085995
        ],
        "unavailable\nwe’ll": [
            1122085995
        ],
        "poddisruptionbudget": [
            1122085995
        ],
        "(pdb)": [
            1122085995
        ],
        "but\ninstead": [
            1122085995
        ],
        "pod-\ndisruptionbudget\n": [
            1122085995
        ],
        "(they\nhave": [
            1122085995
        ],
        "\napp=kubia)": [
            1122085995
        ],
        "pdb": [
            1122085995
        ],
        "kubia-pdb": [
            1122085995
        ],
        "--selector=app=kubia": [
            1122085995
        ],
        "--min-available=3\npoddisruptionbudget": [
            1122085995
        ],
        "kubia-pdb\"": [
            1122085995
        ],
        "created\nsimple": [
            1122085995
        ],
        "pdb’s": [
            1122085995
        ],
        "policy/v1beta1\nkind:": [
            1122085995
        ],
        "poddisruptionbudget\nmetadata:\n": [
            1122085995
        ],
        "kubia-pdb\nspec:\n": [
            1122085995
        ],
        "minavailable:": [
            1122085995
        ],
        "\nstatus:\n": [
            1122085995
        ],
        "minavailable\nfield": [
            1122085995
        ],
        "times\nnotestarting": [
            1122085995
        ],
        "also\nsupports": [
            1122085995
        ],
        "min-\navailable\n": [
            1122085995
        ],
        "evictions": [
            1122085995
        ],
        "are\nunavailable": [
            1122085995
        ],
        "\nwe": [
            1122085995
        ],
        "never\nevict": [
            1122085995
        ],
        "pods\nbelow": [
            1122085995
        ],
        "definition\nhow": [
            1122085995
        ],
        "\ndetermines": [
            1122085995
        ],
        "budget": [
            1122085995
        ],
        "\n\n456chapter": [
            1122085995
        ],
        "minavailable": [
            1122085995
        ],
        "three\nas": [
            1122085995
        ],
        "eviction": [
            1122085995
        ],
        "before\nevicting": [
            1122085995
        ],
        "\n154": [
            1122085995
        ],
        "also\nyour": [
            1122085995
        ],
        "that\nconfiguring": [
            1122085995
        ],
        "a\nhorizontalpodautoscaler": [
            1122085995
        ],
        "pods\nbesides": [
            1122085995
        ],
        "application-provided": [
            1122085995
        ],
        "objects\ndeployed": [
            1122085995
        ],
        "cluster\nvertical": [
            1122085995
        ],
        "yet\neven": [
            1122085995
        ],
        "runs\non": [
            1122085995
        ],
        "provider\nyou": [
            1122085995
        ],
        "deleted\nautomatically": [
            1122085995
        ],
        "and\n--rm": [
            1122085995
        ],
        "options\nin": [
            1122085995
        ],
        "keep\ncertain": [
            1122085995
        ],
        "together\nor": [
            1122085995
        ],
        "apart\n": [
            1122085995
        ],
        "\n\n457\nadvanced": [
            1122085995
        ],
        "scheduling\nkubernetes": [
            1122085995
        ],
        "only\ndone": [
            1122085995
        ],
        "mech-\nanisms": [
            1122085995
        ],
        "this\nchapter\n16.1": [
            1122085995
        ],
        "\ncertain": [
            1122085995
        ],
        "restricting\nthis": [
            1122085995
        ],
        "nodes\ndefining": [
            1122085995
        ],
        "selectors\nco-locating": [
            1122085995
        ],
        "\nkeeping": [
            1122085995
        ],
        "\nanti-affinity\n": [
            1122085995
        ],
        "\n\n458chapter": [
            1122085995
        ],
        "16advanced": [
            1122085995
        ],
        "scheduling\nwhich": [
            1122085995
        ],
        "toler-\nates": [
            1122085995
        ],
        "taints\n": [
            1122085995
        ],
        "which\nyou’ll": [
            1122085995
        ],
        "make\nit": [
            1122085995
        ],
        "specifically\nadding": [
            1122085995
        ],
        "rejecting": [
            1122085995
        ],
        "modify\nexisting": [
            1122085995
        ],
        "tainted": [
            1122085995
        ],
        "they\nwant": [
            1122085995
        ],
        "to\n16.1.1": [
            1122085995
        ],
        "tolerations\nthe": [
            1122085995
        ],
        "taint": [
            1122085995
        ],
        "taints\nyou": [
            1122085995
        ],
        "masterk8s\nname:": [
            1122085995
        ],
        "masterk8s\nrole:\nlabels:": [
            1122085995
        ],
        "betakubernetes.io/arch=amd64\n": [
            1122085995
        ],
        "betakubernetes.io/os=linux\n": [
            1122085995
        ],
        "kubernetesio/hostname=master.k8s\n": [
            1122085995
        ],
        "node-rolekubernetes.io/master=\nannotations:": [
            1122085995
        ],
        "nodealpha.kubernetes.io/ttl=0\n": [
            1122085995
        ],
        "volumeskubernetes.io/controller-managed-attach-detach=true\ntaints:": [
            1122085995
        ],
        "node-rolekubernetes.io/master:noschedule": [
            1122085995
        ],
        "repre-\nsented": [
            1122085995
        ],
        "\n<key>=<value>:<effect>": [
            1122085995
        ],
        "\nnode-rolekubernetes.io/master": [
            1122085995
        ],
        "the\ntaint)": [
            1122085995
        ],
        "\nnoschedule": [
            1122085995
        ],
        "pods\ntolerate": [
            1122085995
        ],
        "161).\n": [
            1122085995
        ],
        "kubeadm\nthe": [
            1122085995
        ],
        "taint\n": [
            1122085995
        ],
        "\n\n459using": [
            1122085995
        ],
        "nodes\ndisplaying": [
            1122085995
        ],
        "tolerations\nin": [
            1122085995
        ],
        "pod\non": [
            1122085995
        ],
        "as\npods": [
            1122085995
        ],
        "pod\nalso": [
            1122085995
        ],
        "toleration": [
            1122085995
        ],
        "pod\nhas": [
            1122085995
        ],
        "kube-proxy-80wqm": [
            1122085995
        ],
        "kube-system\n..\ntolerations:": [
            1122085995
        ],
        "node-rolekubernetes.io/master=:noschedule\n": [
            1122085995
        ],
        "nodealpha.kubernetes.io/notready=:exists:noexecute\n": [
            1122085995
        ],
        "nodealpha.kubernetes.io/unreachable=:exists:noexecute\n...\nas": [
            1122085995
        ],
        "kube-\nproxy": [
            1122085995
        ],
        "\nnotedisregard": [
            1122085995
        ],
        "but\nnot": [
            1122085995
        ],
        "apparently": [
            1122085995
        ],
        "dif-\nferently": [
            1122085995
        ],
        "taint’s/toleration’s": [
            1122085995
        ],
        "\nnull\nunderstanding": [
            1122085995
        ],
        "effects\nthe": [
            1122085995
        ],
        "shown\nlisting": [
            1122085995
        ],
        "tolerations\nsystem": [
            1122085995
        ],
        "master\nnode": [
            1122085995
        ],
        "its\ntoleration": [
            1122085995
        ],
        "matches\nthe": [
            1122085995
        ],
        "taint\nsystem": [
            1122085995
        ],
        "pod\nmaster": [
            1122085995
        ],
        "node\ntaint:\nnode-rolekubernetes.io\n/master:noschedule\ntoleration:\nnode-role.kubernetes.io\n/master:noschedule\nregular": [
            1122085995
        ],
        "pod\nregular": [
            1122085995
        ],
        "taints\nno": [
            1122085995
        ],
        "tolerations\npods": [
            1122085995
        ],
        "tolerations\nmay": [
            1122085995
        ],
        "taints\nfigure": [
            1122085995
        ],
        "tolerates": [
            1122085995
        ],
        "\n\n460chapter": [
            1122085995
        ],
        "yaml)": [
            1122085995
        ],
        "noexecute\ninstead": [
            1122085995
        ],
        "noschedule": [
            1122085995
        ],
        "exist:\nnoschedule": [
            1122085995
        ],
        "tol-\nerate": [
            1122085995
        ],
        "taint\nprefernoschedule": [
            1122085995
        ],
        "soft": [
            1122085995
        ],
        "will\ntry": [
            1122085995
        ],
        "\nnoexecute": [
            1122085995
        ],
        "prefernoschedule": [
            1122085995
        ],
        "schedul-\ning": [
            1122085995
        ],
        "\nnoexecute": [
            1122085995
        ],
        "taint\nto": [
            1122085995
        ],
        "the\nnoexecute": [
            1122085995
        ],
        "\n161.2": [
            1122085995
        ],
        "node\nimagine": [
            1122085995
        ],
        "non-\nproduction": [
            1122085995
        ],
        "workloads": [
            1122085995
        ],
        "utmost": [
            1122085995
        ],
        "non-production": [
            1122085995
        ],
        "never\nrun": [
            1122085995
        ],
        "node1k8s": [
            1122085995
        ],
        "node-type=production:noschedule\nnode": [
            1122085995
        ],
        "node1k8s\"": [
            1122085995
        ],
        "tainted\nthis": [
            1122085995
        ],
        "node-type": [
            1122085995
        ],
        "99999\ndeployment": [
            1122085995
        ],
        "node\ntest-196686-46ngl": [
            1122085995
        ],
        "12s": [
            1122085995
        ],
        "1047.0.1": [
            1122085995
        ],
        "node2k8s\ntest-196686-73p89": [
            1122085995
        ],
        "1047.0.7": [
            1122085995
        ],
        "node2k8s\ntest-196686-77280": [
            1122085995
        ],
        "1047.0.6": [
            1122085995
        ],
        "node2k8s\ntest-196686-h9m8f": [
            1122085995
        ],
        "1047.0.5": [
            1122085995
        ],
        "node2k8s\ntest-196686-p85ll": [
            1122085995
        ],
        "1047.0.4": [
            1122085995
        ],
        "node2k8s\nnow": [
            1122085995
        ],
        "\n161.3": [
            1122085995
        ],
        "taint\nyou": [
            1122085995
        ],
        "toleration\n": [
            1122085995
        ],
        "\n\n461using": [
            1122085995
        ],
        "nodes\napiversion:": [
            1122085995
        ],
        "prod\nspec:\n": [
            1122085995
        ],
        "tolerations:\n": [
            1122085995
        ],
        "effect:": [
            1122085995
        ],
        "production\nnode": [
            1122085995
        ],
        "node\nprod-350605-1ph5h": [
            1122085995
        ],
        "16s": [
            1122085995
        ],
        "1044.0.3": [
            1122085995
        ],
        "node1k8s\nprod-350605-ctqcr": [
            1122085995
        ],
        "node2k8s\nprod-350605-f7pcc": [
            1122085995
        ],
        "17s": [
            1122085995
        ],
        "1044.0.6": [
            1122085995
        ],
        "node1k8s\nprod-350605-k7c8g": [
            1122085995
        ],
        "1047.0.9": [
            1122085995
        ],
        "node2k8s\nprod-350605-rp1nv": [
            1122085995
        ],
        "1044.0.4": [
            1122085995
        ],
        "node1k8s\nas": [
            1122085995
        ],
        "\nnode-type=non-production:noschedule": [
            1122085995
        ],
        "then\nyou’d": [
            1122085995
        ],
        "pods\n16.1.4": [
            1122085995
        ],
        "tolera-\ntions": [
            1122085995
        ],
        "\nequal": [
            1122085995
        ],
        "one)": [
            1122085995
        ],
        "specific\ntaint": [
            1122085995
        ],
        "\nexists": [
            1122085995
        ],
        "operator\nusing": [
            1122085995
        ],
        "scheduling\ntaints": [
            1122085995
        ],
        "(noschedule": [
            1122085995
        ],
        "effect)": [
            1122085995
        ],
        "to\ndefine": [
            1122085995
        ],
        "unpreferred": [
            1122085995
        ],
        "(\nprefernoschedule": [
            1122085995
        ],
        "pods\nfrom": [
            1122085995
        ],
        "(\nnoexecute)\n": [
            1122085995
        ],
        "could\npartition": [
            1122085995
        ],
        "partitions": [
            1122085995
        ],
        "to\nschedule": [
            1122085995
        ],
        "tolerations\nlisting": [
            1122085995
        ],
        "toleration:": [
            1122085995
        ],
        "production-deploymentyaml\nlisting": [
            1122085995
        ],
        "node1\nthis": [
            1122085995
        ],
        "\nproduction": [
            1122085995
        ],
        "\n\n462chapter": [
            1122085995
        ],
        "scheduling\nwhen": [
            1122085995
        ],
        "it\nconfiguring": [
            1122085995
        ],
        "rescheduled\nyou": [
            1122085995
        ],
        "before\nrescheduling": [
            1122085995
        ],
        "becomes\nunready": [
            1122085995
        ],
        "see\ntwo": [
            1122085995
        ],
        "prod-350605-1ph5h": [
            1122085995
        ],
        "yaml\n..\n": [
            1122085995
        ],
        "noexecute": [
            1122085995
        ],
        "nodealpha.kubernetes.io/notready": [
            1122085995
        ],
        "tolerationseconds:": [
            1122085995
        ],
        "nodealpha.kubernetes.io/unreachable": [
            1122085995
        ],
        "unreach-\nable\n": [
            1122085995
        ],
        "if\nthat": [
            1122085995
        ],
        "five-minute": [
            1122085995
        ],
        "spec\nnotethis": [
            1122085995
        ],
        "versions\nof": [
            1122085995
        ],
        "taint-based": [
            1122085995
        ],
        "enable\nthem": [
            1122085995
        ],
        "\n--feature-gates=taint-\nbasedevictions=true\n": [
            1122085995
        ],
        "option\n16.2": [
            1122085995
        ],
        "nodes\ncomparing": [
            1122085995
        ],
        "node-\nselector\n": [
            1122085995
        ],
        "everything\nthat": [
            1122085995
        ],
        "introduced\nlisting": [
            1122085995
        ],
        "rescheduled\nthe": [
            1122085995
        ],
        "unreachable\n": [
            1122085995
        ],
        "\n\n463using": [
            1122085995
        ],
        "nodes\nnode": [
            1122085995
        ],
        "rules\n": [
            1122085995
        ],
        "these\nallow": [
            1122085995
        ],
        "prefer-\nence": [
            1122085995
        ],
        "kubernetes\nwill": [
            1122085995
        ],
        "choose\none": [
            1122085995
        ],
        "labels\nnode": [
            1122085995
        ],
        "do\nbefore": [
            1122085995
        ],
        "are\nthey’re": [
            1122085995
        ],
        "gke-kubia-default-pool-db274c5a-mjnf\nname:": [
            1122085995
        ],
        "gke-kubia-default-pool-db274c5a-mjnf\nrole:\nlabels:": [
            1122085995
        ],
        "betakubernetes.io/fluentd-ds-ready=true\n": [
            1122085995
        ],
        "betakubernetes.io/instance-type=f1-micro\n": [
            1122085995
        ],
        "cloudgoogle.com/gke-nodepool=default-pool\n": [
            1122085995
        ],
        "failure-domainbeta.kubernetes.io/region=europe-west1": [
            1122085995
        ],
        "failure-domainbeta.kubernetes.io/zone=europe-west1-d": [
            1122085995
        ],
        "kubernetesio/hostname=gke-kubia-default-pool-db274c5a-mjnf": [
            1122085995
        ],
        "to\nnode": [
            1122085995
        ],
        "those\nthree": [
            1122085995
        ],
        "follows:\nfailure-domainbeta.kubernetes.io/region": [
            1122085995
        ],
        "geographical": [
            1122085995
        ],
        "region\nthe": [
            1122085995
        ],
        "in\nfailure-domain.beta.kubernetes.io/zone": [
            1122085995
        ],
        "in\nkubernetes.io/hostname": [
            1122085995
        ],
        "hostname\nthese": [
            1122085995
        ],
        "already\nlearned": [
            1122085995
        ],
        "selector\nyou": [
            1122085995
        ],
        "node\nselector": [
            1122085995
        ],
        "rules\n16.2.1": [
            1122085995
        ],
        "rules\nin": [
            1122085995
        ],
        "requires\na": [
            1122085995
        ],
        "field\nshown": [
            1122085995
        ],
        "gke\nthese": [
            1122085995
        ],
        "three\nlabels": [
            1122085995
        ],
        "important\nones": [
            1122085995
        ],
        "affinity\n": [
            1122085995
        ],
        "\n\n464chapter": [
            1122085995
        ],
        "scheduling\napiversion:": [
            1122085995
        ],
        "kubia-gpu\nspec:\n": [
            1122085995
        ],
        "that\ninclude": [
            1122085995
        ],
        "rule\nthe": [
            1122085995
        ],
        "affinity:\n": [
            1122085995
        ],
        "nodeaffinity:\n": [
            1122085995
        ],
        "requiredduringschedulingignoredduringexecution:\n": [
            1122085995
        ],
        "nodeselectorterms:\n": [
            1122085995
        ],
        "matchexpressions:\n": [
            1122085995
        ],
        "gpu\n": [
            1122085995
        ],
        "values:\n": [
            1122085995
        ],
        "true\"\nthe": [
            1122085995
        ],
        "\nmaking": [
            1122085995
        ],
        "nodeaffinity": [
            1122085995
        ],
        "name\nas": [
            1122085995
        ],
        "node-\naffinity\n": [
            1122085995
        ],
        "mean:\nrequiredduringscheduling..": [
            1122085995
        ],
        "node\n...ignoredduringexecution": [
            1122085995
        ],
        "don’t\naffect": [
            1122085995
        ],
        "cur-\nrently": [
            1122085995
        ],
        "node\nthat’s": [
            1122085995
        ],
        "\nignoredduringexecution": [
            1122085995
        ],
        "eventu-\nally": [
            1122085995
        ],
        "\nrequiredduringexecution": [
            1122085995
        ],
        "kubia-gpu-nodeselectoryaml\nlisting": [
            1122085995
        ],
        "kubia-gpu-nodeaffinityyaml\nthis": [
            1122085995
        ],
        "\n\n465using": [
            1122085995
        ],
        "nodes\nremove": [
            1122085995
        ],
        "be\nevicted": [
            1122085995
        ],
        "let’s\nnot": [
            1122085995
        ],
        "longer\nunderstanding": [
            1122085995
        ],
        "nodeselectorterms\nby": [
            1122085995
        ],
        "understand\nthat": [
            1122085995
        ],
        "\nnodeselectorterms": [
            1122085995
        ],
        "which\nexpressions": [
            1122085995
        ],
        "expression": [
            1122085995
        ],
        "a\ngpu": [
            1122085995
        ],
        "162.\nnow": [
            1122085995
        ],
        "prioritize": [
            1122085995
        ],
        "nodes\nduring": [
            1122085995
        ],
        "next\n16.2.2": [
            1122085995
        ],
        "prioritizing": [
            1122085995
        ],
        "\npreferredduringschedulingignoredduringexecution": [
            1122085995
        ],
        "countries": [
            1122085995
        ],
        "datacenter\nrepresents": [
            1122085995
        ],
        "meant\nonly": [
            1122085995
        ],
        "\nzone1": [
            1122085995
        ],
        "gpu\npod\nnode": [
            1122085995
        ],
        "affinity\nrequired": [
            1122085995
        ],
        "label:\ngpu=true\npod\nno": [
            1122085995
        ],
        "affinity\ngpu:": [
            1122085995
        ],
        "true\nnode": [
            1122085995
        ],
        "gpunode": [
            1122085995
        ],
        "gpu\ngpu:": [
            1122085995
        ],
        "true\nthis": [
            1122085995
        ],
        "only\nto": [
            1122085995
        ],
        "gpu=true": [
            1122085995
        ],
        "label\nthis": [
            1122085995
        ],
        "\nscheduled": [
            1122085995
        ],
        "\n\n466chapter": [
            1122085995
        ],
        "scheduling\nmachines": [
            1122085995
        ],
        "have\nenough": [
            1122085995
        ],
        "from\nbeing": [
            1122085995
        ],
        "your\npartners": [
            1122085995
        ],
        "zones": [
            1122085995
        ],
        "that\nlabeling": [
            1122085995
        ],
        "nodes\nfirst": [
            1122085995
        ],
        "that\ndesignates": [
            1122085995
        ],
        "marking": [
            1122085995
        ],
        "a\ndedicated": [
            1122085995
        ],
        "two\nworker": [
            1122085995
        ],
        "nodes)": [
            1122085995
        ],
        "other\nmulti-node": [
            1122085995
        ],
        "it\nruns": [
            1122085995
        ],
        "node\nfirst": [
            1122085995
        ],
        "availability-zone=zone1\nnode": [
            1122085995
        ],
        "share-type=dedicated\nnode": [
            1122085995
        ],
        "node2k8s": [
            1122085995
        ],
        "availability-zone=zone2\nnode": [
            1122085995
        ],
        "node2k8s\"": [
            1122085995
        ],
        "share-type=shared\nnode": [
            1122085995
        ],
        "availability-zone": [
            1122085995
        ],
        "share-type\nname": [
            1122085995
        ],
        "share-type\nmasterk8s": [
            1122085995
        ],
        "v16.4": [
            1122085995
        ],
        "<none>\nnode1k8s": [
            1122085995
        ],
        "zone1": [
            1122085995
        ],
        "dedicated\nnode2k8s": [
            1122085995
        ],
        "zone2": [
            1122085995
        ],
        "shared\nspecifying": [
            1122085995
        ],
        "preferential": [
            1122085995
        ],
        "rules\nwith": [
            1122085995
        ],
        "dedicated\nnodes": [
            1122085995
        ],
        "pref\nspec:\n": [
            1122085995
        ],
        "nodeaffinity:\nlisting": [
            1122085995
        ],
        "preferred-deploymentyaml\n": [
            1122085995
        ],
        "\n\n467using": [
            1122085995
        ],
        "preferredduringschedulingignoredduringexecution:": [
            1122085995
        ],
        "weight:": [
            1122085995
        ],
        "preference:": [
            1122085995
        ],
        "share-type": [
            1122085995
        ],
        "..\nlet’s": [
            1122085995
        ],
        "preference": [
            1122085995
        ],
        "of\na": [
            1122085995
        ],
        "labels\navailability-zone=zone1": [
            1122085995
        ],
        "share-type=dedicated": [
            1122085995
        ],
        "first\npreference": [
            1122085995
        ],
        "\nweight": [
            1122085995
        ],
        "(\nweight": [
            1122085995
        ],
        "20)\nunderstanding": [
            1122085995
        ],
        "work\nif": [
            1122085995
        ],
        "163.\nnodes": [
            1122085995
        ],
        "\navailability-zone": [
            1122085995
        ],
        "ranked": [
            1122085995
        ],
        "weights": [
            1122085995
        ],
        "affinity\nrules": [
            1122085995
        ],
        "\nshared": [
            1122085995
        ],
        "nodes\nyou’re\nspecifying\npreferences\nnot": [
            1122085995
        ],
        "hard\nrequirements\nyou": [
            1122085995
        ],
        "\npreference\nyou": [
            1122085995
        ],
        "\ndedicated": [
            1122085995
        ],
        "\nfour": [
            1122085995
        ],
        "\nthan": [
            1122085995
        ],
        "preference\nnode\ntop": [
            1122085995
        ],
        "priority\navailability": [
            1122085995
        ],
        "1\npod\npriority:": [
            1122085995
        ],
        "2priority:": [
            1122085995
        ],
        "3priority:": [
            1122085995
        ],
        "affinity\npreferred": [
            1122085995
        ],
        "labels:\navail-zone:zone1": [
            1122085995
        ],
        "(weight": [
            1122085995
        ],
        "80)\nshare:dedicated": [
            1122085995
        ],
        "20)\navail-zone:zone1\nshare:dedicated\nnode\navail-zone:zone1\nshare:shared\nnode\navailability": [
            1122085995
        ],
        "2\navail-zone:zone2\nshare:dedicated\nnode\navail-zone:zone2\nshare:shared\nthis": [
            1122085995
        ],
        "are\npreferred": [
            1122085995
        ],
        "labels\nfigure": [
            1122085995
        ],
        "preferences\n": [
            1122085995
        ],
        "\n\n468chapter": [
            1122085995
        ],
        "scheduling\ndeploying": [
            1122085995
        ],
        "cluster\nif": [
            1122085995
        ],
        "not\nall)": [
            1122085995
        ],
        "\nnode1": [
            1122085995
        ],
        "true\n$": [
            1122085995
        ],
        "node\npref-607515-1rnwv": [
            1122085995
        ],
        "4m": [
            1122085995
        ],
        "node2k8s\npref-607515-27wp0": [
            1122085995
        ],
        "1044.0.8": [
            1122085995
        ],
        "node1k8s\npref-607515-5xd0z": [
            1122085995
        ],
        "1044.0.5": [
            1122085995
        ],
        "node1k8s\npref-607515-jx9wt": [
            1122085995
        ],
        "node1k8s\npref-607515-mlgqm": [
            1122085995
        ],
        "node1k8s\nout": [
            1122085995
        ],
        "landed": [
            1122085995
        ],
        "one\nlanded": [
            1122085995
        ],
        "\nnode2": [
            1122085995
        ],
        "land": [
            1122085995
        ],
        "node1?": [
            1122085995
        ],
        "pri-\noritization": [
            1122085995
        ],
        "\nselector-\nspreadpriority\n": [
            1122085995
        ],
        "or\nservice": [
            1122085995
        ],
        "\nnode2\n": [
            1122085995
        ],
        "to\nnode2": [
            1122085995
        ],
        "been\nspread": [
            1122085995
        ],
        "evenly\n16.3": [
            1122085995
        ],
        "co-locating": [
            1122085995
        ],
        "anti-affinity\nyou’ve": [
            1122085995
        ],
        "sometimes\nyou’d": [
            1122085995
        ],
        "pods\ndeployed": [
            1122085995
        ],
        "near": [
            1122085995
        ],
        "the\napp": [
            1122085995
        ],
        "node\nrack,": [
            1122085995
        ],
        "kubernetes\ndeploy": [
            1122085995
        ],
        "pods\nclose": [
            1122085995
        ],
        "with\nan": [
            1122085995
        ],
        "example\n16.3.1": [
            1122085995
        ],
        "node\nyou’ll": [
            1122085995
        ],
        "app=backend": [
            1122085995
        ],
        "999999\ndeployment": [
            1122085995
        ],
        "backend\"": [
            1122085995
        ],
        "scheduled\n": [
            1122085995
        ],
        "\n\n469co-locating": [
            1122085995
        ],
        "anti-affinity\nthis": [
            1122085995
        ],
        "the\napp=backend": [
            1122085995
        ],
        "you’ll\nuse": [
            1122085995
        ],
        "\npodaffinity": [
            1122085995
        ],
        "frontend\nspec:\n": [
            1122085995
        ],
        "podaffinity:": [
            1122085995
        ],
        "requiredduringschedulingignoredduringexecution:": [
            1122085995
        ],
        "topologykey:": [
            1122085995
        ],
        "kubernetesio/hostname": [
            1122085995
        ],
        "labelselector:": [
            1122085995
        ],
        "requirement\nto": [
            1122085995
        ],
        "\ntopologykey": [
            1122085995
        ],
        "that\nhave": [
            1122085995
        ],
        "\napp=backend": [
            1122085995
        ],
        "164).\nlisting": [
            1122085995
        ],
        "frontend-podaffinity-hostyaml\ndefining": [
            1122085995
        ],
        "rules\ndefining": [
            1122085995
        ],
        "\nrequirement": [
            1122085995
        ],
        "preference\nthe": [
            1122085995
        ],
        "selector\nall": [
            1122085995
        ],
        "to\nsome": [
            1122085995
        ],
        "nodeother": [
            1122085995
        ],
        "nodes\nfrontend": [
            1122085995
        ],
        "pods\nbackend\npod\npod": [
            1122085995
        ],
        "affinity\nlabel": [
            1122085995
        ],
        "selector:app=backend\ntopology": [
            1122085995
        ],
        "key:hostname\napp:": [
            1122085995
        ],
        "backend\nfigure": [
            1122085995
        ],
        "are\n": [
            1122085995
        ],
        "\n\n470chapter": [
            1122085995
        ],
        "scheduling\nnoteinstead": [
            1122085995
        ],
        "\nmatchexpressions": [
            1122085995
        ],
        "field\ndeploying": [
            1122085995
        ],
        "affinity\nbefore": [
            1122085995
        ],
        "node\nbackend-257820-qhqj6": [
            1122085995
        ],
        "8m": [
            1122085995
        ],
        "node2k8s\nwhen": [
            1122085995
        ],
        "frontend-podaffinity-hostyaml\ndeployment": [
            1122085995
        ],
        "frontend\"": [
            1122085995
        ],
        "node2k8s\nfrontend-121895-2c1ts": [
            1122085995
        ],
        "13s": [
            1122085995
        ],
        "node2k8s\nfrontend-121895-776m7": [
            1122085995
        ],
        "node2k8s\nfrontend-121895-7ffsm": [
            1122085995
        ],
        "1047.0.8": [
            1122085995
        ],
        "node2k8s\nfrontend-121895-fpgm6": [
            1122085995
        ],
        "node2k8s\nfrontend-121895-vb9ll": [
            1122085995
        ],
        "node2k8s\nall": [
            1122085995
        ],
        "\nlabelselector": [
            1122085995
        ],
        "podaffinity": [
            1122085995
        ],
        "node\nunderstanding": [
            1122085995
        ],
        "rules\nwhat’s": [
            1122085995
        ],
        "sched-\nule": [
            1122085995
        ],
        "(the\nrules": [
            1122085995
        ],
        "back-\nend": [
            1122085995
        ],
        "accident": [
            1122085995
        ],
        "fron-\ntend": [
            1122085995
        ],
        "scheduler’s": [
            1122085995
        ],
        "lines\n...": [
            1122085995
        ],
        "attempting": [
            1122085995
        ],
        "default/backend-257820-qhqj6\n..": [
            1122085995
        ],
        "backend-qhqj6": [
            1122085995
        ],
        "node2k8s:": [
            1122085995
        ],
        "score:": [
            1122085995
        ],
        "(10)\nlisting": [
            1122085995
        ],
        "node2\n": [
            1122085995
        ],
        "\n\n471co-locating": [
            1122085995
        ],
        "anti-affinity\n..": [
            1122085995
        ],
        "node1k8s:": [
            1122085995
        ],
        "(10)\n..": [
            1122085995
        ],
        "interpodaffinitypriority": [
            1122085995
        ],
        "(0)\n..": [
            1122085995
        ],
        "selectorspreadpriority": [
            1122085995
        ],
        "nodeaffinitypriority": [
            1122085995
        ],
        "=>": [
            1122085995
        ],
        "100030\n..": [
            1122085995
        ],
        "100022\n..": [
            1122085995
        ],
        "backend-257820-qhqj6": [
            1122085995
        ],
        "node2k8s\nif": [
            1122085995
        ],
        "\n163.2": [
            1122085995
        ],
        "\ngeographic": [
            1122085995
        ],
        "region\nin": [
            1122085995
        ],
        "backend\npod—for": [
            1122085995
        ],
        "\nco-locating": [
            1122085995
        ],
        "zone\nthe": [
            1122085995
        ],
        "speak": [
            1122085995
        ],
        "i’d\nneed": [
            1122085995
        ],
        "to\nchange": [
            1122085995
        ],
        "failure-domainbeta.kubernetes.io/zone.": [
            1122085995
        ],
        "region\nto": [
            1122085995
        ],
        "region": [
            1122085995
        ],
        "(cloud\nproviders": [
            1122085995
        ],
        "regions": [
            1122085995
        ],
        "split\ninto": [
            1122085995
        ],
        "region)": [
            1122085995
        ],
        "to\nfailure-domainbeta.kubernetes.io/region.\nunderstanding": [
            1122085995
        ],
        "topologykey": [
            1122085995
        ],
        "works\nthe": [
            1122085995
        ],
        "aren’t\nspecial": [
            1122085995
        ],
        "prerequisite": [
            1122085995
        ],
        "\nrack\nlabel": [
            1122085995
        ],
        "165.\n": [
            1122085995
        ],
        "ten": [
            1122085995
        ],
        "as\nrack=rack1": [
            1122085995
        ],
        "rack=rack2": [
            1122085995
        ],
        "podaffinity\nyou’d": [
            1122085995
        ],
        "\ntoplogykey": [
            1122085995
        ],
        "\npod-\naffinity\n": [
            1122085995
        ],
        "nodes\nthey’re": [
            1122085995
        ],
        "the\ntopologykey": [
            1122085995
        ],
        "\n\n472chapter": [
            1122085995
        ],
        "scheduling\nmatches": [
            1122085995
        ],
        "selector\nmatched": [
            1122085995
        ],
        "\nrack": [
            1122085995
        ],
        "\nrack2": [
            1122085995
        ],
        "only\nselect": [
            1122085995
        ],
        "\nrack=rack2": [
            1122085995
        ],
        "label\nnoteby": [
            1122085995
        ],
        "label-\nselector\n\n16.3.3": [
            1122085995
        ],
        "expressing": [
            1122085995
        ],
        "requirements\nearlier": [
            1122085995
        ],
        "to\nexpress": [
            1122085995
        ],
        "any-\nwhere": [
            1122085995
        ],
        "pod\naffinity": [
            1122085995
        ],
        "listing\nfrontend": [
            1122085995
        ],
        "the\nbackend": [
            1122085995
        ],
        "1\nrack": [
            1122085995
        ],
        "1\nrack:": [
            1122085995
        ],
        "rack1\nnode": [
            1122085995
        ],
        "2\nrack:": [
            1122085995
        ],
        "3\n..\nrack:": [
            1122085995
        ],
        "10\nrack:": [
            1122085995
        ],
        "11\nrack": [
            1122085995
        ],
        "rack2\nnode": [
            1122085995
        ],
        "12\nrack:": [
            1122085995
        ],
        "rack2\n..\nnode": [
            1122085995
        ],
        "20\nrack:": [
            1122085995
        ],
        "rack2\nbackend\npod\napp:": [
            1122085995
        ],
        "backend\nfrontend": [
            1122085995
        ],
        "(required)\nlabel": [
            1122085995
        ],
        "key:rack\nfigure": [
            1122085995
        ],
        "\n\n473co-locating": [
            1122085995
        ],
        "anti-affinity\napiversion:": [
            1122085995
        ],
        "podaffinity:\n": [
            1122085995
        ],
        "podaffinityterm:": [
            1122085995
        ],
        "weight": [
            1122085995
        ],
        "labelselector": [
            1122085995
        ],
        "hard-requirement\npodaffinity": [
            1122085995
        ],
        "scenario\ndeploying": [
            1122085995
        ],
        "\nnodeaffinity": [
            1122085995
        ],
        "listing)\nlisting": [
            1122085995
        ],
        "preference\npreferred": [
            1122085995
        ],
        "\nrequired\na": [
            1122085995
        ],
        "\nprevious": [
            1122085995
        ],
        "example\nthe": [
            1122085995
        ],
        "prefer\nnode": [
            1122085995
        ],
        "well\nnode": [
            1122085995
        ],
        "2\nbackend\npod\napp:": [
            1122085995
        ],
        "(preferred)\nlabel": [
            1122085995
        ],
        "key:hostname\nhostname:": [
            1122085995
        ],
        "node2hostname:": [
            1122085995
        ],
        "node1\nfigure": [
            1122085995
        ],
        "\n\n474chapter": [
            1122085995
        ],
        "scheduling\n$": [
            1122085995
        ],
        "node\nbackend-257820-ssrgj": [
            1122085995
        ],
        "1h": [
            1122085995
        ],
        "node2k8s\nfrontend-941083-3mff9": [
            1122085995
        ],
        "node1k8s\nfrontend-941083-7fp7d": [
            1122085995
        ],
        "node2k8s\nfrontend-941083-cq23b": [
            1122085995
        ],
        "node2k8s\nfrontend-941083-m70sw": [
            1122085995
        ],
        "node2k8s\nfrontend-941083-wsjv8": [
            1122085995
        ],
        "node2k8s\n16.3.4": [
            1122085995
        ],
        "co-locate": [
            1122085995
        ],
        "want\nthe": [
            1122085995
        ],
        "called\npod": [
            1122085995
        ],
        "the\npodantiaffinity": [
            1122085995
        ],
        "scheduler\nnever": [
            1122085995
        ],
        "\npodantiaffinity’s": [
            1122085995
        ],
        "167.\nan": [
            1122085995
        ],
        "inter-\nfere": [
            1122085995
        ],
        "another\nexample": [
            1122085995
        ],
        "never\nbrings": [
            1122085995
        ],
        "preferences\nthese": [
            1122085995
        ],
        "running\nsome": [
            1122085995
        ],
        "nodes\npods\npod:": [
            1122085995
        ],
        "foo\npod(required)anti-affinity\nlabel": [
            1122085995
        ],
        "selector:app=foo\ntopology": [
            1122085995
        ],
        "foo\nfigure": [
            1122085995
        ],
        "\n\n475co-locating": [
            1122085995
        ],
        "anti-affinity\nusing": [
            1122085995
        ],
        "deployment\nlet’s": [
            1122085995
        ],
        "configured\napiversion:": [
            1122085995
        ],
        "podantiaffinity:": [
            1122085995
        ],
        "podantiaffinity": [
            1122085995
        ],
        "app=frontend": [
            1122085995
        ],
        "node\nfrontend-286632-0lffz": [
            1122085995
        ],
        "<none>\nfrontend-286632-2rkcz": [
            1122085995
        ],
        "node2k8s\nfrontend-286632-4nwhp": [
            1122085995
        ],
        "<none>\nfrontend-286632-h4686": [
            1122085995
        ],
        "<none>\nfrontend-286632-st222": [
            1122085995
        ],
        "scheduled—one": [
            1122085995
        ],
        "the\nthree": [
            1122085995
        ],
        "\npending": [
            1122085995
        ],
        "schedule\nthem": [
            1122085995
        ],
        "nodes\nusing": [
            1122085995
        ],
        "anti-affinity\nin": [
            1122085995
        ],
        "the\npreferredduringschedulingignoredduringexecution": [
            1122085995
        ],
        "not\nsuch": [
            1122085995
        ],
        "where\nthat’s": [
            1122085995
        ],
        "\nrequiredduringscheduling": [
            1122085995
        ],
        "anti-affinity:": [
            1122085995
        ],
        "frontend-podantiaffinity-hostyaml\nlisting": [
            1122085995
        ],
        "label\ndefining": [
            1122085995
        ],
        "hard-\nrequirements": [
            1122085995
        ],
        "anti-affinity\na": [
            1122085995
        ],
        "\nmachine": [
            1122085995
        ],
        "\napp=frontend": [
            1122085995
        ],
        "\n\n476chapter": [
            1122085995
        ],
        "scheduling\n": [
            1122085995
        ],
        "custom\nnode": [
            1122085995
        ],
        "labels\n16.4": [
            1122085995
        ],
        "or\nare": [
            1122085995
        ],
        "that\nif": [
            1122085995
        ],
        "they\ntolerate": [
            1122085995
        ],
        "taint\nthree": [
            1122085995
        ],
        "prefer-\nnoschedule\n": [
            1122085995
        ],
        "strict": [
            1122085995
        ],
        "should\nwait": [
            1122085995
        ],
        "unreach-\nable": [
            1122085995
        ],
        "unready\nnode": [
            1122085995
        ],
        "it\ncan": [
            1122085995
        ],
        "express": [
            1122085995
        ],
        "preference\npod": [
            1122085995
        ],
        "where\nanother": [
            1122085995
        ],
        "labels)": [
            1122085995
        ],
        "\npod": [
            1122085995
        ],
        "affinity’s": [
            1122085995
        ],
        "(onto": [
            1122085995
        ],
        "availability\nzone": [
            1122085995
        ],
        "region)\npod": [
            1122085995
        ],
        "hard\nrequirements": [
            1122085995
        ],
        "preferences\nin": [
            1122085995
        ],
        "environment\n": [
            1122085995
        ],
        "\n\n477\nbest": [
            1122085995
        ],
        "practices\nfor": [
            1122085995
        ],
        "apps\nwe’ve": [
            1122085995
        ],
        "point\nof": [
            1122085995
        ],
        "misunderstandings": [
            1122085995
        ],
        "explain\nthings": [
            1122085995
        ],
        "few\nadditional": [
            1122085995
        ],
        "point\nthis": [
            1122085995
        ],
        "\nappear": [
            1122085995
        ],
        "application\nadding": [
            1122085995
        ],
        "post-start": [
            1122085995
        ],
        "pre-stop": [
            1122085995
        ],
        "hooks\nproperly": [
            1122085995
        ],
        "breaking": [
            1122085995
        ],
        "requests\nmaking": [
            1122085995
        ],
        "kubernetes\nusing": [
            1122085995
        ],
        "init": [
            1122085995
        ],
        "pod\ndeveloping": [
            1122085995
        ],
        "minikube\n": [
            1122085995
        ],
        "\n\n478chapter": [
            1122085995
        ],
        "17best": [
            1122085995
        ],
        "apps\n171": [
            1122085995
        ],
        "together\nlet’s": [
            1122085995
        ],
        "a\nchance": [
            1122085995
        ],
        "big\npicture": [
            1122085995
        ],
        "application\na": [
            1122085995
        ],
        "statefulset\nobjects": [
            1122085995
        ],
        "service(s)": [
            1122085995
        ],
        "container\nprovides": [
            1122085995
        ],
        "services\nare": [
            1122085995
        ],
        "nodeport-type": [
            1122085995
        ],
        "types\nof": [
            1122085995
        ],
        "secrets—those": [
            1122085995
        ],
        "those\nused": [
            1122085995
        ],
        "are\nusually": [
            1122085995
        ],
        "to\nserviceaccounts": [
            1122085995
        ],
        "\ndefined": [
            1122085995
        ],
        "developer\npod": [
            1122085995
        ],
        "template\ndeployment\nlabels\npod(s)\nlabel": [
            1122085995
        ],
        "selector\nlabels\ncreated": [
            1122085995
        ],
        "runtime\ncreated": [
            1122085995
        ],
        "beforehand\ncontainer(s)\nvolume(s)\nreplicaset(s)\nendpoints\n•": [
            1122085995
        ],
        "probes\n•": [
            1122085995
        ],
        "variables\n•": [
            1122085995
        ],
        "mounts\n•": [
            1122085995
        ],
        "reqs/limits\nhorizontal\npodautoscaler\nstatefulset\ndaemonset\njob\ncronjob\npersistent\nvolume\nconfigmap\nservice\npersistent\nvolume\nclaim\nsecret(s)\nservice\naccount\nstorage\nclass\nlimitrange\nresourcequota\ningress\nimagepullsecret\nfigure": [
            1122085995
        ],
        "application\n": [
            1122085995
        ],
        "\n\n479understanding": [
            1122085995
        ],
        "lifecycle\n": [
            1122085995
        ],
        "to\ninitialize": [
            1122085995
        ],
        "whereas\npods": [
            1122085995
        ],
        "refer-\nenced": [
            1122085995
        ],
        "upfront": [
            1122085995
        ],
        "daemon-\nsets": [
            1122085995
        ],
        "sysad-\nmins": [
            1122085995
        ],
        "horizontalpodautoscalers\nare": [
            1122085995
        ],
        "resourcequota\nobjects": [
            1122085995
        ],
        "a\nwhole)": [
            1122085995
        ],
        "created\nby": [
            1122085995
        ],
        "daemonset)\ncontrollers\n": [
            1122085995
        ],
        "this\ndoesn’t": [
            1122085995
        ],
        "most\nresources": [
            1122085995
        ],
        "for\nmanagement": [
            1122085995
        ],
        "center": [
            1122085995
        ],
        "arguably": [
            1122085995
        ],
        "know\nhow": [
            1122085995
        ],
        "last\nclose": [
            1122085995
        ],
        "pods—this": [
            1122085995
        ],
        "\n172": [
            1122085995
        ],
        "lifecycle\nwe’ve": [
            1122085995
        ],
        "single\napplication": [
            1122085995
        ],
        "significant": [
            1122085995
        ],
        "relocate": [
            1122085995
        ],
        "this\naspect": [
            1122085995
        ],
        "next\n17.2.1": [
            1122085995
        ],
        "relocated\noutside": [
            1122085995
        ],
        "seldom": [
            1122085995
        ],
        "moves": [
            1122085995
        ],
        "and\nmanually": [
            1122085995
        ],
        "kubernetes\napps": [
            1122085995
        ],
        "automatically—no": [
            1122085995
        ],
        "operator\n": [
            1122085995
        ],
        "\n\n480chapter": [
            1122085995
        ],
        "apps\nreconfigures": [
            1122085995
        ],
        "means\napplication": [
            1122085995
        ],
        "relatively\noften": [
            1122085995
        ],
        "change\nwhen": [
            1122085995
        ],
        "(technically": [
            1122085995
        ],
        "replac-\ning": [
            1122085995
        ],
        "relocated)": [
            1122085995
        ],
        "any\nadverse": [
            1122085995
        ],
        "nevertheless": [
            1122085995
        ],
        "that\nto": [
            1122085995
        ],
        "membership": [
            1122085995
        ],
        "a\nclustered": [
            1122085995
        ],
        "basing": [
            1122085995
        ],
        "statefulset\nexpecting": [
            1122085995
        ],
        "disappear\nanother": [
            1122085995
        ],
        "be\navailable": [
            1122085995
        ],
        "involve\nany": [
            1122085995
        ],
        "computationally": [
            1122085995
        ],
        "startup": [
            1122085995
        ],
        "startups": [
            1122085995
        ],
        "make\nthe": [
            1122085995
        ],
        "scanning": [
            1122085995
        ],
        "results\nto": [
            1122085995
        ],
        "files\nare": [
            1122085995
        ],
        "all\nlost": [
            1122085995
        ],
        "172).\n": [
            1122085995
        ],
        "or\nbecause": [
            1122085995
        ],
        "the\noomkiller": [
            1122085995
        ],
        "is\ncompletely": [
            1122085995
        ],
        "again;": [
            1122085995
        ],
        "restarts\nwhen": [
            1122085995
        ],
        "the\nintensive": [
            1122085995
        ],
        "data\nlike": [
            1122085995
        ],
        "pod-scoped": [
            1122085995
        ],
        "live\nand": [
            1122085995
        ],
        "reuse": [
            1122085995
        ],
        "173).\n": [
            1122085995
        ],
        "\n\n481understanding": [
            1122085995
        ],
        "lifecycle\ncontainer\nprocess\nwrites": [
            1122085995
        ],
        "to\nfilesystem\nwritable": [
            1122085995
        ],
        "layer\nread-only": [
            1122085995
        ],
        "layer\nimage": [
            1122085995
        ],
        "layers\ncontainer": [
            1122085995
        ],
        "crashes\nor": [
            1122085995
        ],
        "killed\npod\nnew": [
            1122085995
        ],
        "container\nnew": [
            1122085995
        ],
        "process\nfilesystem\nnew": [
            1122085995
        ],
        "layers\nnew": [
            1122085995
        ],
        "started\n(part": [
            1122085995
        ],
        "pod)\nnew": [
            1122085995
        ],
        "container\nstarts": [
            1122085995
        ],
        "new\nwriteable": [
            1122085995
        ],
        "layer:\nall": [
            1122085995
        ],
        "lost\nfigure": [
            1122085995
        ],
        "restarted\ncontainer\nprocess\nwrites": [
            1122085995
        ],
        "to\ncan": [
            1122085995
        ],
        "files\nfilesystem\nvolumemount\ncontainer": [
            1122085995
        ],
        "process\nfilesystem\nvolumemount\nnew": [
            1122085995
        ],
        "preserved\nin": [
            1122085995
        ],
        "volume\nvolume\nfigure": [
            1122085995
        ],
        "restarts\n": [
            1122085995
        ],
        "\n\n482chapter": [
            1122085995
        ],
        "apps\nusing": [
            1122085995
        ],
        "sometimes\nbut": [
            1122085995
        ],
        "corrupted": [
            1122085995
        ],
        "process\nto": [
            1122085995
        ],
        "the\ncrashloopbackoff": [
            1122085995
        ],
        "status)": [
            1122085995
        ],
        "start\nfrom": [
            1122085995
        ],
        "double-edged": [
            1122085995
        ],
        "sword": [
            1122085995
        ],
        "carefully": [
            1122085995
        ],
        "about\nwhether": [
            1122085995
        ],
        "not\n17.2.2": [
            1122085995
        ],
        "indefinitely\nthe": [
            1122085995
        ],
        "minutes\nduring": [
            1122085995
        ],
        "intervals": [
            1122085995
        ],
        "container’s\nprocess": [
            1122085995
        ],
        "anymore\n": [
            1122085995
        ],
        "removed\nand": [
            1122085995
        ],
        "174).\nyou’d": [
            1122085995
        ],
        "crashing\nbecause": [
            1122085995
        ],
        "node-related": [
            1122085995
        ],
        "sadly\nthat": [
            1122085995
        ],
        "dead—all": [
            1122085995
        ],
        "it\nreplicaset\ndesired": [
            1122085995
        ],
        "3\nactual": [
            1122085995
        ],
        "3\nonly": [
            1122085995
        ],
        "actually\nperforming": [
            1122085995
        ],
        "jobs\nthird": [
            1122085995
        ],
        "crashing\nwith": [
            1122085995
        ],
        "between\nrestarts": [
            1122085995
        ],
        "(crashloopbackoff)\nwe": [
            1122085995
        ],
        "want\nthree": [
            1122085995
        ],
        "pods\npod\nrunning\ncontainer\npod\nrunning\ncontainer\npod\ndead\ncontainer\nfigure": [
            1122085995
        ],
        "reschedule": [
            1122085995
        ],
        "\n\n483understanding": [
            1122085995
        ],
        "lifecycle\ncares": [
            1122085995
        ],
        "replicaset\nwhose": [
            1122085995
        ],
        "replicaset-crashingpodsyaml": [
            1122085995
        ],
        "code\narchive)": [
            1122085995
        ],
        "age\ncrashing-pods-f1tcd": [
            1122085995
        ],
        "\ncrashing-pods-k7l6k": [
            1122085995
        ],
        "6m\ncrashing-pods-z7l3v": [
            1122085995
        ],
        "6m\n$": [
            1122085995
        ],
        "crashing-pods\nname:": [
            1122085995
        ],
        "crashing-pods\nreplicas:": [
            1122085995
        ],
        "crashing-pods-f1tcd\nname:": [
            1122085995
        ],
        "crashing-pods-f1tcd\nnamespace:": [
            1122085995
        ],
        "minikube/192168.99.102\nstart": [
            1122085995
        ],
        "02": [
            1122085995
        ],
        "mar": [
            1122085995
        ],
        "14:02:23": [
            1122085995
        ],
        "+0100\nlabels:": [
            1122085995
        ],
        "app=crashing-pods\nstatus:": [
            1122085995
        ],
        "understandable": [
            1122085995
        ],
        "be\nrestarted": [
            1122085995
        ],
        "be\nresolved": [
            1122085995
        ],
        "rationale": [
            1122085995
        ],
        "likely\nwouldn’t": [
            1122085995
        ],
        "and\nall": [
            1122085995
        ],
        "\n172.3": [
            1122085995
        ],
        "order\none": [
            1122085995
        ],
        "started\nwhen": [
            1122085995
        ],
        "multi-pod": [
            1122085995
        ],
        "built-\nin": [
            1122085995
        ],
        "app\nand": [
            1122085995
        ],
        "your\nlisting": [
            1122085995
        ],
        "crashing\nthe": [
            1122085995
        ],
        "is\ndelaying": [
            1122085995
        ],
        "crashing\nno": [
            1122085995
        ],
        "replicas\nthree": [
            1122085995
        ],
        "\nshown": [
            1122085995
        ],
        "\nrunning\nkubectl": [
            1122085995
        ],
        "\n\n484chapter": [
            1122085995
        ],
        "apps\nwhole": [
            1122085995
        ],
        "pods\nservices,": [
            1122085995
        ],
        "the\norder": [
            1122085995
        ],
        "have\nno": [
            1122085995
        ],
        "precondition": [
            1122085995
        ],
        "is\nmet": [
            1122085995
        ],
        "name\nsuggests": [
            1122085995
        ],
        "pod—this": [
            1122085995
        ],
        "container(s)\n": [
            1122085995
        ],
        "and\nonly": [
            1122085995
        ],
        "means\ninit": [
            1122085995
        ],
        "container(s)—for\nexample": [
            1122085995
        ],
        "met": [
            1122085995,
            1043891123
        ],
        "service\nrequired": [
            1122085995
        ],
        "container\nterminates": [
            1122085995
        ],
        "container(s)": [
            1122085995
        ],
        "7?": [
            1122085995
        ],
        "imagine\nyou": [
            1122085995
        ],
        "\nfortune-client": [
            1122085995
        ],
        "checks\nwhether": [
            1122085995
        ],
        "container\nkeeps": [
            1122085995
        ],
        "retrying": [
            1122085995
        ],
        "main\ncontainer": [
            1122085995
        ],
        "start\nadding": [
            1122085995
        ],
        "pod\ninit": [
            1122085995
        ],
        "the\nspecinitcontainers": [
            1122085995
        ],
        "fortune-client": [
            1122085995
        ],
        "defined\nspec:\n": [
            1122085995
        ],
        "initcontainers:": [
            1122085995
        ],
        "init\n": [
            1122085995
        ],
        "command:\n": [
            1122085995
        ],
        "sh\n": [
            1122085995
        ],
        "-c\n": [
            1122085995
        ],
        "up..;": [
            1122085995
        ],
        "http://fortune": [
            1122085995
        ],
        "/dev/null": [
            1122085995
        ],
        ">/dev/null": [
            1122085995
        ],
        "2>/dev/null": [
            1122085995
        ],
        "break;": [
            1122085995
        ],
        "done;": [
            1122085995
        ],
        "up!": [
            1122085995
        ],
        "container\nlisting": [
            1122085995
        ],
        "fortune-clientyaml\nyou’re": [
            1122085995
        ],
        "\ncontainer\nthe": [
            1122085995
        ],
        "a\nloop": [
            1122085995
        ],
        "\n\n485understanding": [
            1122085995
        ],
        "lifecycle\nwhen": [
            1122085995
        ],
        "pod’s\nstatus": [
            1122085995
        ],
        "age\nfortune-client": [
            1122085995
        ],
        "init:0/1": [
            1122085995
        ],
        "logs:\n$": [
            1122085995
        ],
        "init\nwaiting": [
            1122085995
        ],
        "up..\nwhen": [
            1122085995
        ],
        "init\ncontainer": [
            1122085995
        ],
        "is\ninit": [
            1122085995
        ],
        "the\nfortune-server": [
            1122085995
        ],
        "fortune-serveryaml": [
            1122085995
        ],
        "dependencies\nyou’ve": [
            1122085995
        ],
        "con-\ntainer(s)": [
            1122085995
        ],
        "(making": [
            1122085995
        ],
        "service\nthey": [
            1122085995
        ],
        "go\noffline": [
            1122085995
        ],
        "dependencies\naren’t": [
            1122085995
        ],
        "so\nkubernetes": [
            1122085995
        ],
        "read-\niness": [
            1122085995
        ],
        "update\nthereby": [
            1122085995
        ],
        "\n172.4": [
            1122085995
        ],
        "hooks\nwe’ve": [
            1122085995
        ],
        "hook": [
            1122085995
        ],
        "hooks:\npost-start": [
            1122085995
        ],
        "hooks\npre-stop": [
            1122085995
        ],
        "hooks\nthese": [
            1122085995
        ],
        "starts\nand": [
            1122085995
        ],
        "either\nexecute": [
            1122085995
        ],
        "container\nperform": [
            1122085995
        ],
        "url\n": [
            1122085995
        ],
        "\n\n486chapter": [
            1122085995
        ],
        "apps\nlet’s": [
            1122085995
        ],
        "container\nlifecycle\nusing": [
            1122085995
        ],
        "hook\na": [
            1122085995
        ],
        "started\nyou": [
            1122085995
        ],
        "you’re\nthe": [
            1122085995
        ],
        "those\noperations": [
            1122085995
        ],
        "application\ndeveloped": [
            1122085995
        ],
        "can’t)": [
            1122085995
        ],
        "touch\nthe": [
            1122085995
        ],
        "may\ninitialize": [
            1122085995
        ],
        "job\n": [
            1122085995
        ],
        "somewhat\nmisleading": [
            1122085995
        ],
        "process\nhas": [
            1122085995
        ],
        "to\ncomplete": [
            1122085995
        ],
        "asynchronously": [
            1122085995
        ],
        "two\nways": [
            1122085995
        ],
        "\nwaiting": [
            1122085995
        ],
        "the\nreason": [
            1122085995
        ],
        "\ncontainercreating": [
            1122085995
        ],
        "container\nwill": [
            1122085995
        ],
        "pod-with-poststart-hook\nspec:\n": [
            1122085995
        ],
        "lifecycle:": [
            1122085995
        ],
        "poststart:": [
            1122085995
        ],
        "15;": [
            1122085995
        ],
        "5;": [
            1122085995
        ],
        "typically": [
            1122085995
        ],
        "hook:": [
            1122085995
        ],
        "post-start-hookyaml\nthe": [
            1122085995
        ],
        "starts\nit": [
            1122085995
        ],
        "the\npoststartsh\nscript": [
            1122085995
        ],
        "/bin\ndirectory": [
            1122085995
        ],
        "\n\n487understanding": [
            1122085995
        ],
        "lifecycle\nyou’ll": [
            1122085995
        ],
        "failedpoststarthook": [
            1122085995
        ],
        "listing\nfailedsync": [
            1122085995
        ],
        "syncing": [
            1122085995
        ],
        "skipping:": [
            1122085995
        ],
        "startcontainer\"": [
            1122085995
        ],
        "poststart": [
            1122085995
        ],
        "handler:": [
            1122085995
        ],
        "exited": [
            1122085995
        ],
        "15:": [
            1122085995
        ],
        "by\ndeploying": [
            1122085995
        ],
        "post-start-hook-httpgetyaml": [
            1122085995
        ],
        "archive)\nfailedsync": [
            1122085995
        ],
        "http://1032.0.2:9090/poststart:": [
            1122085995
        ],
        "dial": [
            1122085995
        ],
        "1032.0.2:9090:": [
            1122085995
        ],
        "getsockopt:": [
            1122085995
        ],
        "refused:": [
            1122085995
        ],
        "9090\ninstead": [
            1122085995
        ],
        "fails\nthe": [
            1122085995
        ],
        "command-based": [
            1122085995
        ],
        "with\nsomething": [
            1122085995
        ],
        "my-pod": [
            1122085995
        ],
        "logfiletxt": [
            1122085995
        ],
        "failed)\nthe": [
            1122085995
        ],
        "it\nusing": [
            1122085995
        ],
        "gracefully)": [
            1122085995
        ],
        "initiate": [
            1122085995
        ],
        "graceful": [
            1122085995
        ],
        "receipt": [
            1122085995
        ],
        "third-party\napp": [
            1122085995
        ],
        "modify)": [
            1122085995
        ],
        "a\npost-start": [
            1122085995
        ],
        "com-\nlisting": [
            1122085995
        ],
        "hook\nlisting": [
            1122085995
        ],
        "failed\n": [
            1122085995
        ],
        "\n\n488chapter": [
            1122085995
        ],
        "apps\nmand": [
            1122085995
        ],
        "lifecycle:\n": [
            1122085995
        ],
        "prestop:": [
            1122085995
        ],
        "http://\npod_ip:8080/shutdown": [
            1122085995
        ],
        "container\napart": [
            1122085995
        ],
        "scheme\n(http": [
            1122085995
        ],
        "httpheaders": [
            1122085995
        ],
        "localhost\nbecause": [
            1122085995
        ],
        "hook—an": [
            1122085995
        ],
        "terminated\nif": [
            1122085995
        ],
        "\nfailedprestophook": [
            1122085995
        ],
        "dele-\ntion": [
            1122085995
        ],
        "triggered": [
            1122085995
        ],
        "place)": [
            1122085995
        ],
        "proper\noperation": [
            1122085995
        ],
        "wit-\nnessed": [
            1122085995
        ],
        "developer\nwasn’t": [
            1122085995
        ],
        "sigterm": [
            1122085995
        ],
        "signal\nmany": [
            1122085995
        ],
        "sigterm\nsignal": [
            1122085995
        ],
        "isn’t\nreceived": [
            1122085995
        ],
        "sig-\nnal": [
            1122085995
        ],
        "be\neaten": [
            1122085995
        ],
        "process\n": [
            1122085995
        ],
        "be\nachieved": [
            1122085995
        ],
        "dockerfile:": [
            1122085995
        ],
        "[/mybinary\"]\ninstead": [
            1122085995
        ],
        "/mybinary\nlisting": [
            1122085995
        ],
        "snippet:": [
            1122085995
        ],
        "pre-stop-hook-httpgetyaml\nthis": [
            1122085995
        ],
        "\nhttp://pod_ip:8080/shutdown\n": [
            1122085995
        ],
        "\n\n489understanding": [
            1122085995
        ],
        "mybinary": [
            1122085995
        ],
        "process\nwhereas": [
            1122085995
        ],
        "\nmybinary": [
            1122085995
        ],
        "process\nexecuted": [
            1122085995
        ],
        "process\nunderstanding": [
            1122085995
        ],
        "lifecy-\ncle": [
            1122085995
        ],
        "(most": [
            1122085995
        ],
        "likely\nbecause": [
            1122085995
        ],
        "probe)": [
            1122085995
        ],
        "life-\ntime": [
            1122085995
        ],
        "\n172.5": [
            1122085995
        ],
        "shutdown\nwe’ve": [
            1122085995
        ],
        "touched": [
            1122085995
        ],
        "more\ndetail": [
            1122085995
        ],
        "for\nunderstanding": [
            1122085995
        ],
        "cleanly": [
            1122085995
        ],
        "\ndeletiontimestamp": [
            1122085995
        ],
        "it\npods": [
            1122085995
        ],
        "terminating\neach": [
            1122085995
        ],
        "but\nthe": [
            1122085995
        ],
        "grace": [
            1122085995
        ],
        "timer": [
            1122085995
        ],
        "performed:\n1run": [
            1122085995
        ],
        "finish\n2send": [
            1122085995
        ],
        "container\n3wait": [
            1122085995
        ],
        "shuts": [
            1122085995
        ],
        "grace\nperiod": [
            1122085995
        ],
        "out\n4forcibly": [
            1122085995
        ],
        "175.\npre-stop": [
            1122085995
        ],
        "process\ntermination": [
            1122085995
        ],
        "period\nmain": [
            1122085995
        ],
        "process\ncontainer": [
            1122085995
        ],
        "shutdown\ninitiated\ncontainer": [
            1122085995
        ],
        "running\ntime\nsigterm\nsigkill\nfigure": [
            1122085995
        ],
        "sequence\n": [
            1122085995
        ],
        "\n\n490chapter": [
            1122085995
        ],
        "apps\nspecifying": [
            1122085995
        ],
        "period\nthe": [
            1122085995
        ],
        "spec\nterminationgraceperiodseconds\n": [
            1122085995
        ],
        "fin-\nish": [
            1122085995
        ],
        "cleaning": [
            1122085995
        ],
        "--grace-period=5\nthis": [
            1122085995
        ],
        "when\nall": [
            1122085995
        ],
        "resource\nis": [
            1122085995
        ],
        "immediately\nwithout": [
            1122085995
        ],
        "the\n--force": [
            1122085995
        ],
        "--grace-period=0": [
            1122085995
        ],
        "--force\nbe": [
            1122085995
        ],
        "careful": [
            1122085995
        ],
        "same\ntime": [
            1122085995
        ],
        "same\npersistentvolume)": [
            1122085995
        ],
        "force-deleting": [
            1122085995
        ],
        "shut\ndown": [
            1122085995
        ],
        "malfunction": [
            1122085995
        ],
        "pods\nforcibly": [
            1122085995
        ],
        "and\ncan’t": [
            1122085995
        ],
        "reconnect)": [
            1122085995
        ],
        "the\napplication’s": [
            1122085995
        ],
        "shutdown\nprocedure\nimplementing": [
            1122085995
        ],
        "application\napplications": [
            1122085995
        ],
        "procedure\nand": [
            1122085995
        ],
        "app\nthen": [
            1122085995
        ],
        "predict": [
            1122085995
        ],
        "cleanly?": [
            1122085995
        ],
        "\n\n491understanding": [
            1122085995
        ],
        "lifecycle\npod": [
            1122085995
        ],
        "lost\nshould": [
            1122085995
        ],
        "migrating": [
            1122085995
        ],
        "(through\neither": [
            1122085995
        ],
        "hook)?": [
            1122085995
        ],
        "not!": [
            1122085995
        ],
        "reasons:\na": [
            1122085995
        ],
        "being\nterminated\nyou": [
            1122085995
        ],
        "process\nis": [
            1122085995
        ],
        "killed\nthis": [
            1122085995
        ],
        "node\nthen": [
            1122085995
        ],
        "won’t\neven": [
            1122085995
        ],
        "again)": [
            1122085995
        ],
        "procedure\nreplacing": [
            1122085995
        ],
        "procedures": [
            1122085995
        ],
        "pods\nhow": [
            1122085995
        ],
        "to\ncompletion": [
            1122085995
        ],
        "is\nmigrated": [
            1122085995
        ],
        "pods)?\n": [
            1122085995
        ],
        "(upon": [
            1122085995
        ],
        "signal)": [
            1122085995
        ],
        "new\njob": [
            1122085995
        ],
        "pod’s\ndata": [
            1122085995
        ],
        "single\ntime": [
            1122085995
        ],
        "finds\nthe": [
            1122085995
        ],
        "constantly\nrunning": [
            1122085995
        ],
        "remember\nscaling": [
            1122085995
        ],
        "never\nhappens": [
            1122085995
        ],
        "time)?": [
            1122085995
        ],
        "a\ndata-migrating": [
            1122085995
        ],
        "176).\nto": [
            1122085995
        ],
        "migration": [
            1122085995
        ],
        "occurring": [
            1122085995
        ],
        "data-\nmigrating": [
            1122085995
        ],
        "to\ncome": [
            1122085995
        ],
        "migration\n": [
            1122085995
        ],
        "\n\n492chapter": [
            1122085995
        ],
        "apps\n173": [
            1122085995
        ],
        "(clients": [
            1122085995
        ],
        "providing)": [
            1122085995
        ],
        "you\nobviously": [
            1122085995
        ],
        "to\nfollow": [
            1122085995
        ],
        "all\nconnections": [
            1122085995
        ],
        "up\n17.3.1": [
            1122085995
        ],
        "up\nensuring": [
            1122085995
        ],
        "an\nendpoint": [
            1122085995
        ],
        "may\nremember": [
            1122085995
        ],
        "ready\nuntil": [
            1122085995
        ],
        "requests\nfrom": [
            1122085995
        ],
        "considered\nready": [
            1122085995
        ],
        "immediately—as": [
            1122085995
        ],
        "kube-proxy\nupdates": [
            1122085995
        ],
        "“connec-\ntion": [
            1122085995
        ],
        "errors\n": [
            1122085995
        ],
        "to\nadd": [
            1122085995
        ],
        "many\npod\na-0\npod\na-1\nstatefulset": [
            1122085995
        ],
        "a\nreplicas:": [
            1122085995
        ],
        "2\nscale\ndown\npvc\na-0\npv\npvc\na-1\npv\npod\na-0\nstatefulset": [
            1122085995
        ],
        "1\ntransfers": [
            1122085995
        ],
        "to\nremaining": [
            1122085995
        ],
        "pod(s)\nconnects": [
            1122085995
        ],
        "to\norphaned": [
            1122085995
        ],
        "pvc\ndata-migrating\npod\njob\npvc\na-0\npv\npvc\na-1\npv\nfigure": [
            1122085995
        ],
        "\n\n493ensuring": [
            1122085995
        ],
        "properly\ncases": [
            1122085995
        ],
        "\n173.2": [
            1122085995
        ],
        "shut-down\nnow": [
            1122085995
        ],
        "life—when": [
            1122085995
        ],
        "and\nits": [
            1122085995
        ],
        "containers\nshould": [
            1122085995
        ],
        "its\npre-stop": [
            1122085995
        ],
        "executed)": [
            1122085995
        ],
        "properly?": [
            1122085995
        ],
        "signal?": [
            1122085995
        ],
        "con-\ntinue": [
            1122085995
        ],
        "requests?": [
            1122085995
        ],
        "but\nhaven’t": [
            1122085995
        ],
        "yet?": [
            1122085995
        ],
        "in\nbetween": [
            1122085995
        ],
        "connection)?\nbefore": [
            1122085995
        ],
        "of\nevents": [
            1122085995
        ],
        "deletion\nin": [
            1122085995
        ],
        "in-depth": [
            1122085995
        ],
        "let’s\nexplore": [
            1122085995
        ],
        "the\nstate": [
            1122085995
        ],
        "watchers": [
            1122085995
        ],
        "sequences": [
            1122085995
        ],
        "(marked": [
            1122085995
        ],
        "177.\na2.": [
            1122085995
        ],
        "stop\ncontainers\napi": [
            1122085995
        ],
        "serverkube-proxy\nkubelet\nworker": [
            1122085995
        ],
        "node\nendpoints\ncontroller\nkube-proxy\npod\n(containers)\nclient\ndelete\npod\nb1": [
            1122085995
        ],
        "deletion\nnotification\nb2": [
            1122085995
        ],
        "pod\nas": [
            1122085995
        ],
        "endpoint\na1": [
            1122085995
        ],
        "deletion\nnotification\nb3": [
            1122085995
        ],
        "endpoint\nmodification\nnotification\nb4": [
            1122085995
        ],
        "pod\nfromiptables\nb4": [
            1122085995
        ],
        "pod\nfromiptables\niptables\niptables\nworker": [
            1122085995
        ],
        "\n\n494chapter": [
            1122085995
        ],
        "apps\nin": [
            1122085995
        ],
        "notifica-\ntion": [
            1122085995
        ],
        "initiates": [
            1122085995
        ],
        "172.5": [
            1122085995
        ],
        "app\nresponds": [
            1122085995
        ],
        "ceasing": [
            1122085995
        ],
        "client\ntrying": [
            1122085995
        ],
        "direct\npath": [
            1122085995
        ],
        "kubelet\n": [
            1122085995
        ],
        "events—the": [
            1122085995
        ],
        "leading\nup": [
            1122085995
        ],
        "(sequence": [
            1122085995
        ],
        "figure)\nwhen": [
            1122085995
        ],
        "removes\nthe": [
            1122085995
        ],
        "watchers\nare": [
            1122085995
        ],
        "kube-proxies": [
            1122085995
        ],
        "then\nupdates": [
            1122085995
        ],
        "connections\nfrom": [
            1122085995
        ],
        "that\nremoving": [
            1122085995
        ],
        "connections—clients": [
            1122085995
        ],
        "are\nalready": [
            1122085995
        ],
        "through\nthose": [
            1122085995
        ],
        "connections\n": [
            1122085995
        ],
        "takes\nto": [
            1122085995
        ],
        "rules\nbeing": [
            1122085995
        ],
        "178)": [
            1122085995
        ],
        "first\nreach": [
            1122085995
        ],
        "and\na2": [
            1122085995
        ],
        "send\nsigterm\napi": [
            1122085995
        ],
        "server\napi": [
            1122085995
        ],
        "server\nkubelet\nendpoints\ncontroller\ncontainer(s)\na1": [
            1122085995
        ],
        "watch\nnotification\n(pod": [
            1122085995
        ],
        "modified)\nb1": [
            1122085995
        ],
        "modified)\nb2": [
            1122085995
        ],
        "ip\nfrom": [
            1122085995
        ],
        "endpoints\nkube-proxy\nb4": [
            1122085995
        ],
        "update\niptables\nrules\niptables\nkube-proxy\niptables\ntime\nb3": [
            1122085995
        ],
        "notification\n(endpoints": [
            1122085995
        ],
        "changed)\nfigure": [
            1122085995
        ],
        "timeline": [
            1122085995
        ],
        "\n\n495ensuring": [
            1122085995
        ],
        "properly\nthen": [
            1122085995
        ],
        "the\niptables": [
            1122085995
        ],
        "probability": [
            1122085995
        ],
        "well\nbefore": [
            1122085995
        ],
        "the\ntermination": [
            1122085995
        ],
        "closes": [
            1122085995
        ],
        "connections\nimmediately": [
            1122085995
        ],
        "errors\n(similar": [
            1122085995
        ],
        "capable": [
            1122085995
        ],
        "\nsolving": [
            1122085995
        ],
        "problem\ngoogling": [
            1122085995
        ],
        "probe\nto": [
            1122085995
        ],
        "supposedly": [
            1122085995
        ],
        "to\ncause": [
            1122085995
        ],
        "removal": [
            1122085995
        ],
        "would\nhappen": [
            1122085995
        ],
        "to\nreach": [
            1122085995
        ],
        "\nnull)": [
            1122085995
        ],
        "probe\nis": [
            1122085995
        ],
        "irrelevant\n": [
            1122085995
        ],
        "problem?": [
            1122085995
        ],
        "fully?\n": [
            1122085995
        ],
        "ter-\nmination": [
            1122085995
        ],
        "\niptables\nrules": [
            1122085995
        ],
        "or\nload": [
            1122085995
        ],
        "balancers": [
            1122085995
        ],
        "(\niptables)": [
            1122085995
        ],
        "client-side": [
            1122085995
        ],
        "wait\nuntil": [
            1122085995
        ],
        "them\ndoesn’t": [
            1122085995
        ],
        "respond?": [
            1122085995
        ],
        "response?": [
            1122085995
        ],
        "that\ntime": [
            1122085995
        ],
        "reasonable": [
            1122085995
        ],
        "enough?": [
            1122085995
        ],
        "suffice": [
            1122085995
        ],
        "overloaded": [
            1122085995
        ],
        "the\nnotification": [
            1122085995
        ],
        "solve\nthe": [
            1122085995
        ],
        "5-": [
            1122085995
        ],
        "10-second": [
            1122085995
        ],
        "the\nuser": [
            1122085995
        ],
        "overboard\n": [
            1122085995
        ],
        "\n\n496chapter": [
            1122085995
        ],
        "apps\nbecause": [
            1122085995
        ],
        "promptly": [
            1122085995
        ],
        "will\ncause": [
            1122085995
        ],
        "frus-\ntrating": [
            1122085995
        ],
        "pod\nwrapping": [
            1122085995
        ],
        "section\nto": [
            1122085995
        ],
        "recap—properly": [
            1122085995
        ],
        "steps:\nwait": [
            1122085995
        ],
        "\nclose": [
            1122085995
        ],
        "request\nwait": [
            1122085995
        ],
        "finish\nthen": [
            1122085995
        ],
        "completely\nto": [
            1122085995
        ],
        "carefully\nnot": [
            1122085995
        ],
        "perhaps\n": [
            1122085995
        ],
        "connections\ndelay": [
            1122085995
        ],
        "(few": [
            1122085995
        ],
        "seconds)\nkey:\nconnection\nrequest\niptablesrules\nupdated": [
            1122085995
        ],
        "nodes\n(no": [
            1122085995
        ],
        "connections\nafter": [
            1122085995
        ],
        "point)\nstop\naccepting": [
            1122085995
        ],
        "new\nconnections\nclose": [
            1122085995
        ],
        "inactive\nkeep-alive\nconnections\nand": [
            1122085995
        ],
        "for\nactive": [
            1122085995
        ],
        "finish\nwhen": [
            1122085995
        ],
        "last\nactive": [
            1122085995
        ],
        "request\ncompletes\nshut": [
            1122085995
        ],
        "down\ncompletely\ntime\nsigterm\nfigure": [
            1122085995
        ],
        "signal\n": [
            1122085995
        ],
        "\n\n497making": [
            1122085995
        ],
        "already\nensures": [
            1122085995
        ],
        "in-flight": [
            1122085995
        ],
        "all\nyou": [
            1122085995
        ],
        "need\n17.4": [
            1122085995
        ],
        "kubernetes\ni": [
            1122085995
        ],
        "nicely\nnow": [
            1122085995
        ],
        "kubernetes\n17.4.1": [
            1122085995
        ],
        "images\nwhen": [
            1122085995
        ],
        "app’s\nbinary": [
            1122085995
        ],
        "whole\nos": [
            1122085995
        ],
        "unnecessary\n": [
            1122085995
        ],
        "not\nmost": [
            1122085995
        ],
        "layering": [
            1122085995
        ],
        "undesirable\n": [
            1122085995
        ],
        "demands": [
            1122085995
        ],
        "small\nimages": [
            1122085995
        ],
        "cruft": [
            1122085995
        ],
        "your\nimages": [
            1122085995
        ],
        "go-based": [
            1122085995
        ],
        "perfect": [
            1122085995
        ],
        "for\nkubernetes\ntipuse": [
            1122085995
        ],
        "images\nbut": [
            1122085995
        ],
        "minimal": [
            1122085995
        ],
        "debug\nthe": [
            1122085995
        ],
        "\nping": [
            1122085995
        ],
        "similar\ninside": [
            1122085995
        ],
        "also\ninclude": [
            1122085995
        ],
        "what\nnot": [
            1122085995
        ],
        "you’ll\nneed": [
            1122085995
        ],
        "yourself\n17.4.2": [
            1122085995
        ],
        "wisely\nyou’ll": [
            1122085995
        ],
        "deployment)": [
            1122085995
        ],
        "version\nwhereas": [
            1122085995
        ],
        "tag\nmakes": [
            1122085995
        ],
        "again)\n": [
            1122085995
        ],
        "\n\n498chapter": [
            1122085995
        ],
        "apps\n": [
            1122085995
        ],
        "mandatory": [
            1122085995
        ],
        "designator": [
            1122085995
        ],
        "mutable": [
            1122085995
        ],
        "tags\n(you": [
            1122085995
        ],
        "big\ncaveat": [
            1122085995
        ],
        "run-\ntime": [
            1122085995
        ],
        "slows\ndown": [
            1122085995
        ],
        "mod-\nified": [
            1122085995
        ],
        "worse": [
            1122085995
        ],
        "contacted\n17.4.3": [
            1122085995
        ],
        "labels\ndon’t": [
            1122085995
        ],
        "multiple\nlabels": [
            1122085995
        ],
        "dimension": [
            1122085995
        ],
        "you\n(or": [
            1122085995
        ],
        "team)": [
            1122085995
        ],
        "increases\n": [
            1122085995
        ],
        "like\nthe": [
            1122085995
        ],
        "microservice)": [
            1122085995
        ],
        "to\napplication": [
            1122085995
        ],
        "tier": [
            1122085995
        ],
        "(front-end": [
            1122085995
        ],
        "back-end": [
            1122085995
        ],
        "on)\nenvironment": [
            1122085995
        ],
        "on)\nversion\ntype": [
            1122085995
        ],
        "green": [
            1122085995
        ],
        "blue": [
            1122085995
        ],
        "green/blue": [
            1122085995
        ],
        "on)\ntenant": [
            1122085995
        ],
        "name-\nspaces)\nshard": [
            1122085995
        ],
        "sharded": [
            1122085995
        ],
        "systems\nthis": [
            1122085995
        ],
        "belongs\n17.4.4": [
            1122085995
        ],
        "annotations\nto": [
            1122085995
        ],
        "least\nresources": [
            1122085995
        ],
        "annotation\nwith": [
            1122085995
        ],
        "the\nnames": [
            1122085995
        ],
        "dependen-\ncies": [
            1122085995
        ],
        "information\nand": [
            1122085995
        ],
        "(icon": [
            1122085995
        ],
        "but\nnothing": [
            1122085995
        ],
        "why\n17.4.5": [
            1122085995
        ],
        "terminated\nnothing": [
            1122085995
        ],
        "frustrating": [
            1122085995
        ],
        "terminated\n(or": [
            1122085995
        ],
        "continuously)": [
            1122085995
        ],
        "\n\n499making": [
            1122085995
        ],
        "kubernetes\nmoment": [
            1122085995
        ],
        "lives": [
            1122085995
        ],
        "the\nnecessary": [
            1122085995
        ],
        "triage": [
            1122085995
        ],
        "that\nmakes": [
            1122085995
        ],
        "status\nyou": [
            1122085995
        ],
        "without\neven": [
            1122085995
        ],
        "/dev/termination-log\nbut": [
            1122085995
        ],
        "\nterminationmessagepath": [
            1122085995
        ],
        "container\ndefinition": [
            1122085995
        ],
        "pod-with-termination-message\nspec:\n": [
            1122085995
        ],
        "/var/termination-reason": [
            1122085995
        ],
        "i've": [
            1122085995
        ],
        "crashloopbackoff\nif": [
            1122085995
        ],
        "having\nto": [
            1122085995
        ],
        "po\nname:": [
            1122085995
        ],
        "pod-with-termination-message\n..\ncontainers:\n...\n": [
            1122085995
        ],
        "waiting\n": [
            1122085995
        ],
        "crashloopbackoff\n": [
            1122085995
        ],
        "terminated\n": [
            1122085995
        ],
        "error\n": [
            1122085995
        ],
        "message:": [
            1122085995
        ],
        "ive": [
            1122085995
        ],
        "feb": [
            1122085995
        ],
        "21:38:31": [
            1122085995
        ],
        "+0100\nlisting": [
            1122085995
        ],
        "termination-messageyaml\nlisting": [
            1122085995
        ],
        "describe\nyou’re": [
            1122085995
        ],
        "\ntermination": [
            1122085995
        ],
        "just\nbefore": [
            1122085995
        ],
        "exiting\nyou": [
            1122085995
        ],
        "\ninspect": [
            1122085995
        ],
        "\n\n500chapter": [
            1122085995
        ],
        "6\nas": [
            1122085995
        ],
        "“i’ve": [
            1122085995
        ],
        "enough”": [
            1122085995
        ],
        "/var/ter-\nmination-reason": [
            1122085995
        ],
        "\nlast": [
            1122085995
        ],
        "a\ncompletable": [
            1122085995
        ],
        "termi-\nnation-message-successyaml).": [
            1122085995
        ],
        "that\na": [
            1122085995
        ],
        "app-specific": [
            1122085995
        ],
        "any\nsuch": [
            1122085995
        ],
        "plans": [
            1122085995
        ],
        "it\nnoteif": [
            1122085995
        ],
        "the\nterminationmessagepolicy": [
            1122085995
        ],
        "fallbacktologsonerror": [
            1122085995
        ],
        "message\n(but": [
            1122085995
        ],
        "unsuccessfully)\n17.4.6": [
            1122085995
        ],
        "logs\nwhile": [
            1122085995
        ],
        "reiterate": [
            1122085995
        ],
        "\nkubectl\nlogs\n": [
            1122085995
        ],
        "new\ncontainer’s": [
            1122085995
        ],
        "\n--previous\noption": [
            1122085995
        ],
        "logs\nif": [
            1122085995
        ],
        "log\nfile": [
            1122085995
        ],
        "approach:": [
            1122085995
        ],
        "<pod>": [
            1122085995
        ],
        "<logfile>\nthis": [
            1122085995
        ],
        "to\nkubectl": [
            1122085995
        ],
        "\ncopying": [
            1122085995
        ],
        "cp": [
            1122085995
        ],
        "command\nwhich": [
            1122085995
        ],
        "\nfoo-pod": [
            1122085995
        ],
        "/var/log/\nfoolog\n": [
            1122085995
        ],
        "foo-pod:/var/log/foolog": [
            1122085995
        ],
        "foolog\nto": [
            1122085995
        ],
        "localfile": [
            1122085995
        ],
        "foo-pod:/etc/remotefile\n": [
            1122085995
        ],
        "\n\n501making": [
            1122085995
        ],
        "/etc/remotefile": [
            1122085995
        ],
        "has\nmore": [
            1122085995
        ],
        "containername": [
            1122085995
        ],
        "logging\nin": [
            1122085995
        ],
        "solution\nso": [
            1122085995
        ],
        "(permanently)": [
            1122085995
        ],
        "trends": [
            1122085995
        ],
        "are\ndeleted": [
            1122085995
        ],
        "analysis": [
            1122085995
        ],
        "container\nlogs": [
            1122085995
        ],
        "few\nyaml/json": [
            1122085995
        ],
        "it’s\neven": [
            1122085995
        ],
        "stackdriver": [
            1122085995
        ],
        "done\n": [
            1122085995
        ],
        "elk": [
            1122085995
        ],
        "elasticsearch": [
            1122085995
        ],
        "logstash\nand": [
            1122085995
        ],
        "kibana": [
            1122085995
        ],
        "variation": [
            1122085995
        ],
        "efk": [
            1122085995
        ],
        "logstash": [
            1122085995
        ],
        "fluentd": [
            1122085995
        ],
        "pod-specific\ninformation": [
            1122085995
        ],
        "delivering": [
            1122085995
        ],
        "persistently\nelasticsearch": [
            1122085995
        ],
        "be\nviewed": [
            1122085995
        ],
        "analyzed": [
            1122085995
        ],
        "visualiz-\ning": [
            1122085995
        ],
        "figure\nnotein": [
            1122085995
        ],
        "charts\ncreated": [
            1122085995
        ],
        "logskibana\nweb\nbrowser\nfluentd\nnode": [
            1122085995
        ],
        "logs\nfluentd\nnode": [
            1122085995
        ],
        "logs\nfluentdelasticsearch\nfigure": [
            1122085995
        ],
        "kibana\n": [
            1122085995
        ],
        "\n\n502chapter": [
            1122085995
        ],
        "apps\nhandling": [
            1122085995
        ],
        "statements\nthe": [
            1122085995
        ],
        "elasticsearch\ndata": [
            1122085995
        ],
        "statements": [
            1122085995
        ],
        "spanning": [
            1122085995
        ],
        "lines\nsuch": [
            1122085995
        ],
        "traces": [
            1122085995
        ],
        "centralized\nlogging": [
            1122085995
        ],
        "text\nthis": [
            1122085995
        ],
        "multiline": [
            1122085995
        ],
        "single\nentry": [
            1122085995
        ],
        "human-friendly": [
            1122085995
        ],
        "outputting": [
            1122085995
        ],
        "human-readable": [
            1122085995
        ],
        "output\nwhile": [
            1122085995
        ],
        "requires\nconfiguring": [
            1122085995
        ],
        "sidecar\ncontainer": [
            1122085995
        ],
        "\n175": [
            1122085995
        ],
        "testing\nwe’ve": [
            1122085995
        ],
        "haven’t\ntalked": [
            1122085995
        ],
        "streamline\nthose": [
            1122085995
        ],
        "points\n17.5.1": [
            1122085995
        ],
        "development\nwhen": [
            1122085995
        ],
        "does\nthat": [
            1122085995
        ],
        "development?": [
            1122085995
        ],
        "re-deploy": [
            1122085995
        ],
        "painful\nluckily": [
            1122085995
        ],
        "trouble\n": [
            1122085995
        ],
        "isolated)": [
            1122085995
        ],
        "that—you": [
            1122085995
        ],
        "ide": [
            1122085995
        ],
        "backend_service\n_host\n": [
            1122085995
        ],
        "backend_service_port": [
            1122085995
        ],
        "coordi-\nnates": [
            1122085995
        ],
        "manu-\nally": [
            1122085995
        ],
        "or\ninside": [
            1122085995
        ],
        "least\ntemporarily)": [
            1122085995
        ],
        "\n\n503best": [
            1122085995
        ],
        "testing\nconnecting": [
            1122085995
        ],
        "server\nsimilarly": [
            1122085995
        ],
        "cluster\nduring": [
            1122085995
        ],
        "\nkubectl\ncp\n": [
            1122085995
        ],
        "port)\n": [
            1122085995
        ],
        "using\nhas": [
            1122085995
        ],
        "under\nrunning": [
            1122085995
        ],
        "what-\never": [
            1122085995
        ],
        "avoiding": [
            1122085995
        ],
        "time\ninstead": [
            1122085995
        ],
        "you\nbuild": [
            1122085995
        ],
        "(or\nnot": [
            1122085995
        ],
        "hot-redeploy": [
            1122085995
        ],
        "supported)": [
            1122085995
        ],
        "image\n17.5.2": [
            1122085995
        ],
        "development\nas": [
            1122085995
        ],
        "forces": [
            1122085995
        ],
        "kubernetes\nenvironment\n": [
            1122085995
        ],
        "minikube\ncluster": [
            1122085995
        ],
        "trying\nout": [
            1122085995
        ],
        "that\nmake": [
            1122085995
        ],
        "proper\nmulti-node": [
            1122085995
        ],
        "matter\nmounting": [
            1122085995
        ],
        "minikube\nvm": [
            1122085995
        ],
        "containers\nthrough": [
            1122085995
        ],
        "that\nin": [
            1122085995
        ],
        "https://githubcom/kubernetes/minikube/tree/\nmaster/docs.\nusing": [
            1122085995
        ],
        "images\nif": [
            1122085995
        ],
        "\n\n504chapter": [
            1122085995
        ],
        "apps\nminikube’s": [
            1122085995
        ],
        "docker_host": [
            1122085995
        ],
        "is\nrun": [
            1122085995
        ],
        "machine:\n$": [
            1122085995
        ],
        "eval": [
            1122085995
        ],
        "$(minikube": [
            1122085995
        ],
        "docker-env)\nthis": [
            1122085995
        ],
        "already\nstored": [
            1122085995
        ],
        "immedi-\nately": [
            1122085995
        ],
        "restarted\nbuilding": [
            1122085995
        ],
        "directly\nif": [
            1122085995
        ],
        "to\navoid": [
            1122085995
        ],
        "it\nover": [
            1122085995
        ],
        "(eval": [
            1122085995
        ],
        "docker-env)": [
            1122085995
        ],
        "load)\nas": [
            1122085995
        ],
        "the\nimagepullpolicy": [
            1122085995
        ],
        "you’ve\ncopied": [
            1122085995
        ],
        "over\ncombining": [
            1122085995
        ],
        "com-\nbine": [
            1122085995
        ],
        "other\nworkloads": [
            1122085995
        ],
        "of\nmiles": [
            1122085995
        ],
        "remote\ncluster": [
            1122085995
        ],
        "app\n17.5.3": [
            1122085995
        ],
        "versioning": [
            1122085995
        ],
        "auto-deploying": [
            1122085995
        ],
        "manifests\nbecause": [
            1122085995
        ],
        "to\nwhat": [
            1122085995
        ],
        "desire": [
            1122085995
        ],
        "take\nall": [
            1122085995
        ],
        "system\nenabling": [
            1122085995
        ],
        "audit": [
            1122085995
        ],
        "trail": [
            1122085995
        ],
        "changes\nwhenever": [
            1122085995
        ],
        "reflected": [
            1122085995
        ],
        "\n\n505best": [
            1122085995
        ],
        "testing\n": [
            1122085995
        ],
        "commit)": [
            1122085995
        ],
        "out\nyour": [
            1122085995
        ],
        "(vcs)": [
            1122085995
        ],
        "vcs\nwithout": [
            1122085995
        ],
        "at\nbox": [
            1122085995
        ],
        "coincidently": [
            1122085995
        ],
        "materials)\ndeveloped": [
            1122085995
        ],
        "\nkube-applier": [
            1122085995
        ],
        "described\nyou’ll": [
            1122085995
        ],
        "tool’s": [
            1122085995
        ],
        "https://githubcom/box/kube-applier.\n": [
            1122085995
        ],
        "branches": [
            1122085995
        ],
        "stag-\ning": [
            1122085995
        ],
        "cluster)\n17.5.4": [
            1122085995
        ],
        "\nmanifests\nwe’ve": [
            1122085995
        ],
        "writing\nyaml": [
            1122085995
        ],
        "explain\nto": [
            1122085995
        ],
        "finalizing": [
            1122085995
        ],
        "was\nannounced": [
            1122085995
        ],
        "jsonnet": [
            1122085995
        ],
        "templating": [
            1122085995
        ],
        "language\nfor": [
            1122085995
        ],
        "structures": [
            1122085995
        ],
        "it\nlets": [
            1122085995
        ],
        "parameterized": [
            1122085995
        ],
        "fragments": [
            1122085995
        ],
        "a\nfull": [
            1122085995
        ],
        "repeating": [
            1122085995
        ],
        "locations—much": [
            1122085995
        ],
        "a\nprogramming": [
            1122085995
        ],
        "much\nless": [
            1122085995
        ],
        "example\nlocal": [
            1122085995
        ],
        "./ksonnet-lib/ksonnet.beta.1/k.libsonnet\";\nlocal": [
            1122085995
        ],
        "kcore.v1.container;\nlocal": [
            1122085995
        ],
        "kapps.v1beta1.deployment;\nlocal": [
            1122085995
        ],
        "kubiacontainer": [
            1122085995
        ],
        "containerdefault(kubia\"": [
            1122085995
        ],
        "luksa/kubia:v1\")": [
            1122085995
        ],
        "containerhelpers.namedport(http\"": [
            1122085995
        ],
        "8080);": [
            1122085995
        ],
        "\ndeploymentdefault(kubia\"": [
            1122085995
        ],
        "kubiacontainer)": [
            1122085995
        ],
        "\ndeploymentmixin.spec.replicas(3)": [
            1122085995
        ],
        "kubiaksonnet": [
            1122085995
        ],
        "deployment\nmanifest": [
            1122085995
        ],
        "kubiaksonnet\nlisting": [
            1122085995
        ],
        "ksonnet:": [
            1122085995
        ],
        "kubiaksonnet\nthis": [
            1122085995
        ],
        "http\nthis": [
            1122085995
        ],
        "template\n": [
            1122085995
        ],
        "\n\n506chapter": [
            1122085995
        ],
        "apps\nthe": [
            1122085995
        ],
        "define\nyour": [
            1122085995
        ],
        "duplica-\ntion-free": [
            1122085995
        ],
        "at\nhttps://githubcom/ksonnet/ksonnet-lib.\n17.5.5": [
            1122085995
        ],
        "employing": [
            1122085995
        ],
        "\n(ci/cd)\nwe’ve": [
            1122085995
        ],
        "automating": [
            1122085995
        ],
        "sections\nback": [
            1122085995
        ],
        "ci/cd": [
            1122085995
        ],
        "in\none": [
            1122085995
        ],
        "clusters\n": [
            1122085995
        ],
        "(http://fabric8io)": [
            1122085995
        ],
        "integrated\ndevelopment": [
            1122085995
        ],
        "jenkins": [
            1122085995
        ],
        "open-\nsource": [
            1122085995
        ],
        "pipeline\nfor": [
            1122085995
        ],
        "devops-style": [
            1122085995
        ],
        "on\nkubernetes\n": [
            1122085995
        ],
        "labs": [
            1122085995
        ],
        "https://\ngithubcom/googlecloudplatform/continuous-deployment-on-kubernetes.\n17.6": [
            1122085995
        ],
        "insight": [
            1122085995
        ],
        "into\nhow": [
            1122085995
        ],
        "when\ndeployed": [
            1122085995
        ],
        "to\nshow": [
            1122085995
        ],
        "repre-\nsent": [
            1122085995
        ],
        "kubernetes\nmake": [
            1122085995
        ],
        "moved\nbetween": [
            1122085995
        ],
        "more\nfrequently\nhelp": [
            1122085995
        ],
        "you\nwill)": [
            1122085995
        ],
        "order\nintroduce": [
            1122085995
        ],
        "start\nof": [
            1122085995
        ],
        "met\nteach": [
            1122085995
        ],
        "them\ngain": [
            1122085995
        ],
        "eventual": [
            1122085995
        ],
        "model\nlearn": [
            1122085995
        ],
        "client\nconnections\n": [
            1122085995
        ],
        "\n\n507summary\ngive": [
            1122085995
        ],
        "tips": [
            1122085995
        ],
        "keep-\ning": [
            1122085995
        ],
        "sizes": [
            1122085995
        ],
        "all\nyour": [
            1122085995
        ],
        "terminated\nteach": [
            1122085995
        ],
        "mini-\nkube": [
            1122085995
        ],
        "cluster\nin": [
            1122085995
        ],
        "platform-as-a-service": [
            1122085995
        ],
        "\n\n508\nextending": [
            1122085995
        ],
        "kubernetes\nyou’re": [
            1122085995
        ],
        "api\nobjects": [
            1122085995
        ],
        "have\nextended": [
            1122085995
        ],
        "it\n18.1": [
            1122085995
        ],
        "objects\nthroughout": [
            1122085995
        ],
        "pro-\nvides": [
            1122085995
        ],
        "kubernetes\nusers": [
            1122085995
        ],
        "low-level\ngeneric": [
            1122085995
        ],
        "covers\nadding": [
            1122085995
        ],
        "kubernetes\ncreating": [
            1122085995
        ],
        "object\nadding": [
            1122085995
        ],
        "servers\nself-provisioning": [
            1122085995
        ],
        "catalog\nred": [
            1122085995
        ],
        "platform\ndeis": [
            1122085995
        ],
        "helm\n": [
            1122085995
        ],
        "\n\n509defining": [
            1122085995
        ],
        "ecosystem": [
            1122085995
        ],
        "evolves": [
            1122085995
        ],
        "high-level": [
            1122085995
        ],
        "objects\nwhich": [
            1122085995
        ],
        "specialized": [
            1122085995
        ],
        "today\ninstead": [
            1122085995
        ],
        "create\nand": [
            1122085995
        ],
        "custom\ncontroller": [
            1122085995
        ],
        "on\nthem": [
            1122085995
        ],
        "messaging": [
            1122085995
        ],
        "secrets\ndeployments,": [
            1122085995
        ],
        "kubernetes\nalready": [
            1122085995
        ],
        "\n181.1": [
            1122085995
        ],
        "customresourcedefinitions\nto": [
            1122085995
        ],
        "customresourcedefinition\nobject": [
            1122085995
        ],
        "(crd)": [
            1122085995
        ],
        "customresourcedefinition": [
            1122085995
        ],
        "crd": [
            1122085995
        ],
        "then\ncreate": [
            1122085995
        ],
        "resource\nnoteprior": [
            1122085995
        ],
        "third-\npartyresource": [
            1122085995
        ],
        "but\nwere": [
            1122085995
        ],
        "18.\ncreating": [
            1122085995
        ],
        "if\nthose": [
            1122085995
        ],
        "tangible": [
            1122085995
        ],
        "something\nbased": [
            1122085995
        ],
        "objects)": [
            1122085995
        ],
        "resources\nhave": [
            1122085995
        ],
        "adding\ninstances": [
            1122085995
        ],
        "example\nintroducing": [
            1122085995
        ],
        "customresourcedefinition\nlet’s": [
            1122085995
        ],
        "websites\nas": [
            1122085995
        ],
        "website’s": [
            1122085995
        ],
        "web-\nsite’s": [
            1122085995
        ],
        "(html": [
            1122085995
        ],
        "css": [
            1122085995
        ],
        "png": [
            1122085995
        ],
        "reposi-\ntory": [
            1122085995
        ],
        "website\nresource": [
            1122085995
        ],
        "181.\n": [
            1122085995
        ],
        "\n\n510chapter": [
            1122085995
        ],
        "18extending": [
            1122085995
        ],
        "kubernetes\nkind:": [
            1122085995
        ],
        "field\nand": [
            1122085995
        ],
        "called\ngitrepo": [
            1122085995
        ],
        "name)—it": [
            1122085995
        ],
        "the\nwebsite’s": [
            1122085995
        ],
        "yet\nwhat": [
            1122085995
        ],
        "because\nkubernetes": [
            1122085995
        ],
        "yet:\n$": [
            1122085995
        ],
        "imaginary-kubia-websiteyaml\nerror:": [
            1122085995
        ],
        "imaginary-kubia-websiteyaml\":": [
            1122085995
        ],
        "kind=website\nbefore": [
            1122085995
        ],
        "kubernetes\nrecognize": [
            1122085995
        ],
        "post\nthe": [
            1122085995
        ],
        "apiextensionsk8s.io/v1beta1": [
            1122085995
        ],
        "websitesextensions.example.com": [
            1122085995
        ],
        "scope:": [
            1122085995
        ],
        "imaginary": [
            1122085995
        ],
        "imaginary-kubia-websiteyaml\nlisting": [
            1122085995
        ],
        "website-crdyaml\nwebsite\nkind:": [
            1122085995
        ],
        "website\nmetadata:\nname:": [
            1122085995
        ],
        "kubia\nspec:\ngitrepo:\ngithubcom/.../kubia.git\npod:\nkubia-website\nservice:\nkubia-website\nfigure": [
            1122085995
        ],
        "pod\na": [
            1122085995
        ],
        "kind\nthe": [
            1122085995
        ],
        "\n(used": [
            1122085995
        ],
        "\nresulting": [
            1122085995
        ],
        "pod)\nthe": [
            1122085995
        ],
        "\nrepository": [
            1122085995
        ],
        "\nwebsite’s": [
            1122085995
        ],
        "files\ncustomresourcedefinitions": [
            1122085995
        ],
        "version\nthe": [
            1122085995
        ],
        "full\nname": [
            1122085995
        ],
        "of\nyour\ncustom\nobject\nyou": [
            1122085995
        ],
        "\n\n511defining": [
            1122085995
        ],
        "group:": [
            1122085995
        ],
        "extensionsexample.com": [
            1122085995
        ],
        "names:": [
            1122085995
        ],
        "singular:": [
            1122085995
        ],
        "plural:": [
            1122085995
        ],
        "of\ninstances": [
            1122085995
        ],
        "website-crdyaml": [
            1122085995
        ],
        "archive:\n$": [
            1122085995
        ],
        "website-crd-definitionyaml\ncustomresourcedefinition": [
            1122085995
        ],
        "websitesextensions.example.com\"": [
            1122085995
        ],
        "created\ni’m": [
            1122085995
        ],
        "website?\nthe": [
            1122085995
        ],
        "clashes": [
            1122085995
        ],
        "crd\n(which": [
            1122085995
        ],
        "crd)": [
            1122085995
        ],
        "you\nkeep": [
            1122085995
        ],
        "create\nyour": [
            1122085995
        ],
        "kind:\nwebsite\n": [
            1122085995
        ],
        "nameskind": [
            1122085995
        ],
        "extensionsexam-\nple.com\n": [
            1122085995
        ],
        "to\napps/v1beta1": [
            1122085995
        ],
        "(deployments\nbelong": [
            1122085995
        ],
        "group)": [
            1122085995
        ],
        "(v1beta1": [
            1122085995
        ],
        "deployments)": [
            1122085995
        ],
        "extensionsexample.com/v1.\ncreating": [
            1122085995
        ],
        "resource\nconsidering": [
            1122085995
        ],
        "extensionsexample.com/v1": [
            1122085995
        ],
        "https://githubcom/luksa/kubia-website-example.git\nthe": [
            1122085995
        ],
        "api\ngroup": [
            1122085995
        ],
        "customresourcedefinition\n": [
            1122085995
        ],
        "kubia-websiteyaml\nwebsite": [
            1122085995
        ],
        "kubia-websiteyaml\ndefine": [
            1122085995
        ],
        "resource\nyou": [
            1122085995
        ],
        "\nforms": [
            1122085995
        ],
        "name\nyour": [
            1122085995
        ],
        "version\nthis": [
            1122085995
        ],
        "\nwebsite": [
            1122085995
        ],
        "instance\n": [
            1122085995
        ],
        "\n\n512chapter": [
            1122085995
        ],
        "custom\nwebsite": [
            1122085995
        ],
        "resource\nlist": [
            1122085995
        ],
        "websites\nname": [
            1122085995
        ],
        "kind\nkubia": [
            1122085995
        ],
        "websitev1.extensions.example.com\nas": [
            1122085995
        ],
        "cus-\ntom": [
            1122085995
        ],
        "custom\nobject": [
            1122085995
        ],
        "extensionsexample.com/v1\nkind:": [
            1122085995
        ],
        "website\nmetadata:\n": [
            1122085995
        ],
        "2017-02-26t15:53:21z\n": [
            1122085995
        ],
        "57047\"\n": [
            1122085995
        ],
        "/apis/extensionsexample.com/v1/.../default/websites/kubia\n": [
            1122085995
        ],
        "b2eb6d99-fc3b-11e6-bd71-0800270a1c50\nspec:\n": [
            1122085995
        ],
        "https://githubcom/luksa/kubia-website-example.git\nnote": [
            1122085995
        ],
        "object\nobviously": [
            1122085995
        ],
        "also\ndelete": [
            1122085995
        ],
        "kubia\nwebsite": [
            1122085995
        ],
        "deleted\nnoteyou’re": [
            1122085995
        ],
        "crd\nresource": [
            1122085995
        ],
        "that\nfor": [
            1122085995
        ],
        "\nlet’s": [
            1122085995
        ],
        "object\nyou": [
            1122085995
        ],
        "make\nthem": [
            1122085995
        ],
        "\n\n513defining": [
            1122085995
        ],
        "data\ninstead": [
            1122085995
        ],
        "referenced\nin": [
            1122085995
        ],
        "next\n18.1.2": [
            1122085995
        ],
        "controllers\nto": [
            1122085995
        ],
        "the\ncreation": [
            1122085995
        ],
        "survives": [
            1122085995
        ],
        "controller’s\noperation": [
            1122085995
        ],
        "summarized": [
            1122085995
        ],
        "182.\ni’ve": [
            1122085995
        ],
        "to\nshow": [
            1122085995
        ],
        "crds": [
            1122085995
        ],
        "production-ready\nbecause": [
            1122085995
        ],
        "dockerio/luksa/\nwebsite-controller:latest": [
            1122085995
        ],
        "https://githubcom/luksa/k8s-\nwebsite-controller.": [
            1122085995
        ],
        "does\napi": [
            1122085995
        ],
        "server\nwebsites\nwebsite:\nkubia\ndeployments\ndeployment:\nkubia-website\nservices\nservice:\nkubia-website\nwebsite\ncontroller\nwatches\ncreates\nfigure": [
            1122085995
        ],
        "\nwatches": [
            1122085995
        ],
        "\ncreates": [
            1122085995
        ],
        "\n\n514chapter": [
            1122085995
        ],
        "does\nimmediately": [
            1122085995
        ],
        "requesting\nthe": [
            1122085995
        ],
        "url:\nhttp://localhost:8001/apis/extensionsexample.com/v1/websites?watch=true\nyou": [
            1122085995
        ],
        "port—the": [
            1122085995
        ],
        "which\nruns": [
            1122085995
        ],
        "authentication\n(see": [
            1122085995
        ],
        "183).\nthrough": [
            1122085995
        ],
        "send\nwatch": [
            1122085995
        ],
        "\nadded": [
            1122085995
        ],
        "event\nand": [
            1122085995
        ],
        "containers\n(shown": [
            1122085995
        ],
        "184):": [
            1122085995
        ],
        "synced": [
            1122085995
        ],
        "repo\nthe": [
            1122085995
        ],
        "volume\n(you": [
            1122085995
        ],
        "local\ndirectory": [
            1122085995
        ],
        "git\nrepo’s": [
            1122085995
        ],
        "startup;": [
            1122085995
        ],
        "the\ngit": [
            1122085995
        ],
        "afterward)": [
            1122085995
        ],
        "all\nnodes)": [
            1122085995
        ],
        "web-\nsite": [
            1122085995
        ],
        "port\npod:": [
            1122085995
        ],
        "website-controller\ncontainer:": [
            1122085995
        ],
        "main\nwebsite": [
            1122085995
        ],
        "controller\nget": [
            1122085995
        ],
        "http://localhost:8001/apis/extensions\nexample.com/v1/websites?watch=true\nget": [
            1122085995
        ],
        "https://kubernetes:443/apis/extensions\nexample.com/v1/websites?watch=true\nauthorization:": [
            1122085995
        ],
        "<token>\ncontainer:": [
            1122085995
        ],
        "proxy\nkubectl": [
            1122085995
        ],
        "proxy\napi": [
            1122085995
        ],
        "server\nfigure": [
            1122085995
        ],
        "container)\n": [
            1122085995
        ],
        "\n\n515defining": [
            1122085995
        ],
        "objects\nthe": [
            1122085995
        ],
        "website\nnotemy": [
            1122085995
        ],
        "oversimplified": [
            1122085995
        ],
        "it\nwatches": [
            1122085995
        ],
        "watch\nevents": [
            1122085995
        ],
        "only\nwatch": [
            1122085995
        ],
        "events\nwere": [
            1122085995
        ],
        "pod\nduring": [
            1122085995
        ],
        "a\nlocally": [
            1122085995
        ],
        "it\ninside": [
            1122085995
        ],
        "deployment\napiversion:": [
            1122085995
        ],
        "website-controller\nspec:\n": [
            1122085995
        ],
        "template:\nlisting": [
            1122085995
        ],
        "website-controlleryaml\npod\nwebserver\ncontainer\nweb": [
            1122085995
        ],
        "client\ngit-sync\ncontainer\nserves": [
            1122085995
        ],
        "to\nweb": [
            1122085995
        ],
        "port\nclones": [
            1122085995
        ],
        "repo\ninto": [
            1122085995
        ],
        "and\nkeeps": [
            1122085995
        ],
        "synced\nemptydir\nvolume\nfigure": [
            1122085995
        ],
        "object\nyou’ll": [
            1122085995
        ],
        "\ncontroller\n": [
            1122085995
        ],
        "\n\n516chapter": [
            1122085995
        ],
        "website-controller\n": [
            1122085995
        ],
        "website-controller": [
            1122085995
        ],
        "luksa/website-controller": [
            1122085995
        ],
        "one\ncontainer": [
            1122085995
        ],
        "container\nused": [
            1122085995
        ],
        "spe-\ncial": [
            1122085995
        ],
        "controller:\n$": [
            1122085995
        ],
        "website-controller\nserviceaccount": [
            1122085995
        ],
        "website-controller\"": [
            1122085995
        ],
        "created\nif": [
            1122085995
        ],
        "to\nallow": [
            1122085995
        ],
        "\nwebsite-controller": [
            1122085995
        ],
        "the\ncluster-admin": [
            1122085995
        ],
        "--serviceaccount=default:website-controller\nclusterrolebinding": [
            1122085995
        ],
        "created\nonce": [
            1122085995
        ],
        "deploy\nthe": [
            1122085995
        ],
        "action\nwith": [
            1122085995
        ],
        "listing)": [
            1122085995
        ],
        "has\nreceived": [
            1122085995
        ],
        "event\n$": [
            1122085995
        ],
        "website-controller-2429717411-q43zs": [
            1122085995
        ],
        "main\n2017/02/26": [
            1122085995
        ],
        "16:54:41": [
            1122085995
        ],
        "started\n2017/02/26": [
            1122085995
        ],
        "16:54:47": [
            1122085995
        ],
        "event:": [
            1122085995
        ],
        "added:": [
            1122085995
        ],
        "kubia:": [
            1122085995
        ],
        "https://githubc...\n2017/02/26": [
            1122085995
        ],
        "kubia-website": [
            1122085995
        ],
        "namespa..": [
            1122085995
        ],
        "\n2017/02/26": [
            1122085995
        ],
        "created\n2017/02/26": [
            1122085995
        ],
        "name..": [
            1122085995
        ],
        "controller\nit": [
            1122085995
        ],
        "\nserviceaccount\ntwo": [
            1122085995
        ],
        "\nmain": [
            1122085995
        ],
        "sidecar\n": [
            1122085995
        ],
        "\n\n517defining": [
            1122085995
        ],
        "\nkubia-website": [
            1122085995
        ],
        "a\n201": [
            1122085995
        ],
        "verify\nthat": [
            1122085995
        ],
        "deploysvc,po\nname": [
            1122085995
        ],
        "age\ndeploy/kubia-website": [
            1122085995
        ],
        "4s\ndeploy/website-controller": [
            1122085995
        ],
        "5m\nname": [
            1122085995
        ],
        "age\nsvc/kubernetes": [
            1122085995
        ],
        "1096.0.1": [
            1122085995
        ],
        "38d\nsvc/kubia-website": [
            1122085995
        ],
        "10101.48.23": [
            1122085995
        ],
        "80:32589/tcp": [
            1122085995
        ],
        "4s\nname": [
            1122085995
        ],
        "age\npo/kubia-website-1029415133-rs715": [
            1122085995
        ],
        "4s\npo/website-controller-1571685839-qzmg6": [
            1122085995
        ],
        "5m\nthere": [
            1122085995
        ],
        "\n32589": [
            1122085995
        ],
        "browser\nawesome": [
            1122085995
        ],
        "except\nyour": [
            1122085995
        ],
        "improvement": [
            1122085995
        ],
        "url\nthe": [
            1122085995
        ],
        "instance\nitself": [
            1122085995
        ],
        "exercise\n18.1.3": [
            1122085995
        ],
        "objects\nyou": [
            1122085995
        ],
        "of\ntheir": [
            1122085995
        ],
        "(except\nthe": [
            1122085995
        ],
        "metadata)": [
            1122085995
        ],
        "invalid\nwebsite": [
            1122085995
        ],
        "object\nthen": [
            1122085995
        ],
        "(\nkubectl)": [
            1122085995
        ],
        "the\nwatchers": [
            1122085995
        ],
        "validate\nthe": [
            1122085995
        ],
        "server)": [
            1122085995
        ],
        "have\nlisting": [
            1122085995
        ],
        "kubia-website\n": [
            1122085995
        ],
        "\n\n518chapter": [
            1122085995
        ],
        "unless\nthe": [
            1122085995
        ],
        "and\nreject": [
            1122085995
        ],
        "\ncustomresourcevalidation": [
            1122085995
        ],
        "gate": [
            1122085995
        ],
        "crd\n18.1.4": [
            1122085995
        ],
        "objects\na": [
            1122085995
        ],
        "aggregation\nin": [
            1122085995
        ],
        "main\nkubernetes": [
            1122085995
        ],
        "multiple\naggregated": [
            1122085995
        ],
        "the\naggregated": [
            1122085995
        ],
        "transparently": [
            1122085995
        ],
        "appropriate\napi": [
            1122085995
        ],
        "han-\ndle": [
            1122085995
        ],
        "scenes": [
            1122085995
        ],
        "may\neventually": [
            1122085995
        ],
        "single\nserver": [
            1122085995
        ],
        "aggregator": [
            1122085995
        ],
        "185.\nin": [
            1122085995
        ],
        "website\nobjects": [
            1122085995
        ],
        "validates\nthem": [
            1122085995
        ],
        "you’d\nimplement": [
            1122085995
        ],
        "it\nmain\napi": [
            1122085995
        ],
        "server\ncustom\napi": [
            1122085995
        ],
        "y\ncustom\napi": [
            1122085995
        ],
        "x\nkubectl\nuses": [
            1122085995
        ],
        "instance\nfor": [
            1122085995
        ],
        "resources\nuses": [
            1122085995
        ],
        "customresourcedefinitions\nin": [
            1122085995
        ],
        "storage\nmechanism\netcd\netcd\nfigure": [
            1122085995
        ],
        "aggregation\n": [
            1122085995
        ],
        "\n\n519extending": [
            1122085995
        ],
        "catalog\ncan": [
            1122085995
        ],
        "creating\ninstances": [
            1122085995
        ],
        "example\nregistering": [
            1122085995
        ],
        "server\nto": [
            1122085995
        ],
        "apiservice": [
            1122085995
        ],
        "apiregistrationk8s.io/v1beta1": [
            1122085995
        ],
        "v1alpha1extensions.example.com\nspec:\n": [
            1122085995
        ],
        "v1alpha1": [
            1122085995
        ],
        "priority:": [
            1122085995
        ],
        "150\n": [
            1122085995
        ],
        "website-api": [
            1122085995
        ],
        "sent\nto": [
            1122085995
        ],
        "\nextensionsexample.com\napi": [
            1122085995
        ],
        "pod(s)\nexposed": [
            1122085995
        ],
        "\nwebsite-api": [
            1122085995
        ],
        "clients\nwhile": [
            1122085995
        ],
        "add\ndedicated": [
            1122085995
        ],
        "allows\ncreating": [
            1122085995
        ],
        "resource-specific": [
            1122085995
        ],
        "other\nfeatures": [
            1122085995
        ],
        "extending": [
            1122085995
        ],
        "intensively": [
            1122085995
        ],
        "so\nthey": [
            1122085995
        ],
        "the\nsubject": [
            1122085995
        ],
        "repos": [
            1122085995
        ],
        "http://githubcom/kubernetes.\n18.2": [
            1122085995
        ],
        "\ncatalog\none": [
            1122085995
        ],
        "hot": [
            1122085995
        ],
        "topic\nin": [
            1122085995
        ],
        "(here": [
            1122085995
        ],
        "in\nrelation": [
            1122085995
        ],
        "resources;": [
            1122085995
        ],
        "everything\nlisting": [
            1122085995
        ],
        "\nresource\nthe": [
            1122085995
        ],
        "through\n": [
            1122085995
        ],
        "\n\n520chapter": [
            1122085995
        ],
        "kubernetes\nrequired": [
            1122085995
        ],
        "user\ndeploying": [
            1122085995
        ],
        "general\nservices": [
            1122085995
        ],
        "ticket": [
            1122085995
        ],
        "service\nthis": [
            1122085995
        ],
        "it\nproperly": [
            1122085995
        ],
        "easy-to-use": [
            1122085995
        ],
        "self-service": [
            1122085995
        ],
        "users\nwhose": [
            1122085995
        ],
        "database)": [
            1122085995
        ],
        "“hey": [
            1122085995
        ],
        "it”": [
            1122085995
        ],
        "will\nsoon": [
            1122085995
        ],
        "\n182.1": [
            1122085995
        ],
        "catalog\nas": [
            1122085995
        ],
        "browse\nthrough": [
            1122085995
        ],
        "by\nthemselves": [
            1122085995
        ],
        "resources\nrequired": [
            1122085995
        ],
        "did\nwith": [
            1122085995
        ],
        "resources:\na": [
            1122085995
        ],
        "clusterservicebroker": [
            1122085995
        ],
        "(external)": [
            1122085995
        ],
        "provision\nservices\na": [
            1122085995
        ],
        "clusterserviceclass": [
            1122085995
        ],
        "provisioned\na": [
            1122085995
        ],
        "serviceinstance": [
            1122085995
        ],
        "servicebinding": [
            1122085995
        ],
        "(pods)\nand": [
            1122085995
        ],
        "serviceinstance\nthe": [
            1122085995
        ],
        "and\nexplained": [
            1122085995
        ],
        "paragraphs\nin": [
            1122085995
        ],
        "service\nbroker": [
            1122085995
        ],
        "asks\nthe": [
            1122085995
        ],
        "clusterserviceclass\nresource": [
            1122085995
        ],
        "create\nan": [
            1122085995
        ],
        "to\nclient": [
            1122085995
        ],
        "pods\nservicebinding\nserviceinstance\nclusterserviceclass(es)\nclusterservicebroker\nfigure": [
            1122085995
        ],
        "\n\n521extending": [
            1122085995
        ],
        "catalog\ntheir": [
            1122085995
        ],
        "injected": [
            1122085995
        ],
        "serviceinstance\n": [
            1122085995
        ],
        "187.\nthe": [
            1122085995
        ],
        "sections\n18.2.2": [
            1122085995
        ],
        "\nmanager\nsimilar": [
            1122085995
        ],
        "of\nthree": [
            1122085995
        ],
        "components:\nservice": [
            1122085995
        ],
        "server\netcd": [
            1122085995
        ],
        "storage\ncontroller": [
            1122085995
        ],
        "catalog–related": [
            1122085995
        ],
        "etcd\ninstance": [
            1122085995
        ],
        "alternative\nstorage": [
            1122085995
        ],
        "required)": [
            1122085995
        ],
        "controllers\ndon’t": [
            1122085995
        ],
        "external\nservice": [
            1122085995
        ],
        "registered": [
            1122085995
        ],
        "servicebroker": [
            1122085995
        ],
        "api\nkubernetes": [
            1122085995
        ],
        "clusterexternal": [
            1122085995
        ],
        "system(s)\nkubernetes": [
            1122085995
        ],
        "catalog\nclient": [
            1122085995
        ],
        "pods\nprovisioned\nservices\nbroker": [
            1122085995
        ],
        "a\nbroker": [
            1122085995
        ],
        "b\netcd\nservice\ncatalog\napi": [
            1122085995
        ],
        "server\ncontroller\nmanager\nkubectl\nprovisioned\nservices\nclient": [
            1122085995
        ],
        "the\nprovisioned": [
            1122085995
        ],
        "services\nfigure": [
            1122085995
        ],
        "catalog\n": [
            1122085995
        ],
        "\n\n522chapter": [
            1122085995
        ],
        "kubernetes\n182.3": [
            1122085995
        ],
        "api\na": [
            1122085995
        ],
        "servicebrokers": [
            1122085995
        ],
        "api\nintroducing": [
            1122085995
        ],
        "simple\nit’s": [
            1122085995
        ],
        "operations:\nretrieving": [
            1122085995
        ],
        "/v2/catalog\nprovisioning": [
            1122085995
        ],
        "(put": [
            1122085995
        ],
        "/v2/service_instances/:id)\nupdating": [
            1122085995
        ],
        "(patch": [
            1122085995
        ],
        "/v2/service_instances/:id)\nbinding": [
            1122085995
        ],
        "/v2/service_instances/:id/service_bind-\nings/:binding_id\n)\nunbinding": [
            1122085995
        ],
        "(delete": [
            1122085995
        ],
        "/v2/service_instances/:id/service_bind-\nings/:binding_id\n)\ndeprovisioning": [
            1122085995
        ],
        "/v2/service_instances/:id)\nyou’ll": [
            1122085995
        ],
        "https://githubcom/openservicebro-\nkerapi/servicebroker.\nregistering": [
            1122085995
        ],
        "catalog\nthe": [
            1122085995
        ],
        "man-\nifest": [
            1122085995
        ],
        "servicecatalogk8s.io/v1alpha1": [
            1122085995
        ],
        "database-broker": [
            1122085995
        ],
        "url:": [
            1122085995
        ],
        "http://database-osbapimyorganization.org": [
            1122085995
        ],
        "different\ntypes": [
            1122085995
        ],
        "provision\n": [
            1122085995
        ],
        "retrieves": [
            1122085995
        ],
        "clusterservice-\nclass": [
            1122085995
        ],
        "is\n“postgresql": [
            1122085995
        ],
        "database”)": [
            1122085995
        ],
        "“free”": [
            1122085995
        ],
        "the\nlisting": [
            1122085995
        ],
        "database-brokeryaml\nthe": [
            1122085995
        ],
        "broker\nwhere": [
            1122085995
        ],
        "broker\n(its": [
            1122085995
        ],
        "[osb]": [
            1122085995
        ],
        "url)\n": [
            1122085995
        ],
        "\n\n523extending": [
            1122085995
        ],
        "catalog\ndatabase": [
            1122085995
        ],
        "“premium”\nplan": [
            1122085995
        ],
        "cluster\nusers": [
            1122085995
        ],
        "serviceclasses": [
            1122085995
        ],
        "following\nlisting\n$": [
            1122085995
        ],
        "clusterserviceclasses\nname": [
            1122085995
        ],
        "kind\npostgres-database": [
            1122085995
        ],
        "clusterserviceclassv1alpha1.servicecatalog.k8s.io\nmysql-database": [
            1122085995
        ],
        "serviceclassv1alpha1.servicecatalog.k8s.io\nmongodb-database": [
            1122085995
        ],
        "serviceclassv1alpha1.servicecatalog.k8s.io\nthe": [
            1122085995
        ],
        "clusterserviceclasses": [
            1122085995
        ],
        "bro-\nker": [
            1122085995
        ],
        "we\ndiscussed": [
            1122085995
        ],
        "like\nto": [
            1122085995
        ],
        "an\nexample": [
            1122085995
        ],
        "serviceclass": [
            1122085995
        ],
        "postgres-database": [
            1122085995
        ],
        "servicecatalogk8s.io/v1alpha1\nbindable:": [
            1122085995
        ],
        "true\nbrokername:": [
            1122085995
        ],
        "\ndescription:": [
            1122085995
        ],
        "database\nkind:": [
            1122085995
        ],
        "clusterserviceclass\nmetadata:\n": [
            1122085995
        ],
        "postgres-database\n": [
            1122085995
        ],
        "..\nplanupdatable:": [
            1122085995
        ],
        "false\nplans:\n-": [
            1122085995
        ],
        "description:": [
            1122085995
        ],
        "slow)": [
            1122085995
        ],
        "osbfree:": [
            1122085995
        ],
        "..\n-": [
            1122085995
        ],
        "(very": [
            1122085995
        ],
        "fast)": [
            1122085995
        ],
        "premium": [
            1122085995
        ],
        "plans—a": [
            1122085995
        ],
        "premium\nplan": [
            1122085995
        ],
        "database-broker\nbroker\nlisting": [
            1122085995
        ],
        "definition\nthis": [
            1122085995
        ],
        "\ndatabase-broker\na": [
            1122085995
        ],
        "service\na": [
            1122085995
        ],
        "plan\n": [
            1122085995
        ],
        "\n\n524chapter": [
            1122085995
        ],
        "kubernetes\n182.4": [
            1122085995
        ],
        "inspected": [
            1122085995
        ],
        "\nfree": [
            1122085995
        ],
        "the\npostgres-database": [
            1122085995
        ],
        "\nprovisioning": [
            1122085995
        ],
        "serviceinstance\nto": [
            1122085995
        ],
        "service-\ninstance": [
            1122085995
        ],
        "servicecatalogk8s.io/v1alpha1\nkind:": [
            1122085995
        ],
        "serviceinstance\nmetadata:\n": [
            1122085995
        ],
        "my-postgres-db": [
            1122085995
        ],
        "clusterserviceclassname:": [
            1122085995
        ],
        "clusterserviceplanname:": [
            1122085995
        ],
        "parameters:\n": [
            1122085995
        ],
        "init-db-args:": [
            1122085995
        ],
        "--data-checksums": [
            1122085995
        ],
        "deploying)": [
            1122085995
        ],
        "chosen\nplan": [
            1122085995
        ],
        "cluster-\nserviceclass": [
            1122085995
        ],
        "doc-\numentation\n": [
            1122085995
        ],
        "the\nclusterserviceclass": [
            1122085995
        ],
        "the\nchosen": [
            1122085995
        ],
        "postgresql\ndatabase": [
            1122085995
        ],
        "somewhere—not": [
            1122085995
        ],
        "neither": [
            1122085995
        ],
        "serviceinstance\n..\nstatus:\n": [
            1122085995
        ],
        "asyncopinprogress:": [
            1122085995
        ],
        "conditions:\nlisting": [
            1122085995
        ],
        "database-instanceyaml\nlisting": [
            1122085995
        ],
        "serviceinstance\nyou’re": [
            1122085995
        ],
        "\ninstance": [
            1122085995
        ],
        "want\nadditional": [
            1122085995
        ],
        "\npassed": [
            1122085995
        ],
        "broker\n": [
            1122085995
        ],
        "\n\n525extending": [
            1122085995
        ],
        "2017-05-17t13:57:22z\n": [
            1122085995
        ],
        "provisionedsuccessfully": [
            1122085995
        ],
        "pods?\nto": [
            1122085995
        ],
        "it\nbinding": [
            1122085995
        ],
        "servicebinding\nresource": [
            1122085995
        ],
        "servicebinding\nmetadata:\n": [
            1122085995
        ],
        "my-postgres-db-binding\nspec:\n": [
            1122085995
        ],
        "instanceref:": [
            1122085995
        ],
        "postgres-secret": [
            1122085995
        ],
        "my-postgres-\ndb-binding\n": [
            1122085995
        ],
        "catalog\nto": [
            1122085995
        ],
        "secret\ncalled": [
            1122085995
        ],
        "\npostgres-secret": [
            1122085995
        ],
        "pods?\nnowhere": [
            1122085995
        ],
        "actually\n": [
            1122085995
        ],
        "the\nserviceinstance’s": [
            1122085995
        ],
        "feature\ncalled": [
            1122085995
        ],
        "podpresets": [
            1122085995
        ],
        "secret\nwhere": [
            1122085995
        ],
        "pods\nmanually\n": [
            1122085995
        ],
        "broker\nresponds": [
            1122085995
        ],
        "the\nservicebinding": [
            1122085995
        ],
        "(a\npostgresql": [
            1122085995
        ],
        "servicebinding:": [
            1122085995
        ],
        "my-postgres-db-bindingyaml\nthe": [
            1122085995
        ],
        "\nprovisioned": [
            1122085995
        ],
        "successfully\nit’s": [
            1122085995
        ],
        "used\nyou’re": [
            1122085995
        ],
        "\nearlier\nyou’d": [
            1122085995
        ],
        "\n\n526chapter": [
            1122085995
        ],
        "<base64-encoded": [
            1122085995
        ],
        "database>": [
            1122085995
        ],
        "username>": [
            1122085995
        ],
        "password>": [
            1122085995
        ],
        "secret\nmetadata:\n": [
            1122085995
        ],
        "postgres-secret\n": [
            1122085995
        ],
        "..\ntype:": [
            1122085995
        ],
        "opaque\nbecause": [
            1122085995
        ],
        "before\nprovisioning": [
            1122085995
        ],
        "be\nstarted": [
            1122085995
        ],
        "to\ncreate": [
            1122085995
        ],
        "pre-\nvented": [
            1122085995
        ],
        "resource\n18.2.5": [
            1122085995
        ],
        "unbinding": [
            1122085995
        ],
        "deprovisioning\nonce": [
            1122085995
        ],
        "other\nresources:\n$": [
            1122085995
        ],
        "my-postgres-db-binding\nservicebinding": [
            1122085995
        ],
        "my-postgres-db-binding\"": [
            1122085995
        ],
        "deleted\nwhen": [
            1122085995
        ],
        "postgresql\ndatabase)": [
            1122085995
        ],
        "also:\n$": [
            1122085995
        ],
        "my-postgres-db\nserviceinstance": [
            1122085995
        ],
        "depro-\nvisioning": [
            1122085995
        ],
        "instance\n18.2.6": [
            1122085995
        ],
        "brings\nas": [
            1122085995
        ],
        "to\nexpose": [
            1122085995
        ],
        "instance\nthis": [
            1122085995
        ],
        "\n\n527platforms": [
            1122085995
        ],
        "have\nimplemented": [
            1122085995
        ],
        "and\nexpose": [
            1122085995
        ],
        "broker\nthat": [
            1122085995
        ],
        "deploying\nyour": [
            1122085995
        ],
        "\n183": [
            1122085995
        ],
        "kubernetes\ni’m": [
            1122085995
        ],
        "easily\nextensible": [
            1122085995
        ],
        "previously\ndeveloped": [
            1122085995
        ],
        "re-implementing": [
            1122085995
        ],
        "offerings\n": [
            1122085995
        ],
        "best-known": [
            1122085995
        ],
        "paas": [
            1122085995
        ],
        "deis": [
            1122085995
        ],
        "and\nred": [
            1122085995
        ],
        "of\nwhat": [
            1122085995
        ],
        "stuff": [
            1122085995
        ],
        "offers\n18.3.1": [
            1122085995
        ],
        "platform\nred": [
            1122085995
        ],
        "on\ndeveloper": [
            1122085995
        ],
        "long-term": [
            1122085995
        ],
        "maintenance": [
            1122085995
        ],
        "apps\nopenshift": [
            1122085995
        ],
        "were\nbuilt": [
            1122085995
        ],
        "ground": [
            1122085995,
            186247402
        ],
        "announced": [
            1122085995
        ],
        "decided": [
            1122085995
        ],
        "scratch—\nthis": [
            1122085995
        ],
        "throw\naway": [
            1122085995
        ],
        "tech-\nnology": [
            1122085995
        ],
        "automates": [
            1122085995
        ],
        "auto-\nmates": [
            1122085995
        ],
        "a\nproperly": [
            1122085995
        ],
        "only\nallowed": [
            1122085995
        ],
        "those\nnamespaces": [
            1122085995
        ],
        "network-isolated": [
            1122085995
        ],
        "openshift\nopenshift": [
            1122085995
        ],
        "over-\nview": [
            1122085995
        ],
        "provides\n": [
            1122085995
        ],
        "include\nusers": [
            1122085995
        ],
        "groups\nprojects\n": [
            1122085995
        ],
        "\n\n528chapter": [
            1122085995
        ],
        "kubernetes\ntemplates\nbuildconfigs\ndeploymentconfigs\nimagestreams\nroutes\nand": [
            1122085995
        ],
        "others\nunderstanding": [
            1122085995
        ],
        "projects\nwe’ve": [
            1122085995
        ],
        "users\nunlike": [
            1122085995
        ],
        "individual\nuser": [
            1122085995
        ],
        "in\nit)": [
            1122085995
        ],
        "to\nspecify": [
            1122085995
        ],
        "pre-date": [
            1122085995
        ],
        "role-\nbased": [
            1122085995
        ],
        "vanilla": [
            1122085995
        ],
        "kubernetes\nnamespaces": [
            1122085995
        ],
        "reside\nin": [
            1122085995
        ],
        "templates\nkubernetes": [
            1122085995
        ],
        "or\nyaml": [
            1122085995
        ],
        "be\nparameterizable": [
            1122085995
        ],
        "parameterizable": [
            1122085995
        ],
        "template;": [
            1122085995
        ],
        "placeholders": [
            1122085995
        ],
        "parameter\nvalues": [
            1122085995
        ],
        "instantiate": [
            1122085995
        ],
        "188).\nthe": [
            1122085995
        ],
        "ref-\nerenced": [
            1122085995
        ],
        "json/yaml": [
            1122085995
        ],
        "instantiated": [
            1122085995
        ],
        "needs\ntemplate\nparameters\napp_name=kubia\"\nvol_capacity=\"5": [
            1122085995
        ],
        "gi\n..\npod\nname:": [
            1122085995
        ],
        "$(app_name)\nservice\nname:": [
            1122085995
        ],
        "$(app_name)\ntemplate\npod\nname:": [
            1122085995
        ],
        "kubia\nservice\nname:": [
            1122085995
        ],
        "kubia\npod\nname:": [
            1122085995
        ],
        "kubia\nprocesscreate\nfigure": [
            1122085995
        ],
        "templates\n": [
            1122085995
        ],
        "\n\n529platforms": [
            1122085995
        ],
        "supply": [
            1122085995
        ],
        "template’s\nparameters": [
            1122085995
        ],
        "those\nvalues": [
            1122085995
        ],
        "pre-fabricated": [
            1122085995
        ],
        "arguments)": [
            1122085995
        ],
        "can\nenable": [
            1122085995
        ],
        "ee": [
            1122085995
        ],
        "also\ndeployed": [
            1122085995
        ],
        "command\nbuilding": [
            1122085995
        ],
        "buildconfigs\none": [
            1122085995
        ],
        "imme-\ndiately": [
            1122085995
        ],
        "repository\nholding": [
            1122085995
        ],
        "at\nall—openshift": [
            1122085995
        ],
        "build-\nconfig": [
            1122085995
        ],
        "immediately\nafter": [
            1122085995
        ],
        "committed": [
            1122085995
        ],
        "repos-\nitory": [
            1122085995
        ],
        "changes\nfrom": [
            1122085995
        ],
        "source\nto": [
            1122085995
        ],
        "pomxml": [
            1122085995
        ],
        "maven-formatted": [
            1122085995
        ],
        "maven": [
            1122085995
        ],
        "artifacts": [
            1122085995
        ],
        "are\npackaged": [
            1122085995
        ],
        "internal\ncontainer": [
            1122085995
        ],
        "(provided": [
            1122085995
        ],
        "openshift)": [
            1122085995
        ],
        "run\nin": [
            1122085995
        ],
        "buildconfig": [
            1122085995
        ],
        "not\nworry": [
            1122085995
        ],
        "know\nanything": [
            1122085995
        ],
        "and\ngives": [
            1122085995
        ],
        "and\npush": [
            1122085995
        ],
        "into\ncontainers": [
            1122085995
        ],
        "apps\nfrom": [
            1122085995
        ],
        "code\nautomatically": [
            1122085995
        ],
        "deploymentconfigs\nonce": [
            1122085995
        ],
        "deploymentconfig": [
            1122085995
        ],
        "an\nimagestream": [
            1122085995
        ],
        "imagestream": [
            1122085995
        ],
        "an\nimage": [
            1122085995
        ],
        "to\nnotice": [
            1122085995
        ],
        "189).\n": [
            1122085995
        ],
        "\n\n530chapter": [
            1122085995
        ],
        "kubernetes\na": [
            1122085995
        ],
        "pre-dates": [
            1122085995
        ],
        "transition-\ning": [
            1122085995
        ],
        "post-deployment": [
            1122085995
        ],
        "a\nkubernetes": [
            1122085995
        ],
        "and\nprovides": [
            1122085995
        ],
        "features\nexposing": [
            1122085995
        ],
        "routes\nearly": [
            1122085995
        ],
        "outside\nworld": [
            1122085995
        ],
        "time\nopenshift": [
            1122085995
        ],
        "termination\nand": [
            1122085995
        ],
        "router": [
            1122085995
        ],
        "that\nprovides": [
            1122085995
        ],
        "available\nout": [
            1122085995
        ],
        "\ntrying": [
            1122085995
        ],
        "openshift\nif": [
            1122085995
        ],
        "minishift": [
            1122085995
        ],
        "starter": [
            1122085995
        ],
        "at\nhttps://manageopenshift.com": [
            1122085995
        ],
        "provided\nto": [
            1122085995
        ],
        "\n183.2": [
            1122085995
        ],
        "helm\na": [
            1122085995
        ],
        "a\npaas": [
            1122085995
        ],
        "workflow\npods\nbuilder": [
            1122085995
        ],
        "pod\nreplication\ncontroller\nbuildconfig\ngit": [
            1122085995
        ],
        "repo\ndeploymentconfig\nimagestream\nbuild": [
            1122085995
        ],
        "trigger\nclones": [
            1122085995
        ],
        "new\nimage": [
            1122085995
        ],
        "adds\nit": [
            1122085995
        ],
        "imagestream\nwatches": [
            1122085995
        ],
        "imagestream\nand": [
            1122085995
        ],
        "(similarly": [
            1122085995
        ],
        "a\ndeployment)\nfigure": [
            1122085995
        ],
        "buildconfigs": [
            1122085995
        ],
        "deploymentconfigs": [
            1122085995
        ],
        "openshift\n": [
            1122085995
        ],
        "\n\n531platforms": [
            1122085995
        ],
        "kubernetes\nthey’ve": [
            1122085995
        ],
        "gaining": [
            1122085995
        ],
        "traction": [
            1122085995
        ],
        "kubernetes\ncommunity": [
            1122085995
        ],
        "a\nbrief": [
            1122085995
        ],
        "both\nintroducing": [
            1122085995
        ],
        "workflow\nyou": [
            1122085995
        ],
        "openshift\nwhich": [
            1122085995
        ],
        "compo-\nnents)": [
            1122085995
        ],
        "replicationcontrollers\nwhich": [
            1122085995
        ],
        "developer-friendly": [
            1122085995
        ],
        "\ngit\npush\n": [
            1122085995
        ],
        "openshift\nworkflow": [
            1122085995
        ],
        "roll-\nbacks": [
            1122085995
        ],
        "edge": [
            1122085995
        ],
        "alerting": [
            1122085995
        ],
        "aren’t\navailable": [
            1122085995
        ],
        "work-\nflow": [
            1122085995
        ],
        "https://deis\ncom/workflow.": [
            1122085995
        ],
        "without\nworkflow": [
            1122085995
        ],
        "gained": [
            1122085995
        ],
        "popularity": [
            1122085995
        ],
        "community\ndeploying": [
            1122085995
        ],
        "helm\nhelm": [
            1122085995
        ],
        "(similar": [
            1122085995
        ],
        "managers": [
            1122085995
        ],
        "yum\nor": [
            1122085995
        ],
        "apt": [
            1122085995
        ],
        "homebrew": [
            1122085995
        ],
        "macos)": [
            1122085995
        ],
        "things:\na": [
            1122085995
        ],
        "client)\ntiller": [
            1122085995
        ],
        "cluster\nthose": [
            1122085995
        ],
        "combined\nwith": [
            1122085995
        ],
        "merged": [
            1122085995
        ],
        "chart\nto": [
            1122085995
        ],
        "chart\nand": [
            1122085995
        ],
        "config)": [
            1122085995
        ],
        "\nhelm": [
            1122085995
        ],
        "tiller": [
            1122085995
        ],
        "1810.\n": [
            1122085995
        ],
        "growing": [
            1122085995
        ],
        "maintained": [
            1122085995
        ],
        "https://githubcom/kubernetes/charts.": [
            1122085995
        ],
        "for\napplications": [
            1122085995
        ],
        "mariadb": [
            1122085995
        ],
        "magento": [
            1122085995
        ],
        "memcached": [
            1122085995
        ],
        "mongodb\nopenvpn,": [
            1122085995
        ],
        "phpbb": [
            1122085995
        ],
        "rabbitmq": [
            1122085995
        ],
        "redis": [
            1122085995
        ],
        "wordpress": [
            1122085995
        ],
        "your\nlinux": [
            1122085995
        ],
        "own\nkubernetes": [
            1122085995
        ],
        "helm\nand": [
            1122085995
        ],
        "\n\n532chapter": [
            1122085995
        ],
        "kubernetes\nwhen": [
            1122085995
        ],
        "cluster\ndon’t": [
            1122085995
        ],
        "gone\nthrough": [
            1122085995
        ],
        "prepares": [
            1122085995
        ],
        "the\nhelm": [
            1122085995
        ],
        "one-line": [
            1122085995
        ],
        "is\nclone": [
            1122085995
        ],
        "(pro-\nvided": [
            1122085995
        ],
        "helm’s": [
            1122085995
        ],
        "cluster):\n$": [
            1122085995
        ],
        "my-database": [
            1122085995
        ],
        "stable/mysql\nthis": [
            1122085995
        ],
        "persistentvolu-\nmeclaims": [
            1122085995
        ],
        "yourself\nwith": [
            1122085995
        ],
        "properly\ni’m": [
            1122085995
        ],
        "awesome\ntipone": [
            1122085995
        ],
        "openvpn\nchart": [
            1122085995
        ],
        "openvpn": [
            1122085995
        ],
        "vpn": [
            1122085995
        ],
        "devel-\noping": [
            1122085995
        ],
        "locally\nthese": [
            1122085995
        ],
        "companies\nlike": [
            1122085995
        ],
        "(now": [
            1122085995
        ],
        "microsoft)": [
            1122085995
        ],
        "riding": [
            1122085995
        ],
        "wave": [
            1122085995
        ],
        "yourself!\nkubernetes": [
            1122085995
        ],
        "cluster\nchart\nand\nconfig\nhelm\ncharts\n(files": [
            1122085995
        ],
        "on\nlocal": [
            1122085995
        ],
        "disk)\ntiller\n(pod)\ndeployments\nservices,": [
            1122085995
        ],
        "objects\nhelm\ncli": [
            1122085995
        ],
        "tool\nmanages\ncharts\ncombines": [
            1122085995
        ],
        "and\nconfig": [
            1122085995
        ],
        "release\ncreates": [
            1122085995
        ],
        "objects\ndefined": [
            1122085995
        ],
        "release\nfigure": [
            1122085995
        ],
        "\n\n533summary\n184": [
            1122085995
        ],
        "functionalities\nkubernetes": [
            1122085995
        ],
        "how\ncustom": [
            1122085995
        ],
        "custom-\nresourcedefinition": [
            1122085995
        ],
        "object\ninstances": [
            1122085995
        ],
        "code\na": [
            1122085995
        ],
        "life\nkubernetes": [
            1122085995
        ],
        "aggregation\nkubernetes": [
            1122085995
        ],
        "self-provision": [
            1122085995
        ],
        "services\nand": [
            1122085995
        ],
        "cluster\nplatforms-as-a-service": [
            1122085995
        ],
        "contain-\nerized": [
            1122085995
        ],
        "\na": [
            1122085995
        ],
        "requir-\ning": [
            1122085995
        ],
        "them\nthank": [
            1122085995
        ],
        "learned\nas": [
            1122085995
        ],
        "\n\n534\nappendix": [
            1122085995
        ],
        "a\nusing": [
            1122085995
        ],
        "kubectl\nwith": [
            1122085995
        ],
        "clusters\na1switching": [
            1122085995
        ],
        "\nengine\nthe": [
            1122085995
        ],
        "or\none": [
            1122085995
        ],
        "gke\nswitching": [
            1122085995
        ],
        "minikube\nluckily": [
            1122085995
        ],
        "also\nreconfigures": [
            1122085995
        ],
        "cluster..\n...\nsetting": [
            1122085995
        ],
        "kubeconfig..": [
            1122085995
        ],
        "minikube\nand": [
            1122085995
        ],
        "re-configured": [
            1122085995
        ],
        "again\nswitching": [
            1122085995
        ],
        "gke\nto": [
            1122085995
        ],
        "get-credentials": [
            1122085995
        ],
        "my-gke-cluster\nthis": [
            1122085995
        ],
        "my-gke-cluster\nminikube": [
            1122085995
        ],
        "\n\n535using": [
            1122085995
        ],
        "namespaces\ngoing": [
            1122085995
        ],
        "further\nthese": [
            1122085995
        ],
        "understand\nthe": [
            1122085995
        ],
        "study": [
            1122085995
        ],
        "\na2using": [
            1122085995
        ],
        "namespaces\nif": [
            1122085995
        ],
        "\n--namespace\noption": [
            1122085995
        ],
        "it\na.2.1configuring": [
            1122085995
        ],
        "~/kube/config": [
            1122085995
        ],
        "stored\nsomewhere": [
            1122085995
        ],
        "\nkubeconfig": [
            1122085995
        ],
        "at\nonce": [
            1122085995
        ],
        "(sepa-\nrate": [
            1122085995
        ],
        "colon)\na.2.2understanding": [
            1122085995
        ],
        "file\nan": [
            1122085995
        ],
        "v1\nclusters:\n-": [
            1122085995
        ],
        "certificate-authority:": [
            1122085995
        ],
        "/home/luksa/minikube/ca.crt": [
            1122085995
        ],
        "\ncontexts:\n-": [
            1122085995
        ],
        "context:": [
            1122085995
        ],
        "\ncurrent-context:": [
            1122085995
        ],
        "config\npreferences:": [
            1122085995
        ],
        "{}\nusers:\n-": [
            1122085995
        ],
        "client-certificate:": [
            1122085995
        ],
        "/home/luksa/minikube/apiserver.crt": [
            1122085995
        ],
        "client-key:": [
            1122085995
        ],
        "/home/luksa/minikube/apiserver.key": [
            1122085995
        ],
        "sections:\n■\na": [
            1122085995
        ],
        "clusters\n■\na": [
            1122085995
        ],
        "users\n■\na": [
            1122085995
        ],
        "contexts\n■\nthe": [
            1122085995
        ],
        "context\nlisting": [
            1122085995
        ],
        "a1": [
            1122085995
        ],
        "file\ncontains": [
            1122085995
        ],
        "cluster\ndefines": [
            1122085995
        ],
        "\ncontext\nthe": [
            1122085995
        ],
        "uses\ncontains": [
            1122085995
        ],
        "\ncredentials\n": [
            1122085995
        ],
        "\n\n536appendix": [
            1122085995
        ],
        "clusters\neach": [
            1122085995
        ],
        "context\nuser,": [
            1122085995
        ],
        "\nclusters\na": [
            1122085995
        ],
        "in\nit": [
            1122085995
        ],
        "\ncertificate-authority-data": [
            1122085995
        ],
        "field\nusers\neach": [
            1122085995
        ],
        "a\nusername": [
            1122085995
        ],
        "certificate\nthe": [
            1122085995
        ],
        "\nclient-\ncertificate-data\n": [
            1122085995
        ],
        "client-key-data": [
            1122085995
        ],
        "properties)": [
            1122085995
        ],
        "and\nreferenced": [
            1122085995
        ],
        "a1.\ncontexts\na": [
            1122085995
        ],
        "use\nwhen": [
            1122085995
        ],
        "context\nwhile": [
            1122085995
        ],
        "time\nonly": [
            1122085995
        ],
        "changed\na.2.3listing": [
            1122085995
        ],
        "entries\nyou": [
            1122085995
        ],
        "contexts\nbut": [
            1122085995
        ],
        "commands\nadding": [
            1122085995
        ],
        "set-cluster": [
            1122085995
        ],
        "my-other-cluster": [
            1122085995
        ],
        "--server=https://k8sexample.com:6443": [
            1122085995
        ],
        "--certificate-authority=path/to/the/cafile\nthis": [
            1122085995
        ],
        "https://\nk8sexample.com:6443.": [
            1122085995
        ],
        "run\nkubectl": [
            1122085995
        ],
        "\nset-cluster": [
            1122085995
        ],
        "overwrite\nits": [
            1122085995
        ],
        "credentials\nadding": [
            1122085995
        ],
        "user\nthat": [
            1122085995
        ],
        "--username=foo": [
            1122085995
        ],
        "--password=pass\n": [
            1122085995
        ],
        "\n\n537using": [
            1122085995
        ],
        "token-based": [
            1122085995
        ],
        "instead:\n$": [
            1122085995
        ],
        "--token=mysecrettokenxfdjiq1234\nboth": [
            1122085995
        ],
        "same\ncredentials": [
            1122085995
        ],
        "user\nand": [
            1122085995
        ],
        "\ntying": [
            1122085995
        ],
        "together\na": [
            1122085995
        ],
        "option\n": [
            1122085995
        ],
        "some-context": [
            1122085995
        ],
        "--cluster=my-other-cluster": [
            1122085995
        ],
        "--user=foo": [
            1122085995
        ],
        "--namespace=bar\nthis": [
            1122085995
        ],
        "cluster\nand": [
            1122085995
        ],
        "current\ncontext": [
            1122085995
        ],
        "so:\n$": [
            1122085995
        ],
        "current-context\nminikube\nyou": [
            1122085995
        ],
        "--namespace=another-namespace\nrunning": [
            1122085995
        ],
        "user-friendly": [
            1122085995
        ],
        "kubectl\ntipto": [
            1122085995
        ],
        "this:": [
            1122085995
        ],
        "alias\nkcd=kubectl\n": [
            1122085995
        ],
        "current-context)\n--namespace\n": [
            1122085995
        ],
        "kcd": [
            1122085995
        ],
        "some-\nnamespace\n\na.2.4using": [
            1122085995
        ],
        "contexts\nwhen": [
            1122085995
        ],
        "the\nkubeconfig’s": [
            1122085995
        ],
        "following\ncommand-line": [
            1122085995
        ],
        "options:\n■\n--user": [
            1122085995
        ],
        "file\n■\n--username": [
            1122085995
        ],
        "--password": [
            1122085995
        ],
        "(they\ndon’t": [
            1122085995
        ],
        "\n--client-key": [
            1122085995
        ],
        "--client-certificate": [
            1122085995
        ],
        "--token\n■\n--cluster": [
            1122085995
        ],
        "(must": [
            1122085995
        ],
        "file)\n": [
            1122085995
        ],
        "\n\n538appendix": [
            1122085995
        ],
        "clusters\n■\n--server": [
            1122085995
        ],
        "file)\n■\n--namespace": [
            1122085995
        ],
        "namespace\na.2.5switching": [
            1122085995
        ],
        "\nset-context": [
            1122085995
        ],
        "switch\nbetween": [
            1122085995
        ],
        "(use": [
            1122085995
        ],
        "\nset-clus-\nter\n": [
            1122085995
        ],
        "trivial:\n$": [
            1122085995
        ],
        "use-context": [
            1122085995
        ],
        "my-other-context\nthis": [
            1122085995
        ],
        "switches": [
            1122085995
        ],
        "my-other-context": [
            1122085995
        ],
        "\na2.6listing": [
            1122085995
        ],
        "clusters\nto": [
            1122085995
        ],
        "get-contexts\ncurrent": [
            1122085995
        ],
        "authinfo": [
            1122085995
        ],
        "namespace\n*": [
            1122085995
        ],
        "rpi-cluster": [
            1122085995
        ],
        "admin/rpi-cluster\n": [
            1122085995
        ],
        "rpi-foo": [
            1122085995
        ],
        "admin/rpi-cluster": [
            1122085995
        ],
        "rpi-foo\ncontexts": [
            1122085995
        ],
        "similar:\n$": [
            1122085995
        ],
        "get-clusters\nname\nrpi-cluster\nminikube\ncredentials": [
            1122085995
        ],
        "reasons\na.2.7deleting": [
            1122085995
        ],
        "the\nkubeconfig": [
            1122085995
        ],
        "commands:\n$": [
            1122085995
        ],
        "delete-context": [
            1122085995
        ],
        "my-unused-context\nand\n$": [
            1122085995
        ],
        "delete-cluster": [
            1122085995
        ],
        "my-old-cluster\n": [
            1122085995
        ],
        "\n\n539\nappendix": [
            1122085995
        ],
        "b\nsetting": [
            1122085995
        ],
        "multi-node\ncluster": [
            1122085995
        ],
        "kubeadm\nthis": [
            1122085995
        ],
        "you’ll\nrun": [
            1122085995
        ],
        "virtualbox": [
            1122085995
        ],
        "tool\nb.1setting": [
            1122085995
        ],
        "packages\nfirst": [
            1122085995
        ],
        "installed\nalready": [
            1122085995
        ],
        "https://wwwvirtualbox.org/wiki/downloads.\nonce": [
            1122085995
        ],
        "centos": [
            1122085995
        ],
        "iso": [
            1122085995
        ],
        "from\nwwwcentos.org/download.": [
            1122085995
        ],
        "but\nmake": [
            1122085995
        ],
        "http://kubernetesio": [
            1122085995
        ],
        "website\nb.1.1creating": [
            1122085995
        ],
        "machine\nnext": [
            1122085995
        ],
        "new\nicon": [
            1122085995
        ],
        "upper-left": [
            1122085995
        ],
        "corner": [
            1122085995
        ],
        "“k8s-master”": [
            1122085995
        ],
        "select\nlinux": [
            1122085995
        ],
        "(64-bit)": [
            1122085995
        ],
        "b1.\n": [
            1122085995
        ],
        "the\nhard": [
            1122085995
        ],
        "2gb": [
            1122085995
        ],
        "run\nthree": [
            1122085995
        ],
        "vms)": [
            1122085995
        ],
        "selected\nhere’s": [
            1122085995
        ],
        "case:\n■\nhard": [
            1122085995
        ],
        "vdi": [
            1122085995
        ],
        "(virtualbox": [
            1122085995
        ],
        "image)\n■\nstorage": [
            1122085995
        ],
        "allocated\n■\nfile": [
            1122085995
        ],
        "k8s-master": [
            1122085995
        ],
        "8gb\n": [
            1122085995
        ],
        "\n\n540appendix": [
            1122085995
        ],
        "kubeadm\nb1.2configuring": [
            1122085995
        ],
        "vm\nonce": [
            1122085995
        ],
        "adapter\nbecause": [
            1122085995
        ],
        "bridged": [
            1122085995
        ],
        "same\nway": [
            1122085995
        ],
        "computer\nis": [
            1122085995
        ],
        "usually\nrequire": [
            1122085995
        ],
        "virtual-\nbox": [
            1122085995
        ],
        "settings": [
            1122085995
        ],
        "(next": [
            1122085995
        ],
        "clicked": [
            1122085995
        ],
        "before)": [
            1122085995
        ],
        "left-hand": [
            1122085995
        ],
        "select\nnetwork": [
            1122085995
        ],
        "panel": [
            1122085995
        ],
        "adapter\nas": [
            1122085995
        ],
        "drop-down": [
            1122085995
        ],
        "menu": [
            1122085995
        ],
        "machine’s\nadapter": [
            1122085995
        ],
        "network\nfigure": [
            1122085995
        ],
        "b1": [
            1122085995
        ],
        "virtualbox\n": [
            1122085995
        ],
        "\n\n541setting": [
            1122085995
        ],
        "packages\nb1.3installing": [
            1122085995
        ],
        "system\nyou’re": [
            1122085995
        ],
        "still\nselected": [
            1122085995
        ],
        "window\nselecting": [
            1122085995
        ],
        "disk\nbefore": [
            1122085995
        ],
        "icon\nnext": [
            1122085995
        ],
        "b3)": [
            1122085995
        ],
        "centos\niso": [
            1122085995
        ],
        "vm\nfigure": [
            1122085995
        ],
        "b3": [
            1122085995
        ],
        "\n\n542appendix": [
            1122085995
        ],
        "kubeadm\ninitiating": [
            1122085995
        ],
        "install\nwhen": [
            1122085995
        ],
        "textual": [
            1122085995
        ],
        "cursor": [
            1122085995
        ],
        "to\nselect": [
            1122085995
        ],
        "\nsetting": [
            1122085995
        ],
        "options\nafter": [
            1122085995
        ],
        "welcome": [
            1122085995
        ],
        "appear\nallowing": [
            1122085995
        ],
        "language\nset": [
            1122085995
        ],
        "b4.\ntipwhen": [
            1122085995
        ],
        "keyboard": [
            1122085995
        ],
        "mouse": [
            1122085995
        ],
        "be\ncaptured": [
            1122085995
        ],
        "bottom-right\ncorner": [
            1122085995
        ],
        "right\ncontrol": [
            1122085995
        ],
        "macos\nfirst": [
            1122085995
        ],
        "network\nadapter": [
            1122085995
        ],
        "on/off": [
            1122085995
        ],
        "host\nfigure": [
            1122085995
        ],
        "b4": [
            1122085995
        ],
        "screen\n": [
            1122085995
        ],
        "\n\n543setting": [
            1122085995
        ],
        "packages\nname": [
            1122085995
        ],
        "b5.": [
            1122085995
        ],
        "setting\nup": [
            1122085995
        ],
        "masterk8s.": [
            1122085995
        ],
        "the\ntext": [
            1122085995
        ],
        "name\nto": [
            1122085995
        ],
        "top-left": [
            1122085995
        ],
        "corner\n": [
            1122085995
        ],
        "the\nscreen": [
            1122085995
        ],
        "city": [
            1122085995
        ],
        "return\nto": [
            1122085995
        ],
        "corner\nrunning": [
            1122085995
        ],
        "install\nto": [
            1122085995
        ],
        "bottom-right": [
            1122085995
        ],
        "corner\na": [
            1122085995
        ],
        "b6": [
            1122085995
        ],
        "the\nfigure": [
            1122085995
        ],
        "b5": [
            1122085995
        ],
        "adapter\nfigure": [
            1122085995
        ],
        "rebooting": [
            1122085995
        ],
        "afterward\n": [
            1122085995
        ],
        "\n\n544appendix": [
            1122085995
        ],
        "kubeadm\nroot": [
            1122085995
        ],
        "completes\nclick": [
            1122085995
        ],
        "reboot": [
            1122085995
        ],
        "right\nb.1.4installing": [
            1122085995
        ],
        "kubernetes\nlog": [
            1122085995
        ],
        "features:": [
            1122085995
        ],
        "selinux\nand": [
            1122085995
        ],
        "firewall\ndisabling": [
            1122085995
        ],
        "selinux\nto": [
            1122085995
        ],
        "command:\n#": [
            1122085995
        ],
        "setenforce": [
            1122085995
        ],
        "0\nbut": [
            1122085995
        ],
        "disables": [
            1122085995
        ],
        "reboot)": [
            1122085995
        ],
        "perma-\nnently": [
            1122085995
        ],
        "/etc/selinux/config": [
            1122085995
        ],
        "\nselinux=enforcing": [
            1122085995
        ],
        "to\nselinux=permissive\ndisabling": [
            1122085995
        ],
        "firewall\nyou’ll": [
            1122085995
        ],
        "firewall-related": [
            1122085995
        ],
        "problems\nrun": [
            1122085995
        ],
        "systemctl": [
            1122085995
        ],
        "firewalld": [
            1122085995
        ],
        "firewalld\nremoved": [
            1122085995
        ],
        "symlink": [
            1122085995
        ],
        "/etc/systemd/system/dbus-orgfedoraproject.firewalld1...\nremoved": [
            1122085995
        ],
        "/etc/systemd/system/basictarget.wants/firewalld.service.\nadding": [
            1122085995
        ],
        "yum": [
            1122085995
        ],
        "repo\nto": [
            1122085995
        ],
        "rpm": [
            1122085995
        ],
        "kubernetesrepo": [
            1122085995
        ],
        "/etc/yumrepos.d/": [
            1122085995
        ],
        "/etc/yumrepos.d/kubernetes.repo\n[kubernetes]\nname=kubernetes\nbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\n": [
            1122085995
        ],
        "https://packagescloud.google.com/yum/doc/rpm-package-key.gpg\neof\nnotemake": [
            1122085995
        ],
        "whitespace": [
            1122085995
        ],
        "eof": [
            1122085995
        ],
        "pasting": [
            1122085995
        ],
        "kubernetes-cni\nnow": [
            1122085995
        ],
        "need:\n#": [
            1122085995
        ],
        "kubernetes-cni\nlisting": [
            1122085995
        ],
        "repo\n": [
            1122085995
        ],
        "\n\n545setting": [
            1122085995
        ],
        "packages\nas": [
            1122085995
        ],
        "are:\n■\ndocker—the": [
            1122085995
        ],
        "runtime\n■\nkubelet—the": [
            1122085995
        ],
        "you\n■\nkubeadm—a": [
            1122085995
        ],
        "clusters\n■\nkubectl—the": [
            1122085995
        ],
        "kubernetes\n■\nkubernetes-cni—the": [
            1122085995
        ],
        "interface\nonce": [
            1122085995
        ],
        "kubelet\nservices:\n#": [
            1122085995
        ],
        "docker\n#": [
            1122085995
        ],
        "kubelet\nenabling": [
            1122085995
        ],
        "netbridge.bridge-nf-call-iptables": [
            1122085995
        ],
        "option\ni’ve": [
            1122085995
        ],
        "bridge-nf-call-iptables": [
            1122085995
        ],
        "parameter\nwhich": [
            1122085995
        ],
        "rectify": [
            1122085995
        ],
        "problem\nyou": [
            1122085995
        ],
        "commands:\n#": [
            1122085995
        ],
        "sysctl": [
            1122085995
        ],
        "-w": [
            1122085995
        ],
        "netbridge.bridge-nf-call-iptables=1\n#": [
            1122085995
        ],
        "netbridge.bridge-nf-call-iptables=1\"": [
            1122085995
        ],
        "/etc/sysctld/k8s.conf\ndisabling": [
            1122085995
        ],
        "swap\nthe": [
            1122085995
        ],
        "swap": [
            1122085995
        ],
        "following\ncommand:\n#": [
            1122085995
        ],
        "swapoff": [
            1122085995
        ],
        "s/^/#/": [
            1122085995
        ],
        "/etc/fstab\nb1.5cloning": [
            1122085995
        ],
        "vm\neverything": [
            1122085995
        ],
        "bare": [
            1122085995
        ],
        "metal": [
            1122085995
        ],
        "process\ndescribed": [
            1122085995
        ],
        "times—for": [
            1122085995
        ],
        "vms\nshutting": [
            1122085995
        ],
        "vm\nto": [
            1122085995
        ],
        "shutdown\ncommand:\n#": [
            1122085995
        ],
        "now\ncloning": [
            1122085995
        ],
        "vm\nnow": [
            1122085995
        ],
        "right-click": [
            1122085995
        ],
        "b7": [
            1122085995
        ],
        "k8s-node1": [
            1122085995
        ],
        "or\nk8s-node2": [
            1122085995
        ],
        "reinitialize": [
            1122085995
        ],
        "address\nof": [
            1122085995
        ],
        "cards": [
            1122085995
        ],
        "(because\nthey’re": [
            1122085995
        ],
        "network)\n": [
            1122085995
        ],
        "\n\n546appendix": [
            1122085995
        ],
        "kubeadm\nclick": [
            1122085995
        ],
        "before\nclicking": [
            1122085995
        ],
        "(leave": [
            1122085995
        ],
        "machine\nstate": [
            1122085995
        ],
        "selected)\n": [
            1122085995
        ],
        "by\nselecting": [
            1122085995
        ],
        "vms\nbecause": [
            1122085995
        ],
        "clones": [
            1122085995
        ],
        "root)": [
            1122085995
        ],
        "hostnamectl": [
            1122085995
        ],
        "--static": [
            1122085995
        ],
        "set-hostname": [
            1122085995
        ],
        "node1k8s\nnotebe": [
            1122085995
        ],
        "node\nconfiguring": [
            1122085995
        ],
        "resolution": [
            1122085995
        ],
        "hosts\nyou": [
            1122085995
        ],
        "resolvable": [
            1122085995
        ],
        "a\ndns": [
            1122085995
        ],
        "/etc/hosts": [
            1122085995
        ],
        "vms)\nas": [
            1122085995
        ],
        "listing\n192.168.64.138": [
            1122085995
        ],
        "masterk8s\n192.168.64.139": [
            1122085995
        ],
        "node1k8s\n192.168.64.140": [
            1122085995
        ],
        "node2k8s\nlisting": [
            1122085995
        ],
        "vm\n": [
            1122085995
        ],
        "\n\n547configuring": [
            1122085995
        ],
        "kubeadm\nyou": [
            1122085995
        ],
        "addr": [
            1122085995
        ],
        "and\nfinding": [
            1122085995
        ],
        "\nenp0s3": [
            1122085995
        ],
        "addr\n1:": [
            1122085995
        ],
        "lo:": [
            1122085995
        ],
        "<loopbackup,lower_up>": [
            1122085995
        ],
        "mtu": [
            1122085995
        ],
        "qdisc": [
            1122085995
        ],
        "noqueue": [
            1122085995
        ],
        "qlen": [
            1122085995
        ],
        "link/loopback": [
            1122085995
        ],
        "00:00:00:00:00:00": [
            1122085995
        ],
        "brd": [
            1122085995
        ],
        "00:00:00:00:00:00\n": [
            1122085995
        ],
        "1270.0.1/8": [
            1122085995
        ],
        "lo\n": [
            1122085995
        ],
        "valid_lft": [
            1122085995
        ],
        "forever": [
            1122085995
        ],
        "preferred_lft": [
            1122085995
        ],
        "forever\n": [
            1122085995
        ],
        "::1/128": [
            1122085995
        ],
        "host\n": [
            1122085995
        ],
        "forever\n2:": [
            1122085995
        ],
        "enp0s3:": [
            1122085995
        ],
        "<broadcastmulticast,up,lower_up>": [
            1122085995
        ],
        "pfifo_fast": [
            1122085995
        ],
        "1000\n": [
            1122085995
        ],
        "link/ether": [
            1122085995
        ],
        "08:00:27:db:c3:a4": [
            1122085995
        ],
        "ff:ff:ff:ff:ff:ff\n": [
            1122085995
        ],
        "192168.64.138/24": [
            1122085995
        ],
        "192168.64.255": [
            1122085995
        ],
        "enp0s3\n": [
            1122085995
        ],
        "59414sec": [
            1122085995
        ],
        "59414sec\n": [
            1122085995
        ],
        "fe80::77a9:5ad6:2597:2e1b/64": [
            1122085995
        ],
        "link\n": [
            1122085995
        ],
        "forever\nthe": [
            1122085995
        ],
        "is\n192168.64.138.": [
            1122085995
        ],
        "all\ntheir": [
            1122085995
        ],
        "ips\nb.2configuring": [
            1122085995
        ],
        "kubeadm\nyou’re": [
            1122085995
        ],
        "master\nthanks": [
            1122085995
        ],
        "run\na": [
            1122085995
        ],
        "init\n[kubeadm]": [
            1122085995
        ],
        "warning:": [
            1122085995
        ],
        "\nclusters\n[init]": [
            1122085995
        ],
        "v1.8.4\n...\nyou": [
            1122085995
        ],
        "cluster\nrun": [
            1122085995
        ],
        "[podnetwork]yaml": [
            1122085995
        ],
        "at:\n": [
            1122085995
        ],
        "http://kubernetesio/docs/admin/addons/\nyou": [
            1122085995
        ],
        "join": [
            1122085995,
            1941223023
        ],
        "root:\nkubeadm": [
            1122085995
        ],
        "--token": [
            1122085995
        ],
        "eb38773585d0423978c549": [
            1122085995
        ],
        "192168.64.138:6443": [
            1122085995
        ],
        "\n--discovery-token-ca-cert-hash": [
            1122085995
        ],
        "\nsha256:037d2c5505294af196048a17f184a79411c7b1eac48aaa0ad137075be3d7a847\nnotewrite": [
            1122085995
        ],
        "init’s": [
            1122085995
        ],
        "out-\nput": [
            1122085995
        ],
        "later\nlisting": [
            1122085995
        ],
        "address\nlisting": [
            1122085995
        ],
        "\n\n548appendix": [
            1122085995
        ],
        "kubeadm\nkubeadm": [
            1122085995
        ],
        "etcd\nthe": [
            1122085995
        ],
        "\nb2.1understanding": [
            1122085995
        ],
        "components\nall": [
            1122085995
        ],
        "command\nto": [
            1122085995
        ],
        "their\nyaml": [
            1122085995
        ],
        "/etc/kubernetes/manifests": [
            1122085995
        ],
        "moni-\ntored": [
            1122085995
        ],
        "\nkubectl\nrunning": [
            1122085995
        ],
        "master\nyou": [
            1122085995
        ],
        "configuring\nit": [
            1122085995
        ],
        "/etc/kubernetes/adminconf\nfile.": [
            1122085995
        ],
        "a:\n#": [
            1122085995
        ],
        "kubeconfig=/etc/kubernetes/adminconf\nlisting": [
            1122085995
        ],
        "kube-system\nnamespace)": [
            1122085995
        ],
        "age\netcd-masterk8s": [
            1122085995
        ],
        "21m\nkube-apiserver-masterk8s": [
            1122085995
        ],
        "22m\nkube-controller-manager-masterk8s": [
            1122085995
        ],
        "21m\nkube-dns-3913472980-cn6kz": [
            1122085995
        ],
        "0/3": [
            1122085995
        ],
        "22m\nkube-proxy-qb709": [
            1122085995
        ],
        "22m\nkube-scheduler-masterk8s": [
            1122085995
        ],
        "21m\nlisting": [
            1122085995
        ],
        "nodes\nyou’re": [
            1122085995
        ],
        "nodes\nalthough": [
            1122085995
        ],
        "(you\neither": [
            1122085995
        ],
        "packages)": [
            1122085995
        ],
        "\nkubectl:\n#": [
            1122085995
        ],
        "version\nmasterk8s": [
            1122085995
        ],
        "v18.4\nlisting": [
            1122085995
        ],
        "\n\n549configuring": [
            1122085995
        ],
        "kubeadm\nsee": [
            1122085995
        ],
        "not-\nready": [
            1122085995
        ],
        "nodes\nb.3configuring": [
            1122085995
        ],
        "kubeadm\nwhen": [
            1122085995
        ],
        "the\nmaster": [
            1122085995
        ],
        "it\nalready": [
            1122085995
        ],
        "(repeated": [
            1122085995
        ],
        "listing)\nyou": [
            1122085995
        ],
        "\nsha256:037d2c5505294af196048a17f184a79411c7b1eac48aaa0ad137075be3d7a847\nall": [
            1122085995
        ],
        "the\nmaster’s": [
            1122085995
        ],
        "address/port": [
            1122085995
        ],
        "registered\nby": [
            1122085995
        ],
        "again:\n#": [
            1122085995
        ],
        "3m": [
            1122085995
        ],
        "v18.4\nnode1.k8s": [
            1122085995
        ],
        "3s": [
            1122085995
        ],
        "v18.4\nnode2.k8s": [
            1122085995
        ],
        "v18.4\nokay": [
            1122085995
        ],
        "but\nnone": [
            1122085995
        ],
        "investigate\n": [
            1122085995
        ],
        "more\ninformation": [
            1122085995
        ],
        "\nconditions": [
            1122085995
        ],
        "\nreason": [
            1122085995
        ],
        "and\nmessage\n#": [
            1122085995
        ],
        "node1k8s\n...\nkubeletnotready": [
            1122085995
        ],
        "networkready=false": [
            1122085995
        ],
        "reason:networkpluginnotready": [
            1122085995
        ],
        "message:docker:": [
            1122085995
        ],
        "uninitialized\naccording": [
            1122085995
        ],
        "(cni)\nplugin": [
            1122085995
        ],
        "plugin\nyet": [
            1122085995
        ],
        "now\nlisting": [
            1122085995
        ],
        "command\nlisting": [
            1122085995
        ],
        "\n\n550appendix": [
            1122085995
        ],
        "kubeadm\nb3.1setting": [
            1122085995
        ],
        "weave": [
            1122085995
        ],
        "alternatives": [
            1122085995
        ],
        "http://kuber-\nnetesio/docs/admin/addons/.\n": [
            1122085995
        ],
        "add-ons)": [
            1122085995
        ],
        "https://cloudweave.works/k8s/net?k8s-version=$(kubectl": [
            1122085995
        ],
        "tr": [
            1122085995
        ],
        "\\n')\nthis": [
            1122085995
        ],
        "(refer": [
            1122085995
        ],
        "12\nfor": [
            1122085995
        ],
        "deployed\nalongside": [
            1122085995
        ],
        "daemonset)\n": [
            1122085995
        ],
        "your\nnodes": [
            1122085995
        ],
        "ready:\n#": [
            1122085995
        ],
        "9m": [
            1122085995
        ],
        "v18.4\nand": [
            1122085995
        ],
        "an\noverlay": [
            1122085995
        ],
        "age\nkube-system": [
            1122085995
        ],
        "etcd-masterk8s": [
            1122085995
        ],
        "kube-apiserver-masterk8s": [
            1122085995
        ],
        "kube-controller-manager-masterk8s": [
            1122085995
        ],
        "kube-dns-3913472980-cn6kz": [
            1122085995
        ],
        "kube-proxy-hcqnx": [
            1122085995
        ],
        "24m\nkube-system": [
            1122085995
        ],
        "kube-proxy-jvdlr": [
            1122085995
        ],
        "kube-proxy-qb709": [
            1122085995
        ],
        "kube-scheduler-masterk8s": [
            1122085995
        ],
        "weave-net-58zbk": [
            1122085995
        ],
        "7m\nkube-system": [
            1122085995
        ],
        "weave-net-91kjd": [
            1122085995
        ],
        "weave-net-vt279": [
            1122085995
        ],
        "7m\nb4using": [
            1122085995
        ],
        "machine\nup": [
            1122085995
        ],
        "/etc/kubernetes/adminconf": [
            1122085995
        ],
        "mas-\nter": [
            1122085995
        ],
        "scp": [
            1122085995
        ],
        "root@192168.64.138:/etc/kubernetes/admin.conf": [
            1122085995
        ],
        "~/kube/config2\nlisting": [
            1122085995
        ],
        "b8": [
            1122085995
        ],
        "net\n": [
            1122085995
        ],
        "\n\n551using": [
            1122085995
        ],
        "machine\nreplace": [
            1122085995
        ],
        "~/kube/config2": [
            1122085995
        ],
        "kubeconfig=~/kube/config2\nkubectl": [
            1122085995
        ],
        "unset\nthe": [
            1122085995
        ],
        "\n\n552\nappendix": [
            1122085995
        ],
        "c\nusing": [
            1122085995
        ],
        "container\nruntimes\nc1replacing": [
            1122085995
        ],
        "rkt\nwe’ve": [
            1122085995
        ],
        "rock-it)": [
            1122085995
        ],
        "as\nthose": [
            1122085995
        ],
        "differs": [
            1122085995
        ],
        "in\nminikube\n": [
            1122085995
        ],
        "pod\n(running": [
            1122085995
        ],
        "individual\ncontainers": [
            1122085995
        ],
        "tam-\npered": [
            1122085995
        ],
        "with)": [
            1122085995
        ],
        "client-server": [
            1122085995
        ],
        "architecture\nthat": [
            1122085995
        ],
        "rkt\nis": [
            1122085995
        ],
        "docker-formatted": [
            1122085995
        ],
        "to\nrepackage": [
            1122085995
        ],
        "rkt\nc.1.1configuring": [
            1122085995
        ],
        "rkt\nas": [
            1122085995
        ],
        "kubernetes\ncomponent": [
            1122085995
        ],
        "rkt\ninstead": [
            1122085995
        ],
        "\n--container-runtime=rkt": [
            1122085995
        ],
        "support\nfor": [
            1122085995
        ],
        "mature": [
            1122085995
        ],
        "get\nyou": [
            1122085995
        ],
        "started\n": [
            1122085995
        ],
        "\n\n553replacing": [
            1122085995
        ],
        "rkt\nc1.2trying": [
            1122085995
        ],
        "exe-\ncutable": [
            1122085995
        ],
        "options:\n$": [
            1122085995
        ],
        "--container-runtime=rkt": [
            1122085995
        ],
        "--network-plugin=cni": [
            1122085995
        ],
        "run\nso": [
            1122085995
        ],
        "it\nrunning": [
            1122085995
        ],
        "pod\nonce": [
            1122085995
        ],
        "--port": [
            1122085995
        ],
        "8080\ndeployment": [
            1122085995
        ],
        "created\nwhen": [
            1122085995
        ],
        "pods\nname:": [
            1122085995
        ],
        "kubia-3604679414-l1nn3\n..\nstatus:": [
            1122085995
        ],
        "replicaset/kubia-3604679414\ncontainers:\n": [
            1122085995
        ],
        "rkt://87a138ce-..-96e375852997:kubia": [
            1122085995
        ],
        "rkt://sha512-5bbc5c7df6148d30d74e0..": [
            1122085995
        ],
        "to\nhttp": [
            1122085995
        ],
        "kubectl\nport-forward\n": [
            1122085995
        ],
        "\ninspecting": [
            1122085995
        ],
        "ssh\nlisting": [
            1122085995
        ],
        "c1": [
            1122085995
        ],
        "rkt\nthe": [
            1122085995
        ],
        "\nmention": [
            1122085995
        ],
        "\ndocker\n": [
            1122085995
        ],
        "\n\n554appendix": [
            1122085995
        ],
        "runtimes\nthen": [
            1122085995
        ],
        "list\nuuid": [
            1122085995
        ],
        "..\n4900e0a5": [
            1122085995
        ],
        "k8s-dashboard": [
            1122085995
        ],
        "gcrio/google_containers/kun...": [
            1122085995
        ],
        "..\n564a6234": [
            1122085995
        ],
        "nginx-ingr-ctrlr": [
            1122085995
        ],
        "gcrio/google_containers/ngi...": [
            1122085995
        ],
        "..\n5dcafffd": [
            1122085995
        ],
        "dflt-http-backend": [
            1122085995
        ],
        "gcrio/google_containers/def...": [
            1122085995
        ],
        "..\n707a306c": [
            1122085995
        ],
        "kube-addon-manager": [
            1122085995
        ],
        "gcrio/google-containers/kub...": [
            1122085995
        ],
        "..\n87a138ce": [
            1122085995
        ],
        "registry-1docker.io/luksa/k...": [
            1122085995
        ],
        "..\nd97f5c29": [
            1122085995
        ],
        "kubedns": [
            1122085995
        ],
        "gcrio/google_containers/k8s...": [
            1122085995
        ],
        "dnsmasq": [
            1122085995
        ],
        "gcrio/google_containers/k8...": [
            1122085995
        ],
        "gcrio/google_containers/k8...\nyou": [
            1122085995
        ],
        "ones\ndeployed": [
            1122085995
        ],
        "\nuuid": [
            1122085995
        ],
        "columns?": [
            1122085995
        ],
        "they\nbelong": [
            1122085995
        ],
        "\nkubedns": [
            1122085995
        ],
        "pod\n(instead": [
            1122085995
        ],
        "uuid": [
            1122085995
        ],
        "tried": [
            1122085995
        ],
        "appreciate": [
            1122085995
        ],
        "easier\nit": [
            1122085995
        ],
        "infrastructure\ncontainer": [
            1122085995
        ],
        "of\nrkt’s": [
            1122085995
        ],
        "with\nrkt’s": [
            1122085995
        ],
        "\nrkt": [
            1122085995
        ],
        "list\nid": [
            1122085995
        ],
        "used\nsha512-a9c3": [
            1122085995
        ],
        "..addon-manager:v6.4-beta.1": [
            1122085995
        ],
        "245mib": [
            1122085995
        ],
        "ago\nsha512-a078": [
            1122085995
        ],
        "../rkt/stage1-coreos:1.24.0": [
            1122085995
        ],
        "224mib": [
            1122085995
        ],
        "ago\nsha512-5bbc": [
            1122085995
        ],
        "..ker.io/luksa/kubia:latest": [
            1122085995
        ],
        "13gib": [
            1122085995
        ],
        "ago\nsha512-3931": [
            1122085995
        ],
        "..es-dashboard-amd64:v1.6.1": [
            1122085995
        ],
        "257mib": [
            1122085995
        ],
        "ago\nsha512-2826": [
            1122085995
        ],
        "..ainers/defaultbackend:1.0": [
            1122085995
        ],
        "15mib": [
            1122085995
        ],
        "ago\nsha512-8b59": [
            1122085995
        ],
        "..s-controller:0.9.0-beta.4": [
            1122085995
        ],
        "233mib": [
            1122085995
        ],
        "ago\nsha512-7b59": [
            1122085995
        ],
        "..dns-kube-dns-amd64:1.14.2": [
            1122085995
        ],
        "100mib": [
            1122085995
        ],
        "ago\nsha512-39c6": [
            1122085995
        ],
        "..nsmasq-nanny-amd64:1.14.2": [
            1122085995
        ],
        "86mib": [
            1122085995
        ],
        "ago\nsha512-89fe": [
            1122085995
        ],
        "..-dns-sidecar-amd64:1.14.2": [
            1122085995
        ],
        "85mib": [
            1122085995
        ],
        "ago\nthese": [
            1122085995
        ],
        "(oci": [
            1122085995
        ],
        "initiative)": [
            1122085995
        ],
        "acbuild\nlisting": [
            1122085995
        ],
        "list\nlisting": [
            1122085995
        ],
        "c3": [
            1122085995
        ],
        "\n\n555using": [
            1122085995
        ],
        "cri\ntool": [
            1122085995
        ],
        "(available": [
            1122085995
        ],
        "https://githubcom/containers/build)": [
            1122085995
        ],
        "rkt\ndoing": [
            1122085995
        ],
        "you\nstarted": [
            1122085995
        ],
        "https://coreos\ncom/rkt": [
            1122085995
        ],
        "https://kubernetesio/docs": [
            1122085995
        ],
        "information\nc.2using": [
            1122085995
        ],
        "cri\nkubernetes’": [
            1122085995
        ],
        "rkt\nboth": [
            1122085995
        ],
        "integrated": [
            1122085995
        ],
        "(cri)": [
            1122085995
        ],
        "cri\nis": [
            1122085995
        ],
        "plug": [
            1122085995
        ],
        "without\nhaving": [
            1122085995
        ],
        "cri": [
            1122085995
        ],
        "uses\nboth": [
            1122085995
        ],
        "directly)\nc.2.1introducing": [
            1122085995
        ],
        "cri-o": [
            1122085995
        ],
        "runtime\nbeside": [
            1122085995
        ],
        "to\ndirectly": [
            1122085995
        ],
        "launch": [
            1122085995
        ],
        "oci-compliant": [
            1122085995
        ],
        "\n--container-runtime=crio\nc.2.2running": [
            1122085995
        ],
        "we\nexplored": [
            1122085995
        ],
        "system\nbut": [
            1122085995
        ],
        "frakti\nallows": [
            1122085995
        ],
        "hypervi-\nsor": [
            1122085995
        ],
        "isola-\ntion": [
            1122085995
        ],
        "mirantis": [
            1122085995
        ],
        "virtlet": [
            1122085995
        ],
        "which\nmakes": [
            1122085995
        ],
        "qcow2": [
            1122085995
        ],
        "is\none": [
            1122085995
        ],
        "qemu": [
            1122085995
        ],
        "tool)": [
            1122085995
        ],
        "that?\n": [
            1122085995
        ],
        "\n\n556\nappendix": [
            1122085995
        ],
        "d\ncluster": [
            1122085995
        ],
        "federation\nin": [
            1122085995
        ],
        "kubernetes\ncan": [
            1122085995
        ],
        "racks\nor": [
            1122085995
        ],
        "dark?\n": [
            1122085995
        ],
        "susceptible": [
            1122085995
        ],
        "datacenter-wide": [
            1122085995
        ],
        "outages": [
            1122085995
        ],
        "be\ndeployed": [
            1122085995
        ],
        "those\ndatacenters": [
            1122085995
        ],
        "routed\nto": [
            1122085995
        ],
        "zones\n": [
            1122085995
        ],
        "latency\nbetween": [
            1122085995
        ],
        "disconnected\nfrom": [
            1122085995
        ],
        "locations\na": [
            1122085995
        ],
        "location\nwe’ll": [
            1122085995
        ],
        "appendix\nd.1introducing": [
            1122085995
        ],
        "federation\nkubernetes": [
            1122085995
        ],
        "clusters\nthrough": [
            1122085995
        ],
        "locations": [
            1122085995
        ],
        "different\ncloud": [
            1122085995
        ],
        "(hybrid": [
            1122085995
        ],
        "cloud)": [
            1122085995
        ],
        "motiva-\ntion": [
            1122085995
        ],
        "super-cluster": [
            1122085995
        ],
        "cloud\nprovider’s": [
            1122085995
        ],
        "privacy-sensitive": [
            1122085995
        ],
        "cloud\nanother": [
            1122085995
        ],
        "on-premises\ncluster": [
            1122085995
        ],
        "cluster’s\n": [
            1122085995
        ],
        "\n\n557understanding": [
            1122085995
        ],
        "architecture\ncapacity": [
            1122085995
        ],
        "spill": [
            1122085995
        ],
        "cloud-based": [
            1122085995
        ],
        "infrastructure\nd.2understanding": [
            1122085995
        ],
        "architecture\nlet’s": [
            1122085995
        ],
        "clusters\ncan": [
            1122085995
        ],
        "federated": [
            1122085995
        ],
        "applications\nacross": [
            1122085995
        ],
        "but\nacross": [
            1122085995
        ],
        "things:\n■\netcd": [
            1122085995
        ],
        "objects\n■\nfederation": [
            1122085995
        ],
        "server\n■\nfederation": [
            1122085995
        ],
        "manager\nthis": [
            1122085995
        ],
        "the\nfederated": [
            1122085995
        ],
        "controllers\nthat": [
            1122085995
        ],
        "feder-\nated": [
            1122085995
        ],
        "the\nunderlying": [
            1122085995
        ],
        "clusters’": [
            1122085995
        ],
        "d1.\nsan": [
            1122085995
        ],
        "francisco\ncontrol": [
            1122085995
        ],
        "plane\netcd\nfederation": [
            1122085995
        ],
        "manager\ncontroller\nmanager\napi": [
            1122085995
        ],
        "server\netcd\nfederation\napi": [
            1122085995
        ],
        "server\nscheduler\nworker": [
            1122085995
        ],
        "node\nkubelet\nworker": [
            1122085995
        ],
        "node\nkubelet\ncontrol": [
            1122085995
        ],
        "plane\netcd\ncontroller\nmanager\napi": [
            1122085995
        ],
        "node\nkubelet\nlondon\nother\nlocations\nfigure": [
            1122085995
        ],
        "d1": [
            1122085995
        ],
        "locations\n": [
            1122085995
        ],
        "\n\n558appendix": [
            1122085995
        ],
        "acluster": [
            1122085995
        ],
        "federation\nd3understanding": [
            1122085995
        ],
        "\nd3.1introducing": [
            1122085995
        ],
        "resources\nat": [
            1122085995
        ],
        "supported:\n■\nnamespaces\n■\nconfigmaps": [
            1122085995
        ],
        "secrets\n■\nservices": [
            1122085995
        ],
        "ingresses\n■\ndeployments": [
            1122085995
        ],
        "daemonsets\n■\nhorizontalpodautoscalers\nnotecheck": [
            1122085995
        ],
        "up-to-\ndate": [
            1122085995
        ],
        "cluster\nobject": [
            1122085995
        ],
        "object\nrepresents": [
            1122085995
        ],
        "fed-\nerated": [
            1122085995
        ],
        "d2.\nsan": [
            1122085995
        ],
        "francisco\nnamespace:": [
            1122085995
        ],
        "foo\nreplicaset": [
            1122085995
        ],
        "x-432\nreplicas:": [
            1122085995
        ],
        "x-432-5\ndeployment": [
            1122085995
        ],
        "x\nreplicas:3\nfederated": [
            1122085995
        ],
        "resources\nsecret": [
            1122085995
        ],
        "w\nsecret": [
            1122085995
        ],
        "y\nnamespace:": [
            1122085995
        ],
        "x\nreplicas:5\nsecret": [
            1122085995
        ],
        "y\nlondon\nnamespace:": [
            1122085995
        ],
        "x-432-8\ndeployment": [
            1122085995
        ],
        "x\nreplicas:2\nsecret": [
            1122085995
        ],
        "y\ncluster:\nlondon\ncluster:\nsan": [
            1122085995
        ],
        "francisco\nsecret": [
            1122085995
        ],
        "wingress": [
            1122085995
        ],
        "z\nfigure": [
            1122085995
        ],
        "d2": [
            1122085995
        ],
        "\n\n559understanding": [
            1122085995
        ],
        "objects\nd3.2understanding": [
            1122085995
        ],
        "do\nfor": [
            1122085995
        ],
        "cluster-scoped": [
            1122085995
        ],
        "them\nuntil": [
            1122085995
        ],
        "modified\nversions": [
            1122085995
        ],
        "federated\nversions": [
            1122085995
        ],
        "synchronization": [
            1122085995
        ],
        "one-directional": [
            1122085995
        ],
        "only—from": [
            1122085995
        ],
        "server\ndown": [
            1122085995
        ],
        "cluster\nthe": [
            1122085995
        ],
        "namespace\nwith": [
            1122085995
        ],
        "daemonsets\n": [
            1122085995
        ],
        "pod\nreplicas": [
            1122085995
        ],
        "fed-\neration": [
            1122085995
        ],
        "deployments/replicasets": [
            1122085995
        ],
        "counts": [
            1122085995
        ],
        "federated\ndeployment": [
            1122085995
        ],
        "overridden\nnotecurrently": [
            1122085995
        ],
        "individu-\nally": [
            1122085995
        ],
        "clusters’\npods": [
            1122085995
        ],
        "server\na": [
            1122085995
        ],
        "any\ningress": [
            1122085995
        ],
        "an\ningress": [
            1122085995
        ],
        "multi-cluster-wide": [
            1122085995
        ],
        "the\nservices": [
            1122085995
        ],
        "\nnoteas": [
            1122085995
        ],
        "required\nfor": [
            1122085995
        ],
        "can\nlearn": [
            1122085995
        ],
        "http://\nkubernetesio/docs/.\n": [
            1122085995
        ],
        "\n\n": [
            1122085995
        ],
        "\n\n561\nindex\nsymbols\n$(env_var)": [
            1122085995
        ],
        "syntax199\n$(env_variable_name)": [
            1122085995
        ],
        "\nsyntax205\n$(var)": [
            1122085995
        ],
        "syntax198\n*": [
            1122085995
        ],
        "(asterisk)": [
            1122085995
        ],
        "character117\n-": [
            1122085995
        ],
        "(double": [
            1122085995
        ],
        "dash)125\nnumerics\n137": [
            1122085995
        ],
        "code88\n143": [
            1122085995
        ],
        "code89\n8080": [
            1122085995
        ],
        "port48": [
            1122085995
        ],
        "67\n8888": [
            1122085995
        ],
        "port67\na\n-a": [
            1122085995
        ],
        "option35\nabac": [
            1122085995
        ],
        "(attribute-based": [
            1122085995
        ],
        "\ncontrol)353\naccess": [
            1122085995
        ],
        "rbac\naccounts": [
            1122085995
        ],
        "accounts\nactions353–354\nactivedeadlineseconds": [
            1122085995
        ],
        "\nproperty116\nad": [
            1122085995
        ],
        "tasks112\nadapters\nnetwork": [
            1122085995
        ],
        "\nvms": [
            1122085995
        ],
        "540\nnode": [
            1122085995
        ],
        "304–305\nadded": [
            1122085995
        ],
        "event514\nadd-ons328–330\ncomponents": [
            1122085995
        ],
        "310\ndeploying": [
            1122085995
        ],
        "328–329\ndns": [
            1122085995
        ],
        "329\ningress": [
            1122085995
        ],
        "329\nusing": [
            1122085995
        ],
        "329–330\naddresses": [
            1122085995
        ],
        "attribute138\naddresses": [
            1122085995
        ],
        "servers239–240\nadmin": [
            1122085995
        ],
        "\nwith372\nalgorithms": [
            1122085995
        ],
        "\nscheduling319\naliases\ncreating": [
            1122085995
        ],
        "\nservices": [
            1122085995
        ],
        "134\nfor": [
            1122085995
        ],
        "41–42\nall": [
            1122085995
        ],
        "keyword82\n--all": [
            1122085995
        ],
        "option81\n--all-namespaces": [
            1122085995
        ],
        "option144\nallocatable": [
            1122085995
        ],
        "resources408\nallowed": [
            1122085995
        ],
        "\nconfiguring394–395\nadding": [
            1122085995
        ],
        "395\ndropping": [
            1122085995
        ],
        "395\nspecifying": [
            1122085995
        ],
        "394\nallowedcapabilities394\nalwayspullimages317\namazon": [
            1122085995
        ],
        "aws\nambassador": [
            1122085995
        ],
        "containers\ncommunicating": [
            1122085995
        ],
        "245\npatterns": [
            1122085995
        ],
        "244\nrunning": [
            1122085995
        ],
        "244–245\nsimplifying": [
            1122085995
        ],
        "commu-\nnication": [
            1122085995
        ],
        "243–245\nannotations\nadding": [
            1122085995
        ],
        "76\ndescribing": [
            1122085995
        ],
        "through\n498\nmodifying": [
            1122085995
        ],
        "76\nof": [
            1122085995
        ],
        "75–76\nupdating": [
            1122085995
        ],
        "232\napi": [
            1122085995
        ],
        "(application": [
            1122085995
        ],
        "inter-\nface)\naggregation": [
            1122085995
        ],
        "518–519\ncustom": [
            1122085995
        ],
        "\nobjects": [
            1122085995
        ],
        "518–519\ndefining": [
            1122085995
        ],
        "508–519\nautomating": [
            1122085995
        ],
        "\ncontrollers": [
            1122085995
        ],
        "513–517\ncrd": [
            1122085995
        ],
        "509–513\nproviding": [
            1122085995
        ],
        "\nservers": [
            1122085995
        ],
        "518–519\nvalidating": [
            1122085995
        ],
        "517–518\nfederated": [
            1122085995
        ],
        "558–559\nservice": [
            1122085995
        ],
        "521\nsee": [
            1122085995
        ],
        "api\napi": [
            1122085995
        ],
        "servers233–248": [
            1122085995
        ],
        "316–318": [
            1122085995
        ],
        "\n346–374\naccessing": [
            1122085995
        ],
        "234–235\nauthentication": [
            1122085995
        ],
        "346–353\ngroups": [
            1122085995
        ],
        "346–348\nservice": [
            1122085995
        ],
        "348–353\nusers": [
            1122085995
        ],
        "347–348\nauthorizing": [
            1122085995
        ],
        "317\ncommunicating": [
            1122085995
        ],
        "238–243\ncommunicating": [
            1122085995
        ],
        "295–297\ncommunicating": [
            1122085995
        ],
        "245\n": [
            1122085995
        ],
        "\n\nindex562\napi": [
            1122085995
        ],
        "(continued)\nconnecting": [
            1122085995
        ],
        "503\nconnecting": [
            1122085995
        ],
        "299\nexploring": [
            1122085995
        ],
        "235–236": [
            1122085995
        ],
        "248\nfinding": [
            1122085995
        ],
        "239–240\nmodifying": [
            1122085995
        ],
        "317\nnotifying": [
            1122085995
        ],
        "318–319\npersistently": [
            1122085995
        ],
        "318\nrunning": [
            1122085995
        ],
        "343\nrunning": [
            1122085995
        ],
        "326–327\nsecuring": [
            1122085995
        ],
        "353–373\nbinding": [
            1122085995
        ],
        "\naccounts": [
            1122085995
        ],
        "359–360\ndefault": [
            1122085995
        ],
        "clusterrolebindings\n371–373\ndefault": [
            1122085995
        ],
        "371–373\ngranting": [
            1122085995
        ],
        "\npermissions": [
            1122085995
        ],
        "373\nincluding": [
            1122085995
        ],
        "361\nrbac": [
            1122085995
        ],
        "\nplugins": [
            1122085995
        ],
        "353–354\nrbac": [
            1122085995
        ],
        "355–357\nusing": [
            1122085995
        ],
        "clusterrolebindings\n362–371\nusing": [
            1122085995
        ],
        "362–371\nusing": [
            1122085995
        ],
        "358–359\nusing": [
            1122085995
        ],
        "358–359\nsimplifying": [
            1122085995
        ],
        "communication\n243–245\nambassador": [
            1122085995
        ],
        "\npatterns": [
            1122085995
        ],
        "244–245\nusing": [
            1122085995
        ],
        "commu-\nnicate": [
            1122085995
        ],
        "246–248\nbuilding": [
            1122085995
        ],
        "\nopenapi": [
            1122085995
        ],
        "248\nbuilding": [
            1122085995
        ],
        "\nswagger": [
            1122085995
        ],
        "248\ninteracting": [
            1122085995
        ],
        "\njava": [
            1122085995
        ],
        "247–248\nusing": [
            1122085995
        ],
        "246–247\nusing": [
            1122085995
        ],
        "\ntokens": [
            1122085995
        ],
        "352–353\nvalidating": [
            1122085995
        ],
        "318\nverifying": [
            1122085995
        ],
        "240–241\nsee": [
            1122085995
        ],
        "api\napiversion": [
            1122085995
        ],
        "property106": [
            1122085995
        ],
        "510\napp=kubia": [
            1122085995
        ],
        "label98": [
            1122085995
        ],
        "123\napp=pc": [
            1122085995
        ],
        "selector72\napplication": [
            1122085995
        ],
        "logs65–66\napplication": [
            1122085995
        ],
        "\napi\napplication": [
            1122085995
        ],
        "\nopenshift": [
            1122085995
        ],
        "\nplatform528–529\napplications\naccessing": [
            1122085995
        ],
        "32\nbest": [
            1122085995
        ],
        "\ndeveloping": [
            1122085995
        ],
        "\n506–507\nauto-deploying": [
            1122085995
        ],
        "\nmanifests": [
            1122085995
        ],
        "504–505\nemploying": [
            1122085995
        ],
        "506\nensuring": [
            1122085995
        ],
        "\nhandled": [
            1122085995
        ],
        "492–\n497\nksonnet": [
            1122085995
        ],
        "\nwriting": [
            1122085995
        ],
        "505–506\npod": [
            1122085995
        ],
        "lifecycles": [
            1122085995
        ],
        "479–491\nusing": [
            1122085995
        ],
        "503–504\nversioning": [
            1122085995
        ],
        "504–505\ncompromised": [
            1122085995
        ],
        "373\ncontainerized": [
            1122085995
        ],
        "191–192\ncreating": [
            1122085995
        ],
        "290–291\ncreating": [
            1122085995
        ],
        "268\ndeploying": [
            1122085995
        ],
        "\nstatefulsets": [
            1122085995
        ],
        "291–295\ncreating": [
            1122085995
        ],
        "292–294\ncreating": [
            1122085995
        ],
        "291–292\ncreating": [
            1122085995
        ],
        "294\nexamining": [
            1122085995
        ],
        "295\nexamining": [
            1122085995
        ],
        "pods\n294–295\ndescribing": [
            1122085995
        ],
        "498\ndescriptions": [
            1122085995
        ],
        "19–20\nexamining": [
            1122085995
        ],
        "51–52\ndisplaying": [
            1122085995
        ],
        "51\ndisplaying": [
            1122085995
        ],
        "51\ninspecting": [
            1122085995
        ],
        "52\nexposing": [
            1122085995
        ],
        "255\nhandling": [
            1122085995
        ],
        "500–502\ncopying": [
            1122085995
        ],
        "500–501\ncopying": [
            1122085995
        ],
        "500–501\nhandling": [
            1122085995
        ],
        "\nstatements": [
            1122085995
        ],
        "\nlogging": [
            1122085995
        ],
        "501\nhighly": [
            1122085995
        ],
        "341\nhorizontally": [
            1122085995
        ],
        "48–50\nincreasing": [
            1122085995
        ],
        "49\nrequests": [
            1122085995
        ],
        "50\nresults": [
            1122085995
        ],
        "scale-out": [
            1122085995
        ],
        "49–50\nvisualizing": [
            1122085995
        ],
        "50\nimplementing": [
            1122085995
        ],
        "han-\ndlers": [
            1122085995
        ],
        "490–491\nin": [
            1122085995
        ],
        "415–416\nkilling": [
            1122085995
        ],
        "479–482\nexpecting": [
            1122085995
        ],
        "480\nexpecting": [
            1122085995
        ],
        "\nchange": [
            1122085995
        ],
        "480\nusing": [
            1122085995
        ],
        "\nrestarts": [
            1122085995
        ],
        "480–482\nmaking": [
            1122085995
        ],
        "497\nmanaging": [
            1122085995
        ],
        "497–502\nmonolithic": [
            1122085995
        ],
        "microservices\n3–6\nmulti-tier": [
            1122085995
        ],
        "59\nnodejs\ncreating": [
            1122085995
        ],
        "28–29\ndeploying": [
            1122085995
        ],
        "42–44\nnon-horizontally": [
            1122085995
        ],
        "\nleader-election": [
            1122085995
        ],
        "341\noverview": [
            1122085995
        ],
        "478–479\nproviding": [
            1122085995
        ],
        "6\nproviding": [
            1122085995
        ],
        "498–500\nrelocating": [
            1122085995
        ],
        "480–482\nrunning": [
            1122085995
        ],
        "19–21": [
            1122085995
        ],
        "497–502\nin": [
            1122085995
        ],
        "555\nkeeping": [
            1122085995
        ],
        "20–\n21\n": [
            1122085995
        ],
        "\n\nindex563\napplications": [
            1122085995
        ],
        "(continued)\nlocating": [
            1122085995
        ],
        "21\noutside": [
            1122085995
        ],
        "\ndevelopment": [
            1122085995
        ],
        "502–503\nscaling": [
            1122085995
        ],
        "21\nthrough": [
            1122085995
        ],
        "\nyaml": [
            1122085995
        ],
        "255\nshut-down": [
            1122085995
        ],
        "496–497\nsplitting": [
            1122085995
        ],
        "3–4\ntagging": [
            1122085995
        ],
        "497–498\nupdating": [
            1122085995
        ],
        "261–278\nblocking": [
            1122085995
        ],
        "\nversions": [
            1122085995
        ],
        "274–278\ncontrolling": [
            1122085995
        ],
        "271–273\ncreating": [
            1122085995
        ],
        "deployments\n262–264\npausing": [
            1122085995
        ],
        "process\n273–274\nrolling": [
            1122085995
        ],
        "\ndeployments": [
            1122085995
        ],
        "268–270\nupdating": [
            1122085995
        ],
        "deployments\n264–268\nusing": [
            1122085995
        ],
        "497–498\nusing": [
            1122085995
        ],
        "\nvs": [
            1122085995
        ],
        "498\nusing": [
            1122085995
        ],
        "\nreceiving": [
            1122085995
        ],
        "\nsignal": [
            1122085995
        ],
        "488–489\nargs": [
            1122085995
        ],
        "array196\narguments\ndefining": [
            1122085995
        ],
        "193–195\ncmd": [
            1122085995
        ],
        "193\nentrypoint": [
            1122085995
        ],
        "instruction\n193\nmaking": [
            1122085995
        ],
        "images\n194–195\nshell": [
            1122085995
        ],
        "forms": [
            1122085995
        ],
        "193–194\noverriding": [
            1122085995
        ],
        "kubernetes\n195–196\nsee": [
            1122085995
        ],
        "argu-\nments\nasterisks117\nat-most-one": [
            1122085995
        ],
        "semantics290\nattribute-based": [
            1122085995
        ],
        "\nabac\nauthenticating\napi": [
            1122085995
        ],
        "347–348\nservice": [
            1122085995
        ],
        "\n351–\n353\nusers": [
            1122085995
        ],
        "347–348\nclients": [
            1122085995
        ],
        "317\ncreating": [
            1122085995
        ],
        "223\nwith": [
            1122085995
        ],
        "241–242\nauthorization": [
            1122085995
        ],
        "rbac\n353–354\nauthorizations\nclients": [
            1122085995
        ],
        "317\ngranting": [
            1122085995
        ],
        "373\nservice": [
            1122085995
        ],
        "349\nauto-deploying": [
            1122085995
        ],
        "\nmanifests504–505\nautomatic": [
            1122085995
        ],
        "scaling23\nautomating": [
            1122085995
        ],
        "controllers\n513–517\nrunning": [
            1122085995
        ],
        "515–516\nwebsite": [
            1122085995
        ],
        "514–515\nautoscaler": [
            1122085995
        ],
        "\ndeployments446–447\nautoscaling\nmetrics": [
            1122085995
        ],
        "450\nprocess": [
            1122085995
        ],
        "438–441\ncalculating": [
            1122085995
        ],
        "439\nobtaining": [
            1122085995
        ],
        "metrics\n438–439\nupdating": [
            1122085995
        ],
        "\nscaled": [
            1122085995
        ],
        "440\nsee": [
            1122085995
        ],
        "autoscaling\navailability": [
            1122085995
        ],
        "zones\nco-locating": [
            1122085995
        ],
        "471\ndeploying": [
            1122085995
        ],
        "same\n471–472\navailability-zone": [
            1122085995
        ],
        "label467\naws": [
            1122085995
        ],
        "services)37": [
            1122085995
        ],
        "\n174": [
            1122085995
        ],
        "454\nawselasticblockstore": [
            1122085995
        ],
        "volume162": [
            1122085995
        ],
        "\n174\nazuredisk": [
            1122085995
        ],
        "174\nazurefile": [
            1122085995
        ],
        "volume174\nb\nbackend": [
            1122085995
        ],
        "\nto502\nbackend-database129–130\nbase": [
            1122085995
        ],
        "images29\nbash": [
            1122085995
        ],
        "process33\nbatch": [
            1122085995
        ],
        "\nendpoints236–237\nbesteffort": [
            1122085995
        ],
        "\nto417\nbinary": [
            1122085995
        ],
        "for217\nbinding\nroles": [
            1122085995
        ],
        "accounts\n359–360\nservice": [
            1122085995
        ],
        "525\nto": [
            1122085995
        ],
        "namespaces\n377–379\nblocking": [
            1122085995
        ],
        "rollouts274–278\nconfiguring": [
            1122085995
        ],
        "deadlines": [
            1122085995
        ],
        "278\ndefining": [
            1122085995
        ],
        "\nprevent": [
            1122085995
        ],
        "275\nminreadyseconds": [
            1122085995
        ],
        "274–275\npreventing": [
            1122085995
        ],
        "277–278\nupdating": [
            1122085995
        ],
        "276–277\nblue-green": [
            1122085995
        ],
        "deployment253\nborg16\nbrokers": [
            1122085995
        ],
        "brokers\nbuildconfigs": [
            1122085995
        ],
        "using529\nburstable": [
            1122085995
        ],
        "\npods418\nbusybox": [
            1122085995
        ],
        "image26": [
            1122085995
        ],
        "112\nc\n-c": [
            1122085995
        ],
        "option66": [
            1122085995
        ],
        "245\nca": [
            1122085995
        ],
        "(certificate": [
            1122085995
        ],
        "authority)170": [
            1122085995
        ],
        "\n240\n--cacert": [
            1122085995
        ],
        "option240\ncadvisor431\ncanary": [
            1122085995
        ],
        "release69": [
            1122085995
        ],
        "273\ncapabilities\nadding": [
            1122085995
        ],
        "containers\n385–386": [
            1122085995
        ],
        "395\nkernel": [
            1122085995
        ],
        "384–385\nspecifying": [
            1122085995
        ],
        "394\nsee": [
            1122085995
        ],
        "capabilities\ncapacity": [
            1122085995
        ],
        "nodes407–408\ncap_chown": [
            1122085995
        ],
        "capability385": [
            1122085995
        ],
        "395\ncap_sys_time": [
            1122085995
        ],
        "capability384\ncategorizing": [
            1122085995
        ],
        "\nlabels74\ncentos": [
            1122085995
        ],
        "image541\ncentral": [
            1122085995
        ],
        "processing": [
            1122085995
        ],
        "cpus\ncephfs": [
            1122085995
        ],
        "volume163\ncert": [
            1122085995
        ],
        "secrets221\ncertificate-authority-data": [
            1122085995
        ],
        "field536\ncertificates": [
            1122085995
        ],
        "tls147–149\nci/cd": [
            1122085995
        ],
        "(continuous": [
            1122085995
        ],
        "\ndelivery)506\ncinder": [
            1122085995
        ],
        "volume163\nclaiming\nbenefits": [
            1122085995
        ],
        "182\npersistentvolumes": [
            1122085995
        ],
        "179–181\nclient": [
            1122085995
        ],
        "downloading39\n": [
            1122085995
        ],
        "\n\nindex564\nclient": [
            1122085995
        ],
        "through\n246–248\nbuilding": [
            1122085995
        ],
        "248\nexploring": [
            1122085995
        ],
        "246–247\nclient-certificate-data": [
            1122085995
        ],
        "\nproperty536\nclient-key-data": [
            1122085995
        ],
        "property536\nclients\nauthenticating": [
            1122085995
        ],
        "317\nauthorizing": [
            1122085995
        ],
        "317\ncustom": [
            1122085995
        ],
        "519\nhandling": [
            1122085995
        ],
        "492–497\nnon-preservation": [
            1122085995
        ],
        "142\nnotifying": [
            1122085995
        ],
        "318–319\npods": [
            1122085995
        ],
        "\nsecrets": [
            1122085995
        ],
        "525–526\npreventing": [
            1122085995
        ],
        "\nconnections": [
            1122085995
        ],
        "492–497\nsequence": [
            1122085995
        ],
        "\ndeletion": [
            1122085995
        ],
        "493–495\ntroubleshooting": [
            1122085995
        ],
        "495–496\nsee": [
            1122085995
        ],
        "clients\ncloning": [
            1122085995
        ],
        "vms545–547\nchanging": [
            1122085995
        ],
        "546\nconfiguring": [
            1122085995
        ],
        "\nhosts": [
            1122085995
        ],
        "546–547\nshutting": [
            1122085995
        ],
        "545\ncloud": [
            1122085995
        ],
        "from452–453\ncluster": [
            1122085995
        ],
        "autoscaler452–453\nenabling": [
            1122085995
        ],
        "454\nrelinquishing": [
            1122085995
        ],
        "453\nrequesting": [
            1122085995
        ],
        "infrastructure\n452–453\ncluster": [
            1122085995
        ],
        "observing332–333\ncluster": [
            1122085995
        ],
        "federation556–559\narchitecture": [
            1122085995
        ],
        "557\nfederated": [
            1122085995
        ],
        "558–559\noverview": [
            1122085995
        ],
        "556–557\ncluster": [
            1122085995
        ],
        "431\ndisplaying": [
            1122085995
        ],
        "431\nhorizontal": [
            1122085995
        ],
        "452–456\nsecuring": [
            1122085995
        ],
        "375–403\nconfiguring": [
            1122085995
        ],
        "secu-\nrity": [
            1122085995
        ],
        "380–389\nisolating": [
            1122085995
        ],
        "networks\n399–402\nrestricting": [
            1122085995
        ],
        "security-\nrelated": [
            1122085995
        ],
        "389–399\nusing": [
            1122085995
        ],
        "376–380\nclustered": [
            1122085995
        ],
        "stores303–304\ncluster-internal": [
            1122085995
        ],
        "connect-\ning": [
            1122085995
        ],
        "servers299\nclusterip": [
            1122085995
        ],
        "field154": [
            1122085995
        ],
        "293\nclusterip": [
            1122085995
        ],
        "service45\ncluster-level": [
            1122085995
        ],
        "to362–365\nclusterrolebindings\ncombing": [
            1122085995
        ],
        "\nclusterroles": [
            1122085995
        ],
        "370–371\ncombining": [
            1122085995
        ],
        "\nrolebindings": [
            1122085995
        ],
        "370–371\ndefault": [
            1122085995
        ],
        "371–373\nusing": [
            1122085995
        ],
        "362–371\nallowing": [
            1122085995
        ],
        "362–365\nallowing": [
            1122085995
        ],
        "365–367\nclusterroles362–371\nallowing": [
            1122085995
        ],
        "\nurls": [
            1122085995
        ],
        "365–367\ncombining": [
            1122085995
        ],
        "clusterrole-\nbindings": [
            1122085995
        ],
        "371–373\nallowing": [
            1122085995
        ],
        "372\nallowing": [
            1122085995
        ],
        "372\ngranting": [
            1122085995
        ],
        "372\nusing": [
            1122085995
        ],
        "367–370\nclusters\nadding": [
            1122085995
        ],
        "536\ncombining": [
            1122085995
        ],
        "504\nconfirming": [
            1122085995
        ],
        "38\nconnecting": [
            1122085995
        ],
        "\noutside": [
            1122085995
        ],
        "131–134\ncreating": [
            1122085995
        ],
        "134\nservice": [
            1122085995
        ],
        "39\ndeleting": [
            1122085995
        ],
        "538\nenabling": [
            1122085995
        ],
        "356\netcd": [
            1122085995
        ],
        "342–343\nhighly": [
            1122085995
        ],
        "running\n341–345\nmaking": [
            1122085995
        ],
        "342–345\nhosted": [
            1122085995
        ],
        "38–41\nin": [
            1122085995
        ],
        "36–42\nrunning": [
            1122085995
        ],
        "\nclusters": [
            1122085995
        ],
        "minikube\n37–38\nsetting": [
            1122085995
        ],
        "41–42\nsetting": [
            1122085995
        ],
        "\ncompletion": [
            1122085995
        ],
        "41–42\nin": [
            1122085995
        ],
        "536\nkubernetes": [
            1122085995
        ],
        "of\n18–19\nlimiting": [
            1122085995
        ],
        "454–456\nlisting": [
            1122085995
        ],
        "538\nlisting": [
            1122085995
        ],
        "237–238\nlisting": [
            1122085995
        ],
        "40\nlisting": [
            1122085995
        ],
        "523\nlocal": [
            1122085995
        ],
        "37–38\nmodifying": [
            1122085995
        ],
        "536\nmulti-node": [
            1122085995
        ],
        "kubeadm\n539–551\nconfiguring": [
            1122085995
        ],
        "masters": [
            1122085995
        ],
        "547–549\nconfiguring": [
            1122085995
        ],
        "549–550\nsetting": [
            1122085995
        ],
        "\nsystems": [
            1122085995
        ],
        "539\nsetting": [
            1122085995
        ],
        "\npackages": [
            1122085995
        ],
        "539\noverview": [
            1122085995
        ],
        "39\nrunning": [
            1122085995
        ],
        "433\nrunning": [
            1122085995
        ],
        "433\nsecuring": [
            1122085995
        ],
        "355–357\n": [
            1122085995
        ],
        "\n\nindex565\nclusters": [
            1122085995
        ],
        "(continued)\nstarting": [
            1122085995
        ],
        "37–38\ntesting": [
            1122085995
        ],
        "124\ntwo-node": [
            1122085995
        ],
        "468\ntying": [
            1122085995
        ],
        "537\nusing": [
            1122085995
        ],
        "machines\n550–551\nusing": [
            1122085995
        ],
        "534–538\nadding": [
            1122085995
        ],
        "\nentries": [
            1122085995
        ],
        "536–537\nconfiguring": [
            1122085995
        ],
        "535\ncontents": [
            1122085995
        ],
        "535–536\ndeleting": [
            1122085995
        ],
        "536–537\nmodifying": [
            1122085995
        ],
        "536–537\nswitching": [
            1122085995
        ],
        "\ncontexts": [
            1122085995
        ],
        "538\nswitching": [
            1122085995
        ],
        "534–535\nsee": [
            1122085995
        ],
        "clusterrolebindings;": [
            1122085995
        ],
        "\nclusterroles;": [
            1122085995
        ],
        "rolebind-\nings;": [
            1122085995
        ],
        "roles;": [
            1122085995
        ],
        "nodes\ncmd": [
            1122085995
        ],
        "instruction193\ncni": [
            1122085995
        ],
        "(container": [
            1122085995
        ],
        "\ninterface)335": [
            1122085995
        ],
        "549\ncollecting": [
            1122085995
        ],
        "usages\n430–432\ncommand": [
            1122085995
        ],
        "column334\ncommand-line": [
            1122085995
        ],
        "arguments\npassing": [
            1122085995
        ],
        "204–205\npassing": [
            1122085995
        ],
        "192–196\ncommand-line": [
            1122085995
        ],
        "\nkubectl41–42\ncommands\ndefining": [
            1122085995
        ],
        "forms\n193–194\noverriding": [
            1122085995
        ],
        "kubernetes\n195–196\nremotely": [
            1122085995
        ],
        "124–126\ncommunication\nbetween": [
            1122085995
        ],
        "337–338\nbetween": [
            1122085995
        ],
        "336–337\nof": [
            1122085995
        ],
        "311\nwith": [
            1122085995
        ],
        "352–353\nwith": [
            1122085995
        ],
        "295–297\ncompletions": [
            1122085995
        ],
        "property114\ncomponents\nadd-ons": [
            1122085995
        ],
        "310\ncommunicating": [
            1122085995
        ],
        "311\ninvolved": [
            1122085995
        ],
        "330\nisolating": [
            1122085995
        ],
        "\ntechnologies": [
            1122085995
        ],
        "8\nof": [
            1122085995
        ],
        "310\nmaking": [
            1122085995
        ],
        "available\n342–345\nusing": [
            1122085995
        ],
        "344\nof": [
            1122085995
        ],
        "310–312\nrunning": [
            1122085995
        ],
        "311–312\nrunning": [
            1122085995
        ],
        "548–549\ncomponentstatus311\ncomputational": [
            1122085995
        ],
        "resources404–436\nlimiting": [
            1122085995
        ],
        "412–416\nexceeding": [
            1122085995
        ],
        "414–415\nlimits": [
            1122085995
        ],
        "415–416\nsetting": [
            1122085995
        ],
        "412–413\nlimiting": [
            1122085995
        ],
        "425–429\nlimiting": [
            1122085995
        ],
        "427–428\nresourcequota": [
            1122085995
        ],
        "425–427\nspecifying": [
            1122085995
        ],
        "427\nspecifying": [
            1122085995
        ],
        "429\nspecifying": [
            1122085995
        ],
        "429\nmonitoring": [
            1122085995
        ],
        "\nusage": [
            1122085995
        ],
        "430–434\nanalyzing": [
            1122085995
        ],
        "\nstatistics": [
            1122085995
        ],
        "432–434\ncollecting": [
            1122085995
        ],
        "\nusages": [
            1122085995
        ],
        "430–432\nretrieving": [
            1122085995
        ],
        "430–432\nstoring": [
            1122085995
        ],
        "432–434\npod": [
            1122085995
        ],
        "417–421\ndefining": [
            1122085995
        ],
        "417–419\nkilling": [
            1122085995
        ],
        "420–\n421\nrequesting": [
            1122085995
        ],
        "405–412\ncreating": [
            1122085995
        ],
        "405–406\ndefining": [
            1122085995
        ],
        "resources\n411–412\neffect": [
            1122085995
        ],
        "\ncpu": [
            1122085995
        ],
        "411\neffect": [
            1122085995
        ],
        "406–410\nrequesting": [
            1122085995
        ],
        "411–412\nsetting": [
            1122085995
        ],
        "421–425\napplying": [
            1122085995
        ],
        "424–425\ncreating": [
            1122085995
        ],
        "422–423\nenforcing": [
            1122085995
        ],
        "423–424\nlimitrange": [
            1122085995
        ],
        "resources\n421–422\nconditions": [
            1122085995
        ],
        "\nselectors72\nconfig": [
            1122085995
        ],
        "application211–213\nediting": [
            1122085995
        ],
        "212\nsignaling": [
            1122085995
        ],
        "\nconfig": [
            1122085995
        ],
        "212\nupdating": [
            1122085995
        ],
        "213\nupdating": [
            1122085995
        ],
        "automatically\n212–213\nconfig-fileconf": [
            1122085995
        ],
        "file201\nconfig_foo-bar": [
            1122085995
        ],
        "variable204\nconfigmap": [
            1122085995
        ],
        "volume\nexamining": [
            1122085995
        ],
        "\ncontents": [
            1122085995
        ],
        "209\nexposing": [
            1122085995
        ],
        "205–211\nsetting": [
            1122085995
        ],
        "211\nconfigmaps\ncreating": [
            1122085995
        ],
        "199–201": [
            1122085995
        ],
        "206–207\ncombining": [
            1122085995
        ],
        "201\nentries": [
            1122085995
        ],
        "201\nfrom": [
            1122085995
        ],
        "201\nusing": [
            1122085995
        ],
        "command\n200–201\ndecoupling": [
            1122085995
        ],
        "configurations": [
            1122085995
        ],
        "198–213\nediting": [
            1122085995
        ],
        "212\nexposing": [
            1122085995
        ],
        "205–211\nexamining": [
            1122085995
        ],
        "contents\n209\nmounting": [
            1122085995
        ],
        "210\n": [
            1122085995
        ],
        "\n\nindex566\nconfigmaps": [
            1122085995
        ],
        "(continued)\nsetting": [
            1122085995
        ],
        "211\nverifying": [
            1122085995
        ],
        "208\nexposing": [
            1122085995
        ],
        "volume\n209–210\nmounting": [
            1122085995
        ],
        "files\n210–211\nnon-existing": [
            1122085995
        ],
        "203\noverview": [
            1122085995
        ],
        "198\npassing": [
            1122085995
        ],
        "202–205\nupdating": [
            1122085995
        ],
        "213\nusing": [
            1122085995
        ],
        "207–208\nversus": [
            1122085995
        ],
        "217–218\nconfigurations": [
            1122085995
        ],
        "decoupling": [
            1122085995
        ],
        "\nconfigmap\ncreating": [
            1122085995
        ],
        "200–201\npassing": [
            1122085995
        ],
        "211–213\nusing": [
            1122085995
        ],
        "205–211\nconfiguring\nallowed": [
            1122085995
        ],
        "394–395\nadding": [
            1122085995
        ],
        "394\ncontainer": [
            1122085995
        ],
        "380–389\nadding": [
            1122085995
        ],
        "\ncapabilities": [
            1122085995
        ],
        "384–385\ndropping": [
            1122085995
        ],
        "385–386\npreventing": [
            1122085995
        ],
        "382\npreventing": [
            1122085995
        ],
        "\nfilesystems": [
            1122085995
        ],
        "386–387\nrunning": [
            1122085995
        ],
        "381–382\nrunning": [
            1122085995
        ],
        "\nmode": [
            1122085995
        ],
        "382–384\nrunning": [
            1122085995
        ],
        "specify-\ning": [
            1122085995
        ],
        "contexts\n381\nsharing": [
            1122085995
        ],
        "\nusers": [
            1122085995
        ],
        "387–389\ncontainerized": [
            1122085995
        ],
        "applications\n191–192\ndeadlines": [
            1122085995
        ],
        "278\ndefault": [
            1122085995
        ],
        "394\ndisallowed": [
            1122085995
        ],
        "394\nhost": [
            1122085995
        ],
        "\ningress": [
            1122085995
        ],
        "145\ningress": [
            1122085995
        ],
        "147–149\ninterval": [
            1122085995
        ],
        "\nimages": [
            1122085995
        ],
        "194–195\njob": [
            1122085995
        ],
        "117\nkubernetes": [
            1122085995
        ],
        "552\nlocation": [
            1122085995
        ],
        "535\nmasters": [
            1122085995
        ],
        "547–549\nrunning": [
            1122085995
        ],
        "548–549\nrunning": [
            1122085995
        ],
        "ini-\ntialize": [
            1122085995
        ],
        "547–548\nname": [
            1122085995
        ],
        "hosts\n546–547\nnetwork": [
            1122085995
        ],
        "540\npod": [
            1122085995
        ],
        "462\nproperties": [
            1122085995
        ],
        "88–89\nresource": [
            1122085995
        ],
        "451\nschedule": [
            1122085995
        ],
        "117\nservice": [
            1122085995
        ],
        "\nmanually": [
            1122085995
        ],
        "132–134\ncreating": [
            1122085995
        ],
        "133–134\ncreating": [
            1122085995
        ],
        "132–133\nsession": [
            1122085995
        ],
        "126\ntab": [
            1122085995
        ],
        "41–42\nworker": [
            1122085995
        ],
        "kubeadm\n549–550\nconnections\nexternal": [
            1122085995
        ],
        "141–142\npreventing": [
            1122085995
        ],
        "breakage": [
            1122085995
        ],
        "\nshut": [
            1122085995
        ],
        "493–497\nsequence": [
            1122085995
        ],
        "495–496\npreventing": [
            1122085995
        ],
        "492–493\nsignaling": [
            1122085995
        ],
        "\naccept": [
            1122085995
        ],
        "149–153\nto": [
            1122085995
        ],
        "503\nto": [
            1122085995
        ],
        "502\nto": [
            1122085995
        ],
        "131–\n134\nto": [
            1122085995
        ],
        "130\nto": [
            1122085995
        ],
        "\nbalancers": [
            1122085995
        ],
        "139–141\ncontainer": [
            1122085995
        ],
        "images\nbuilding": [
            1122085995
        ],
        "29–32\nimage": [
            1122085995
        ],
        "30–31\noverview": [
            1122085995
        ],
        "30\nwith": [
            1122085995
        ],
        "31–32\ncreating": [
            1122085995
        ],
        "29\ncreating": [
            1122085995
        ],
        "28–29\nin": [
            1122085995
        ],
        "\nplatform": [
            1122085995
        ],
        "15\npushing": [
            1122085995
        ],
        "\nregistry": [
            1122085995
        ],
        "35–36\npushing": [
            1122085995
        ],
        "\nhub": [
            1122085995
        ],
        "\nmachines": [
            1122085995
        ],
        "36\ntagging": [
            1122085995
        ],
        "35\nremoving": [
            1122085995
        ],
        "34–35\nrunning": [
            1122085995
        ],
        "32–33\naccessing": [
            1122085995
        ],
        "32\nlisting": [
            1122085995
        ],
        "32\nobtaining": [
            1122085995
        ],
        "33\nstopping": [
            1122085995
        ],
        "34–35\nversioning": [
            1122085995
        ],
        "28\nviewing": [
            1122085995
        ],
        "33–34\nisolating": [
            1122085995
        ],
        "34\nprocesses": [
            1122085995
        ],
        "\noperating": [
            1122085995
        ],
        "34\nrunning": [
            1122085995
        ],
        "shells": [
            1122085995
        ],
        "33\nwith": [
            1122085995
        ],
        "393\ncontainer": [
            1122085995
        ],
        "\ncni\ncontainer": [
            1122085995
        ],
        "\nsee": [
            1122085995
        ],
        "kubernetes-cni\ncontainer": [
            1122085995
        ],
        "specifying63–65\n": [
            1122085995
        ],
        "\n\nindex567\ncontainer": [
            1122085995
        ],
        "runtimes552–555\nreplacing": [
            1122085995
        ],
        "rkt\n552–555\nconfiguring": [
            1122085995
        ],
        "552\nusing": [
            1122085995
        ],
        "minikube\n553–555\nusing": [
            1122085995
        ],
        "555\ncri-o": [
            1122085995
        ],
        "runtime\n555\nrunning": [
            1122085995
        ],
        "555\ncontainer_cpu_request\n_millicores": [
            1122085995
        ],
        "variable228\ncontainercreating486\ncontainer_memory_limit\n_kibibytes": [
            1122085995
        ],
        "variable228\n--container-runtime=rkt": [
            1122085995
        ],
        "option\n552–553\ncontainers7–16\nadding": [
            1122085995
        ],
        "395\nadding": [
            1122085995
        ],
        "capa-\nbilities": [
            1122085995
        ],
        "384–385\ncomparing": [
            1122085995
        ],
        "8–10\nconfiguring": [
            1122085995
        ],
        "380–389\npreventing": [
            1122085995
        ],
        "382\nrunning": [
            1122085995
        ],
        "381\ncopying": [
            1122085995
        ],
        "from\n500–501\ncopying": [
            1122085995
        ],
        "from\n500–501\ndetermining": [
            1122085995
        ],
        "418\ndocker": [
            1122085995
        ],
        "platform\n12–15\nbuilding": [
            1122085995
        ],
        "13\ncomparing": [
            1122085995
        ],
        "14–15\nconcepts": [
            1122085995
        ],
        "12–13\ndistributing": [
            1122085995
        ],
        "13\nimage": [
            1122085995
        ],
        "15\nportability": [
            1122085995
        ],
        "15\nrunning": [
            1122085995
        ],
        "13\ndropping": [
            1122085995
        ],
        "385–386": [
            1122085995
        ],
        "395\nexisting": [
            1122085995
        ],
        "33\nexploring": [
            1122085995
        ],
        "33\nimages\ncreating": [
            1122085995
        ],
        "290–291\nlisting": [
            1122085995
        ],
        "554–555\ninit": [
            1122085995
        ],
        "484–485\ninstead": [
            1122085995
        ],
        "555\nisolating": [
            1122085995
        ],
        "34\nlimiting": [
            1122085995
        ],
        "412–416\nlimits": [
            1122085995
        ],
        "415–416\nlinux": [
            1122085995
        ],
        "8\nlisting": [
            1122085995
        ],
        "32\nlocating": [
            1122085995
        ],
        "21\nmaking": [
            1122085995
        ],
        "497\nmechanisms": [
            1122085995
        ],
        "11\nmounting": [
            1122085995
        ],
        "503\nmultiple": [
            1122085995
        ],
        "56–57\nobtaining": [
            1122085995
        ],
        "33\nof": [
            1122085995
        ],
        "pods\nrequesting": [
            1122085995
        ],
        "405–412\nresources": [
            1122085995
        ],
        "for\n411–412\nrunning": [
            1122085995
        ],
        "332\norganizing": [
            1122085995
        ],
        "58–60\nsplitting": [
            1122085995
        ],
        "59\nsplitting": [
            1122085995
        ],
        "59\noverview": [
            1122085995
        ],
        "8–12\nisolating": [
            1122085995
        ],
        "11\nlimiting": [
            1122085995
        ],
        "11–12\npartial": [
            1122085995
        ],
        "57\npassing": [
            1122085995
        ],
        "argu-\nments": [
            1122085995
        ],
        "192–196\ndefining": [
            1122085995
        ],
        "193–195\ndefining": [
            1122085995
        ],
        "193–195\noverriding": [
            1122085995
        ],
        "195–196\noverriding": [
            1122085995
        ],
        "195–196\nrunning": [
            1122085995
        ],
        "195–196\npassing": [
            1122085995
        ],
        "202–203\npods": [
            1122085995
        ],
        "determin-\ning": [
            1122085995
        ],
        "419\npost-start": [
            1122085995
        ],
        "\nhooks": [
            1122085995
        ],
        "486–487\npre-stop\nusing": [
            1122085995
        ],
        "hooks\n487–488\nusing": [
            1122085995
        ],
        "signal\n488–489\npreventing": [
            1122085995
        ],
        "386–387\nprocesses": [
            1122085995
        ],
        "34\nremotely": [
            1122085995
        ],
        "124–126\nremoving": [
            1122085995
        ],
        "34–35\nrkt": [
            1122085995
        ],
        "alter-\nnative": [
            1122085995
        ],
        "15–16\nrunning": [
            1122085995
        ],
        "20–21\neffect": [
            1122085995
        ],
        "descrip-\ntions": [
            1122085995
        ],
        "19–20\ninspecting": [
            1122085995
        ],
        "\nvm": [
            1122085995
        ],
        "553–554\nviewing": [
            1122085995
        ],
        "33–34\nrunning": [
            1122085995
        ],
        "503\nrunning": [
            1122085995
        ],
        "130–131\nseeing": [
            1122085995
        ],
        "416\nseeing": [
            1122085995
        ],
        "196–198\nconfiguring": [
            1122085995
        ],
        "variables\n197\ndisadvantages": [
            1122085995
        ],
        "198\nreferring": [
            1122085995
        ],
        "198\nsetting": [
            1122085995
        ],
        "412–413\ncreating": [
            1122085995
        ],
        "412–413\novercommitting": [
            1122085995
        ],
        "413\nsetting": [
            1122085995
        ],
        "550\nsharing": [
            1122085995
        ],
        "163–169\nusing": [
            1122085995
        ],
        "volume\n163–166\nusing": [
            1122085995
        ],
        "166–169\nsharing": [
            1122085995
        ],
        "57\nsharing": [
            1122085995
        ],
        "387–389\nspecifying": [
            1122085995
        ],
        "197\nspecifying": [
            1122085995
        ],
        "retriev-\ning": [
            1122085995
        ],
        "66\n": [
            1122085995
        ],
        "\n\nindex568\ncontainers": [
            1122085995
        ],
        "(continued)\nspecifying": [
            1122085995
        ],
        "394\nstopping": [
            1122085995
        ],
        "34–35\ntargeting": [
            1122085995
        ],
        "489\nusing": [
            1122085995
        ],
        "213–223\nconfigmaps": [
            1122085995
        ],
        "versus": [
            1122085995
        ],
        "secrets\n217–218\ncreating": [
            1122085995
        ],
        "216\ndefault": [
            1122085995
        ],
        "214–215\nimage": [
            1122085995
        ],
        "222–223\noverview": [
            1122085995
        ],
        "214\nusing": [
            1122085995
        ],
        "pods\n218–222\nusing": [
            1122085995
        ],
        "480–482\nwhen": [
            1122085995
        ],
        "59–60\nwith": [
            1122085995
        ],
        "\nhandling": [
            1122085995
        ],
        "420–421\nsee": [
            1122085995
        ],
        "images;": [
            1122085995
        ],
        "side-\ncar": [
            1122085995
        ],
        "containers\n--containers": [
            1122085995
        ],
        "option432\ncontainers": [
            1122085995
        ],
        "docker-based13\ncontentagent": [
            1122085995
        ],
        "container162\ncontexts\ncurrent": [
            1122085995
        ],
        "536\ndeleting": [
            1122085995
        ],
        "538\nin": [
            1122085995
        ],
        "536\nlisting": [
            1122085995
        ],
        "538\nusing": [
            1122085995
        ],
        "537–538\nsee": [
            1122085995
        ],
        "contexts\ncontinuous": [
            1122085995
        ],
        "delivery6–7\nbenefits": [
            1122085995
        ],
        "7\nrole": [
            1122085995
        ],
        "7\ncontinuous": [
            1122085995
        ],
        "con-\ntinuous": [
            1122085995
        ],
        "ci/cd\ncontrol": [
            1122085995
        ],
        "plane18–19\ncomponents": [
            1122085995
        ],
        "344\nmaking": [
            1122085995
        ],
        "342–345\nensuring": [
            1122085995
        ],
        "343–344\nensuring": [
            1122085995
        ],
        "\nscheduler": [
            1122085995
        ],
        "343–344\nrunning": [
            1122085995
        ],
        "342–343\nrunning": [
            1122085995
        ],
        "343\nusing": [
            1122085995
        ],
        "components\n344\ncontroller": [
            1122085995
        ],
        "component19\ncontrollers145": [
            1122085995
        ],
        "321\n–326": [
            1122085995
        ],
        "330–333\nchain": [
            1122085995
        ],
        "331–332\ndeployment": [
            1122085995
        ],
        "331\nkubelet": [
            1122085995
        ],
        "332\nreplicaset": [
            1122085995
        ],
        "332\nscheduler": [
            1122085995
        ],
        "\nnewly": [
            1122085995
        ],
        "332\ncomponents": [
            1122085995
        ],
        "330\ncustom": [
            1122085995
        ],
        "513–517\ndaemonset": [
            1122085995
        ],
        "324\ndeployment": [
            1122085995
        ],
        "324\nendpoints": [
            1122085995
        ],
        "325\nensuring": [
            1122085995
        ],
        "343–344\njob": [
            1122085995
        ],
        "324\nnamespace": [
            1122085995
        ],
        "325\nnode": [
            1122085995
        ],
        "324\nobserving": [
            1122085995
        ],
        "332–333\noverview": [
            1122085995
        ],
        "326\npersistentvolume": [
            1122085995
        ],
        "325–326\nremoving": [
            1122085995
        ],
        "100\nreplicaset": [
            1122085995
        ],
        "324\nreplication": [
            1122085995
        ],
        "323–324\nrunning": [
            1122085995
        ],
        "515–516\nservice": [
            1122085995
        ],
        "324\nstatefulset": [
            1122085995
        ],
        "324\nwebsite": [
            1122085995
        ],
        "514–515\ncopies": [
            1122085995
        ],
        "of21\ncopying\nfiles": [
            1122085995
        ],
        "containers\n500–501\nimages": [
            1122085995
        ],
        "\ndirectly": [
            1122085995
        ],
        "504\nlogs": [
            1122085995
        ],
        "500–501\ncpus": [
            1122085995
        ],
        "(central": [
            1122085995
        ],
        "units)\ncreating": [
            1122085995
        ],
        "442–443\ncreating": [
            1122085995
        ],
        "425–426\ndisplaying": [
            1122085995
        ],
        "usage\nfor": [
            1122085995
        ],
        "431\nfor": [
            1122085995
        ],
        "431–432\nnodes": [
            1122085995
        ],
        "416\nscaling": [
            1122085995
        ],
        "441–447\nautomatic": [
            1122085995
        ],
        "\nevents": [
            1122085995
        ],
        "444–445\ncreating": [
            1122085995
        ],
        "442–443\nmaximum": [
            1122085995
        ],
        "447\nmodifying": [
            1122085995
        ],
        "447\ntriggering": [
            1122085995
        ],
        "scale-ups": [
            1122085995
        ],
        "445–446\nusing": [
            1122085995
        ],
        "446–447\ntime": [
            1122085995
        ],
        "411\ncrashloopbackoff\n414": [
            1122085995
        ],
        "499\ncrd": [
            1122085995
        ],
        "(customresource-\ndefinitions)509–513\ncreating": [
            1122085995
        ],
        "510–511\ncreating": [
            1122085995
        ],
        "511–512\ndeleting": [
            1122085995
        ],
        "512–513\nexamples": [
            1122085995
        ],
        "509–510\nretrieving": [
            1122085995
        ],
        "512\ncreation_method": [
            1122085995
        ],
        "label72\ncredentials": [
            1122085995
        ],
        "credentials\ncri": [
            1122085995
        ],
        "inter-\nface)": [
            1122085995
        ],
        "run-\ntimes": [
            1122085995
        ],
        "through555\ncronjob": [
            1122085995
        ],
        "creating\n116–117\ncrud": [
            1122085995
        ],
        "\ndelete)316\ncsr": [
            1122085995
        ],
        "(certificatesigning-\nrequest)148\ncurl": [
            1122085995
        ],
        "command124–126": [
            1122085995
        ],
        "130–131": [
            1122085995
        ],
        "\n167\ncurl": [
            1122085995
        ],
        "containers\n244–245\ncurl_ca_bundle": [
            1122085995
        ],
        "variable241\ncurrent": [
            1122085995
        ],
        "column49\ncustom": [
            1122085995
        ],
        "with195–196\ncustom-namespaceyaml": [
            1122085995
        ],
        "file78\nd\n-d": [
            1122085995
        ],
        "flag32\ndaemonsets324": [
            1122085995
        ],
        "559\ncreating": [
            1122085995
        ],
        "111\ncreating": [
            1122085995
        ],
        "110\nexamples": [
            1122085995
        ],
        "109\nrunning": [
            1122085995
        ],
        "108–112\nrunning": [
            1122085995
        ],
        "109–112\nadding": [
            1122085995
        ],
        "111\nremoving": [
            1122085995
        ],
        "111–112\nrunning": [
            1122085995
        ],
        "109\ndashboard52–53\naccessing": [
            1122085995
        ],
        "52\naccessing": [
            1122085995
        ],
        "53\n": [
            1122085995
        ],
        "\n\nindex569\ndata\nbinary": [
            1122085995
        ],
        "217\npassing": [
            1122085995
        ],
        "217–218\ncreating": [
            1122085995
        ],
        "secrets\n214–215\nimage": [
            1122085995
        ],
        "pods\n218–222\npersisted": [
            1122085995
        ],
        "pod\n173–174\nusing": [
            1122085995
        ],
        "480–482\nwriting": [
            1122085995
        ],
        "mon-\ngodb": [
            1122085995
        ],
        "173\nwritten": [
            1122085995
        ],
        "\ndisappearing": [
            1122085995
        ],
        "480\ndata": [
            1122085995
        ],
        "clustered303–304\ndeadlines": [
            1122085995
        ],
        "\nrollouts278\ndeclarative": [
            1122085995
        ],
        "scaling103\ndecoupling\nconfigurations": [
            1122085995
        ],
        "198–213\nconfigmaps": [
            1122085995
        ],
        "198–199\ncreating": [
            1122085995
        ],
        "configmaps\n200–201\npassing": [
            1122085995
        ],
        "204\npassing": [
            1122085995
        ],
        "\narguments": [
            1122085995
        ],
        "202–203\nupdating": [
            1122085995
        ],
        "211\npods": [
            1122085995
        ],
        "176–184\nbenefits": [
            1122085995
        ],
        "182\npersistentvolumeclaims\n176–177": [
            1122085995
        ],
        "179–181\npersistentvolumes": [
            1122085995
        ],
        "176–184\ndefault": [
            1122085995
        ],
        "\nconfiguring394–\n395": [
            1122085995
        ],
        "403\nadding": [
            1122085995
        ],
        "394\ndefault": [
            1122085995
        ],
        "policy397\ndefault": [
            1122085995
        ],
        "secret214–215\ndefaultaddcapabilities394\ndefaultmode": [
            1122085995
        ],
        "property232\ndeis": [
            1122085995
        ],
        "manager\n530–533\ndeleted": [
            1122085995
        ],
        "event515\ndeleting\nclusters": [
            1122085995
        ],
        "538\ncontexts": [
            1122085995
        ],
        "538\ninstances": [
            1122085995
        ],
        "512–513\npersistentvolumeclaims": [
            1122085995
        ],
        "288\npet": [
            1122085995
        ],
        "297–298\npods": [
            1122085995
        ],
        "80–82": [
            1122085995
        ],
        "252–253": [
            1122085995
        ],
        "306\nby": [
            1122085995
        ],
        "80–81\nby": [
            1122085995
        ],
        "80\nforcibly": [
            1122085995
        ],
        "307\nin": [
            1122085995
        ],
        "81–82\nmanually": [
            1122085995
        ],
        "306–307\nsequence": [
            1122085995
        ],
        "493–495\nusing": [
            1122085995
        ],
        "80\npods": [
            1122085995
        ],
        "307\nreplicationcontrollers": [
            1122085995
        ],
        "103–104\nresources": [
            1122085995
        ],
        "82\ndeletiontimestamp": [
            1122085995
        ],
        "field489": [
            1122085995
        ],
        "495\ndelivery": [
            1122085995
        ],
        "delivery\ndependencies\ninter-pod": [
            1122085995
        ],
        "485\noverview": [
            1122085995
        ],
        "5\ndeploying\nadd-ons": [
            1122085995
        ],
        "328–329\napplications": [
            1122085995
        ],
        "pods\n294–295\napplications": [
            1122085995
        ],
        "simplifying": [
            1122085995
        ],
        "21–22\nmicroservices": [
            1122085995
        ],
        "5\nnewly": [
            1122085995
        ],
        "deploymentconfigs\n529–530\nnodejs": [
            1122085995
        ],
        "42–44\nbehind": [
            1122085995
        ],
        "44\nlisting": [
            1122085995
        ],
        "43–44\npods\nin": [
            1122085995
        ],
        "\nzone": [
            1122085995
        ],
        "471–472\nco-locating": [
            1122085995
        ],
        "\navailability": [
            1122085995
        ],
        "471\ntopologykey": [
            1122085995
        ],
        "471–472\nin": [
            1122085995
        ],
        "\nregions": [
            1122085995
        ],
        "468\non": [
            1122085995
        ],
        "\ninter-pod": [
            1122085995
        ],
        "affinity\n468–471\nwith": [
            1122085995
        ],
        "\nids": [
            1122085995
        ],
        "393\nwith": [
            1122085995
        ],
        "470\nwith": [
            1122085995
        ],
        "393\nprivileged": [
            1122085995
        ],
        "\npodsecuritypolicy": [
            1122085995
        ],
        "\nallow": [
            1122085995
        ],
        "396–397\nresources": [
            1122085995
        ],
        "helm\n531–533\nversions": [
            1122085995
        ],
        "269\ndeployment": [
            1122085995
        ],
        "controllers\ncreating": [
            1122085995
        ],
        "331\noverview": [
            1122085995
        ],
        "resource\nbenefits": [
            1122085995
        ],
        "267–268\ncreating": [
            1122085995
        ],
        "262–264\nmanifest": [
            1122085995
        ],
        "262\nrolling": [
            1122085995
        ],
        "\nrevision": [
            1122085995
        ],
        "270\nstrategies": [
            1122085995
        ],
        "264–265\nupdating": [
            1122085995
        ],
        "264–268\nslowing": [
            1122085995
        ],
        "\nupdates": [
            1122085995
        ],
        "265\ntriggering": [
            1122085995
        ],
        "265–267\nusing": [
            1122085995
        ],
        "\ndeclaratively": [
            1122085995
        ],
        "271–273\npausing": [
            1122085995
        ],
        "268–270\ndeploymentconfigs": [
            1122085995
        ],
        "with529–530\n": [
            1122085995
        ],
        "\n\nindex570\ndeployments250–279\ndisplaying": [
            1122085995
        ],
        "270\nperforming": [
            1122085995
        ],
        "254–261\nobsolescence": [
            1122085995
        ],
        "roll-\ning-update": [
            1122085995
        ],
        "260–261\nperforming": [
            1122085995
        ],
        "256–260\nrunning": [
            1122085995
        ],
        "254–255\nrolling": [
            1122085995
        ],
        "268–270\ncreating": [
            1122085995
        ],
        "269\nrolling": [
            1122085995
        ],
        "270\nundoing": [
            1122085995
        ],
        "269\nrollouts": [
            1122085995
        ],
        "263\nupdating": [
            1122085995
        ],
        "251–253\ndeleting": [
            1122085995
        ],
        "252–253\nreplacing": [
            1122085995
        ],
        "pods\n252–253\nupdating": [
            1122085995
        ],
        "276–277\nusing": [
            1122085995
        ],
        "475\nusing": [
            1122085995
        ],
        "446–447\ndeprovisioning": [
            1122085995
        ],
        "instances526\ndescribe": [
            1122085995
        ],
        "command409\ndescriptors\njson": [
            1122085995
        ],
        "61–67\nyaml\ncreating": [
            1122085995
        ],
        "63–65\ncreating": [
            1122085995
        ],
        "61–67\nof": [
            1122085995
        ],
        "61–63\ndesired": [
            1122085995
        ],
        "column49\ndevelopers": [
            1122085995
        ],
        "\ndelivery7\ndevops7\ndirectories\nmounting": [
            1122085995
        ],
        "210\nusing": [
            1122085995
        ],
        "282\ndirectory": [
            1122085995
        ],
        "attack353\ndisabling": [
            1122085995
        ],
        "firewalls544\ndisallowed": [
            1122085995
        ],
        "394\ndisk=ssd": [
            1122085995
        ],
        "label110\ndisks": [
            1122085995
        ],
        "\ndisappearing480\ndisruptions": [
            1122085995
        ],
        "services454–456\ndistributing": [
            1122085995
        ],
        "images13\ndns": [
            1122085995
        ],
        "system)300\ndiscovering": [
            1122085995
        ],
        "through\n155–156\ndiscovering": [
            1122085995
        ],
        "129\nimplementing": [
            1122085995
        ],
        "301–302\nrecords": [
            1122085995
        ],
        "155–156\nservers": [
            1122085995
        ],
        "329\ndnspolicy": [
            1122085995
        ],
        "property129\ndocker": [
            1122085995
        ],
        "platform25–54\nclusters": [
            1122085995
        ],
        "37–38\nsetting": [
            1122085995
        ],
        "41–42\nusing": [
            1122085995
        ],
        "\ngke": [
            1122085995
        ],
        "38–41\ncomparing": [
            1122085995
        ],
        "12–13\ncontainer": [
            1122085995
        ],
        "26–36\nbuilding": [
            1122085995
        ],
        "29–32\ncreating": [
            1122085995
        ],
        "28–29\npushing": [
            1122085995
        ],
        "35–36\nremoving": [
            1122085995
        ],
        "32–33\nstopping": [
            1122085995
        ],
        "34–35\nviewing": [
            1122085995
        ],
        "33–34\ndefining": [
            1122085995
        ],
        "\ninstruction": [
            1122085995
        ],
        "193\nmaking": [
            1122085995
        ],
        "194–195\nshell": [
            1122085995
        ],
        "193–194\ndefining": [
            1122085995
        ],
        "193–194\nimages\nbuilding": [
            1122085995
        ],
        "13\ndistributing": [
            1122085995
        ],
        "13\nlayers": [
            1122085995
        ],
        "13\ninstalling": [
            1122085995
        ],
        "26–28\nregistries": [
            1122085995
        ],
        "\nauthenticating": [
            1122085995
        ],
        "223\nrkt": [
            1122085995
        ],
        "\nkubernetes\naccessing": [
            1122085995
        ],
        "applications\n45–47\ndeploying": [
            1122085995
        ],
        "42–44\nexamining": [
            1122085995
        ],
        "51–52\nhorizontally": [
            1122085995
        ],
        "48–50\nlogical": [
            1122085995
        ],
        "47–48\nusing": [
            1122085995
        ],
        "52–53\nrunning": [
            1122085995
        ],
        "26–28\nbehind": [
            1122085995
        ],
        "27\nrunning": [
            1122085995
        ],
        "27\nversioning": [
            1122085995
        ],
        "28\ndocker": [
            1122085995
        ],
        "hub\npushing": [
            1122085995
        ],
        "36\nusing": [
            1122085995
        ],
        "222\ndocker": [
            1122085995
        ],
        "id35\ndocker": [
            1122085995
        ],
        "command35\ndocker": [
            1122085995
        ],
        "platform\nreplacing": [
            1122085995
        ],
        "552–555\nconfiguring": [
            1122085995
        ],
        "503–504\ndocker": [
            1122085995
        ],
        "command548\ndocker": [
            1122085995
        ],
        "command44\ndocker": [
            1122085995
        ],
        "registry13\ndocker": [
            1122085995
        ],
        "command26\ndocker": [
            1122085995
        ],
        "installing544–545\ndisabling": [
            1122085995
        ],
        "544\ndisabling": [
            1122085995
        ],
        "544\nenabling": [
            1122085995
        ],
        "netbridge.bridge-nf-\ncall-iptables": [
            1122085995
        ],
        "545\ndockerfile\nbuilding": [
            1122085995
        ],
        "29\n": [
            1122085995
        ],
        "\n\nindex571\ndocker_host": [
            1122085995
        ],
        "variable32": [
            1122085995
        ],
        "504\ndocker-registry": [
            1122085995
        ],
        "definitions223\ndocuments": [
            1122085995
        ],
        "\ndatabase173\ndoesnotexist": [
            1122085995
        ],
        "operator107\ndomain": [
            1122085995
        ],
        "dns\ndouble": [
            1122085995
        ],
        "character125\ndownloading": [
            1122085995
        ],
        "binaries39\ndowntime": [
            1122085995
        ],
        "\ninstances341\ndownward": [
            1122085995
        ],
        "api\nbenefits": [
            1122085995
        ],
        "233\npassing": [
            1122085995
        ],
        "through\n226–233\navailable": [
            1122085995
        ],
        "226–227\nexposing": [
            1122085995
        ],
        "227–230\npassing": [
            1122085995
        ],
        "230–233\ndownwardapi": [
            1122085995
        ],
        "\nin163": [
            1122085995
        ],
        "230–233\nbenefits": [
            1122085995
        ],
        "233\nreferring": [
            1122085995
        ],
        "\nspecification": [
            1122085995
        ],
        "233\nupdating": [
            1122085995
        ],
        "232\nupdating": [
            1122085995
        ],
        "232\ndownwardapiitems": [
            1122085995
        ],
        "attribute231\ndsl": [
            1122085995
        ],
        "(domain-specific-\nlanguage)248\ndynamic": [
            1122085995
        ],
        "persistentvolumes\n184–189\ndefining": [
            1122085995
        ],
        "storage-\nclass": [
            1122085995
        ],
        "185\nrequesting": [
            1122085995
        ],
        "\npersistentvolume-\nclaims": [
            1122085995
        ],
        "185–187\nwithout": [
            1122085995
        ],
        "187–189\nwithout": [
            1122085995
        ],
        "class\ncreating": [
            1122085995
        ],
        "188–189\nexamining": [
            1122085995
        ],
        "\nclasses": [
            1122085995
        ],
        "188\nlisting": [
            1122085995
        ],
        "187–188\npersistentvolumeclaims": [
            1122085995
        ],
        "\npersistent-volumes": [
            1122085995
        ],
        "189\ne\necho": [
            1122085995
        ],
        "command27\nedit": [
            1122085995
        ],
        "modifi-\ncation": [
            1122085995
        ],
        "with372\nediting\nconfigmap": [
            1122085995
        ],
        "212\ndefinitions": [
            1122085995
        ],
        "102–103\neditor": [
            1122085995
        ],
        "variable102\nefk": [
            1122085995
        ],
        "stack501\nelastic": [
            1122085995
        ],
        "aws174\nelk": [
            1122085995
        ],
        "stack501\nemptydir": [
            1122085995
        ],
        "volume162–166\ncreating": [
            1122085995
        ],
        "164–165\nseeing": [
            1122085995
        ],
        "165\nspecifying": [
            1122085995
        ],
        "166\nusing": [
            1122085995
        ],
        "163–164\n--enable-swagger-ui=true": [
            1122085995
        ],
        "\noption248\nendpoints": [
            1122085995
        ],
        "controllers325\nendpoints": [
            1122085995
        ],
        "selectors\n133–134\nentrypoint": [
            1122085995
        ],
        "instruction193\nenv": [
            1122085995
        ],
        "command128\nenv": [
            1122085995
        ],
        "label72\nenv=debug": [
            1122085995
        ],
        "label70\nenv=devel": [
            1122085995
        ],
        "label105\nenv=prod": [
            1122085995
        ],
        "label70\nenv=production": [
            1122085995
        ],
        "label105\nenvfrom": [
            1122085995
        ],
        "attribute204\nenvironment": [
            1122085995
        ],
        "variables\nconfiguring": [
            1122085995
        ],
        "through\n197–198\ndisadvantages": [
            1122085995
        ],
        "\nhardcoding": [
            1122085995
        ],
        "198\ndiscovering": [
            1122085995
        ],
        "128–129\nexposing": [
            1122085995
        ],
        "227–230\nexposing": [
            1122085995
        ],
        "221–222\npassing": [
            1122085995
        ],
        "202–203\nreferring": [
            1122085995
        ],
        "198\nspecifying": [
            1122085995
        ],
        "197\nephemeral": [
            1122085995
        ],
        "pods121\netcd": [
            1122085995
        ],
        "cluster518\netcd": [
            1122085995
        ],
        "stores312–316\nensuring": [
            1122085995
        ],
        "314–315\nensuring": [
            1122085995
        ],
        "\nclustered": [
            1122085995
        ],
        "315\nensuring": [
            1122085995
        ],
        "314–315\nnumber": [
            1122085995
        ],
        "316\nrunning": [
            1122085995
        ],
        "342–343\nstoring": [
            1122085995
        ],
        "313–314\netcd": [
            1122085995
        ],
        "component19\nexec": [
            1122085995
        ],
        "\nforms193–194\nexec": [
            1122085995
        ],
        "probe86": [
            1122085995
        ],
        "150\nexists": [
            1122085995
        ],
        "operator107": [
            1122085995
        ],
        "461\nexplain": [
            1122085995
        ],
        "command175\nexposing\nmultiple": [
            1122085995
        ],
        "126–127\nmultiple": [
            1122085995
        ],
        "146–147\nmapping": [
            1122085995
        ],
        "147\nmapping": [
            1122085995
        ],
        "146\nservices": [
            1122085995
        ],
        "142–149\nbenefits": [
            1122085995
        ],
        "142–143\nconfiguring": [
            1122085995
        ],
        "\ntls": [
            1122085995
        ],
        "147–149\ncreating": [
            1122085995
        ],
        "144\nusing": [
            1122085995
        ],
        "controllers\n143–144\nservices": [
            1122085995
        ],
        "\nroutes": [
            1122085995
        ],
        "530\nservices": [
            1122085995
        ],
        "clients\n134–142\nexternal": [
            1122085995
        ],
        "141–142\nthrough": [
            1122085995
        ],
        "138–141\nusing": [
            1122085995
        ],
        "135–138\nexternal": [
            1122085995
        ],
        "clients\nallowing": [
            1122085995
        ],
        "137–138\nexposing": [
            1122085995
        ],
        "134–142\nexternal": [
            1122085995
        ],
        "connections\n141–142\nthrough": [
            1122085995
        ],
        "\nfor134\nexternalip138\nexternal-ip": [
            1122085995
        ],
        "column136\nexternalname": [
            1122085995
        ],
        "\ncreating134\nexternal-traffic": [
            1122085995
        ],
        "annotation141\n": [
            1122085995
        ],
        "\n\nindex572\nf\nfabric8": [
            1122085995
        ],
        "\nwith247–248\nfailedpoststarthook487\nfailedprestophook488\nfallbacktologsonerror500\nfeatures": [
            1122085995
        ],
        "fea-\ntures\nfederation": [
            1122085995
        ],
        "federation\nfiles\ncopying": [
            1122085995
        ],
        "500–501\ncreating": [
            1122085995
        ],
        "201\nin": [
            1122085995
        ],
        "\nfile": [
            1122085995
        ],
        "211\nin": [
            1122085995
        ],
        "\npassing": [
            1122085995
        ],
        "through\n230–233\nin": [
            1122085995
        ],
        "168\nmounted": [
            1122085995
        ],
        "208\nmounting": [
            1122085995
        ],
        "503\nmounting": [
            1122085995
        ],
        "210–211\nmounting": [
            1122085995
        ],
        "210\non": [
            1122085995
        ],
        "169–170\nupdating": [
            1122085995
        ],
        "212–213\nusing": [
            1122085995
        ],
        "211\nfilesystems\nof": [
            1122085995
        ],
        "containers\nisolating": [
            1122085995
        ],
        "34\npreventing": [
            1122085995
        ],
        "386–387\nworker": [
            1122085995
        ],
        "169–170\nfirewalls\nchanging": [
            1122085995
        ],
        "137–138\ndisabling": [
            1122085995
        ],
        "544\nflannel312\nflat": [
            1122085995
        ],
        "inter-pod58\nflexvolume": [
            1122085995
        ],
        "volume163\nflocker": [
            1122085995
        ],
        "volume163\nfoo": [
            1122085995
        ],
        "namespace358\nfoodefault.svc.cluster.local": [
            1122085995
        ],
        "\ndomain286\nfoo_secret": [
            1122085995
        ],
        "variable221\n--force": [
            1122085995
        ],
        "option490\nfortune\nimages": [
            1122085995
        ],
        "194–195\npods": [
            1122085995
        ],
        "\nintervals": [
            1122085995
        ],
        "195–196\nfortune": [
            1122085995
        ],
        "command\n163": [
            1122085995
        ],
        "165\nfortune": [
            1122085995
        ],
        "in197\nfortune-config": [
            1122085995
        ],
        "https\n218–219\nfortune-https": [
            1122085995
        ],
        "directory216\nfortune-https": [
            1122085995
        ],
        "\npods219–220\nfortuneloop": [
            1122085995
        ],
        "container209\nfortuneloopsh": [
            1122085995
        ],
        "script164\nfortune-podyaml": [
            1122085995
        ],
        "file164\nforwarding": [
            1122085995
        ],
        "pod67\nfqdn": [
            1122085995
        ],
        "(fully": [
            1122085995
        ],
        "\nname)129–130": [
            1122085995
        ],
        "134\nfrom": [
            1122085995
        ],
        "directive497\n--from-file": [
            1122085995
        ],
        "argument201\nfsgroup": [
            1122085995
        ],
        "policies392–394\ndeploying": [
            1122085995
        ],
        "\nimage": [
            1122085995
        ],
        "393\nusing": [
            1122085995
        ],
        "392–393\nfsgroup": [
            1122085995
        ],
        "property388\ng\ngateways58\ngce": [
            1122085995
        ],
        "\nengine)185": [
            1122085995
        ],
        "454\ncreating": [
            1122085995
        ],
        "171–172\nusing": [
            1122085995
        ],
        "171–174\ncreating": [
            1122085995
        ],
        "\ndisks": [
            1122085995
        ],
        "171–172\ncreating": [
            1122085995
        ],
        "gceper-\nsistentdisk": [
            1122085995
        ],
        "172\nre-creating": [
            1122085995
        ],
        "173–174\nverifying": [
            1122085995
        ],
        "\npersisted": [
            1122085995
        ],
        "173–174\nwriting": [
            1122085995
        ],
        "docu-\nments": [
            1122085995
        ],
        "\ndatabase": [
            1122085995
        ],
        "173\ngcepersistentdisk": [
            1122085995
        ],
        "volumes\ncreating": [
            1122085995
        ],
        "172\noverview": [
            1122085995
        ],
        "190\ngcloud": [
            1122085995
        ],
        "command39": [
            1122085995
        ],
        "171\ngcloud": [
            1122085995
        ],
        "command97\n--generator": [
            1122085995
        ],
        "flag42\ngeographical": [
            1122085995
        ],
        "regions\nco-locating": [
            1122085995
        ],
        "471–472\ngit": [
            1122085995
        ],
        "repositories\nas": [
            1122085995
        ],
        "166–169\nfiles": [
            1122085995
        ],
        "168\ngitrepo": [
            1122085995
        ],
        "169\nsidecar": [
            1122085995
        ],
        "168\ncloned": [
            1122085995
        ],
        "\nserving": [
            1122085995
        ],
        "167\nprivate": [
            1122085995
        ],
        "168\ngit": [
            1122085995
        ],
        "container168\ngitrepo": [
            1122085995
        ],
        "field510": [
            1122085995
        ],
        "517\ngitrepo": [
            1122085995
        ],
        "volumes\nfiles": [
            1122085995
        ],
        "168\noverview": [
            1122085995
        ],
        "169\nusing": [
            1122085995
        ],
        "\nrepositories": [
            1122085995
        ],
        "168\ngke": [
            1122085995
        ],
        "\nengine)36": [
            1122085995
        ],
        "454\naccessing": [
            1122085995
        ],
        "52\nand": [
            1122085995
        ],
        "\nbetween": [
            1122085995
        ],
        "534–535\nswitching": [
            1122085995
        ],
        "534\nswitching": [
            1122085995
        ],
        "534\nusing": [
            1122085995
        ],
        "with\n38–41\ncreating": [
            1122085995
        ],
        "39\ndownloading": [
            1122085995
        ],
        "\nbinaries": [
            1122085995
        ],
        "39\ngetting": [
            1122085995
        ],
        "39\nlisting": [
            1122085995
        ],
        "40\nretrieving": [
            1122085995
        ],
        "41\nsetting": [
            1122085995
        ],
        "\nprojects": [
            1122085995
        ],
        "39\nglusterfs": [
            1122085995
        ],
        "volume\n163\ngoogle": [
            1122085995
        ],
        "\nprojects39\ngoogle": [
            1122085995
        ],
        "gce\ngoogle": [
            1122085995
        ],
        "registry35\ngoverning": [
            1122085995
        ],
        "292–294\noverview": [
            1122085995
        ],
        "285–287\ngpu=true": [
            1122085995
        ],
        "label75": [
            1122085995
        ],
        "464–465\ngrafana": [
            1122085995
        ],
        "suite\nanalyzing": [
            1122085995
        ],
        "433–434\noverview": [
            1122085995
        ],
        "432\nrunning": [
            1122085995
        ],
        "433\ngroup": [
            1122085995
        ],
        "(gid)381\ngrouping": [
            1122085995
        ],
        "\nnamespaces76–80\n": [
            1122085995
        ],
        "\n\nindex573\ngroups347–348\nassigning": [
            1122085995
        ],
        "396–399\nin": [
            1122085995
        ],
        "528\nguaranteed": [
            1122085995
        ],
        "\nto417–418\nguarantees": [
            1122085995
        ],
        "statefulsets289–290\nat-most-one": [
            1122085995
        ],
        "290\nimplications": [
            1122085995
        ],
        "\nidentity": [
            1122085995
        ],
        "289–290\nimplications": [
            1122085995
        ],
        "289–290\nh\nhardcoding": [
            1122085995
        ],
        "disadvantages": [
            1122085995
        ],
        "of198\nhardware": [
            1122085995
        ],
        "\nof22\nheadless": [
            1122085995
        ],
        "154–155\ndns": [
            1122085995
        ],
        "155–156\nusing": [
            1122085995
        ],
        "154–156\ndiscovering": [
            1122085995
        ],
        "156\ndiscovering": [
            1122085995
        ],
        "\ndns": [
            1122085995
        ],
        "155–156\nhealth": [
            1122085995
        ],
        "checking22–23\nhealth": [
            1122085995
        ],
        "endpoint89\nheapster": [
            1122085995
        ],
        "\nenabling431\nhello": [
            1122085995
        ],
        "container26–28\nbehind": [
            1122085995
        ],
        "28\nhelm": [
            1122085995
        ],
        "\nmanager\nhooks": [
            1122085995
        ],
        "hooks\nhops": [
            1122085995
        ],
        "hops\nhorizontal": [
            1122085995
        ],
        "\npods438–451\nautoscaling": [
            1122085995
        ],
        "438–441\nmetrics": [
            1122085995
        ],
        "450\nscaling\ndown": [
            1122085995
        ],
        "replicas\n450–451\non": [
            1122085995
        ],
        "441–447\non": [
            1122085995
        ],
        "448\non": [
            1122085995
        ],
        "448–450\nhorizontal": [
            1122085995
        ],
        "\nusage442–443\nhorizontal": [
            1122085995
        ],
        "\nnodes452–456\ncluster": [
            1122085995
        ],
        "452–453\nenabling": [
            1122085995
        ],
        "454\nlimiting": [
            1122085995
        ],
        "scale-\ndown": [
            1122085995
        ],
        "454–456\nhost": [
            1122085995
        ],
        "header147\nhost": [
            1122085995
        ],
        "namespaces\n377–379\nhost": [
            1122085995
        ],
        "\npods376\n–380\nhost": [
            1122085995
        ],
        "in34\nhost": [
            1122085995
        ],
        "to377–379\nhostipc": [
            1122085995
        ],
        "property380\nhostnames\nchanging": [
            1122085995
        ],
        "546\nexpecting": [
            1122085995
        ],
        "480\nhostnetwork": [
            1122085995
        ],
        "property376\nhostpath": [
            1122085995
        ],
        "volumes162": [
            1122085995
        ],
        "169–170\nhostport": [
            1122085995
        ],
        "property377\nhosts\nconfiguring": [
            1122085995
        ],
        "546–547\nmapping": [
            1122085995
        ],
        "146\nhpa": [
            1122085995
        ],
        "(horizontalpodautoscaler)\nmodifying": [
            1122085995
        ],
        "447\noverview": [
            1122085995
        ],
        "443\nhtml-generator": [
            1122085995
        ],
        "container165\nhttp": [
            1122085995
        ],
        "probe85": [
            1122085995
        ],
        "150\nhttp-based": [
            1122085995
        ],
        "\ncreating86–87\nhttpget": [
            1122085995
        ],
        "probe87\nhttps": [
            1122085995
        ],
        "(hypertext": [
            1122085995
        ],
        "\nprotocol": [
            1122085995
        ],
        "secure)\n218–219\nhybrid": [
            1122085995
        ],
        "cloud556\nhypervisors9\ni\nidentities\nnetwork": [
            1122085995
        ],
        "285–287\ngoverning": [
            1122085995
        ],
        "\noverview": [
            1122085995
        ],
        "285–287\nscaling": [
            1122085995
        ],
        "287\nof": [
            1122085995
        ],
        "verifying\n240–241\nproviding": [
            1122085995
        ],
        "pods\n282–284\nstable": [
            1122085995
        ],
        "of\n289–290\nif": [
            1122085995
        ],
        "statement268\nignoredduringexecution464\nimage": [
            1122085995
        ],
        "layers15": [
            1122085995
        ],
        "30–31\nimage": [
            1122085995
        ],
        "secrets222–223": [
            1122085995
        ],
        "\n351\ncreating": [
            1122085995
        ],
        "authenticat-\ning": [
            1122085995
        ],
        "\nregistries": [
            1122085995
        ],
        "223\nspecifying": [
            1122085995
        ],
        "223\nusing": [
            1122085995
        ],
        "222\nimage": [
            1122085995
        ],
        "registry\npushing": [
            1122085995
        ],
        "35–36\nrunning": [
            1122085995
        ],
        "35\npushing": [
            1122085995
        ],
        "36\nimagepullpolicy\noverview": [
            1122085995
        ],
        "317\nusing": [
            1122085995
        ],
        "497–498\nimagepullsecrets": [
            1122085995
        ],
        "field222\nimages\nbuilding\nfrom": [
            1122085995
        ],
        "\nbuildconfigs": [
            1122085995
        ],
        "529\nlocally": [
            1122085995
        ],
        "504\nusing": [
            1122085995
        ],
        "503–504\ncopying": [
            1122085995
        ],
        "504\ndeploying": [
            1122085995
        ],
        "deploymentconfigs\n529–530\ndocker\nbuilding": [
            1122085995
        ],
        "13\nportability": [
            1122085995
        ],
        "13\nfortune\nconfiguring": [
            1122085995
        ],
        "194–195\nconfiguring": [
            1122085995
        ],
        "197\nof": [
            1122085995
        ],
        "290–291": [
            1122085995
        ],
        "497\nlisting": [
            1122085995
        ],
        "554–555\npushing": [
            1122085995
        ],
        "36\npushing": [
            1122085995
        ],
        "registry\n35–36\nrunning": [
            1122085995
        ],
        "497–498\nsee": [
            1122085995
        ],
        "images\nimagestream529\nin": [
            1122085995
        ],
        "operator107\n": [
            1122085995
        ],
        "\n\nindex574\ninfluxdb": [
            1122085995
        ],
        "database\noverview": [
            1122085995
        ],
        "433\ningress": [
            1122085995
        ],
        "resource135\naccessing": [
            1122085995
        ],
        "145\naccessing": [
            1122085995
        ],
        "145\nbenefits": [
            1122085995
        ],
        "144\ncreating": [
            1122085995
        ],
        "147–149\nexposing": [
            1122085995
        ],
        "146\nexposing": [
            1122085995
        ],
        "142–149\nobtaining": [
            1122085995
        ],
        "145\noverview": [
            1122085995
        ],
        "145\ningresses559\ninit": [
            1122085995
        ],
        "\npods484–485\ninitialdelayseconds": [
            1122085995
        ],
        "property88\ninitializing": [
            1122085995
        ],
        "\ninit547–548\ninitiating": [
            1122085995
        ],
        "os542\ninspecting": [
            1122085995
        ],
        "capacity407–408\ninstalling\ndocker": [
            1122085995
        ],
        "544–545\ndisabling": [
            1122085995
        ],
        "netbridge.bridge-\nnf-call-iptables": [
            1122085995
        ],
        "545\nkubeadm": [
            1122085995
        ],
        "544–545\nkubectl": [
            1122085995
        ],
        "544–545\nkubelet": [
            1122085995
        ],
        "544–545\nkubernetes": [
            1122085995
        ],
        "544–545\nadding": [
            1122085995
        ],
        "\nrepo": [
            1122085995
        ],
        "\noption": [
            1122085995
        ],
        "545\nkubernetes-cni": [
            1122085995
        ],
        "544–545\nminikube": [
            1122085995
        ],
        "37\nos": [
            1122085995
        ],
        "541–544\ninitiating": [
            1122085995
        ],
        "542\nrunning": [
            1122085995
        ],
        "543–544\nselecting": [
            1122085995
        ],
        "541\nsetting": [
            1122085995
        ],
        "542–543\ninternet": [
            1122085995
        ],
        "ip\ninter-pod": [
            1122085995
        ],
        "nodes468–\n471\ndeploying": [
            1122085995
        ],
        "\naffinity": [
            1122085995
        ],
        "470\nspecifying": [
            1122085995
        ],
        "469\nusing": [
            1122085995
        ],
        "470–471\ninterval": [
            1122085995
        ],
        "images\noverview": [
            1122085995
        ],
        "194–195\nthrough": [
            1122085995
        ],
        "197\nintervals": [
            1122085995
        ],
        "\nwith195–196\nip": [
            1122085995
        ],
        "(internet": [
            1122085995
        ],
        "protocol)\ncontainers": [
            1122085995
        ],
        "57\nexternal": [
            1122085995
        ],
        "46–47\ningress": [
            1122085995
        ],
        "145\nlocal": [
            1122085995
        ],
        "480\nof": [
            1122085995
        ],
        "142\nservice": [
            1122085995
        ],
        "131\nip": [
            1122085995
        ],
        "addresses121\nipc": [
            1122085995
        ],
        "communication)\noverview": [
            1122085995
        ],
        "379–380\niptables": [
            1122085995
        ],
        "rules327–328": [
            1122085995
        ],
        "339–340": [
            1122085995
        ],
        "\n345": [
            1122085995
        ],
        "494–495\niscsi": [
            1122085995
        ],
        "volume163\nisolating\ncomponents": [
            1122085995
        ],
        "8\ncontainer": [
            1122085995
        ],
        "34\ncontainers": [
            1122085995
        ],
        "11\nnetworks": [
            1122085995
        ],
        "401–402\npartially": [
            1122085995
        ],
        "57\npod": [
            1122085995
        ],
        "399–402\nprocesses": [
            1122085995
        ],
        "11\n-it": [
            1122085995
        ],
        "option33\nitems": [
            1122085995
        ],
        "attribute138\nj\njob": [
            1122085995
        ],
        "controllers324\njob": [
            1122085995
        ],
        "resources112\nconfiguring": [
            1122085995
        ],
        "117\nrunning": [
            1122085995
        ],
        "114–116\nrunning": [
            1122085995
        ],
        "\nparallel": [
            1122085995
        ],
        "115\nrunning": [
            1122085995
        ],
        "\nsequentially": [
            1122085995
        ],
        "115\nscaling": [
            1122085995
        ],
        "116\nrunning": [
            1122085995
        ],
        "114\njobs\ndefining": [
            1122085995
        ],
        "113–114\nlisting": [
            1122085995
        ],
        "237–238\nretrieving": [
            1122085995
        ],
        "238\nscheduling": [
            1122085995
        ],
        "116–118\ncreating": [
            1122085995
        ],
        "116–117\noverview": [
            1122085995
        ],
        "117\njson": [
            1122085995
        ],
        "format\ncreating": [
            1122085995
        ],
        "descriptors\nsending": [
            1122085995
        ],
        "pods\n66–67\nusing": [
            1122085995
        ],
        "65\nviewing": [
            1122085995
        ],
        "logs\n65–66\nmanifests": [
            1122085995
        ],
        "505–506\njsonpath138\nk\nkeep-alive": [
            1122085995
        ],
        "connections140\nkernel": [
            1122085995
        ],
        "\ncontainers384–385\nkernels": [
            1122085995
        ],
        "\nnetbridge.bridge-nf-call-\niptables": [
            1122085995
        ],
        "option545\nkey": [
            1122085995
        ],
        "secrets221\nkey=value": [
            1122085995
        ],
        "format232\nkilling": [
            1122085995
        ],
        "applications479–482\nexpecting": [
            1122085995
        ],
        "\ndisappear": [
            1122085995
        ],
        "480–482\nksonnet": [
            1122085995
        ],
        "manifests505–506\nkube": [
            1122085995
        ],
        "config\nadding": [
            1122085995
        ],
        "536–537\nadding": [
            1122085995
        ],
        "536\nadding": [
            1122085995
        ],
        "credentials\n536–537\ntying": [
            1122085995
        ],
        "537\nlisting": [
            1122085995
        ],
        "credentials\n536–537\nmodifying": [
            1122085995
        ],
        "536\nmodifying": [
            1122085995
        ],
        "\ncredentials": [
            1122085995
        ],
        "536–537\ntying": [
            1122085995
        ],
        "537\n": [
            1122085995
        ],
        "\n\nindex575\nkube": [
            1122085995
        ],
        "(continued)\nmodifying": [
            1122085995
        ],
        "537\nkubeadm": [
            1122085995
        ],
        "command549\nkubeadm": [
            1122085995
        ],
        "tool\nconfiguring": [
            1122085995
        ],
        "with\n547–549\nconfiguring": [
            1122085995
        ],
        "549–550\ninstalling": [
            1122085995
        ],
        "544–545\nrunning": [
            1122085995
        ],
        "with\n548–549\nlisting": [
            1122085995
        ],
        "548–549\nlisting": [
            1122085995
        ],
        "548\nrunning": [
            1122085995
        ],
        "\nmasters": [
            1122085995
        ],
        "547–548\nsetting": [
            1122085995
        ],
        "539–551\nsetting": [
            1122085995
        ],
        "539\nusing": [
            1122085995
        ],
        "550–551\nkubeconfig": [
            1122085995
        ],
        "files\nconfiguring": [
            1122085995
        ],
        "535–536\nkubeconfig": [
            1122085995
        ],
        "variable535\nkubectl\naliases": [
            1122085995
        ],
        "41–42\ncommand-line": [
            1122085995
        ],
        "completion\n41–42\nconfiguring": [
            1122085995
        ],
        "41–42\nconfirming": [
            1122085995
        ],
        "communicat-\ning": [
            1122085995
        ],
        "38\ncreating": [
            1122085995
        ],
        "398\ninstalling": [
            1122085995
        ],
        "38\nlogs": [
            1122085995
        ],
        "66\nperforming": [
            1122085995
        ],
        "256–260\nreplacing": [
            1122085995
        ],
        "\nreplicationcontrollers\n259–260\nsteps": [
            1122085995
        ],
        "commences\n258–259\nproxy\naccessing": [
            1122085995
        ],
        "234–235\nexploring": [
            1122085995
        ],
        "235–236\nusing": [
            1122085995
        ],
        "537–538\nusing": [
            1122085995
        ],
        "users\n537–538\nusing": [
            1122085995
        ],
        "538\ndeleting": [
            1122085995
        ],
        "534–535\nusing": [
            1122085995
        ],
        "535–538\nadding": [
            1122085995
        ],
        "538\nkubectl": [
            1122085995
        ],
        "command76\nkubectl": [
            1122085995
        ],
        "command266": [
            1122085995
        ],
        "\n276–277": [
            1122085995
        ],
        "504\nkubectl": [
            1122085995
        ],
        "command443\nkubectl": [
            1122085995
        ],
        "command38": [
            1122085995
        ],
        "\n52\nkubectl": [
            1122085995
        ],
        "tool39\nkubectl": [
            1122085995
        ],
        "command500\nkubectl": [
            1122085995
        ],
        "command65": [
            1122085995
        ],
        "\n114": [
            1122085995
        ],
        "255\nkubectl": [
            1122085995
        ],
        "\ncommand200\n–201\nkubectl": [
            1122085995
        ],
        "\ncommand519\nkubectl": [
            1122085995
        ],
        "command65\nkubectl": [
            1122085995
        ],
        "\ncommand78–79\nkubectl": [
            1122085995
        ],
        "command359\nkubectl": [
            1122085995
        ],
        "command104\nkubectl": [
            1122085995
        ],
        "command41": [
            1122085995
        ],
        "\n76": [
            1122085995
        ],
        "\n549": [
            1122085995
        ],
        "553\nkubectl": [
            1122085995
        ],
        "\ncommand409\nkubectl": [
            1122085995
        ],
        "\ncommand215": [
            1122085995
        ],
        "499\nkubectl": [
            1122085995
        ],
        "command151\nkubectl": [
            1122085995
        ],
        "method266\nkubectl": [
            1122085995
        ],
        "command124": [
            1122085995
        ],
        "\n130": [
            1122085995
        ],
        "446\nkubectl": [
            1122085995
        ],
        "command64": [
            1122085995
        ],
        "505\nkubectl": [
            1122085995
        ],
        "command66": [
            1122085995
        ],
        "123\nkubectl": [
            1122085995
        ],
        "command48": [
            1122085995
        ],
        "95\nkubectl": [
            1122085995
        ],
        "events332\nkubectl": [
            1122085995
        ],
        "command51": [
            1122085995
        ],
        "124\nkubectl": [
            1122085995
        ],
        "\ncommand523\nkubectl": [
            1122085995
        ],
        "command46\nkubectl": [
            1122085995
        ],
        "command129\nkubectl": [
            1122085995
        ],
        "command87": [
            1122085995
        ],
        "502\nkubectl": [
            1122085995
        ],
        "\ncommand67\nkubectl": [
            1122085995
        ],
        "command244\n": [
            1122085995
        ],
        "503\nkubectl": [
            1122085995
        ],
        "process514–515\nkubectl": [
            1122085995
        ],
        "\ncommand257": [
            1122085995
        ],
        "260–261": [
            1122085995
        ],
        "264\nkubectl": [
            1122085995
        ],
        "\ncommand271\nkubectl": [
            1122085995
        ],
        "command61": [
            1122085995
        ],
        "82\nkubectl": [
            1122085995
        ],
        "command102–103": [
            1122085995
        ],
        "\n116\nkubectl": [
            1122085995
        ],
        "command\n265–266\nkubectl": [
            1122085995
        ],
        "tool\ninstalling": [
            1122085995
        ],
        "548\nkube-dns129\nkube_editor": [
            1122085995
        ],
        "\nvariable102\nkubelet": [
            1122085995
        ],
        "\ninstalling544–545\nkubelets326–327\noverview": [
            1122085995
        ],
        "326\nrunning": [
            1122085995
        ],
        "332\nrunning": [
            1122085995
        ],
        "326–327\n": [
            1122085995
        ],
        "\n\nindex576\nkube-proxy19": [
            1122085995
        ],
        "345\noverview": [
            1122085995
        ],
        "339\nusing": [
            1122085995
        ],
        "339–340\nkube-public": [
            1122085995
        ],
        "namespace77\nkubernetes16–24\narchitecture": [
            1122085995
        ],
        "310–330\nadd-ons": [
            1122085995
        ],
        "328–330\napi": [
            1122085995
        ],
        "316–319\nclusters": [
            1122085995
        ],
        "18–19\ncomponents": [
            1122085995
        ],
        "310–312\ncomponents": [
            1122085995
        ],
        "310\ncomponents": [
            1122085995
        ],
        "\nworker": [
            1122085995
        ],
        "310\ncontroller": [
            1122085995
        ],
        "321–326\netcd": [
            1122085995
        ],
        "312–316\nkubelet": [
            1122085995
        ],
        "326–327\nscheduler": [
            1122085995
        ],
        "319–321\nservice": [
            1122085995
        ],
        "327–328\nbenefits": [
            1122085995
        ],
        "21–24\nautomatic": [
            1122085995
        ],
        "23\nhealth": [
            1122085995
        ],
        "22–23\nimproving": [
            1122085995
        ],
        "22\nself-healing": [
            1122085995
        ],
        "22–23\nsimplifying": [
            1122085995
        ],
        "21–22\nsimplifying": [
            1122085995
        ],
        "23–24\ndashboard": [
            1122085995
        ],
        "52–53\naccessing": [
            1122085995
        ],
        "53\ninstalling": [
            1122085995
        ],
        "545\nmaster": [
            1122085995
        ],
        "\nseen": [
            1122085995
        ],
        "305\norigins": [
            1122085995
        ],
        "16\noverriding": [
            1122085995
        ],
        "in\n195–196\noverriding": [
            1122085995
        ],
        "in\n195–196\noverview": [
            1122085995
        ],
        "16–18\nfocusing": [
            1122085995
        ],
        "\nfeatures": [
            1122085995
        ],
        "17–18\nimproving": [
            1122085995
        ],
        "18\nrunning": [
            1122085995
        ],
        "19–21\neffect": [
            1122085995
        ],
        "descrip-\ntion": [
            1122085995
        ],
        "19–20\nkeeping": [
            1122085995
        ],
        "20–21\nlocating": [
            1122085995
        ],
        "21\nscaling": [
            1122085995
        ],
        "21\nwhen": [
            1122085995
        ],
        "2–7\ncontinuous": [
            1122085995
        ],
        "6–7\nmicroservices": [
            1122085995
        ],
        "3–6\nproviding": [
            1122085995
        ],
        "6\nkubernetes": [
            1122085995
        ],
        "interface)545\nkubernetes": [
            1122085995
        ],
        "plane18\nkubernetes_service_host": [
            1122085995
        ],
        "\nvariable239\nkubernetes_service_port": [
            1122085995
        ],
        "\nvariable239\nkube-scheduler": [
            1122085995
        ],
        "resource344\nkube-system": [
            1122085995
        ],
        "namespace77": [
            1122085995
        ],
        "\n314": [
            1122085995
        ],
        "548\nkubia42\nkubia-2qneh100\nkubia-container32–33\nkubia-dmdck100\nkubia-gpuyaml": [
            1122085995
        ],
        "file74\nkubia-manual63\nkubia-rcyaml": [
            1122085995
        ],
        "file93\nkubia_service_host": [
            1122085995
        ],
        "\nvariable129\nkubia_service_port": [
            1122085995
        ],
        "\nvariable129\nkubia-svcyaml": [
            1122085995
        ],
        "file123\nkubia-website": [
            1122085995
        ],
        "service517\nl\nlabel": [
            1122085995
        ],
        "selectors\nchanging": [
            1122085995
        ],
        "100–101\ndeleting": [
            1122085995
        ],
        "80\neffect": [
            1122085995
        ],
        "93\nlisting": [
            1122085995
        ],
        "71–72\nlisting": [
            1122085995
        ],
        "71–72\nreplicasets": [
            1122085995
        ],
        "107\nusing": [
            1122085995
        ],
        "72\nlabels\nadding": [
            1122085995
        ],
        "111\nadding": [
            1122085995
        ],
        "\nreplicationcontrollers": [
            1122085995
        ],
        "99\ncategorizing": [
            1122085995
        ],
        "74\nconstraining": [
            1122085995
        ],
        "73–75\ncategorizing": [
            1122085995
        ],
        "74\nscheduling": [
            1122085995
        ],
        "74–75\nmulti-dimensional": [
            1122085995
        ],
        "single-\ndimensional": [
            1122085995
        ],
        "498\nof": [
            1122085995
        ],
        "modifying\n70–71\nof": [
            1122085995
        ],
        "99–100\norganizing": [
            1122085995
        ],
        "71\noverview": [
            1122085995
        ],
        "68–69\nremoving": [
            1122085995
        ],
        "111–112\nspecifying": [
            1122085995
        ],
        "69–70\nupdating": [
            1122085995
        ],
        "232\nlan": [
            1122085995
        ],
        "(local": [
            1122085995
        ],
        "network)58\nlatest": [
            1122085995
        ],
        "tag28\n--leader-elect": [
            1122085995
        ],
        "option343\nleader-election\nfor": [
            1122085995
        ],
        "341\nusing": [
            1122085995
        ],
        "344\nleastrequestedpriority407\nlibraries\nbuilding": [
            1122085995
        ],
        "248\nsee": [
            1122085995
        ],
        "libraries\nlifecycles": [
            1122085995
        ],
        "pods479–491\nadding": [
            1122085995
        ],
        "485–489\nhooks\nadding": [
            1122085995
        ],
        "485–489\ntargeting": [
            1122085995
        ],
        "container\n486–487\nusing": [
            1122085995
        ],
        "container\n487–488\nusing": [
            1122085995
        ],
        "signal\n488–489\nkilling": [
            1122085995
        ],
        "479–482\npod": [
            1122085995
        ],
        "shutdowns": [
            1122085995
        ],
        "489–491\nrelocating": [
            1122085995
        ],
        "479–482\nrescheduling": [
            1122085995
        ],
        "pods\n482–483\nrescheduling": [
            1122085995
        ],
        "482–483\nstarting": [
            1122085995
        ],
        "\norder": [
            1122085995
        ],
        "483–485\nlimiting": [
            1122085995
        ],
        "resources\navailable": [
            1122085995
        ],
        "429\n": [
            1122085995
        ],
        "\n\nindex577\nlimiting": [
            1122085995
        ],
        "(continued)\navailable": [
            1122085995
        ],
        "412–413\nlimitrange": [
            1122085995
        ],
        "creating\n422–423\nlimits\nas": [
            1122085995
        ],
        "415–416\ncontainers": [
            1122085995
        ],
        "416\ncontainers": [
            1122085995
        ],
        "\nmemory": [
            1122085995
        ],
        "415–416\nenforcing": [
            1122085995
        ],
        "423–424\nexceeding": [
            1122085995
        ],
        "414–415\nfor": [
            1122085995
        ],
        "\ndefaults": [
            1122085995
        ],
        "421–425\novercommitting": [
            1122085995
        ],
        "412–413\nsee": [
            1122085995
        ],
        "limits\nlinux": [
            1122085995
        ],
        "\n(cgroups)11\nlinux": [
            1122085995
        ],
        "namespaces11\nlinux": [
            1122085995
        ],
        "os\ncontainer": [
            1122085995
        ],
        "isolat-\ning": [
            1122085995
        ],
        "8\nnamespaces": [
            1122085995
        ],
        "11\nlisting\nall": [
            1122085995
        ],
        "32\ncluster": [
            1122085995
        ],
        "40\nclusters": [
            1122085995
        ],
        "538\ncontainer": [
            1122085995
        ],
        "554–555\ncontexts": [
            1122085995
        ],
        "538\njob": [
            1122085995
        ],
        "237–238\nkube": [
            1122085995
        ],
        "537\nnodes": [
            1122085995
        ],
        "548–549\npersistentvolumeclaims": [
            1122085995
        ],
        "180\npersistentvolumes": [
            1122085995
        ],
        "180–181\npods": [
            1122085995
        ],
        "43–44": [
            1122085995
        ],
        "71–72": [
            1122085995
        ],
        "\n548\nservices": [
            1122085995
        ],
        "46\nservices": [
            1122085995
        ],
        "523\nstorage": [
            1122085995
        ],
        "187–188\nsubsets": [
            1122085995
        ],
        "71–72\nliveness": [
            1122085995
        ],
        "probes85–86\nconfiguring": [
            1122085995
        ],
        "of\n88–89\ncreating": [
            1122085995
        ],
        "89–90\nhttp-based": [
            1122085995
        ],
        "86–87\nimplementing": [
            1122085995
        ],
        "90\nkeeping": [
            1122085995
        ],
        "90\nwhat": [
            1122085995
        ],
        "\ncheck": [
            1122085995
        ],
        "89–90\nin": [
            1122085995
        ],
        "87–88\nload": [
            1122085995
        ],
        "balancers\nconnecting": [
            1122085995
        ],
        "139–141\nexternal": [
            1122085995
        ],
        "138–141\nloadbalancer": [
            1122085995
        ],
        "139\noverview": [
            1122085995
        ],
        "530\nlocal": [
            1122085995
        ],
        "lan\nlocal": [
            1122085995
        ],
        "\nfrom550–551\nlogrotator": [
            1122085995
        ],
        "container162\nlogs\ncentralized": [
            1122085995
        ],
        "501\ncopying": [
            1122085995
        ],
        "\nlogs": [
            1122085995
        ],
        "66\nof": [
            1122085995
        ],
        "500–502\nof": [
            1122085995
        ],
        "66\npod": [
            1122085995
        ],
        "66\nsee": [
            1122085995
        ],
        "logs\nlogvol162\nls": [
            1122085995
        ],
        "command151\nm\nmanifests\ndeployment": [
            1122085995
        ],
        "262\njson": [
            1122085995
        ],
        "505–506\nresources": [
            1122085995
        ],
        "504–505\nyaml": [
            1122085995
        ],
        "505–506\nmapping\ndifferent": [
            1122085995
        ],
        "147\ndifferent": [
            1122085995
        ],
        "146\nmaster": [
            1122085995
        ],
        "node18\nmasters\nconfiguring": [
            1122085995
        ],
        "\ninitialize": [
            1122085995
        ],
        "547–548\nrunning": [
            1122085995
        ],
        "548\nmatchexpressions": [
            1122085995
        ],
        "property107": [
            1122085995
        ],
        "\n465": [
            1122085995
        ],
        "470\nmatchlabels": [
            1122085995
        ],
        "field105": [
            1122085995
        ],
        "\n470\nmaxsurge": [
            1122085995
        ],
        "property271–272\nmaxunavailable": [
            1122085995
        ],
        "property\n271–274": [
            1122085995
        ],
        "455\nmedium": [
            1122085995
        ],
        "\nvolume166\nmemory\ncreating": [
            1122085995
        ],
        "431–432\nkilling": [
            1122085995
        ],
        "low\n420–421\nhandling": [
            1122085995
        ],
        "420–421\nsequence": [
            1122085995
        ],
        "420\nnodes": [
            1122085995
        ],
        "containers\n415–416\nscaling": [
            1122085995
        ],
        "448\nsecret": [
            1122085995
        ],
        "221\nmetadata\ncontainer-level": [
            1122085995
        ],
        "\nspecifications": [
            1122085995
        ],
        "233\nexposing": [
            1122085995
        ],
        "226–233\navailable": [
            1122085995
        ],
        "226–227\npassing": [
            1122085995
        ],
        "volume\n230–\n233\nmetadata": [
            1122085995
        ],
        "section201\nmetadataname": [
            1122085995
        ],
        "field510\nmetadataresourceversion": [
            1122085995
        ],
        "\nfield313\nmetric": [
            1122085995
        ],
        "types\nobject": [
            1122085995
        ],
        "on\n449–450\npods\nobtaining": [
            1122085995
        ],
        "438–439\nscaling": [
            1122085995
        ],
        "449\nresource": [
            1122085995
        ],
        "449\nmetrics\nappropriate": [
            1122085995
        ],
        "autoscaling\n450\ncustom": [
            1122085995
        ],
        "on\n448–450\nmodifying": [
            1122085995
        ],
        "447\n": [
            1122085995
        ],
        "\n\nindex578\nmicroservices3–6\ndeploying": [
            1122085995
        ],
        "5\ndivergence": [
            1122085995
        ],
        "5–6\nscaling": [
            1122085995
        ],
        "4\nsplitting": [
            1122085995
        ],
        "3–4\nminavailable": [
            1122085995
        ],
        "field455\nminikube": [
            1122085995
        ],
        "command553\nminikube": [
            1122085995
        ],
        "command503\nminikube": [
            1122085995
        ],
        "node408\nminikube": [
            1122085995
        ],
        "tool\naccessing": [
            1122085995
        ],
        "53\nand": [
            1122085995
        ],
        "534–535\ncombining": [
            1122085995
        ],
        "504\ninspecting": [
            1122085995
        ],
        "553–554\ninstalling": [
            1122085995
        ],
        "37\nrunning": [
            1122085995
        ],
        "with\n37–38\nstarting": [
            1122085995
        ],
        "37–38\nswitching": [
            1122085995
        ],
        "503–504\nbuilding": [
            1122085995
        ],
        "504\ncopying": [
            1122085995
        ],
        "504\nmounting": [
            1122085995
        ],
        "503\nusing": [
            1122085995
        ],
        "503–504\nusing": [
            1122085995
        ],
        "553–555\nlisting": [
            1122085995
        ],
        "images\n554–555\nrunning": [
            1122085995
        ],
        "553\nminikube": [
            1122085995
        ],
        "(virtual": [
            1122085995
        ],
        "machine)\ndirectly": [
            1122085995
        ],
        "\nbuild": [
            1122085995
        ],
        "503–504\nminishift530\nminreadyseconds": [
            1122085995
        ],
        "attribute265": [
            1122085995
        ],
        "\n274–275\nmongodb": [
            1122085995
        ],
        "to173\nmonitoring": [
            1122085995
        ],
        "\nusage430–434\nanalyzing": [
            1122085995
        ],
        "statistics\n432–434\ncollecting": [
            1122085995
        ],
        "usages\n430–432\nretrieving": [
            1122085995
        ],
        "usages\n430–\n432\nstoring": [
            1122085995
        ],
        "statistics\n432–434\nmonolithic": [
            1122085995
        ],
        "\nmicroservices3–6\nmostrequestedpriority407\nmount": [
            1122085995
        ],
        "(mnt)": [
            1122085995
        ],
        "namespace11\nmountable": [
            1122085995
        ],
        "secrets350–351\nmounted": [
            1122085995
        ],
        "using208\nmounting\nconfigmap": [
            1122085995
        ],
        "files\n210–211\ndirectories": [
            1122085995
        ],
        "210\nfortune-https": [
            1122085995
        ],
        "219–220\nlocal": [
            1122085995
        ],
        "503\nlocal": [
            1122085995
        ],
        "503\nmulti-tier": [
            1122085995
        ],
        "pods59\nmustrunas": [
            1122085995
        ],
        "using392–393\nmustrunasnonroot": [
            1122085995
        ],
        "fields394\nmy-nginx-configconf": [
            1122085995
        ],
        "entry209\nmy-postgres-db": [
            1122085995
        ],
        "service525\nmysql_root_password": [
            1122085995
        ],
        "\nvariable192\nn\n-n": [
            1122085995
        ],
        "flag79\n-n": [
            1122085995
        ],
        "option537\nnames\nconfiguring": [
            1122085995
        ],
        "546–547\ndeleting": [
            1122085995
        ],
        "80\nof": [
            1122085995
        ],
        "66\nretrieving": [
            1122085995
        ],
        "238\nnameskind": [
            1122085995
        ],
        "property511\nnamespace": [
            1122085995
        ],
        "controllers325\n--namespace": [
            1122085995
        ],
        "option535": [
            1122085995
        ],
        "537\nnamespacelifecycle317\nnamespaces\naccessing": [
            1122085995
        ],
        "367–370\ncreating\nfrom": [
            1122085995
        ],
        "78\nwith": [
            1122085995
        ],
        "78–79\ndeleting": [
            1122085995
        ],
        "81–82\ndiscovering": [
            1122085995
        ],
        "77–78\nenabling": [
            1122085995
        ],
        "399\ngranting": [
            1122085995
        ],
        "\nadmin": [
            1122085995
        ],
        "372\ngrouping": [
            1122085995
        ],
        "76–80\nhost": [
            1122085995
        ],
        "\nports": [
            1122085995
        ],
        "using\n377–379\nhost": [
            1122085995
        ],
        "pods\n376–380\nincluding": [
            1122085995
        ],
        "361\nipc": [
            1122085995
        ],
        "379–380\nisolating": [
            1122085995
        ],
        "between\n401–402\nisolation": [
            1122085995
        ],
        "79–80\nlimiting": [
            1122085995
        ],
        "resources\n425–427\nspecifying": [
            1122085995
        ],
        "429\nlinux": [
            1122085995
        ],
        "11\nmanaging": [
            1122085995
        ],
        "79\nnode": [
            1122085995
        ],
        "376–377\nnode": [
            1122085995
        ],
        "379–380\nof": [
            1122085995
        ],
        "242\npods": [
            1122085995
        ],
        "400\nsetting": [
            1122085995
        ],
        "421\n–425\napplying": [
            1122085995
        ],
        "421–422\nsetting": [
            1122085995
        ],
        "resources\n421–422\nusing": [
            1122085995
        ],
        "multiple\n535–538\nadding": [
            1122085995
        ],
        "entries\n536–537\nconfiguring": [
            1122085995
        ],
        "535\n": [
            1122085995
        ],
        "\n\nindex579\nnamespaces": [
            1122085995
        ],
        "(continued)\ncontents": [
            1122085995
        ],
        "538\nwhy": [
            1122085995
        ],
        "77\nnas": [
            1122085995
        ],
        "(network-attached": [
            1122085995
        ],
        "\nstorage)171\nnat": [
            1122085995
        ],
        "\ntranslation)58": [
            1122085995
        ],
        "335\nnetbridge.bridge-nf-call-iptables": [
            1122085995
        ],
        "\nkernel": [
            1122085995
        ],
        "option545\nnetwork": [
            1122085995
        ],
        "(net)": [
            1122085995
        ],
        "namespace11\nnetwork": [
            1122085995
        ],
        "hops": [
            1122085995
        ],
        "preventing141\nnetworks\nbetween": [
            1122085995
        ],
        "335–338\narchitecture": [
            1122085995
        ],
        "335–336\ncni": [
            1122085995
        ],
        "\ninterface)": [
            1122085995
        ],
        "338\nenabling": [
            1122085995
        ],
        "337–338\nenabling": [
            1122085995
        ],
        "336–337\noverview": [
            1122085995
        ],
        "336–338\nconfiguring": [
            1122085995
        ],
        "540\nenabling": [
            1122085995
        ],
        "399\nisolating": [
            1122085995
        ],
        "401–402\nlocal": [
            1122085995
        ],
        "67\nnode": [
            1122085995
        ],
        "\nadapters": [
            1122085995
        ],
        "304–305\nof": [
            1122085995
        ],
        "550\nproviding": [
            1122085995
        ],
        "identities\n285–287\ngoverning": [
            1122085995
        ],
        "287\nsecuring": [
            1122085995
        ],
        "contexts\n380–389\nisolating": [
            1122085995
        ],
        "389–\n399\nusing": [
            1122085995
        ],
        "376–380\nsimulating": [
            1122085995
        ],
        "304–306\nchecking": [
            1122085995
        ],
        "\nmaster": [
            1122085995
        ],
        "305\npods": [
            1122085995
        ],
        "305–306\nsee": [
            1122085995
        ],
        "networks;": [
            1122085995
        ],
        "net-\nworks\nnfs": [
            1122085995
        ],
        "system)175\nnfs": [
            1122085995
        ],
        "volume162\nnginx": [
            1122085995
        ],
        "container334": [
            1122085995
        ],
        "514\nnginx": [
            1122085995
        ],
        "software\nsignaling": [
            1122085995
        ],
        "212\nusing": [
            1122085995
        ],
        "secrets\n221\nusing": [
            1122085995
        ],
        "221\nverifying": [
            1122085995
        ],
        "208\nnode": [
            1122085995
        ],
        "affinity\nspecifying": [
            1122085995
        ],
        "463–465\nnodeaffinity": [
            1122085995
        ],
        "\nnames": [
            1122085995
        ],
        "464–465\nnodeselectorterms": [
            1122085995
        ],
        "465\nspecifying": [
            1122085995
        ],
        "466–467\nusing": [
            1122085995
        ],
        "462–468\nexamining": [
            1122085995
        ],
        "462–463\nprioritizing": [
            1122085995
        ],
        "465–468\nvs": [
            1122085995
        ],
        "462–463\nnode": [
            1122085995
        ],
        "controllers324\nnode": [
            1122085995
        ],
        "failures304–307\ndeleting": [
            1122085995
        ],
        "306–307\nsimulating": [
            1122085995
        ],
        "305–306\nshutting": [
            1122085995
        ],
        "304–305\nnodeaffinity": [
            1122085995
        ],
        "names\n464–465\nnodejs\ncreating": [
            1122085995
        ],
        "43–44\nnodelost306\nnode_name": [
            1122085995
        ],
        "variable228\nnodeport": [
            1122085995
        ],
        "services\nchanging": [
            1122085995
        ],
        "access\n137–138\ncreating": [
            1122085995
        ],
        "135–136\nexamining": [
            1122085995
        ],
        "136–137\nusing": [
            1122085995
        ],
        "135–138\nnodes\nadding": [
            1122085995
        ],
        "460\napplications": [
            1122085995
        ],
        "51–52\nassigning": [
            1122085995
        ],
        "332\nchecking": [
            1122085995
        ],
        "305\nconfiguring": [
            1122085995
        ],
        "462\ncpu": [
            1122085995
        ],
        "416\ncreating": [
            1122085995
        ],
        "39\ncreating": [
            1122085995
        ],
        "408–409\ndisplaying": [
            1122085995
        ],
        "458\nenabling": [
            1122085995
        ],
        "same\n336–337\nexamining": [
            1122085995
        ],
        "labels\n462–463\nfinding": [
            1122085995
        ],
        "320\ninspecting": [
            1122085995
        ],
        "407–408\nlabeling": [
            1122085995
        ],
        "466\nlisting": [
            1122085995
        ],
        "548–549\nmemory": [
            1122085995
        ],
        "415–416\nnetwork": [
            1122085995
        ],
        "376–377\npid": [
            1122085995
        ],
        "379–380\npreferences": [
            1122085995
        ],
        "467\nprioritizing": [
            1122085995
        ],
        "465–468\ndeploying": [
            1122085995
        ],
        "468\nspecifying": [
            1122085995
        ],
        "466–467\nrelinquishing": [
            1122085995
        ],
        "453\nreplicationcontrollers": [
            1122085995
        ],
        "respond-\ning": [
            1122085995
        ],
        "97–98\nrequesting": [
            1122085995
        ],
        "452–453\nrunning": [
            1122085995
        ],
        "on\nadding": [
            1122085995
        ],
        "111\n": [
            1122085995
        ],
        "\n\nindex580\nnodes": [
            1122085995
        ],
        "(continued)\nremoving": [
            1122085995
        ],
        "111–112\nwith": [
            1122085995
        ],
        "109–112\nrunning": [
            1122085995
        ],
        "109\nscheduler": [
            1122085995
        ],
        "407\nscheduling": [
            1122085995
        ],
        "specific\n74–75\nselecting": [
            1122085995
        ],
        "320–321\nselectors": [
            1122085995
        ],
        "462–463\nsimulating": [
            1122085995
        ],
        "304–305\nusing": [
            1122085995
        ],
        "\ndeploy": [
            1122085995
        ],
        "468–471\ndeploying": [
            1122085995
        ],
        "470–471\nusing": [
            1122085995
        ],
        "462–468\ncomparing": [
            1122085995
        ],
        "462–463\nspecifying": [
            1122085995
        ],
        "463–465\nusing": [
            1122085995
        ],
        "\nwhether": [
            1122085995
        ],
        "406\nusing": [
            1122085995
        ],
        "457–462\ntaints": [
            1122085995
        ],
        "458–460\nusing": [
            1122085995
        ],
        "461–462\nusing": [
            1122085995
        ],
        "457–462\nadding": [
            1122085995
        ],
        "460–461\ntolerations": [
            1122085995
        ],
        "461–462\nworker\ncategorizing": [
            1122085995
        ],
        "74\ncomponents": [
            1122085995
        ],
        "310\nsee": [
            1122085995
        ],
        "\naffinity;": [
            1122085995
        ],
        "failures;": [
            1122085995
        ],
        "filesystems;": [
            1122085995
        ],
        "nodes\nnodeselector": [
            1122085995
        ],
        "field75": [
            1122085995
        ],
        "462–464\nnodeselectorterms\n465\nnoexecute460\nnon-resource": [
            1122085995
        ],
        "to365–367\nnoops7\nnoschedule460\nnotin": [
            1122085995
        ],
        "operator107\nnotready": [
            1122085995
        ],
        "status97": [
            1122085995
        ],
        "305\nnotterminating": [
            1122085995
        ],
        "scope429\nntp": [
            1122085995
        ],
        "\ndaemon385\nntp": [
            1122085995
        ],
        "ntp\no\n-o": [
            1122085995
        ],
        "option312\nobject": [
            1122085995
        ],
        "fields64\nobject": [
            1122085995
        ],
        "\non449–450\noci": [
            1122085995
        ],
        "(open": [
            1122085995
        ],
        "\ninitiative)15": [
            1122085995
        ],
        "554\nomega16\nonlylocal": [
            1122085995
        ],
        "annotation142\noom": [
            1122085995
        ],
        "(outofmemory)": [
            1122085995
        ],
        "score420\nopenapi": [
            1122085995
        ],
        "with248\nopenservicebroker": [
            1122085995
        ],
        "api522–523\nlisting": [
            1122085995
        ],
        "523\noverview": [
            1122085995
        ],
        "522\nregistering": [
            1122085995
        ],
        "522–523\nopenshift": [
            1122085995
        ],
        "platform\noperations": [
            1122085995
        ],
        "team2\noptimistic": [
            1122085995
        ],
        "\ncontrol313\nos": [
            1122085995
        ],
        "(operating": [
            1122085995
        ],
        "systems)": [
            1122085995
        ],
        "\ninstalling541–544\ninitiating": [
            1122085995
        ],
        "options\n542–543\noutofmemoryerrors85\nout-of-range": [
            1122085995
        ],
        "with393\novercommitting": [
            1122085995
        ],
        "limits413\noverriding": [
            1122085995
        ],
        "\ncommands195–196\n--overwrite": [
            1122085995
        ],
        "option70": [
            1122085995
        ],
        "99\np\nparallelism": [
            1122085995
        ],
        "property114\npath": [
            1122085995
        ],
        "traversa": [
            1122085995
        ],
        "attack353\npaths": [
            1122085995
        ],
        "to146\npatterns": [
            1122085995
        ],
        "\ncontainers244\npausing": [
            1122085995
        ],
        "rollouts273–274\npd": [
            1122085995
        ],
        "(persistent": [
            1122085995
        ],
        "disk)185\npdb": [
            1122085995
        ],
        "(poddisruptionbudget)455\npeers\ndiscovering": [
            1122085995
        ],
        "statefulsets\n299–304\nclustered": [
            1122085995
        ],
        "303–304\nimplementing": [
            1122085995
        ],
        "301–302\nsrv": [
            1122085995
        ],
        "300\nupdating": [
            1122085995
        ],
        "statefulsets\n302–303\nimplementing": [
            1122085995
        ],
        "301–302\npending": [
            1122085995
        ],
        "status44\npermissions373\npersistent": [
            1122085995
        ],
        "gce171–172\npersistent": [
            1122085995
        ],
        "storage171–175\nspecifying": [
            1122085995
        ],
        "427\nusing": [
            1122085995
        ],
        "173–174\nusing": [
            1122085995
        ],
        "174–175\nusing": [
            1122085995
        ],
        "174\nusing": [
            1122085995
        ],
        "175\nusing": [
            1122085995
        ],
        "technologies\n175\nwriting": [
            1122085995
        ],
        "173\npersistentvolume": [
            1122085995
        ],
        "\ncontrollers325–326\npersistentvolumeclaims": [
            1122085995
        ],
        "pvc\npersistentvolumes176–177": [
            1122085995
        ],
        "288\nbenefits": [
            1122085995
        ],
        "182\nclaiming": [
            1122085995
        ],
        "179–181\ncreating": [
            1122085995
        ],
        "179\n–180\nlisting": [
            1122085995
        ],
        "180\ncreating": [
            1122085995
        ],
        "177–178\ndynamic": [
            1122085995
        ],
        "of\n184–189\ndefining": [
            1122085995
        ],
        "\npersistentvolumeclaims\n185–187\nwithout": [
            1122085995
        ],
        "187–189\n": [
            1122085995
        ],
        "\n\nindex581\npersistentvolumes": [
            1122085995
        ],
        "(continued)\ndynamically": [
            1122085995
        ],
        "186–187\nlisting": [
            1122085995
        ],
        "180–181\npre-provisioned": [
            1122085995
        ],
        "189\nreclaiming": [
            1122085995
        ],
        "automatically\n183–184\nreclaiming": [
            1122085995
        ],
        "183\nrecycling": [
            1122085995
        ],
        "183–184\npet": [
            1122085995
        ],
        "pods295–299\ncommunicating": [
            1122085995
        ],
        "servers\n295–297\nconnecting": [
            1122085995
        ],
        "299\ndeleting": [
            1122085995
        ],
        "297–298\nexposing": [
            1122085995
        ],
        "services\n298–299\nscaling": [
            1122085995
        ],
        "298\npetsets284\nphotonpersistentdisk": [
            1122085995
        ],
        "volume163\npid": [
            1122085995
        ],
        "namespaces379–380\npinging": [
            1122085995
        ],
        "ip131\nplatforms527–533\ndeis": [
            1122085995
        ],
        "530–533\nhelm": [
            1122085995
        ],
        "530–533\nred": [
            1122085995
        ],
        "527–530\napplication": [
            1122085995
        ],
        "templates\n528–529\nautomatically": [
            1122085995
        ],
        "\ndeploymentconfigs\n529–530\nbuilding": [
            1122085995
        ],
        "529\nexposing": [
            1122085995
        ],
        "routes": [
            1122085995
        ],
        "530\ngroups": [
            1122085995
        ],
        "528\nprojects": [
            1122085995
        ],
        "528\nresources": [
            1122085995
        ],
        "527–528\nusers": [
            1122085995
        ],
        "528\nusing": [
            1122085995
        ],
        "530\npod": [
            1122085995
        ],
        "affinity\nco-locating": [
            1122085995
        ],
        "468–476\ndeploying": [
            1122085995
        ],
        "avail-\nability": [
            1122085995
        ],
        "471–472\ndeploying": [
            1122085995
        ],
        "geo-\ngraphical": [
            1122085995
        ],
        "471–\n472\ndeploying": [
            1122085995
        ],
        "471–472\nexpressing": [
            1122085995
        ],
        "pref-\nerences": [
            1122085995
        ],
        "472–\n473\ndeploying": [
            1122085995
        ],
        "468–471\nusing": [
            1122085995
        ],
        "470–471\nsee": [
            1122085995
        ],
        "affinity\npodaffinity": [
            1122085995
        ],
        "\nhard": [
            1122085995
        ],
        "requirements\n472–473\npodantiaffinity": [
            1122085995
        ],
        "property474\npoddisruptionbudget": [
            1122085995
        ],
        "\nresource455\npoddisruptionbudget": [
            1122085995
        ],
        "pdb\npod_ip": [
            1122085995
        ],
        "variable228\npod_name": [
            1122085995
        ],
        "variable228\npod_namespace": [
            1122085995
        ],
        "variable228\npods20": [
            1122085995
        ],
        "47–55": [
            1122085995
        ],
        "83\naccessing": [
            1122085995
        ],
        "264\nadding": [
            1122085995
        ],
        "to\n484–485\nadding": [
            1122085995
        ],
        "151–153\nadding": [
            1122085995
        ],
        "template\n151–152\nhitting": [
            1122085995
        ],
        "\nready": [
            1122085995
        ],
        "153\nmodifying": [
            1122085995
        ],
        "152\nobserving": [
            1122085995
        ],
        "152\nadding": [
            1122085995
        ],
        "460–461\nadvanced": [
            1122085995
        ],
        "321\nannotating": [
            1122085995
        ],
        "75–76\nadding": [
            1122085995
        ],
        "76\nlooking": [
            1122085995
        ],
        "75–76\nmodifying": [
            1122085995
        ],
        "76\nassigning": [
            1122085995
        ],
        "418\nassigning": [
            1122085995
        ],
        "332\nassigning": [
            1122085995
        ],
        "351–353\nassigning": [
            1122085995
        ],
        "417\nassigning": [
            1122085995
        ],
        "417–418\nattracting": [
            1122085995
        ],
        "462–463\nexamining": [
            1122085995
        ],
        "463\n–465\ncalculating": [
            1122085995
        ],
        "439\nco-locating": [
            1122085995
        ],
        "468–476\nexpressing": [
            1122085995
        ],
        "472–473\nusing": [
            1122085995
        ],
        "468–471\nwith": [
            1122085995
        ],
        "468–476\ncommunicating": [
            1122085995
        ],
        "238–243\nauthenticating": [
            1122085995
        ],
        "241–242\nfinding": [
            1122085995
        ],
        "\naddresses": [
            1122085995
        ],
        "239–240\nrunning": [
            1122085995
        ],
        "239\nverifying": [
            1122085995
        ],
        "identity\n240–241\ncommunicating": [
            1122085995
        ],
        "243\nconfiguring": [
            1122085995
        ],
        "462\nconnecting": [
            1122085995
        ],
        "\nforwarders": [
            1122085995
        ],
        "67\ncontainers\nresources": [
            1122085995
        ],
        "332\ncontainers": [
            1122085995
        ],
        "57\ncontainers": [
            1122085995
        ],
        "\nspace": [
            1122085995
        ],
        "57\ncreating": [
            1122085995
        ],
        "164–165\nas": [
            1122085995
        ],
        "398–399\nfrom": [
            1122085995
        ],
        "61–67\nfrom": [
            1122085995
        ],
        "descriptors\n61–67\nexamining": [
            1122085995
        ],
        "descrip-\ntors": [
            1122085995
        ],
        "pods\n61–63\nviewing": [
            1122085995
        ],
        "65–66\nmanually": [
            1122085995
        ],
        "281\nnew": [
            1122085995
        ],
        "96\nspecific": [
            1122085995
        ],
        "\neach": [
            1122085995
        ],
        "373\nusing": [
            1122085995
        ],
        "351–352\nwith": [
            1122085995
        ],
        "172\nwith": [
            1122085995
        ],
        "65\nwith": [
            1122085995
        ],
        "332\nwith": [
            1122085995
        ],
        "412–413\nwith": [
            1122085995
        ],
        "requests\n405–406\nyaml": [
            1122085995
        ],
        "63–65\n": [
            1122085995
        ],
        "\n\nindex582\npods": [
            1122085995
        ],
        "(continued)\ndead": [
            1122085995
        ],
        "482–483\ndecoupling": [
            1122085995
        ],
        "technologies\n176–184\nbenefits": [
            1122085995
        ],
        "182\nusing": [
            1122085995
        ],
        "176–177": [
            1122085995
        ],
        "\n179–181\nusing": [
            1122085995
        ],
        "persistentvolumes\n176–184\ndefinitions": [
            1122085995
        ],
        "62–63\ndeleting": [
            1122085995
        ],
        "82\nforcibly": [
            1122085995
        ],
        "306–307\nold": [
            1122085995
        ],
        "252–253\nusing": [
            1122085995
        ],
        "80\ndeploying\nin": [
            1122085995
        ],
        "468\nmanaged": [
            1122085995
        ],
        "84–118\nwith": [
            1122085995
        ],
        "\nout-of-range": [
            1122085995
        ],
        "393\ndiscovering": [
            1122085995
        ],
        "242\ndiscovering": [
            1122085995
        ],
        "dns\n155–156\ndisplaying": [
            1122085995
        ],
        "for\n431–432\ndisplaying": [
            1122085995
        ],
        "431–432\ndisplaying": [
            1122085995
        ],
        "459\nflat": [
            1122085995
        ],
        "58\nfortune": [
            1122085995
        ],
        "195–196\nfreeing": [
            1122085995
        ],
        "\nschedule": [
            1122085995
        ],
        "410\nhorizontal": [
            1122085995
        ],
        "of\n438–451\nautoscaling": [
            1122085995
        ],
        "450\nscaling": [
            1122085995
        ],
        "441\n–447\nscaling": [
            1122085995
        ],
        "448\nscaling": [
            1122085995
        ],
        "448–450\nscaling": [
            1122085995
        ],
        "450–451\nhorizontally": [
            1122085995
        ],
        "102–103\ndeclarative": [
            1122085995
        ],
        "103\nscaling": [
            1122085995
        ],
        "\nscale": [
            1122085995
        ],
        "definitions\n102–103\nscaling": [
            1122085995
        ],
        "102\nin": [
            1122085995
        ],
        "165\nin": [
            1122085995
        ],
        "400\ninspecting": [
            1122085995
        ],
        "\ndescribe": [
            1122085995
        ],
        "52\nisolating": [
            1122085995
        ],
        "399–402\nenabling": [
            1122085995
        ],
        "401–402\nlifecycles": [
            1122085995
        ],
        "479–491\nadding": [
            1122085995
        ],
        "hooks\n485–489\nkilling": [
            1122085995
        ],
        "479–482\nrelocating": [
            1122085995
        ],
        "applications\n479–482\nlisting": [
            1122085995
        ],
        "548\nlisting": [
            1122085995
        ],
        "selectors\n71–72\nlogs": [
            1122085995
        ],
        "66\nmanaged": [
            1122085995
        ],
        "99\nmanaged": [
            1122085995
        ],
        "99–100\nmarked": [
            1122085995
        ],
        "307\nmodifying": [
            1122085995
        ],
        "existing\n70–71\nmodifying": [
            1122085995
        ],
        "451–452\nmonitoring": [
            1122085995
        ],
        "432–434\nmoving": [
            1122085995
        ],
        "\nreplicationcontrollers\n98–101\nadding": [
            1122085995
        ],
        "99\nchanging": [
            1122085995
        ],
        "selector\n100–101\nmulti-container": [
            1122085995
        ],
        "66\nnetworking": [
            1122085995
        ],
        "335–338\ncni": [
            1122085995
        ],
        "336–337\nnetwork": [
            1122085995
        ],
        "architecture\n335–336\nnetworking": [
            1122085995
        ],
        "336–338\nnot": [
            1122085995
        ],
        "409–410\nof": [
            1122085995
        ],
        "525–526\norganizing": [
            1122085995
        ],
        "59\norganizing": [
            1122085995
        ],
        "67–71\noverview": [
            1122085995
        ],
        "48–58": [
            1122085995
        ],
        "60\npartial": [
            1122085995
        ],
        "57\npartially": [
            1122085995
        ],
        "rescheduling\n482–483\nperforming": [
            1122085995
        ],
        "112–116\ndefining": [
            1122085995
        ],
        "113–114\njob": [
            1122085995
        ],
        "112\nrunning": [
            1122085995
        ],
        "\ninstances": [
            1122085995
        ],
        "114–116\nseeing": [
            1122085995
        ],
        "114\npreventing": [
            1122085995
        ],
        "\nconnection": [
            1122085995
        ],
        "492–497\nprioritizing": [
            1122085995
        ],
        "465–468\nlabeling": [
            1122085995
        ],
        "466\nnode": [
            1122085995
        ],
        "467\nspecifying": [
            1122085995
        ],
        "466–467\n": [
            1122085995
        ],
        "\n\nindex583\npods": [
            1122085995
        ],
        "(continued)\nproviding": [
            1122085995
        ],
        "282–284\nqos": [
            1122085995
        ],
        "420–421\nreading": [
            1122085995
        ],
        "218\nreattaching": [
            1122085995
        ],
        "289\nre-creating": [
            1122085995
        ],
        "173–174\nreferencing": [
            1122085995
        ],
        "\nmaps": [
            1122085995
        ],
        "203\nremoving": [
            1122085995
        ],
        "100\nrepelling": [
            1122085995
        ],
        "\ntaints": [
            1122085995
        ],
        "460\ntaints": [
            1122085995
        ],
        "461–462\nrepelling": [
            1122085995
        ],
        "\ntolerations": [
            1122085995
        ],
        "overview\n458–460\nusing": [
            1122085995
        ],
        "461–462\nreplacing": [
            1122085995
        ],
        "259–260\nreplicationcontrollers": [
            1122085995
        ],
        "95\nrequesting": [
            1122085995
        ],
        "405–412\ndefining": [
            1122085995
        ],
        "411–412\neffect": [
            1122085995
        ],
        "406–410\nrequests": [
            1122085995
        ],
        "hit-\nting": [
            1122085995
        ],
        "50\nrestricting": [
            1122085995
        ],
        "389–399\nassigning": [
            1122085995
        ],
        "podsecu-\nritypolicies": [
            1122085995
        ],
        "396–399\nassigning": [
            1122085995
        ],
        "396–399\nconfiguring": [
            1122085995
        ],
        "394–395\nconfiguring": [
            1122085995
        ],
        "394–395\nconstraining": [
            1122085995
        ],
        "vol-\numes": [
            1122085995
        ],
        "395\nfsgroup": [
            1122085995
        ],
        "392–394\npodsecuritypolicy": [
            1122085995
        ],
        "389–392\nrunasuser": [
            1122085995
        ],
        "392–394\nsupplementalgroups": [
            1122085995
        ],
        "\npolicies": [
            1122085995
        ],
        "392–394\nretrieving": [
            1122085995
        ],
        "65\nrunning": [
            1122085995
        ],
        "333–334": [
            1122085995
        ],
        "553\nin": [
            1122085995
        ],
        "382–384\non": [
            1122085995
        ],
        "nodes\nadding": [
            1122085995
        ],
        "111\ndaemonsets": [
            1122085995
        ],
        "109–112\nremoving": [
            1122085995
        ],
        "111–112\non": [
            1122085995
        ],
        "109\none": [
            1122085995
        ],
        "108–112\nshell": [
            1122085995
        ],
        "130–131\nwithout": [
            1122085995
        ],
        "381\nwithout": [
            1122085995
        ],
        "\nmanifest": [
            1122085995
        ],
        "155\nrunning": [
            1122085995
        ],
        "515–516\nscheduler": [
            1122085995
        ],
        "74–75\nseeing": [
            1122085995
        ],
        "65\nselecting": [
            1122085995
        ],
        "for\n320–321\nsending": [
            1122085995
        ],
        "66–67\nsequence": [
            1122085995
        ],
        "493–495\nsetting": [
            1122085995
        ],
        "421–425\nsetting": [
            1122085995
        ],
        "421–425\nshutdowns": [
            1122085995
        ],
        "489–491\nimplementing": [
            1122085995
        ],
        "shut-\ndown": [
            1122085995
        ],
        "490–491\nreplacing": [
            1122085995
        ],
        "\nprocedures": [
            1122085995
        ],
        "dedi-\ncated": [
            1122085995
        ],
        "491\nspecifying": [
            1122085995
        ],
        "\nperiods": [
            1122085995
        ],
        "490\nsignaling": [
            1122085995
        ],
        "149–153\nspecifying": [
            1122085995
        ],
        "\nstates": [
            1122085995
        ],
        "69–70\nspecifying": [
            1122085995
        ],
        "469\nspinning": [
            1122085995
        ],
        "252–253\nstarting": [
            1122085995
        ],
        "483–484\nstarting": [
            1122085995
        ],
        "483–485\nbest": [
            1122085995
        ],
        "dependencies\n485\ninit": [
            1122085995
        ],
        "484\nstatic": [
            1122085995
        ],
        "326–327\nstopping": [
            1122085995
        ],
        "80–82\ntemplates\nchanging": [
            1122085995
        ],
        "101\neffect": [
            1122085995
        ],
        "93\nusing": [
            1122085995
        ],
        "\ntemplates": [
            1122085995
        ],
        "288\nupdating": [
            1122085995
        ],
        "251–253\nusing": [
            1122085995
        ],
        "283–284\nusing": [
            1122085995
        ],
        "in\n163–164\nusing": [
            1122085995
        ],
        "gce-\npersistentdisk": [
            1122085995
        ],
        "volumes\n172\nwriting": [
            1122085995
        ],
        "173\nusing": [
            1122085995
        ],
        "\ndiscover": [
            1122085995
        ],
        "154–156\nusing": [
            1122085995
        ],
        "376–380\nbinding": [
            1122085995
        ],
        "377–379\nusing": [
            1122085995
        ],
        "379–380\nusing": [
            1122085995
        ],
        "74–75\nusing": [
            1122085995
        ],
        "80\ncreating": [
            1122085995
        ],
        "78–79\n": [
            1122085995
        ],
        "\n\nindex584\npods": [
            1122085995
        ],
        "(continued)\ndiscovering": [
            1122085995
        ],
        "77–78\nisolation": [
            1122085995
        ],
        "79–80\nmanaging": [
            1122085995
        ],
        "79\nnamespaces": [
            1122085995
        ],
        "77\nusing": [
            1122085995
        ],
        "376–377\nusing": [
            1122085995
        ],
        "281–282\nusing": [
            1122085995
        ],
        "181\nusing": [
            1122085995
        ],
        "222\nexposing": [
            1122085995
        ],
        "221–222\nmodifying": [
            1122085995
        ],
        "\nhttps": [
            1122085995
        ],
        "218–219\nmounting": [
            1122085995
        ],
        "219–220\nsecret": [
            1122085995
        ],
        "221\nusing": [
            1122085995
        ],
        "\nscheduling\ncategorizing": [
            1122085995
        ],
        "74–75\nverifying": [
            1122085995
        ],
        "173–174\nvertical": [
            1122085995
        ],
        "451–452\nautomatically": [
            1122085995
        ],
        "451\nmodifying": [
            1122085995
        ],
        "running\n451–452\nweb": [
            1122085995
        ],
        "\ncloned": [
            1122085995
        ],
        "167\nwhen": [
            1122085995
        ],
        "59–60\nwhy": [
            1122085995
        ],
        "56–57\nwith": [
            1122085995
        ],
        "deter-\nmining": [
            1122085995
        ],
        "419\nwith": [
            1122085995
        ],
        "pods\npodsecuritypolicy": [
            1122085995
        ],
        "resources\n389–392\nassigning": [
            1122085995
        ],
        "396–399\ncreating": [
            1122085995
        ],
        "398\ncreating": [
            1122085995
        ],
        "398–399\nwith": [
            1122085995
        ],
        "397–398\ncreating": [
            1122085995
        ],
        "\ndeployed": [
            1122085995
        ],
        "396–397\nexamining": [
            1122085995
        ],
        "391–392\noverview": [
            1122085995
        ],
        "390–391\npodspec.containers.args": [
            1122085995
        ],
        "field204\npolicy": [
            1122085995
        ],
        "of393\nport": [
            1122085995
        ],
        "forwarding66\nport": [
            1122085995
        ],
        "spaces": [
            1122085995
        ],
        "sharing57\nportability": [
            1122085995
        ],
        "\nimages15\nports\nexposing": [
            1122085995
        ],
        "126–127\nlocal": [
            1122085995
        ],
        "67\nnamed": [
            1122085995
        ],
        "127–128\nsee": [
            1122085995
        ],
        "ports\npost-start": [
            1122085995
        ],
        "hooks485\nprefernoschedule460\npreserving": [
            1122085995
        ],
        "volumes\n480–482\npre-stop": [
            1122085995
        ],
        "hooks485\nprivate": [
            1122085995
        ],
        "hub222\nprivilege": [
            1122085995
        ],
        "escalation372\nprivileged": [
            1122085995
        ],
        "containers\ndeployment": [
            1122085995
        ],
        "396–397\noverview": [
            1122085995
        ],
        "403\nprivileged": [
            1122085995
        ],
        "\nin382–384\nprivileged": [
            1122085995
        ],
        "policy397\nprivileged": [
            1122085995
        ],
        "property383\nprobes": [
            1122085995
        ],
        "probes;": [
            1122085995
        ],
        "probes\nprocedures": [
            1122085995
        ],
        "proce-\ndures\nprocess": [
            1122085995
        ],
        "(pid)": [
            1122085995
        ],
        "namespace11\nprocesses\nisolating": [
            1122085995
        ],
        "11\nkilling": [
            1122085995
        ],
        "\nlow": [
            1122085995
        ],
        "420–421\nhandling": [
            1122085995
        ],
        "420\nmultiple": [
            1122085995
        ],
        "56–57\npreventing": [
            1122085995
        ],
        "386–387\nprojects": [
            1122085995
        ],
        "platform528\nprovisioning": [
            1122085995
        ],
        "provi-\nsioning\nprovisioningfailed": [
            1122085995
        ],
        "event186\nproxies327–328\npublichtml": [
            1122085995
        ],
        "volume162\npushing\nimages": [
            1122085995
        ],
        "36\nimages": [
            1122085995
        ],
        "35\npvc": [
            1122085995
        ],
        "(persistentvolumeclaims)\n176–177": [
            1122085995
        ],
        "532\nbound": [
            1122085995
        ],
        "189\ncreating": [
            1122085995
        ],
        "179–180": [
            1122085995
        ],
        "288\ncreating": [
            1122085995
        ],
        "188–189\ndeleting": [
            1122085995
        ],
        "288\nexamining": [
            1122085995
        ],
        "295\nlisting": [
            1122085995
        ],
        "180\nreattaching": [
            1122085995
        ],
        "289\nrequesting": [
            1122085995
        ],
        "in\n185–187\nby": [
            1122085995
        ],
        "definition\n185–186\nexamining": [
            1122085995
        ],
        "186–187\nexamining": [
            1122085995
        ],
        "186–187\nusing": [
            1122085995
        ],
        "187\nusing": [
            1122085995
        ],
        "181\nq\nqcow2": [
            1122085995
        ],
        "format555\nqemu": [
            1122085995
        ],
        "tool555\nqos": [
            1122085995
        ],
        "(quality": [
            1122085995
        ],
        "service)417\nqos": [
            1122085995
        ],
        "classes417–421\ndefining": [
            1122085995
        ],
        "417–419\nassigning": [
            1122085995
        ],
        "417–418\nhandling": [
            1122085995
        ],
        "420–421\n": [
            1122085995
        ],
        "\n\nindex585\nqos": [
            1122085995
        ],
        "(continued)\nkilling": [
            1122085995
        ],
        "420\nof": [
            1122085995
        ],
        "418\nof": [
            1122085995
        ],
        "419\nsequence": [
            1122085995
        ],
        "420\nspecifying": [
            1122085995
        ],
        "429\nqps": [
            1122085995
        ],
        "(queries-per-second)439": [
            1122085995
        ],
        "\n449\nquobyte": [
            1122085995
        ],
        "volume163\nquorum315\nquotas": [
            1122085995
        ],
        "specifying427–429\nr\nracks": [
            1122085995
        ],
        "in471–472\nrbac": [
            1122085995
        ],
        "(role-based": [
            1122085995
        ],
        "\ncontrol)242\nauthorization": [
            1122085995
        ],
        "353–354\nplugins": [
            1122085995
        ],
        "354\nresources": [
            1122085995
        ],
        "355–357\ncreating": [
            1122085995
        ],
        "357\nenabling": [
            1122085995
        ],
        "356\nlisting": [
            1122085995
        ],
        "357\nrunning": [
            1122085995
        ],
        "357\nsecuring": [
            1122085995
        ],
        "clusterroles\n371–373\ngranting": [
            1122085995
        ],
        "397–398\nrbd": [
            1122085995
        ],
        "volume163\nrc": [
            1122085995
        ],
        "(replicationcontroller)46": [
            1122085995
        ],
        "95\nreadiness": [
            1122085995
        ],
        "probes\nadding": [
            1122085995
        ],
        "151–152\nhitting": [
            1122085995
        ],
        "152\nbenefits": [
            1122085995
        ],
        "151\ndefining": [
            1122085995
        ],
        "153\ndefining": [
            1122085995
        ],
        "275\nincluding": [
            1122085995
        ],
        "153\noperation": [
            1122085995
        ],
        "150\noverview": [
            1122085995
        ],
        "149–151\npreventing": [
            1122085995
        ],
        "277–278\ntypes": [
            1122085995
        ],
        "150\nreading": [
            1122085995
        ],
        "\npods218\nread-only": [
            1122085995
        ],
        "clusterrole372\nreadonlymany": [
            1122085995
        ],
        "mode180\nreadonlyrootfilesystem": [
            1122085995
        ],
        "\nproperty387\n": [
            1122085995
        ],
        "392\nreadwritemany": [
            1122085995
        ],
        "mode180\nreadwriteonce": [
            1122085995
        ],
        "mode180\nready": [
            1122085995
        ],
        "column44": [
            1122085995
        ],
        "152\nreconciliation": [
            1122085995
        ],
        "loops92\nrecords": [
            1122085995
        ],
        "dns155–156\nre-creating": [
            1122085995
        ],
        "pods173–174\nrecycling": [
            1122085995
        ],
        "persistentvolumes\n183–184\nautomatically": [
            1122085995
        ],
        "183–184\nmanually": [
            1122085995
        ],
        "183\nred": [
            1122085995
        ],
        "\nplatform527–530\napplication": [
            1122085995
        ],
        "528–529\nautomatically": [
            1122085995
        ],
        "\nbuilt": [
            1122085995
        ],
        "530\nreferencing": [
            1122085995
        ],
        "pods203\nregistering\ncustom": [
            1122085995
        ],
        "519\nservice": [
            1122085995
        ],
        "522–523\nregistries": [
            1122085995
        ],
        "with223\nrel=canary": [
            1122085995
        ],
        "label80\nrelinquishing": [
            1122085995
        ],
        "nodes453\nreloading": [
            1122085995
        ],
        "\nto212\nrelocating": [
            1122085995
        ],
        "480–482\nremoving\ncontainers": [
            1122085995
        ],
        "34–35\nlabels": [
            1122085995
        ],
        "111–112\npods": [
            1122085995
        ],
        "100\nrepelling\npods": [
            1122085995
        ],
        "458\n–460\nusing": [
            1122085995
        ],
        "461–462\npods": [
            1122085995
        ],
        "pods\nby": [
            1122085995
        ],
        "259–260\noverview": [
            1122085995
        ],
        "252\nreplica": [
            1122085995
        ],
        "count49": [
            1122085995
        ],
        "92\nreplicas\nrunning": [
            1122085995
        ],
        "281–282\ncreating": [
            1122085995
        ],
        "281\nusing": [
            1122085995
        ],
        "282\nusing": [
            1122085995
        ],
        "281–282\nscaling": [
            1122085995
        ],
        "450–451\nupdating": [
            1122085995
        ],
        "440\nreplicasets104–108": [
            1122085995
        ],
        "559\ncontrollers\ncreating": [
            1122085995
        ],
        "332\ncreating": [
            1122085995
        ],
        "331\ncreating": [
            1122085995
        ],
        "106–107": [
            1122085995
        ],
        "263–264\ndefining": [
            1122085995
        ],
        "105–106\nexamining": [
            1122085995
        ],
        "106–107\nusing": [
            1122085995
        ],
        "281–282\nvs": [
            1122085995
        ],
        "105\nvs": [
            1122085995
        ],
        "284–285\n": [
            1122085995
        ],
        "\n\nindex586\nreplicating": [
            1122085995
        ],
        "pods281–284\nproviding": [
            1122085995
        ],
        "282–284\nrunning": [
            1122085995
        ],
        "\nseparate": [
            1122085995
        ],
        "281–282\nreplication": [
            1122085995
        ],
        "controllers47–48\nreplication": [
            1122085995
        ],
        "managers323–324\nreplicationcontrollers90–104\nbenefits": [
            1122085995
        ],
        "93\nchanging": [
            1122085995
        ],
        "101\ncreating": [
            1122085995
        ],
        "93–94\ncreating": [
            1122085995
        ],
        "96\ndeleting": [
            1122085995
        ],
        "103–104\ngetting": [
            1122085995
        ],
        "about\n95–96\nhorizontally": [
            1122085995
        ],
        "pods\n102–103\nin": [
            1122085995
        ],
        "94–98\nmoving": [
            1122085995
        ],
        "\nscope": [
            1122085995
        ],
        "98–101\nadding": [
            1122085995
        ],
        "selectors\n100–101\nchanging": [
            1122085995
        ],
        "99–100\nremoving": [
            1122085995
        ],
        "100\noperation": [
            1122085995
        ],
        "91–93\nparts": [
            1122085995
        ],
        "92–93\nperforming": [
            1122085995
        ],
        "with\n254–261\nobsolescence": [
            1122085995
        ],
        "\nrolling-update\n260–261\nperforming": [
            1122085995
        ],
        "254–255\nreconciliation": [
            1122085995
        ],
        "92\nreplacing": [
            1122085995
        ],
        "259–260\nresponding": [
            1122085995
        ],
        "95\nresponding": [
            1122085995
        ],
        "97–98\nscaling": [
            1122085995
        ],
        "102\nvs": [
            1122085995
        ],
        "284–285\nrepo": [
            1122085995
        ],
        "\nmanager544\nrequests\nfor": [
            1122085995
        ],
        "421–425\nfrom": [
            1122085995
        ],
        "492–497\npreventing": [
            1122085995
        ],
        "493–497\npreventing": [
            1122085995
        ],
        "492–493\nmodifying": [
            1122085995
        ],
        "\nadmission": [
            1122085995
        ],
        "317\nsending": [
            1122085995
        ],
        "66–67\nconnecting": [
            1122085995
        ],
        "forwarders": [
            1122085995
        ],
        "67\nforwarding": [
            1122085995
        ],
        "67\nsee": [
            1122085995
        ],
        "requests\nrequireddropcapabilities394–395\nrequiredduringscheduling464": [
            1122085995
        ],
        "\n475\nrescaling": [
            1122085995
        ],
        "automatically444–445\nrescheduling\ndead": [
            1122085995
        ],
        "482–483\npartially": [
            1122085995
        ],
        "482–483\nresource": [
            1122085995
        ],
        "\nwith412–413\nresource": [
            1122085995
        ],
        "on449\nresource": [
            1122085995
        ],
        "requests\nautomatically": [
            1122085995
        ],
        "451\ncreating": [
            1122085995
        ],
        "405–406\ndefault": [
            1122085995
        ],
        "424–425\ndefining": [
            1122085995
        ],
        "406–410\ncreating": [
            1122085995
        ],
        "408–409\nfreeing": [
            1122085995
        ],
        "410\ninspecting": [
            1122085995
        ],
        "capacity\n407–408\npods": [
            1122085995
        ],
        "scheduled\n409–410\nscheduler": [
            1122085995
        ],
        "406\nscheduler": [
            1122085995
        ],
        "407\nfor": [
            1122085995
        ],
        "411–412\nmodifying": [
            1122085995
        ],
        "451–\n452\nresourcefieldref233\nresourcenames": [
            1122085995
        ],
        "field358\nresourcequota": [
            1122085995
        ],
        "resources\n425–427\ncreating": [
            1122085995
        ],
        "425–426\ncreating": [
            1122085995
        ],
        "\nresourcequota": [
            1122085995
        ],
        "427\ninspecting": [
            1122085995
        ],
        "426\nresources\naccessing": [
            1122085995
        ],
        "367–370\nallowing": [
            1122085995
        ],
        "\nclusterrole": [
            1122085995
        ],
        "372\nanalyzing": [
            1122085995
        ],
        "432–434\ngrafana": [
            1122085995
        ],
        "432\ninfluxdb": [
            1122085995
        ],
        "433\nusing": [
            1122085995
        ],
        "\ncharts": [
            1122085995
        ],
        "434\nanalyzing": [
            1122085995
        ],
        "\ngrafana": [
            1122085995
        ],
        "433–434\nauto-deploying": [
            1122085995
        ],
        "manifests\n504–505\navailable": [
            1122085995
        ],
        "429\navailable": [
            1122085995
        ],
        "415–416\ncluster-level": [
            1122085995
        ],
        "362–365\ncollecting": [
            1122085995
        ],
        "430–432\ndisplaying": [
            1122085995
        ],
        "431–432\nenabling": [
            1122085995
        ],
        "431\ncustom\nautomating": [
            1122085995
        ],
        "513–517\ncreating": [
            1122085995
        ],
        "of\n511–512\nretrieving": [
            1122085995
        ],
        "512\ndeleting": [
            1122085995
        ],
        "82\n": [
            1122085995
        ],
        "\n\nindex587\nresources": [
            1122085995
        ],
        "(continued)\ndeploying": [
            1122085995
        ],
        "helm\n531–533\ndescribing": [
            1122085995
        ],
        "498\nfederated\nfunctions": [
            1122085995
        ],
        "559\nversions": [
            1122085995
        ],
        "558\nfreeing": [
            1122085995
        ],
        "410\nin": [
            1122085995
        ],
        "527–528\nlimiting": [
            1122085995
        ],
        "11–12\nmodifying": [
            1122085995
        ],
        "318–319\nof": [
            1122085995
        ],
        "430–434\nread-only": [
            1122085995
        ],
        "372\nrequesting": [
            1122085995
        ],
        "406–410\nretrieving": [
            1122085995
        ],
        "usage\n431–432\ndisplaying": [
            1122085995
        ],
        "431\nsetting": [
            1122085995
        ],
        "413\nstoring": [
            1122085995
        ],
        "313–314\nstoring": [
            1122085995
        ],
        "318\nstoring": [
            1122085995
        ],
        "434\nusing": [
            1122085995
        ],
        "group\n76–80\ncreating": [
            1122085995
        ],
        "78–79\ndiscovering": [
            1122085995
        ],
        "pods\n77–78\nisolation": [
            1122085995
        ],
        "77\nvalidating": [
            1122085995
        ],
        "318\nversioning": [
            1122085995
        ],
        "504–505\nsee": [
            1122085995
        ],
        "resources\nrest": [
            1122085995
        ],
        "api234–238\naccessing": [
            1122085995
        ],
        "\nrest": [
            1122085995
        ],
        "236–237\nexploring": [
            1122085995
        ],
        "proxy\n235–236\nlisting": [
            1122085995
        ],
        "238\n--restart=never": [
            1122085995
        ],
        "option446\nrestartpolicy": [
            1122085995
        ],
        "property113\nrestarts": [
            1122085995
        ],
        "containers480–482\nrestful": [
            1122085995
        ],
        "\ntransfer)": [
            1122085995
        ],
        "apis4\nresuming": [
            1122085995
        ],
        "rollouts274\nretrieving\ninstances": [
            1122085995
        ],
        "512\njob": [
            1122085995
        ],
        "238\nresource": [
            1122085995
        ],
        "431\nretry": [
            1122085995
        ],
        "probes90\nrevisionhistorylimit": [
            1122085995
        ],
        "property270\nrevisions": [
            1122085995
        ],
        "\nto270\nrhel": [
            1122085995
        ],
        "(red": [
            1122085995
        ],
        "\nlinux)12\nrkt": [
            1122085995
        ],
        "system\nconfiguring": [
            1122085995
        ],
        "552\nreplacing": [
            1122085995
        ],
        "552–555\nusing": [
            1122085995
        ],
        "553–555\ninspecting": [
            1122085995
        ],
        "vm\n553–554\nlisting": [
            1122085995
        ],
        "553\n--rm": [
            1122085995
        ],
        "option446\nrole": [
            1122085995
        ],
        "resources\nbinding": [
            1122085995
        ],
        "359–360\ncreating": [
            1122085995
        ],
        "357–359\nusing": [
            1122085995
        ],
        "358\n–359\nrole-based": [
            1122085995
        ],
        "\nrbac\nrolebindings\ncombining": [
            1122085995
        ],
        "clusterroles\n370–371\ncombining": [
            1122085995
        ],
        "370–371\nincluding": [
            1122085995
        ],
        "361\nusing": [
            1122085995
        ],
        "358–359\nrolling": [
            1122085995
        ],
        "deployments\n268–270\ncreating": [
            1122085995
        ],
        "269\ndisplaying": [
            1122085995
        ],
        "\nhistory": [
            1122085995
        ],
        "270\nto": [
            1122085995
        ],
        "updates253\nperforming": [
            1122085995
        ],
        "\nrolling-update": [
            1122085995
        ],
        "260–261\nrunning": [
            1122085995
        ],
        "254–255\nperforming": [
            1122085995
        ],
        "kubectl\n256–260\nreplacing": [
            1122085995
        ],
        "commences\n258–259\nslowing": [
            1122085995
        ],
        "265–267\nrolling-update": [
            1122085995
        ],
        "\nkubectl260–261\nrollouts\nblocking": [
            1122085995
        ],
        "274–278\nconfiguring": [
            1122085995
        ],
        "276–277\n": [
            1122085995
        ],
        "\n\nindex588\nrollouts": [
            1122085995
        ],
        "(continued)\nconfiguring": [
            1122085995
        ],
        "278\ncontrolling": [
            1122085995
        ],
        "271–273\nmaxsurge": [
            1122085995
        ],
        "271–272\nmaxunavailable": [
            1122085995
        ],
        "property\n271–273\nof": [
            1122085995
        ],
        "deployments\ndisplaying": [
            1122085995
        ],
        "270\ndisplaying": [
            1122085995
        ],
        "263\npausing": [
            1122085995
        ],
        "273–274\npreventing\nby": [
            1122085995
        ],
        "274\nwith": [
            1122085995
        ],
        "probes\n277–278\nresuming": [
            1122085995
        ],
        "274\nundoing": [
            1122085995
        ],
        "269\nroot": [
            1122085995
        ],
        "as382\nroutes": [
            1122085995
        ],
        "exter-\nnally": [
            1122085995
        ],
        "with530\nrpi-cluster": [
            1122085995
        ],
        "context538\nrpi-foo": [
            1122085995
        ],
        "context538\nrun": [
            1122085995
        ],
        "command27": [
            1122085995
        ],
        "42–43": [
            1122085995
        ],
        "47\nrunasuser": [
            1122085995
        ],
        "mustrun-\nasnonroot": [
            1122085995
        ],
        "in394\nrunasuser": [
            1122085995
        ],
        "392–393\nusing": [
            1122085995
        ],
        "394\nrunasuser": [
            1122085995
        ],
        "property381": [
            1122085995
        ],
        "393\nruntimes": [
            1122085995
        ],
        "runtimes\ns\nscaled": [
            1122085995
        ],
        "on440\nscale-downs456\nof": [
            1122085995
        ],
        "dis-\nruption": [
            1122085995
        ],
        "454–456\nto": [
            1122085995
        ],
        "450–451\nscaleio": [
            1122085995
        ],
        "volume163\nscale-out": [
            1122085995
        ],
        "of49–50\nscale-ups456\nof": [
            1122085995
        ],
        "446–447\ntriggering": [
            1122085995
        ],
        "445–446\nscaling\napplications": [
            1122085995
        ],
        "horizontally\n48–50\nautomatic": [
            1122085995
        ],
        "23\nbased": [
            1122085995
        ],
        "utilization\n441–447\nautomatic": [
            1122085995
        ],
        "events\n444–445\ncreating": [
            1122085995
        ],
        "446–447\nbased": [
            1122085995
        ],
        "metrics\n448–450\nobject": [
            1122085995
        ],
        "449–450\npods": [
            1122085995
        ],
        "449\nbased": [
            1122085995
        ],
        "448\ncluster": [
            1122085995
        ],
        "horizontally\n452–456\ncluster": [
            1122085995
        ],
        "autoscaler\n454\nlimiting": [
            1122085995
        ],
        "454–456\ndown": [
            1122085995
        ],
        "450–451\nhorizontal": [
            1122085995
        ],
        "438–451\nautoscaling": [
            1122085995
        ],
        "450\njob": [
            1122085995
        ],
        "116\nmaximum": [
            1122085995
        ],
        "447\nmicroservices": [
            1122085995
        ],
        "4\nnumber": [
            1122085995
        ],
        "21\npods": [
            1122085995
        ],
        "replicationcontrol-\nler": [
            1122085995
        ],
        "102–103\nscaling": [
            1122085995
        ],
        "102\nreplicationcontrollers\n259–260\nsplitting": [
            1122085995
        ],
        "59\nstatefulsets": [
            1122085995
        ],
        "298\nvertical": [
            1122085995
        ],
        "running\n451–452\nsee": [
            1122085995
        ],
        "autoscaling\nscaling": [
            1122085995
        ],
        "out3\nscaling": [
            1122085995
        ],
        "up3\nschedule": [
            1122085995
        ],
        "configuring117\nscheduler319–321\nadvanced": [
            1122085995
        ],
        "321\nassigning": [
            1122085995
        ],
        "332\ndefault": [
            1122085995
        ],
        "algorithm\n319\nensuring": [
            1122085995
        ],
        "343–344\nfinding": [
            1122085995
        ],
        "320\nselecting": [
            1122085995
        ],
        "320–321\nusing": [
            1122085995
        ],
        "321\nusing": [
            1122085995
        ],
        "select-\ning": [
            1122085995
        ],
        "407\nusing": [
            1122085995
        ],
        "406\nscheduling44": [
            1122085995
        ],
        "457–476\nco-locating": [
            1122085995
        ],
        "468–471\ndefault": [
            1122085995
        ],
        "319\neffect": [
            1122085995
        ],
        "407–408\npod": [
            1122085995
        ],
        "409–410\nscheduler": [
            1122085995
        ],
        "407\njobs": [
            1122085995
        ],
        "117\npods\naway": [
            1122085995
        ],
        "474–476\nto": [
            1122085995
        ],
        "constrain\n73–75\nusing": [
            1122085995
        ],
        "\nconstrain": [
            1122085995
        ],
        "73–75\nusing": [
            1122085995
        ],
        "321\n": [
            1122085995
        ],
        "\n\nindex589\nscheduling": [
            1122085995
        ],
        "(continued)\nusing": [
            1122085995
        ],
        "sched-\nuling": [
            1122085995
        ],
        "465–468\nspecifying": [
            1122085995
        ],
        "during\n461–462\nusing": [
            1122085995
        ],
        "461–462\nsee": [
            1122085995
        ],
        "scheduler\nsecret": [
            1122085995
        ],
        "volume163\nsecrets\ncreating": [
            1122085995
        ],
        "223\ndefault": [
            1122085995
        ],
        "214–215\nexposing": [
            1122085995
        ],
        "221–222\nimage": [
            1122085995
        ],
        "351\nmountable": [
            1122085995
        ],
        "350–351\noverview": [
            1122085995
        ],
        "214\nreading": [
            1122085995
        ],
        "218\nusing": [
            1122085995
        ],
        "217\nusing": [
            1122085995
        ],
        "222\nmodifying": [
            1122085995
        ],
        "525–526\nusing": [
            1122085995
        ],
        "222–223\nversus": [
            1122085995
        ],
        "217–218\nsee": [
            1122085995
        ],
        "secrets\nsecuring\ncluster": [
            1122085995
        ],
        "376–380\nclusters": [
            1122085995
        ],
        "371–373\ndefault": [
            1122085995
        ],
        "358–359\nnetworks": [
            1122085995
        ],
        "376–380\nsecurity": [
            1122085995
        ],
        "contexts\nof": [
            1122085995
        ],
        "380–389\nrunning": [
            1122085995
        ],
        "381\nsetting": [
            1122085995
        ],
        "387\nsecuritycontext": [
            1122085995
        ],
        "property\n380": [
            1122085995
        ],
        "\n383": [
            1122085995
        ],
        "387\nsecuritycontextcapabilities": [
            1122085995
        ],
        "\nfield394\nsecuritycontextcapabilities.drop": [
            1122085995
        ],
        "\nproperty386\nsecuritycontextreadonlyroot-\nfilesystem": [
            1122085995
        ],
        "property386\nsecuritycontextrunasuser": [
            1122085995
        ],
        "\nproperty381\nsecurity-enhanced": [
            1122085995
        ],
        "\nselinux\nsecurity-related": [
            1122085995
        ],
        "restrict-\ning": [
            1122085995
        ],
        "pods389–399\nassigning": [
            1122085995
        ],
        "392–394\nselecting": [
            1122085995
        ],
        "disks541\nselector": [
            1122085995
        ],
        "property106\nselectormatchlabels106\nselectors\nconstraining": [
            1122085995
        ],
        "74–75\ncreating": [
            1122085995
        ],
        "without\n133–134\ncreating": [
            1122085995
        ],
        "without\n132–133\nselectorspreadpriority": [
            1122085995
        ],
        "\nfunction468\nself-healing22–23\nselinux": [
            1122085995
        ],
        "(security-enhanced": [
            1122085995
        ],
        "\nlinux)\ndisabling": [
            1122085995
        ],
        "544\noverview": [
            1122085995
        ],
        "380\nsemantics": [
            1122085995
        ],
        "at-most-one290\nserver": [
            1122085995
        ],
        "\nto400\nservers346–374\napi": [
            1122085995
        ],
        "503\nauthentication": [
            1122085995
        ],
        "347–348\nsecuring": [
            1122085995
        ],
        "359–360\n": [
            1122085995
        ],
        "\n\nindex590\nservers": [
            1122085995
        ],
        "(continued)\ndefault": [
            1122085995
        ],
        "352–353\nservice": [
            1122085995
        ],
        "accounts348–349\nassigning": [
            1122085995
        ],
        "351–353\nbinding": [
            1122085995
        ],
        "349–351\ncreating": [
            1122085995
        ],
        "373\ncustom\ncreating": [
            1122085995
        ],
        "use\n351–352\nusing": [
            1122085995
        ],
        "servers\n352–353\nimage": [
            1122085995
        ],
        "351\nincluding": [
            1122085995
        ],
        "361\nmountable": [
            1122085995
        ],
        "350–351\nserviceaccount": [
            1122085995
        ],
        "resources\n348–349\ntying": [
            1122085995
        ],
        "authorizations": [
            1122085995
        ],
        "349\nservice": [
            1122085995
        ],
        "brokers522–523\nlisting": [
            1122085995
        ],
        "523\nregistering": [
            1122085995
        ],
        "522–523\nservice": [
            1122085995
        ],
        "catalog519–527\nbenefits": [
            1122085995
        ],
        "526–527\ncontroller": [
            1122085995
        ],
        "521\ndeprovisioning": [
            1122085995
        ],
        "526\nopenservicebroker": [
            1122085995
        ],
        "api\n522–523\nlisting": [
            1122085995
        ],
        "522\noverview": [
            1122085995
        ],
        "520–\n521\nprovisioning": [
            1122085995
        ],
        "524–526\nbinding": [
            1122085995
        ],
        "instances\n525\nprovisioning": [
            1122085995
        ],
        "524–525\nusing": [
            1122085995
        ],
        "525–526\nregistering": [
            1122085995
        ],
        "521\nunbinding": [
            1122085995
        ],
        "526\nusing": [
            1122085995
        ],
        "524–526\nservice": [
            1122085995
        ],
        "controllers324\nservice": [
            1122085995
        ],
        "endpoints\nmanually": [
            1122085995
        ],
        "132–133\noverview": [
            1122085995
        ],
        "131–132\nservice": [
            1122085995
        ],
        "proxy327–328\nserviceaccount": [
            1122085995
        ],
        "resources348–349\nservice_account": [
            1122085995
        ],
        "\nvariable228\nserviceaccountname": [
            1122085995
        ],
        "\nproperty373\nservice_host": [
            1122085995
        ],
        "variable129\nservice_port": [
            1122085995
        ],
        "variable129\nservice-reader": [
            1122085995
        ],
        "role359\nservices47": [
            1122085995
        ],
        "120–158\naccessing": [
            1122085995
        ],
        "264\naccessing": [
            1122085995
        ],
        "46–47\naccessing": [
            1122085995
        ],
        "145\nconfiguring": [
            1122085995
        ],
        "\npointing": [
            1122085995
        ],
        "145\nhow": [
            1122085995
        ],
        "145\nobtaining": [
            1122085995
        ],
        "145\navailable": [
            1122085995
        ],
        "523\nbackend": [
            1122085995
        ],
        "502\nbinding": [
            1122085995
        ],
        "525\ncluster-internal": [
            1122085995
        ],
        "299\nconfiguring": [
            1122085995
        ],
        "41\nconfiguring": [
            1122085995
        ],
        "126\nconnecting": [
            1122085995
        ],
        "139–141\ncreating": [
            1122085995
        ],
        "122–128\nremotely": [
            1122085995
        ],
        "124–126\nthrough": [
            1122085995
        ],
        "123\nthrough": [
            1122085995
        ],
        "\ndescriptors": [
            1122085995
        ],
        "123\nusing": [
            1122085995
        ],
        "127–128\ncreating": [
            1122085995
        ],
        "selectors\n132–133\ndedicated": [
            1122085995
        ],
        "283–284\ndiscovering": [
            1122085995
        ],
        "128–131\nconnecting": [
            1122085995
        ],
        "130\npinging": [
            1122085995
        ],
        "131\nrunning": [
            1122085995
        ],
        "130–131\nthrough": [
            1122085995
        ],
        "129\nthrough": [
            1122085995
        ],
        "128–129\ndisruptions": [
            1122085995
        ],
        "454–456\nexamining": [
            1122085995
        ],
        "124\nexamples": [
            1122085995
        ],
        "121–122\nexposing": [
            1122085995
        ],
        "255\nexposing": [
            1122085995
        ],
        "controllers\n143–144\nexposing": [
            1122085995
        ],
        "530\nexposing": [
            1122085995
        ],
        "126–127\nexposing": [
            1122085995
        ],
        "298–299\nexposing": [
            1122085995
        ],
        "138–141\nconnecting": [
            1122085995
        ],
        "balancers\n139–141\ncreating": [
            1122085995
        ],
        "139\nexposing": [
            1122085995
        ],
        "141–142\nusing": [
            1122085995
        ],
        "services\n135–138\ngoverning\ncreating": [
            1122085995
        ],
        "292–\n294\noverview": [
            1122085995
        ],
        "285–287\nimplementing": [
            1122085995
        ],
        "338–340\n": [
            1122085995
        ],
        "\n\nindex591\nservices": [
            1122085995
        ],
        "(continued)\nlisting": [
            1122085995
        ],
        "46\nlisting": [
            1122085995
        ],
        "357\nliving": [
            1122085995
        ],
        "131–134\noverview": [
            1122085995
        ],
        "121–131\nprovisioning": [
            1122085995
        ],
        "524–526\nrequests": [
            1122085995
        ],
        "50\nrunning": [
            1122085995
        ],
        "255\nsignaling": [
            1122085995
        ],
        "connections\n149–153\ntesting": [
            1122085995
        ],
        "124\ntroubleshooting": [
            1122085995
        ],
        "156\nusing": [
            1122085995
        ],
        "524–526\nwhy": [
            1122085995
        ],
        "48\nsee": [
            1122085995
        ],
        "services\nsessionaffinity": [
            1122085995
        ],
        "property126\nsessions": [
            1122085995
        ],
        "\nservices126\nset-context": [
            1122085995
        ],
        "command538\nshare-type": [
            1122085995
        ],
        "label467\nsharing\ndata": [
            1122085995
        ],
        "volume\n166–169\nvolumes": [
            1122085995
        ],
        "387–389\nshell": [
            1122085995
        ],
        "forms\n193–194\nshells\nrunning": [
            1122085995
        ],
        "130–131\nrunning": [
            1122085995
        ],
        "33\nshutdown": [
            1122085995
        ],
        "handlers": [
            1122085995
        ],
        "implement-\ning": [
            1122085995
        ],
        "applications490–491\nshutdown": [
            1122085995
        ],
        "pods153\nshutdowns\ncritical": [
            1122085995
        ],
        "\nshut-down": [
            1122085995
        ],
        "491\ndedicated": [
            1122085995
        ],
        "criti-\ncal": [
            1122085995
        ],
        "491\nof": [
            1122085995
        ],
        "490\nshutting": [
            1122085995
        ],
        "vms545\nsidecar": [
            1122085995
        ],
        "container60": [
            1122085995
        ],
        "168\nsig": [
            1122085995
        ],
        "(special": [
            1122085995
        ],
        "group)448\nsigkill89\nsignaling": [
            1122085995
        ],
        "connections\n149–153\nsigterm": [
            1122085995
        ],
        "signal487–491": [
            1122085995
        ],
        "\n495\nsimulating": [
            1122085995
        ],
        "network304–306\nchecking": [
            1122085995
        ],
        "status\n305–306\nshutting": [
            1122085995
        ],
        "304–305\nsingle-node": [
            1122085995
        ],
        "local37–38\nsleep-interval": [
            1122085995
        ],
        "entry221\nslowing": [
            1122085995
        ],
        "updates265\nsnat": [
            1122085995
        ],
        "(source": [
            1122085995
        ],
        "\ntranslation)142\nspec": [
            1122085995
        ],
        "section75\nspeccontainers.ports": [
            1122085995
        ],
        "field377\nspecifications": [
            1122085995
        ],
        "\ncontainer-level": [
            1122085995
        ],
        "\nin233\nspecinitcontainers": [
            1122085995
        ],
        "field484\nspecreplicas": [
            1122085995
        ],
        "field102–103\nspectemplate.spec.contain-\ners.image": [
            1122085995
        ],
        "attribute303\nspinning": [
            1122085995
        ],
        "pods252–253\nsplitting\napplications": [
            1122085995
        ],
        "\nmicroservices": [
            1122085995
        ],
        "3–4\ninto": [
            1122085995
        ],
        "59\nsrv": [
            1122085995
        ],
        "records300\nssd": [
            1122085995
        ],
        "(solid-state": [
            1122085995
        ],
        "drive)109\nssd": [
            1122085995
        ],
        "class427\nstartingdeadlineseconds": [
            1122085995
        ],
        "\nfield118\nstart-up": [
            1122085995
        ],
        "selecting541\nstateful": [
            1122085995
        ],
        "pods308\nexamining": [
            1122085995
        ],
        "294–295\noverview": [
            1122085995
        ],
        "284\nreplicating": [
            1122085995
        ],
        "281–284\nproviding": [
            1122085995
        ],
        "281–282\nstatefulset": [
            1122085995
        ],
        "controllers324\nstatefulset": [
            1122085995
        ],
        "resources280–308\ncreating": [
            1122085995
        ],
        "294\ncreating": [
            1122085995
        ],
        "images\n290–291\ndeploying": [
            1122085995
        ],
        "291–292\nexamining": [
            1122085995
        ],
        "pods\n294–295\ndiscovering": [
            1122085995
        ],
        "299–304\nclustered": [
            1122085995
        ],
        "300\nguarantees": [
            1122085995
        ],
        "289–290\nat-most-one": [
            1122085995
        ],
        "289–290\nnode": [
            1122085995
        ],
        "304–307\ndeleting": [
            1122085995
        ],
        "manually\n306–307\nsimulating": [
            1122085995
        ],
        "disconnec-\ntion": [
            1122085995
        ],
        "network\n304–306\noverview": [
            1122085995
        ],
        "284–290\nproviding": [
            1122085995
        ],
        "287–289\nproviding": [
            1122085995
        ],
        "\nidentities": [
            1122085995
        ],
        "285–287\nreplicating": [
            1122085995
        ],
        "pods\n281–284\nproviding": [
            1122085995
        ],
        "298\nupdating": [
            1122085995
        ],
        "302–303\nusing": [
            1122085995
        ],
        "290–295\nvs": [
            1122085995
        ],
        "284–285\nvs": [
            1122085995
        ],
        "replicationcontrollers\n284–285\nwith": [
            1122085995
        ],
        "295–299\ncommunicating": [
            1122085995
        ],
        "cluster-inter-\nnal": [
            1122085995
        ],
        "298–299\nstates": [
            1122085995
        ],
        "new50\nstatistics": [
            1122085995
        ],
        "\nconsumption432–434\n": [
            1122085995
        ],
        "\n\nindex592\nstatus\nof": [
            1122085995
        ],
        "305–306\nstatus": [
            1122085995
        ],
        "attribute138\nstatus": [
            1122085995
        ],
        "column183\nstatus": [
            1122085995
        ],
        "section63\nstatuscertificate": [
            1122085995
        ],
        "field148\nstatusqosclass": [
            1122085995
        ],
        "field419\nstopping\ncontainers": [
            1122085995
        ],
        "34–35\npods": [
            1122085995
        ],
        "82\nstorage\ndefining": [
            1122085995
        ],
        "185\nproviding": [
            1122085995
        ],
        "287–289\ncreating": [
            1122085995
        ],
        "288\ndeleting": [
            1122085995
        ],
        "288\nreattaching": [
            1122085995
        ],
        "289\nusing": [
            1122085995
        ],
        "templates\n288\nrunning": [
            1122085995
        ],
        "281–282\nstable": [
            1122085995
        ],
        "of\n289–290\nsee": [
            1122085995
        ],
        "storage\nstorage": [
            1122085995
        ],
        "classes\ncreating": [
            1122085995
        ],
        "specifying\n188–189\ndefault": [
            1122085995
        ],
        "188\ndynamic": [
            1122085995
        ],
        "187–189\nlisting": [
            1122085995
        ],
        "187–188\nrequesting": [
            1122085995
        ],
        "185–187\ncreating": [
            1122085995
        ],
        "185–186\nexamining": [
            1122085995
        ],
        "187\nstorage": [
            1122085995
        ],
        "technologies\ndecoupling": [
            1122085995
        ],
        "176–184\nusing": [
            1122085995
        ],
        "175\nstorage": [
            1122085995
        ],
        "volumes160\nstorageclass": [
            1122085995
        ],
        "\nthrough185\nstorageclass-fast-hostpathyaml": [
            1122085995
        ],
        "\nfile187\nstorageclassname": [
            1122085995
        ],
        "attribute\n188–189\nstoring\nhistorical": [
            1122085995
        ],
        "432–434\nanalyzing": [
            1122085995
        ],
        "433–434\ngrafana": [
            1122085995
        ],
        "434\nresources": [
            1122085995
        ],
        "318\nstringdata": [
            1122085995
        ],
        "field218\nsubpath": [
            1122085995
        ],
        "property210–211\nsubsets": [
            1122085995
        ],
        "\nselectors71–72\nlisting": [
            1122085995
        ],
        "71–72\nusing": [
            1122085995
        ],
        "72\nsupplementalgroups": [
            1122085995
        ],
        "\npolicies392–394\ndeploying": [
            1122085995
        ],
        "392–393\nsupplementalgroups": [
            1122085995
        ],
        "property\n388–389\nswagger": [
            1122085995
        ],
        "framework248\nsymlink213\nsynchandler": [
            1122085995
        ],
        "field323\nsysadmins": [
            1122085995
        ],
        "\ndelivery7\nsystem:authenticated": [
            1122085995
        ],
        "group348": [
            1122085995
        ],
        "\n366\nsystem:discovery": [
            1122085995
        ],
        "role\n373\nsystem:serviceaccounts:": [
            1122085995
        ],
        "348\nsystem:unauthenticated": [
            1122085995
        ],
        "\ngroup348": [
            1122085995
        ],
        "366\nsys_time": [
            1122085995
        ],
        "capability394\nt\ntab": [
            1122085995
        ],
        "\nkubectl41–42\ntagging": [
            1122085995
        ],
        "497–498\nunder": [
            1122085995
        ],
        "35\ntags28\ntaints\ncustom": [
            1122085995
        ],
        "460\neffects": [
            1122085995
        ],
        "459–460\nof": [
            1122085995
        ],
        "458\noverview": [
            1122085995
        ],
        "458–462\nrepelling": [
            1122085995
        ],
        "457–462\nusing": [
            1122085995
        ],
        "scheduling\n461–462\ntargeting": [
            1122085995
        ],
        "containers489\ntcp": [
            1122085995
        ],
        "packets126\ntcp": [
            1122085995
        ],
        "socket86": [
            1122085995
        ],
        "150\ntemplates\nfor": [
            1122085995
        ],
        "117\npods\nadding": [
            1122085995
        ],
        "151–152\nchanging": [
            1122085995
        ],
        "288\nvolume": [
            1122085995
        ],
        "288\nterm": [
            1122085995
        ],
        "variable33\nterminating": [
            1122085995
        ],
        "about498–500\nterminating": [
            1122085995
        ],
        "scope429\ntermination": [
            1122085995
        ],
        "\nspecifying490\nterminationmessagepath": [
            1122085995
        ],
        "field499\nterminationmessagepolicy": [
            1122085995
        ],
        "\nfield500\ntesting": [
            1122085995
        ],
        "\nclusters124\nthirdpartyresource": [
            1122085995
        ],
        "objects509\ntime": [
            1122085995
        ],
        "sharing\ntls": [
            1122085995
        ],
        "(transport": [
            1122085995
        ],
        "\nsecurity)147–149\ntmpfs": [
            1122085995
        ],
        "filesystem166\ntoken": [
            1122085995
        ],
        "variable241\ntokens": [
            1122085995
        ],
        "\naccounts352–353\ntolerations\nadding": [
            1122085995
        ],
        "460–461\nof": [
            1122085995
        ],
        "459\noverview": [
            1122085995
        ],
        "scheduling\n461–462\n": [
            1122085995
        ],
        "\n\nindex593\ntopologykey471–473\ntriggering\nrolling": [
            1122085995
        ],
        "265–267\nscale-ups": [
            1122085995
        ],
        "445–446\ntroubleshooting": [
            1122085995
        ],
        "services156\nu\nubuntu:latest": [
            1122085995
        ],
        "image164\nudp": [
            1122085995
        ],
        "packets126\nunbinding": [
            1122085995
        ],
        "instances526\nundoing": [
            1122085995
        ],
        "rollouts269\nuniversal": [
            1122085995
        ],
        "locators": [
            1122085995
        ],
        "\nurls\nunschedulable": [
            1122085995
        ],
        "nodes109\nupdates": [
            1122085995
        ],
        "updates\nupdating\nannotations": [
            1122085995
        ],
        "232\napplication": [
            1122085995
        ],
        "\nrestarting": [
            1122085995
        ],
        "application\n211–213\napplications": [
            1122085995
        ],
        "deployments\n264–268\napplications": [
            1122085995
        ],
        "pods\n252–253\nconfigmap": [
            1122085995
        ],
        "213\ndeployment": [
            1122085995
        ],
        "264–268\ndeployment": [
            1122085995
        ],
        "strategies\n264–265\nslowing": [
            1122085995
        ],
        "265–267\ndeployments": [
            1122085995
        ],
        "276–277\nfiles": [
            1122085995
        ],
        "212–213\nlabels": [
            1122085995
        ],
        "232\nreplica": [
            1122085995
        ],
        "440\nstatefulsets": [
            1122085995
        ],
        "302–303\nurls": [
            1122085995
        ],
        "(universal": [
            1122085995
        ],
        "\nlocators)365–367\n--user": [
            1122085995
        ],
        "argument359\nuser": [
            1122085995
        ],
        "536\n–537\ntying": [
            1122085995
        ],
        "537\nuser": [
            1122085995
        ],
        "directive381\nusers347–348\nassigning": [
            1122085995
        ],
        "deployed\n396–397\nwith": [
            1122085995
        ],
        "398–399\nin": [
            1122085995
        ],
        "536\nin": [
            1122085995
        ],
        "528\nout-of-range": [
            1122085995
        ],
        "393\nrunning": [
            1122085995
        ],
        "381–382\nsharing": [
            1122085995
        ],
        "387–389\nusing": [
            1122085995
        ],
        "537–538\nuts": [
            1122085995
        ],
        "namespace11\nv\nv1": [
            1122085995
        ],
        "creating254\nvalidating\ncustom": [
            1122085995
        ],
        "517–518\nresources": [
            1122085995
        ],
        "318\nvaluefrom": [
            1122085995
        ],
        "field202\nvalues": [
            1122085995
        ],
        "property107\nvalues": [
            1122085995
        ],
        "in198\nvariables": [
            1122085995
        ],
        "vari-\nables\nvcs": [
            1122085995
        ],
        "(version": [
            1122085995
        ],
        "\nsystem)505\nvdi": [
            1122085995
        ],
        "image)\n539\nverifying": [
            1122085995
        ],
        "\nservers240–241\nversioning\ncontainer": [
            1122085995
        ],
        "28\nresource": [
            1122085995
        ],
        "504–505\nversions\ncreating": [
            1122085995
        ],
        "269\nof": [
            1122085995
        ],
        "558\nvertical": [
            1122085995
        ],
        "pods\n451–452\nautomatically": [
            1122085995
        ],
        "running\n451–452\nveth": [
            1122085995
        ],
        "pair336–337\nview": [
            1122085995
        ],
        "read-\nonly": [
            1122085995
        ],
        "\nwith372\nvisualizing": [
            1122085995
        ],
        "states50\nvms": [
            1122085995
        ],
        "machines)8": [
            1122085995
        ],
        "26\ncloning": [
            1122085995
        ],
        "545–547\nchanging": [
            1122085995
        ],
        "546–547\ncomparing": [
            1122085995
        ],
        "8–10\ncomparing": [
            1122085995
        ],
        "14–15\nconfiguring": [
            1122085995
        ],
        "540\ncreating": [
            1122085995
        ],
        "539\ninstead": [
            1122085995
        ],
        "555\nshutting": [
            1122085995
        ],
        "545\nsee": [
            1122085995
        ],
        "vm\nvolume": [
            1122085995
        ],
        "templates288\nvolume": [
            1122085995
        ],
        "column186\nvolumeclaimtemplates293\nvolumes159–177": [
            1122085995
        ],
        "190\naccessing": [
            1122085995
        ],
        "169–170\nexamining": [
            1122085995
        ],
        "170\nhostpath": [
            1122085995
        ],
        "169–170\nconstraining": [
            1122085995
        ],
        "395\ndecoupling": [
            1122085995
        ],
        "underly-\ning": [
            1122085995
        ],
        "technologies\n176–184\nclaims": [
            1122085995
        ],
        "182\npersistentvolumeclaims\n176–181\npersistentvolumes": [
            1122085995
        ],
        "176–184\ndynamic": [
            1122085995
        ],
        "184–189\ndefining": [
            1122085995
        ],
        "187–189\noverview": [
            1122085995
        ],
        "160–163\navailable": [
            1122085995
        ],
        "types\n162–163\nexamples": [
            1122085995
        ],
        "160–162\nsharing": [
            1122085995
        ],
        "163–169\nsharing": [
            1122085995
        ],
        "387–389\n": [
            1122085995
        ],
        "\n\nindex594\nvolumes": [
            1122085995
        ],
        "(continued)\nspecifications": [
            1122085995
        ],
        "233\nusing": [
            1122085995
        ],
        "207–208\nusing": [
            1122085995
        ],
        "163–166\ncreating": [
            1122085995
        ],
        "164–165\nin": [
            1122085995
        ],
        "163–164\nseeing": [
            1122085995
        ],
        "\npoint": [
            1122085995
        ],
        "169\nrunning": [
            1122085995
        ],
        "serv-\ning": [
            1122085995
        ],
        "167\nsidecar": [
            1122085995
        ],
        "168\nusing": [
            1122085995
        ],
        "\nprivate": [
            1122085995
        ],
        "repositories\n168\nusing": [
            1122085995
        ],
        "171–175\nunderlying": [
            1122085995
        ],
        "171–174\nusing": [
            1122085995
        ],
        "480–482\nsee": [
            1122085995
        ],
        "volume;": [
            1122085995
        ],
        "volumes\nvspherevolume": [
            1122085995
        ],
        "volume163\nw\nwatch": [
            1122085995
        ],
        "command445\nweb": [
            1122085995
        ],
        "accessing45–47\naccessing": [
            1122085995
        ],
        "46–47\ncreating": [
            1122085995
        ],
        "45\nlisting": [
            1122085995
        ],
        "46\nweb": [
            1122085995
        ],
        "browsers140\nweb": [
            1122085995
        ],
        "repository167\nwebhook": [
            1122085995
        ],
        "plugin353\nwebserver": [
            1122085995
        ],
        "container162\nwebsite": [
            1122085995
        ],
        "controllers514–515\nwebsite-crdyaml": [
            1122085995
        ],
        "file511\nworker": [
            1122085995
        ],
        "nodes18": [
            1122085995
        ],
        "43\naccessing": [
            1122085995
        ],
        "169–170\ncategorizing": [
            1122085995
        ],
        "310\nconfiguring": [
            1122085995
        ],
        "549–550\nworker()": [
            1122085995
        ],
        "method323\nworkflow": [
            1122085995
        ],
        "plat-\nform\nx\n-xmx": [
            1122085995
        ],
        "option416\ny\nyaml": [
            1122085995
        ],
        "110\ncreating": [
            1122085995
        ],
        "78\ncreating": [
            1122085995
        ],
        "61–67\nsending": [
            1122085995
        ],
        "logs\n65–66\ncreating": [
            1122085995
        ],
        "123\nexamining": [
            1122085995
        ],
        "61–63\nexposing": [
            1122085995
        ],
        "255\nmanifests": [
            1122085995
        ],
        "505–506\nrunning": [
            1122085995
        ],
        "255\nrunning": [
            1122085995
        ],
        "155\nyum": [
            1122085995
        ],
        "files544\n": [
            1122085995
        ],
        "\n\nkubernetes": [
            1122085995
        ],
        "(continued)\n*": [
            1122085995
        ],
        "book\nresource": [
            1122085995
        ],
        "version]descriptionsection\nscaling\nhorizontalpodautoscaler": [
            1122085995
        ],
        "\n[autoscaling/v2beta1**]\nautomatically": [
            1122085995
        ],
        "metric\n151\npoddisruptionbudget": [
            1122085995
        ],
        "\n[policy/v1beta1]\ndefines": [
            1122085995
        ],
        "\nremain": [
            1122085995
        ],
        "evacuating": [
            1122085995
        ],
        "nodes\n153.3\nresources\nlimitrange": [
            1122085995
        ],
        "(limits)": [
            1122085995
        ],
        "namespace\n144\nresourcequota": [
            1122085995
        ],
        "(quota)": [
            1122085995
        ],
        "namespace\n145\ncluster": [
            1122085995
        ],
        "state\nnode*": [
            1122085995
        ],
        "(no)": [
            1122085995
        ],
        "[v1]represents": [
            1122085995
        ],
        "node22.2\ncluster*": [
            1122085995
        ],
        "[federation/v1beta1]a": [
            1122085995
        ],
        "(used": [
            1122085995
        ],
        "federation)app": [
            1122085995
        ],
        "d\ncomponentstatus*": [
            1122085995
        ],
        "(cs)": [
            1122085995
        ],
        "[v1]status": [
            1122085995
        ],
        "component111.1\nevent": [
            1122085995
        ],
        "(ev)": [
            1122085995
        ],
        "cluster112.3\nsecurity\nserviceaccount": [
            1122085995
        ],
        "(sa)": [
            1122085995
        ],
        "[v1]an": [
            1122085995
        ],
        "pods121.2\nrole": [
            1122085995
        ],
        "[rbacauthorization.k8s.io/v1]defines": [
            1122085995
        ],
        "(per": [
            1122085995
        ],
        "namespace)\n122.3\nclusterrole*": [
            1122085995
        ],
        "\n[rbacauthorization.k8s.io/v1]\nlike": [
            1122085995
        ],
        "\ngrant": [
            1122085995
        ],
        "namespaces\n122.4\nrolebinding": [
            1122085995
        ],
        "\n[rbacauthorization.k8s.io/v1]\ndefines": [
            1122085995
        ],
        "\nrole": [
            1122085995
        ],
        "\n122.3\nclusterrolebinding*": [
            1122085995
        ],
        "namespaces122.4\npodsecuritypolicy*": [
            1122085995
        ],
        "(psp)": [
            1122085995
        ],
        "\n[extensions/v1beta1]\na": [
            1122085995
        ],
        "security-\nsensitive": [
            1122085995
        ],
        "use\n133.1\nnetworkpolicy": [
            1122085995
        ],
        "(netpol)": [
            1122085995
        ],
        "\n[networkingk8s.io/v1]\nisolates": [
            1122085995
        ],
        "other\n134\ncertificatesigningrequest*": [
            1122085995
        ],
        "\n[certificatesk8s.io/v1beta1]\na": [
            1122085995
        ],
        "certificate54.4\next.\ncustomresourcedefinition*": [
            1122085995
        ],
        "\n[apiextensionsk8s.io/v1beta1]\ndefines": [
            1122085995
        ],
        "\n181\n": [
            1122085995
        ],
        "\n\nmarko": [
            1122085995
        ],
        "lukša\nk\nubernetes": [
            1122085995
        ],
        "“helmsman”": [
            1122085995
        ],
        "waters": [
            1122085995
        ],
        "orchestra-\ntion": [
            1122085995
        ],
        "safely": [
            1122085995
        ],
        "fl": [
            1122085995
        ],
        "ow": [
            1122085995
        ],
        "\ndistributed": [
            1122085995
        ],
        "\nmaximum": [
            1122085995
        ],
        "effi": [
            1122085995
        ],
        "ciency": [
            1122085995
        ],
        "eliminating": [
            1122085995
        ],
        "designs\nkubernetes": [
            1122085995
        ],
        "\ncontainer-based": [
            1122085995
        ],
        "fi": [
            1122085995
        ],
        "rst": [
            1122085995
        ],
        "gradually": [
            1122085995
        ],
        "expand": [
            1122085995
        ],
        "deepening": [
            1122085995
        ],
        "navigate": [
            1122085995
        ],
        "high-value": [
            1122085995
        ],
        "tuning": [
            1122085995
        ],
        "\nwhat’s": [
            1122085995
        ],
        "inside\n●\n": [
            1122085995
        ],
        "internals\n●\n": [
            1122085995
        ],
        "cluster\n●\n": [
            1122085995
        ],
        "clusters\n●\n": [
            1122085995
        ],
        "downtime\nwritten": [
            1122085995
        ],
        "\nfamiliarity": [
            1122085995
        ],
        "systems\nmarko": [
            1122085995
        ],
        "openshift\nto": [
            1122085995
        ],
        "ebook": [
            1122085995
        ],
        "pdf": [
            1122085995
        ],
        "epub": [
            1122085995
        ],
        "kindle": [
            1122085995
        ],
        "\nowners": [
            1122085995
        ],
        "\nwwwmanning.com/books/kubernetes-in-action\n$59.99": [
            1122085995
        ],
        "$7999": [
            1122085995
        ],
        "[including": [
            1122085995
        ],
        "ebook]\nkubernetes": [
            1122085995
        ],
        "action\nsoftware": [
            1122085995
        ],
        "development/operations\nmanning\n“\nauthoritative": [
            1122085995
        ],
        "\nexhaustive": [
            1122085995
        ],
        "\nstyle": [
            1122085995
        ],
        "\nlifecycle": [
            1122085995
        ],
        "application\n”\n": [
            1122085995
        ],
        "\n—antonio": [
            1122085995
        ],
        "system1\n“\nthe": [
            1122085995
        ],
        "real-\nworld": [
            1122085995
        ],
        "\njust": [
            1122085995
        ],
        "road": [
            1122085995
        ],
        "them\n”\n": [
            1122085995
        ],
        "\n—paolo": [
            1122085995
        ],
        "hat\n“\nan": [
            1122085995
        ],
        "discussion": [
            1122085995
        ],
        "must-have!\n”\n—al": [
            1122085995
        ],
        "uspto": [
            1122085995
        ],
        "\n“\nthe": [
            1122085995
        ],
        "professional": [
            1122085995
        ],
        "kubernaut": [
            1122085995
        ],
        "reading\n”\n": [
            1122085995
        ],
        "\n—csaba": [
            1122085995
        ],
        "sári\nchimera": [
            1122085995
        ],
        "entertainment\nsee": [
            1122085995
        ],
        "delhi:": [
            2119133144
        ],
        "congress": [
            2119133144,
            1478412827,
            1043891123,
            1118639836,
            410929361,
            186247402
        ],
        "nationalist": [
            2119133144
        ],
        "party": [
            2119133144
        ],
        "(ncp)": [
            2119133144
        ],
        "wednesday": [
            2119133144,
            186247402
        ],
        "stable\"": [
            2119133144
        ],
        "government": [
            2119133144
        ],
        "maharashtra": [
            2119133144,
            1478412827,
            1941223023
        ],
        "engagement": [
            2119133144
        ],
        "shiv": [
            2119133144,
            1043891123,
            1941223023,
            410929361,
            186247402
        ],
        "sena": [
            2119133144,
            1118639836,
            1941223023,
            410929361,
            186247402
        ],
        "out\r\nthere": [
            2119133144
        ],
        "ncp": [
            2119133144,
            1478412827,
            410929361
        ],
        "nawab": [
            2119133144
        ],
        "malik": [
            2119133144
        ],
        "addressing": [
            2119133144
        ],
        "harkishen": [
            2119133144,
            1478412827,
            1043891123,
            1118639836,
            410929361,
            186247402
        ],
        "prithviraj": [
            2119133144
        ],
        "chavan": [
            2119133144
        ],
        "round": [
            2119133144
        ],
        "leadership": [
            2119133144
        ],
        "delhi": [
            2119133144
        ],
        "held": [
            1478412827
        ],
        "meeting": [
            1478412827
        ],
        "prime": [
            1478412827
        ],
        "minister": [
            1478412827
        ],
        "narendra": [
            1478412827
        ],
        "modi": [
            1478412827
        ],
        "chief": [
            1478412827,
            410929361
        ],
        "sharad": [
            1478412827,
            1043891123
        ],
        "pawar": [
            1478412827
        ],
        "fueled": [
            1478412827
        ],
        "speculation": [
            1478412827
        ],
        "bjp": [
            1478412827,
            1941223023
        ],
        "manoeuvre": [
            1478412827
        ],
        "pawars": [
            1043891123
        ],
        "senior": [
            1043891123
        ],
        "senas": [
            1043891123
        ],
        "sanjay": [
            1043891123
        ],
        "raut": [
            1043891123
        ],
        "sorted": [
            1118639836
        ],
        "agenda": [
            1118639836
        ],
        "sonia": [
            1941223023,
            186247402
        ],
        "gandhi": [
            1941223023,
            186247402
        ],
        "reluctant": [
            1941223023
        ],
        "ideologically": [
            1941223023
        ],
        "contrasting": [
            1941223023
        ],
        "pro-hindutva": [
            1941223023
        ],
        "fell": [
            1941223023
        ],
        "long-time": [
            1941223023
        ],
        "ally": [
            1941223023,
            186247402
        ],
        "won": [
            1941223023
        ],
        "circulation": [
            410929361
        ],
        "ministership": [
            410929361
        ],
        "uddhav": [
            410929361
        ],
        "thackeray": [
            410929361
        ],
        "deputy": [
            410929361
        ],
        "ministers": [
            410929361
        ],
        "committee": [
            186247402
        ],
        "vastly": [
            186247402
        ],
        "mismatched": [
            186247402
        ],
        "parties": [
            186247402
        ],
        "asked": [
            186247402
        ],
        "tone": [
            186247402
        ],
        "hindutva": [
            186247402
        ],
        "rhetoric": [
            186247402
        ],
        "anathema": [
            186247402
        ],
        "urged": [
            186247402
        ],
        "kerala": [
            186247402
        ],
        "indian": [
            186247402
        ],
        "muslim": [
            186247402
        ],
        "league": [
            186247402
        ],
        "(iuml)": [
            186247402
        ],
        "snub": [
            186247402
        ]
    }
}